# Comparing `tmp/aigc_zoo-0.1.11.post1-py3-none-any.whl.zip` & `tmp/aigc_zoo-0.1.11.post2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,41 +1,42 @@
-Zip file size: 64001 bytes, number of entries: 39
--rw-rw-rw-  2.0 fat       80 b- defN 23-Jun-26 02:52 aigc_zoo/__init__.py
--rw-rw-rw-  2.0 fat       77 b- defN 23-Jun-26 02:52 aigc_zoo/model_zoo/__init__.py
--rw-rw-rw-  2.0 fat       66 b- defN 23-Jun-26 02:52 aigc_zoo/model_zoo/baichuan/__init__.py
--rw-rw-rw-  2.0 fat     5578 b- defN 23-Jul-10 04:30 aigc_zoo/model_zoo/baichuan/llm_model.py
--rw-rw-rw-  2.0 fat     9574 b- defN 23-Jun-26 02:52 aigc_zoo/model_zoo/baichuan/tokenization_baichuan.py
--rw-rw-rw-  2.0 fat       66 b- defN 23-Jun-26 02:52 aigc_zoo/model_zoo/baichuan2/__init__.py
--rw-rw-rw-  2.0 fat     5575 b- defN 23-Jul-11 04:15 aigc_zoo/model_zoo/baichuan2/llm_model.py
--rw-rw-rw-  2.0 fat     8720 b- defN 23-Jul-11 03:13 aigc_zoo/model_zoo/baichuan2/tokenization_baichuan.py
--rw-rw-rw-  2.0 fat       77 b- defN 23-Jun-26 02:52 aigc_zoo/model_zoo/chatglm/__init__.py
--rw-rw-rw-  2.0 fat    17281 b- defN 23-Jul-12 00:39 aigc_zoo/model_zoo/chatglm/llm_model.py
--rw-rw-rw-  2.0 fat     6435 b- defN 23-Jul-10 04:30 aigc_zoo/model_zoo/chatglm/ppo_model.py
--rw-rw-rw-  2.0 fat     9700 b- defN 23-Jul-10 04:30 aigc_zoo/model_zoo/chatglm/reward_model.py
--rw-rw-rw-  2.0 fat    17037 b- defN 23-Jun-26 02:52 aigc_zoo/model_zoo/chatglm/tokenization_chatglm.py
--rw-rw-rw-  2.0 fat       77 b- defN 23-Jun-26 02:52 aigc_zoo/model_zoo/chatglm2/__init__.py
--rw-rw-rw-  2.0 fat     9942 b- defN 23-Jul-11 07:45 aigc_zoo/model_zoo/chatglm2/chatglm_model.py
--rw-rw-rw-  2.0 fat     9252 b- defN 23-Jun-26 01:20 aigc_zoo/model_zoo/chatglm2/tokenization_chatglm.py
--rw-rw-rw-  2.0 fat       77 b- defN 23-Jun-26 02:52 aigc_zoo/model_zoo/llm/__init__.py
--rw-rw-rw-  2.0 fat     6344 b- defN 23-Jul-10 04:30 aigc_zoo/model_zoo/llm/ilql_model.py
--rw-rw-rw-  2.0 fat     5443 b- defN 23-Jul-10 04:30 aigc_zoo/model_zoo/llm/llm_model.py
--rw-rw-rw-  2.0 fat     6298 b- defN 23-Jul-10 04:30 aigc_zoo/model_zoo/llm/ppo_model.py
--rw-rw-rw-  2.0 fat     9732 b- defN 23-Jul-10 04:30 aigc_zoo/model_zoo/llm/reward_model.py
--rw-rw-rw-  2.0 fat     6704 b- defN 23-Jul-10 04:30 aigc_zoo/model_zoo/llm/rrhf_model.py
--rw-rw-rw-  2.0 fat       77 b- defN 23-Jun-26 02:52 aigc_zoo/model_zoo/moss/__init__.py
--rw-rw-rw-  2.0 fat     4554 b- defN 23-Jul-10 04:30 aigc_zoo/model_zoo/moss/llm_model.py
--rw-rw-rw-  2.0 fat     2050 b- defN 23-Jun-26 02:52 aigc_zoo/model_zoo/moss/moss_model.py
--rw-rw-rw-  2.0 fat       77 b- defN 23-Jun-26 02:52 aigc_zoo/model_zoo/rwkv4/__init__.py
--rw-rw-rw-  2.0 fat     5529 b- defN 23-Jul-10 04:30 aigc_zoo/model_zoo/rwkv4/llm_model.py
--rw-rw-rw-  2.0 fat       77 b- defN 23-Jun-26 02:52 aigc_zoo/model_zoo/t5/__init__.py
--rw-rw-rw-  2.0 fat     4950 b- defN 23-Jul-10 04:30 aigc_zoo/model_zoo/t5/llm_model.py
--rw-rw-rw-  2.0 fat     5848 b- defN 23-Jul-10 04:30 aigc_zoo/model_zoo/t5/ppo_model.py
--rw-rw-rw-  2.0 fat     9098 b- defN 23-Jul-10 04:30 aigc_zoo/model_zoo/t5/reward_model.py
--rw-rw-rw-  2.0 fat       77 b- defN 23-Jun-26 02:52 aigc_zoo/utils/__init__.py
--rw-rw-rw-  2.0 fat     2664 b- defN 23-Jul-12 07:02 aigc_zoo/utils/llm_generate.py
--rw-rw-rw-  2.0 fat    13002 b- defN 23-Jul-12 07:04 aigc_zoo/utils/moss_generate.py
--rw-rw-rw-  2.0 fat    11357 b- defN 23-Jul-12 07:50 aigc_zoo-0.1.11.post1.dist-info/LICENSE
--rw-rw-rw-  2.0 fat      357 b- defN 23-Jul-12 07:50 aigc_zoo-0.1.11.post1.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-Jul-12 07:50 aigc_zoo-0.1.11.post1.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        9 b- defN 23-Jul-12 07:50 aigc_zoo-0.1.11.post1.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     3621 b- defN 23-Jul-12 07:50 aigc_zoo-0.1.11.post1.dist-info/RECORD
-39 files, 197574 bytes uncompressed, 58063 bytes compressed:  70.6%
+Zip file size: 64253 bytes, number of entries: 40
+-rw-rw-rw-  2.0 fat       80 b- defN 23-Jun-16 14:31 aigc_zoo/__init__.py
+-rw-rw-rw-  2.0 fat       77 b- defN 23-Jun-16 11:35 aigc_zoo/model_zoo/__init__.py
+-rw-rw-rw-  2.0 fat       66 b- defN 23-Jun-16 11:35 aigc_zoo/model_zoo/baichuan/__init__.py
+-rw-rw-rw-  2.0 fat     5578 b- defN 23-Jul-11 11:21 aigc_zoo/model_zoo/baichuan/llm_model.py
+-rw-rw-rw-  2.0 fat     9574 b- defN 23-Jun-16 11:35 aigc_zoo/model_zoo/baichuan/tokenization_baichuan.py
+-rw-rw-rw-  2.0 fat       66 b- defN 23-Jul-11 11:21 aigc_zoo/model_zoo/baichuan2/__init__.py
+-rw-rw-rw-  2.0 fat     5575 b- defN 23-Jul-11 11:21 aigc_zoo/model_zoo/baichuan2/llm_model.py
+-rw-rw-rw-  2.0 fat     8720 b- defN 23-Jul-11 11:21 aigc_zoo/model_zoo/baichuan2/tokenization_baichuan.py
+-rw-rw-rw-  2.0 fat       77 b- defN 23-Jun-16 11:35 aigc_zoo/model_zoo/chatglm/__init__.py
+-rw-rw-rw-  2.0 fat    17057 b- defN 23-Jul-13 10:54 aigc_zoo/model_zoo/chatglm/llm_model.py
+-rw-rw-rw-  2.0 fat     6435 b- defN 23-Jul-11 11:21 aigc_zoo/model_zoo/chatglm/ppo_model.py
+-rw-rw-rw-  2.0 fat     9700 b- defN 23-Jul-11 11:21 aigc_zoo/model_zoo/chatglm/reward_model.py
+-rw-rw-rw-  2.0 fat    17037 b- defN 23-Jun-16 11:35 aigc_zoo/model_zoo/chatglm/tokenization_chatglm.py
+-rw-rw-rw-  2.0 fat       77 b- defN 23-Jul-04 13:32 aigc_zoo/model_zoo/chatglm2/__init__.py
+-rw-rw-rw-  2.0 fat      114 b- defN 23-Jul-13 11:01 aigc_zoo/model_zoo/chatglm2/chatglm_model.py
+-rw-rw-rw-  2.0 fat     9839 b- defN 23-Jul-13 11:01 aigc_zoo/model_zoo/chatglm2/llm_model.py
+-rw-rw-rw-  2.0 fat     9252 b- defN 23-Jul-04 13:32 aigc_zoo/model_zoo/chatglm2/tokenization_chatglm.py
+-rw-rw-rw-  2.0 fat       77 b- defN 23-Jun-16 11:35 aigc_zoo/model_zoo/llm/__init__.py
+-rw-rw-rw-  2.0 fat     6344 b- defN 23-Jul-11 11:21 aigc_zoo/model_zoo/llm/ilql_model.py
+-rw-rw-rw-  2.0 fat     5443 b- defN 23-Jul-11 11:21 aigc_zoo/model_zoo/llm/llm_model.py
+-rw-rw-rw-  2.0 fat     6298 b- defN 23-Jul-11 11:21 aigc_zoo/model_zoo/llm/ppo_model.py
+-rw-rw-rw-  2.0 fat     9732 b- defN 23-Jul-11 11:21 aigc_zoo/model_zoo/llm/reward_model.py
+-rw-rw-rw-  2.0 fat     6704 b- defN 23-Jul-11 11:21 aigc_zoo/model_zoo/llm/rrhf_model.py
+-rw-rw-rw-  2.0 fat       77 b- defN 23-Jun-16 11:35 aigc_zoo/model_zoo/moss/__init__.py
+-rw-rw-rw-  2.0 fat     4554 b- defN 23-Jul-11 11:21 aigc_zoo/model_zoo/moss/llm_model.py
+-rw-rw-rw-  2.0 fat     2050 b- defN 23-Jun-16 14:52 aigc_zoo/model_zoo/moss/moss_model.py
+-rw-rw-rw-  2.0 fat       77 b- defN 23-Jun-16 11:35 aigc_zoo/model_zoo/rwkv4/__init__.py
+-rw-rw-rw-  2.0 fat     5529 b- defN 23-Jul-11 11:21 aigc_zoo/model_zoo/rwkv4/llm_model.py
+-rw-rw-rw-  2.0 fat       77 b- defN 23-Jun-16 11:35 aigc_zoo/model_zoo/t5/__init__.py
+-rw-rw-rw-  2.0 fat     4950 b- defN 23-Jul-11 11:21 aigc_zoo/model_zoo/t5/llm_model.py
+-rw-rw-rw-  2.0 fat     5848 b- defN 23-Jul-11 11:21 aigc_zoo/model_zoo/t5/ppo_model.py
+-rw-rw-rw-  2.0 fat     9098 b- defN 23-Jul-11 11:21 aigc_zoo/model_zoo/t5/reward_model.py
+-rw-rw-rw-  2.0 fat       77 b- defN 23-Jun-16 11:35 aigc_zoo/utils/__init__.py
+-rw-rw-rw-  2.0 fat     2664 b- defN 23-Jul-13 10:54 aigc_zoo/utils/llm_generate.py
+-rw-rw-rw-  2.0 fat    13198 b- defN 23-Jul-13 11:08 aigc_zoo/utils/moss_generate.py
+-rw-rw-rw-  2.0 fat    11357 b- defN 23-Jul-13 11:10 aigc_zoo-0.1.11.post2.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat      372 b- defN 23-Jul-13 11:10 aigc_zoo-0.1.11.post2.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Jul-13 11:10 aigc_zoo-0.1.11.post2.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        9 b- defN 23-Jul-13 11:10 aigc_zoo-0.1.11.post2.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     3717 b- defN 23-Jul-13 11:10 aigc_zoo-0.1.11.post2.dist-info/RECORD
+40 files, 197668 bytes uncompressed, 58159 bytes compressed:  70.6%
```

## zipnote {}

```diff
@@ -39,14 +39,17 @@
 
 Filename: aigc_zoo/model_zoo/chatglm2/__init__.py
 Comment: 
 
 Filename: aigc_zoo/model_zoo/chatglm2/chatglm_model.py
 Comment: 
 
+Filename: aigc_zoo/model_zoo/chatglm2/llm_model.py
+Comment: 
+
 Filename: aigc_zoo/model_zoo/chatglm2/tokenization_chatglm.py
 Comment: 
 
 Filename: aigc_zoo/model_zoo/llm/__init__.py
 Comment: 
 
 Filename: aigc_zoo/model_zoo/llm/ilql_model.py
@@ -96,23 +99,23 @@
 
 Filename: aigc_zoo/utils/llm_generate.py
 Comment: 
 
 Filename: aigc_zoo/utils/moss_generate.py
 Comment: 
 
-Filename: aigc_zoo-0.1.11.post1.dist-info/LICENSE
+Filename: aigc_zoo-0.1.11.post2.dist-info/LICENSE
 Comment: 
 
-Filename: aigc_zoo-0.1.11.post1.dist-info/METADATA
+Filename: aigc_zoo-0.1.11.post2.dist-info/METADATA
 Comment: 
 
-Filename: aigc_zoo-0.1.11.post1.dist-info/WHEEL
+Filename: aigc_zoo-0.1.11.post2.dist-info/WHEEL
 Comment: 
 
-Filename: aigc_zoo-0.1.11.post1.dist-info/top_level.txt
+Filename: aigc_zoo-0.1.11.post2.dist-info/top_level.txt
 Comment: 
 
-Filename: aigc_zoo-0.1.11.post1.dist-info/RECORD
+Filename: aigc_zoo-0.1.11.post2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## aigc_zoo/model_zoo/chatglm/llm_model.py

```diff
@@ -54,21 +54,21 @@
         return scores
 
 class MyChatGLMForConditionalGeneration(ChatGLMForConditionalGeneration):
     def __init__(self,config):
         super(MyChatGLMForConditionalGeneration, self).__init__(config)
 
     @torch.no_grad()
-    def generate_for_continue_writing(self,tokenizer, query: str, max_length: int = 2048, num_beams=1,
+    def generate_for_continue_writing(self,tokenizer, query: str,
         do_sample=True, top_p=0.7, temperature=0.95, logits_processor=None, **kwargs
     ):
         if logits_processor is None:
             logits_processor = LogitsProcessorList()
         logits_processor.append(InvalidScoreLogitsProcessor())
-        gen_kwargs = {"max_length": max_length, "num_beams": num_beams, "do_sample": do_sample, "top_p": top_p,
+        gen_kwargs = {"do_sample": do_sample, "top_p": top_p,
                       "temperature": temperature, "logits_processor": logits_processor, **kwargs}
 
         output_scores = gen_kwargs.get('output_scores', False)
         if output_scores:
             gen_kwargs['return_dict_in_generate'] = True
 
         tokenizer: ChatGLMTokenizer
@@ -83,22 +83,22 @@
             score = outputs.scores[0]
             return score
         outputs = outputs.tolist()[0][len(inputs_ids[0]):]
         response = tokenizer.decode(outputs)
         response = self.process_response(response)
         return response
     @torch.no_grad()
-    def chat(self, tokenizer, query: str, history: List[Tuple[str, str]] = None, max_length: int = 2048, num_beams=1,
+    def chat(self, tokenizer, query: str, history: List[Tuple[str, str]] = None,
              do_sample=True, top_p=0.7, temperature=0.95, logits_processor=None, **kwargs):
         if history is None:
             history = []
         if logits_processor is None:
             logits_processor = LogitsProcessorList()
         logits_processor.append(InvalidScoreLogitsProcessor())
-        gen_kwargs = {"max_length": max_length, "num_beams": num_beams, "do_sample": do_sample, "top_p": top_p,
+        gen_kwargs = {"do_sample": do_sample, "top_p": top_p,
                       "temperature": temperature, "logits_processor": logits_processor, **kwargs}
         output_scores = gen_kwargs.get('output_scores', False)
         if output_scores:
             gen_kwargs['return_dict_in_generate'] = True
 
         if not history:
             prompt = query
@@ -116,22 +116,22 @@
         outputs = outputs.tolist()[0][len(inputs["input_ids"][0]):]
         response = tokenizer.decode(outputs)
         response = self.process_response(response)
         history = history + [(query, response)]
         return response, history
 
     @torch.no_grad()
-    def stream_chat(self, tokenizer, query: str, history: List[Tuple[str, str]] = None, max_length: int = 2048,
+    def stream_chat(self, tokenizer, query: str, history: List[Tuple[str, str]] = None,
                     do_sample=True, top_p=0.7, temperature=0.95, logits_processor=None, **kwargs):
         if history is None:
             history = []
         if logits_processor is None:
             logits_processor = LogitsProcessorList()
         logits_processor.append(InvalidScoreLogitsProcessor())
-        gen_kwargs = {"max_length": max_length, "do_sample": do_sample, "top_p": top_p,
+        gen_kwargs = {"do_sample": do_sample, "top_p": top_p,
                       "temperature": temperature, "logits_processor": logits_processor, **kwargs}
         if not history:
             prompt = query
         else:
             prompt = ""
             for i, (old_query, response) in enumerate(history):
                 prompt += "[Round {}]\n问：{}\n答：{}\n".format(i, old_query, response)
```

## aigc_zoo/model_zoo/chatglm2/chatglm_model.py

```diff
@@ -1,223 +1,5 @@
 # coding=utf8
 # @Time    : 2023/5/12 20:20
 # @Author  : tk
 # @FileName: chatglm_model
-
-import copy
-import os
-import re
-import warnings
-from typing import List, Tuple, Optional, Callable
-import torch
-from deep_training.nlp.models.chatglm2.modeling_chatglm import ChatGLMForConditionalGeneration,ChatGLMConfig, logger,setup_model_profile
-from deep_training.nlp.models.transformer import TransformerBase
-from torch import nn
-from transformers import LogitsProcessorList, LogitsProcessor, GenerationConfig, StoppingCriteriaList
-from .tokenization_chatglm import ChatGLMTokenizer
-from deep_training.trainer.pl.modelweighter import *
-import logging
-logger = logging.getLogger(__name__)
-
-
-
-
-def build_masks_and_position_ids_glm(batch_input_ids, ctxlens):
-    max_len = batch_input_ids.size(1)
-    batch_position_ids, batch_attention_mask = [], []
-    for input_ids,ctxlen in zip(batch_input_ids,ctxlens):
-        position_ids = list(range(1,max_len+1))
-        assert ctxlen <= max_len
-        attention_mask = [1] * ctxlen + [0] * (max_len - ctxlen)
-        batch_position_ids.append(torch.tensor(position_ids,dtype=torch.long))
-        batch_attention_mask.append(torch.tensor(attention_mask,dtype=torch.bool))
-
-    batch_attention_mask = torch.stack(batch_attention_mask, dim=0)
-    batch_position_ids = torch.stack(batch_position_ids, dim=0)
-    return batch_attention_mask,batch_position_ids
-
-class InvalidScoreLogitsProcessor(LogitsProcessor):
-    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:
-        if torch.isnan(scores).any() or torch.isinf(scores).any():
-            scores.zero_()
-            scores[..., 5] = 5e4
-        return scores
-
-class MyChatGLMForConditionalGeneration(ChatGLMForConditionalGeneration):
-    def __init__(self,config):
-        super(MyChatGLMForConditionalGeneration, self).__init__(config)
-
-
-    def build_inputs(self, tokenizer, query: str, history: List[Tuple[str, str]] = None):
-        if history is None:
-            history = []
-        prompt = ""
-        for i, (old_query, response) in enumerate(history):
-            prompt += "[Round {}]\n\n问：{}\n\n答：{}\n\n".format(i + 1, old_query, response)
-        prompt += "[Round {}]\n\n问：{}\n\n答：".format(len(history) + 1, query)
-        inputs = tokenizer([prompt], return_tensors="pt")
-        inputs = inputs.to(self.device)
-        return inputs
-
-    def build_stream_inputs(self, tokenizer, query: str, history: List[Tuple[str, str]] = None):
-        if history:
-            prompt = "\n\n[Round {}]\n\n问：{}\n\n答：".format(len(history) + 1, query)
-            input_ids = tokenizer.encode(prompt, add_special_tokens=False)
-            input_ids = input_ids[1:]
-            inputs = tokenizer.batch_encode_plus([(input_ids, None)], return_tensors="pt", add_special_tokens=False)
-        else:
-            prompt = "[Round {}]\n\n问：{}\n\n答：".format(len(history) + 1, query)
-            inputs = tokenizer([prompt], return_tensors="pt")
-        inputs = inputs.to(self.device)
-        return inputs
-
-
-    @torch.no_grad()
-    def chat(self, tokenizer, query: str, history: List[Tuple[str, str]] = None, max_length: int = 2048, num_beams=1,
-             do_sample=True, top_p=0.8, temperature=0.8, logits_processor=None, **kwargs):
-        if history is None:
-            history = []
-        if logits_processor is None:
-            logits_processor = LogitsProcessorList()
-        logits_processor.append(InvalidScoreLogitsProcessor())
-        gen_kwargs = {"max_length": max_length, "num_beams": num_beams, "do_sample": do_sample, "top_p": top_p,
-                      "temperature": temperature, "logits_processor": logits_processor, **kwargs}
-
-        output_scores = gen_kwargs.get('output_scores', False)
-        if output_scores:
-            gen_kwargs['return_dict_in_generate'] = True
-
-        # inputs = self.build_inputs(tokenizer, query, history=history)
-        if not history:
-            prompt = query
-        else:
-            prompt = ""
-            for i, (old_query, response) in enumerate(history):
-                prompt += "[Round {}]\n问：{}\n答：{}\n".format(i, old_query, response)
-            prompt += "[Round {}]\n问：{}\n答：".format(len(history), query)
-
-        inputs = tokenizer([prompt], return_tensors="pt")
-        inputs = inputs.to(self.device)
-        outputs = self.generate(**inputs, **gen_kwargs)
-        if output_scores:
-            score = outputs.scores[0]
-            return score
-
-        outputs = outputs.tolist()[0][len(inputs["input_ids"][0]):]
-        response = tokenizer.decode(outputs)
-        response = self.process_response(response)
-        history = history + [(query, response)]
-        return response, history
-
-
-
-class MyTransformerChatGlmLMHeadModel(TransformerBase):
-    def __init__(self, *args,**kwargs):
-        load_in_8bit = kwargs.get('load_in_8bit', False)
-        load_in_4bit = kwargs.get('load_in_4bit', False)
-        if not load_in_4bit:
-            quantization_config = kwargs.get("quantization_config", None)
-            if quantization_config:
-                load_in_4bit = quantization_config.load_in_4bit
-
-        if not load_in_8bit and not load_in_4bit:
-            kwargs.pop("device_map", None)
-            kwargs.pop("quantization_config", None)
-        super(MyTransformerChatGlmLMHeadModel, self).__init__(*args,**kwargs)
-        self.set_model(self.from_pretrained(MyChatGLMForConditionalGeneration, *args, **kwargs))
-
-        # for param in self.model.parameters():
-        #     param.requires_grad = False  # freeze the model - train adapters later
-        #     if param.ndim == 1:
-        #         # cast the small parameters (e.g. layernorm) to fp32 for stability
-        #         param.data = param.data.to(torch.float32)
-
-        # class CastOutputToFloat(nn.Sequential):
-        #     def forward(self, x):
-        #         return super().forward(x).to(torch.float32)
-        #
-        # self.model.lm_head = CastOutputToFloat(self.model.lm_head)
-
-
-    def enable_input_require_grads(self):
-        pass
-        # setattr(self.model, 'model_parallel', True)
-        # setattr(self.model, 'is_parallelizable', True)
-        # self.model.enable_input_require_grads()
-
-
-
-
-
-
-
-
-
-class MyTransformer(MyTransformerChatGlmLMHeadModel,ModelWeightMinMax, with_pl=True):
-    def __init__(self, *args,new_num_tokens=None, **kwargs):
-        lora_args: LoraArguments = kwargs.pop('lora_args',None)
-        num_layers_freeze = kwargs.pop('num_layers_freeze',-1)
-        super(MyTransformer, self).__init__(*args, **kwargs)
-        self.lora_args = lora_args
-
-        #可能添加新词
-        self.resize_token_embs(new_num_tokens)
-
-        if lora_args is not None and lora_args.with_lora:
-            self.backbone.enable_input_require_grads()
-            model = LoraModel(self.backbone, lora_args)
-            print('==' * 30,'lora info')
-            model.print_trainable_parameters()
-            self.set_model(model, copy_attr=False)
-
-            # for name, module in model.named_modules():
-            #     if isinstance(module, LoraLayer):
-            #         module = module.to(torch.bfloat16)
-            #     if 'norm' in name:
-            #         module = module.to(torch.float32)
-            #     if 'lm_head' in name or 'embed_tokens' in name:
-            #         if hasattr(module, 'weight'):
-            #             if module.weight.dtype == torch.float32:
-            #                 module = module.to(torch.bfloat16)
-
-        elif num_layers_freeze > 0 and self.config.pre_seq_len is None:  # 非 lora freeze 非 ptuning模式
-            M: nn.Module = self.backbone
-            for param in M.named_parameters():
-                result = re.match(re.compile('.*transformer.layers.(\\d+)'),param[0])
-                if result is not None:
-                    n_layer = int(result.group(1))
-                    if n_layer < num_layers_freeze:
-                        param[1].requires_grad = False
-                        print('freeze layer',param[0])
-
-    def resize_token_embs(self, new_num_tokens):
-        if new_num_tokens is not None:
-            logger.info(f"new_num_tokens:{new_num_tokens}")
-            model: PreTrainedModel = self.backbone.model
-            embedding_size = model.get_input_embeddings().weight.shape[0]
-            if new_num_tokens > embedding_size:
-                # lora ptv2 二次加载权重需备份原此词表
-                if (self.lora_args is not None and self.lora_args.with_lora) or (
-                        self.prompt_args is not None and self.prompt_args.with_prompt):
-                    config = model.config
-                    if config.task_specific_params is None:
-                        config.task_specific_params = {}
-                    config.task_specific_params['vocab_size'] = config.vocab_size
-
-                logger.info("resize the embedding size by the size of the tokenizer")
-                # print('before',self.config)
-                model.resize_token_embeddings(new_num_tokens)
-                # print('after',self.config)
-
-    def get_model_lr(self, model=None, lr=None):
-        # for n, p in self.named_parameters():
-        #     print(n, p.requires_grad)
-        lr = lr if lr is not None else self.config.task_specific_params['learning_rate']
-        if self.lora_args is not None and self.lora_args.with_lora:
-            return [(self.backbone, lr)]
-        return super(MyTransformer, self).get_model_lr(model, lr)
-
-    def get_llm_model(self) -> MyChatGLMForConditionalGeneration:
-        if self.lora_args is not None and self.lora_args.with_lora:
-            return self.backbone.model.model
-        return self.backbone.model
-
+from .llm_model import *
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## aigc_zoo/utils/moss_generate.py

```diff
@@ -1,11 +1,12 @@
 # -*- coding: utf-8 -*-
 # @Time:  22:42
 # @Author: tk
 # @File：moss_genetate
+import math
 import typing
 
 import torch
 import time
 from ..model_zoo.moss.llm_model import MossTokenizer,MossConfig,MyMossForCausalLM
 
 
@@ -56,22 +57,21 @@
             "temperature": 0.7,
             "top_k": 40,
             "top_p": 0.8,
             "length_penalty": 1,
             "max_time": 60,
             "repetition_penalty": 1.02,
             "num_return_sequences": 1,
-            "max_iterations": 512,
             "regulation_start": 512,
             "prefix_length": len(self.prefix),
         }
         
 
     @torch.no_grad()
-    def generate_text(self,text: str,do_sample=False, top_p=0.7, temperature=0.95,**kwargs):
+    def generate(self,text: str,do_sample=False, top_p=0.7, temperature=0.95,**kwargs):
         output_scores = kwargs.get('output_scores', False)
         if output_scores:
             kwargs['return_dict_in_generate'] = True
 
         tokens = self.tokenizer.encode_plus(text,  return_tensors='pt')
         input_ids, attention_mask = tokens['input_ids'], tokens['attention_mask']
 
@@ -125,22 +125,28 @@
     @torch.no_grad()
     def chat_inner(self, input_ids,
                    attention_mask,
                    temperature=0.7,
                    repetition_penalty=1.1,
                    top_k=0,
                    top_p=0.92,
-                   max_iterations=2048,
                    regulation_start=512,
                    length_penalty=1,
                    max_time=60,
                    extra_ignored_tokens=None,
                    **kwargs):
         """
         """
+        max_new_tokens = 2048
+        if 'max_new_tokens' in kwargs:
+            max_new_tokens = kwargs['max_new_tokens']
+        else:
+            if 'max_length' in kwargs:
+                max_new_tokens = max(kwargs['max_length'] - input_ids.size(1),0)
+
         output_scores = kwargs.get('output_scores', False)
         scores = () if output_scores else None
 
 
         assert input_ids.dtype == torch.int64 and attention_mask.dtype == torch.int64
 
         self.bsz, self.seqlen = input_ids.shape
@@ -158,15 +164,15 @@
 
         moss_start = torch.tensor([True] * self.bsz, device=input_ids.device)
         moss_stop = torch.tensor([False] * self.bsz, device=input_ids.device)
 
         generations, start_time = torch.ones(self.bsz, 1, dtype=torch.int64), time.time()
 
         past_key_values = None
-        for i in range(int(max_iterations)):
+        for i in range(int(max_new_tokens)):
             logits, past_key_values = self.infer_(input_ids if i == 0 else new_generated_id,
                                                   attention_mask,
                                                   past_key_values)
 
             if i == 0:
                 logits = logits.gather(1, last_token_indices.view(self.bsz, 1, 1).repeat(1, 1, self.config.vocab_size)).squeeze(1)
             else:
```

## Comparing `aigc_zoo-0.1.11.post1.dist-info/LICENSE` & `aigc_zoo-0.1.11.post2.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `aigc_zoo-0.1.11.post1.dist-info/RECORD` & `aigc_zoo-0.1.11.post2.dist-info/RECORD`

 * *Files 7% similar despite different names*

```diff
@@ -3,20 +3,21 @@
 aigc_zoo/model_zoo/baichuan/__init__.py,sha256=2WFhbzYihC48B1ZSY2a42c1l8F8gIxxWh8slLoxGEt0,66
 aigc_zoo/model_zoo/baichuan/llm_model.py,sha256=9vqXeEyvqgjdQmviJnPylFqNtqNR3hyMLnmb1t1VNUI,5578
 aigc_zoo/model_zoo/baichuan/tokenization_baichuan.py,sha256=ZUGfjVTCcNiKfG_JV8TaZsdw7acnUXXxkOpH-wGwht4,9574
 aigc_zoo/model_zoo/baichuan2/__init__.py,sha256=2WFhbzYihC48B1ZSY2a42c1l8F8gIxxWh8slLoxGEt0,66
 aigc_zoo/model_zoo/baichuan2/llm_model.py,sha256=-MYekAWzGJO9FX9e79LJTcisgY-kOA7aSwKNvhjT94o,5575
 aigc_zoo/model_zoo/baichuan2/tokenization_baichuan.py,sha256=15HCHJatue1VgvRZ86HbnaYMgodHt0MHZsi2pIUBYTY,8720
 aigc_zoo/model_zoo/chatglm/__init__.py,sha256=RFimplzKNFznknZ0MzZDOhcvck-tAJQ53rTCMX-shS8,77
-aigc_zoo/model_zoo/chatglm/llm_model.py,sha256=wc4fhxRzM3X6vZ4Ko1TQZ1-l2PhdLRFn-oDfIgRB618,17281
+aigc_zoo/model_zoo/chatglm/llm_model.py,sha256=YLHqq5MaQMZLnie0ea6BTgj5aYV2-abatZIVyL_IJHU,17057
 aigc_zoo/model_zoo/chatglm/ppo_model.py,sha256=MjbAJm22oitIMJrJ-RxbHJv0df4m80G4BQsBXorwqac,6435
 aigc_zoo/model_zoo/chatglm/reward_model.py,sha256=Xa5yONvwlIknxMEK2Y59gE_our5iICtsUyaPmi86by0,9700
 aigc_zoo/model_zoo/chatglm/tokenization_chatglm.py,sha256=IMyHa8uPOgE0ia1DYp8Lx-IZ0N4TKSh1HVlA5sDdw-s,17037
 aigc_zoo/model_zoo/chatglm2/__init__.py,sha256=3cMgJqib4OqHJHv37rn3xHzyBbGl9s1zpTqd549QDmw,77
-aigc_zoo/model_zoo/chatglm2/chatglm_model.py,sha256=SL3pNCBGo24VBMC9pqwhiAHJwaQMZK9jHpWNAOfYqHc,9942
+aigc_zoo/model_zoo/chatglm2/chatglm_model.py,sha256=5bqiInqxxSAi0DB4zDZmzEQ_v4DluJQbnhTlx8R31Ik,114
+aigc_zoo/model_zoo/chatglm2/llm_model.py,sha256=l4iUZjJWUy3hpb9v6bbb9s6oVUCqWLJmm7QD0MirU40,9839
 aigc_zoo/model_zoo/chatglm2/tokenization_chatglm.py,sha256=eU2mw3psfMURCPVVqB0yLPDJuR4XtftFnnM6kiDCZZI,9252
 aigc_zoo/model_zoo/llm/__init__.py,sha256=RFimplzKNFznknZ0MzZDOhcvck-tAJQ53rTCMX-shS8,77
 aigc_zoo/model_zoo/llm/ilql_model.py,sha256=ZMpmamGRakLJx3V3ynS8XUN8s0IXLpNAD8WdRfDbdcw,6344
 aigc_zoo/model_zoo/llm/llm_model.py,sha256=x176u-JFBww6RtAKzOUevUL7iQQ6t50xrlg2vom-uaM,5443
 aigc_zoo/model_zoo/llm/ppo_model.py,sha256=tH0Rk1Oy4AOFrvs7tYtrmAjJY5Xoxugk3XILVvLfic4,6298
 aigc_zoo/model_zoo/llm/reward_model.py,sha256=Wa7fkI0z83Lg9uy8GZBN_JQ7kkhpnf6YLzZSjqm1YCQ,9732
 aigc_zoo/model_zoo/llm/rrhf_model.py,sha256=bTN95boS_ndI9_-ux0xHeL1B7vgQXvEKdJZezyTO0zg,6704
@@ -27,13 +28,13 @@
 aigc_zoo/model_zoo/rwkv4/llm_model.py,sha256=_EjnUp_TnzFWiotKSW6VqhZfHkupT_91yyy6KhWqyyQ,5529
 aigc_zoo/model_zoo/t5/__init__.py,sha256=RFimplzKNFznknZ0MzZDOhcvck-tAJQ53rTCMX-shS8,77
 aigc_zoo/model_zoo/t5/llm_model.py,sha256=CJmPXz4Q04geCgMFiN8AyPYLwVfthW0UvdZhipkUdWw,4950
 aigc_zoo/model_zoo/t5/ppo_model.py,sha256=eXm5sKi4WgISU95r25OCJhrP_ziYy3wRwALn1bf6_24,5848
 aigc_zoo/model_zoo/t5/reward_model.py,sha256=YhWq_Q6zNgQYlyrR3R79NDUkf4BZZiSB8gMzjS2n9kY,9098
 aigc_zoo/utils/__init__.py,sha256=Oc9cllKC2z1rKpYMLkfRj01Jud2WLQaZ_Dd89t4sreY,77
 aigc_zoo/utils/llm_generate.py,sha256=92TvTXISdU7GSrBgMYJoFyicWxkXrm_-02O4jcDnMp0,2664
-aigc_zoo/utils/moss_generate.py,sha256=P098NOvpytYjSSmtK_1dhVQEvZBURLfFHQ0YBWx8Fr4,13002
-aigc_zoo-0.1.11.post1.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-aigc_zoo-0.1.11.post1.dist-info/METADATA,sha256=JzLXchI4ojk857d6__v8NmeMp3OIKyH8IiJ_dT4QEFI,357
-aigc_zoo-0.1.11.post1.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
-aigc_zoo-0.1.11.post1.dist-info/top_level.txt,sha256=dl7_T4oT72ShHHM2khyOXJL4Kyvq0u_TdVolu-lKbnY,9
-aigc_zoo-0.1.11.post1.dist-info/RECORD,,
+aigc_zoo/utils/moss_generate.py,sha256=NYcvqjm3NbutslYlF8_7_BiHNBMPPI0mZzVui4nmkIU,13198
+aigc_zoo-0.1.11.post2.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+aigc_zoo-0.1.11.post2.dist-info/METADATA,sha256=bLqgWbIldHw5N-2k-IxlJxBj-wGIhuR5ndW-hRO93Jk,372
+aigc_zoo-0.1.11.post2.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+aigc_zoo-0.1.11.post2.dist-info/top_level.txt,sha256=dl7_T4oT72ShHHM2khyOXJL4Kyvq0u_TdVolu-lKbnY,9
+aigc_zoo-0.1.11.post2.dist-info/RECORD,,
```

