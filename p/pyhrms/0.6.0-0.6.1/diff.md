# Comparing `tmp/pyhrms-0.6.0.zip` & `tmp/pyhrms-0.6.1.zip`

## zipinfo {}

```diff
@@ -1,17 +1,17 @@
-Zip file size: 73821 bytes, number of entries: 15
-drwxrwxrwx  2.0 fat        0 b- stor 23-Jul-03 06:42 pyhrms-0.6.0/
-drwxrwxrwx  2.0 fat        0 b- stor 23-Jul-03 06:42 pyhrms-0.6.0/pyhrms/
-drwxrwxrwx  2.0 fat        0 b- stor 23-Jul-03 06:42 pyhrms-0.6.0/pyhrms.egg-info/
--rw-rw-rw-  2.0 fat     1088 b- defN 21-Nov-11 20:05 pyhrms-0.6.0/License.txt
--rw-rw-rw-  2.0 fat    23460 b- defN 23-Jul-03 06:42 pyhrms-0.6.0/PKG-INFO
--rw-rw-rw-  2.0 fat    22548 b- defN 23-Jul-03 06:41 pyhrms-0.6.0/README.rst
--rw-rw-rw-  2.0 fat       42 b- defN 23-Jul-03 06:42 pyhrms-0.6.0/setup.cfg
--rw-rw-rw-  2.0 fat      899 b- defN 23-Jul-03 06:41 pyhrms-0.6.0/setup.py
--rw-rw-rw-  2.0 fat   217077 b- defN 23-Jul-03 06:38 pyhrms-0.6.0/pyhrms/pyhrms.py
--rw-rw-rw-  2.0 fat     1311 b- defN 23-Jul-03 06:38 pyhrms-0.6.0/pyhrms/__init__.py
--rw-rw-rw-  2.0 fat        1 b- defN 23-Jul-03 06:42 pyhrms-0.6.0/pyhrms.egg-info/dependency_links.txt
--rw-rw-rw-  2.0 fat    23460 b- defN 23-Jul-03 06:42 pyhrms-0.6.0/pyhrms.egg-info/PKG-INFO
--rw-rw-rw-  2.0 fat      157 b- defN 23-Jul-03 06:42 pyhrms-0.6.0/pyhrms.egg-info/requires.txt
--rw-rw-rw-  2.0 fat      216 b- defN 23-Jul-03 06:42 pyhrms-0.6.0/pyhrms.egg-info/SOURCES.txt
--rw-rw-rw-  2.0 fat        7 b- defN 23-Jul-03 06:42 pyhrms-0.6.0/pyhrms.egg-info/top_level.txt
-15 files, 290266 bytes uncompressed, 71775 bytes compressed:  75.3%
+Zip file size: 77143 bytes, number of entries: 15
+drwxrwxrwx  2.0 fat        0 b- stor 23-Jul-13 08:56 pyhrms-0.6.1/
+drwxrwxrwx  2.0 fat        0 b- stor 23-Jul-13 08:56 pyhrms-0.6.1/pyhrms/
+drwxrwxrwx  2.0 fat        0 b- stor 23-Jul-13 08:56 pyhrms-0.6.1/pyhrms.egg-info/
+-rw-rw-rw-  2.0 fat     1088 b- defN 21-Nov-11 20:05 pyhrms-0.6.1/License.txt
+-rw-rw-rw-  2.0 fat    23613 b- defN 23-Jul-13 08:56 pyhrms-0.6.1/PKG-INFO
+-rw-rw-rw-  2.0 fat    22695 b- defN 23-Jul-13 08:02 pyhrms-0.6.1/README.rst
+-rw-rw-rw-  2.0 fat       42 b- defN 23-Jul-13 08:56 pyhrms-0.6.1/setup.cfg
+-rw-rw-rw-  2.0 fat      899 b- defN 23-Jul-13 07:54 pyhrms-0.6.1/setup.py
+-rw-rw-rw-  2.0 fat   221826 b- defN 23-Jul-13 08:04 pyhrms-0.6.1/pyhrms/pyhrms.py
+-rw-rw-rw-  2.0 fat     1448 b- defN 23-Jul-13 07:56 pyhrms-0.6.1/pyhrms/__init__.py
+-rw-rw-rw-  2.0 fat        1 b- defN 23-Jul-13 08:56 pyhrms-0.6.1/pyhrms.egg-info/dependency_links.txt
+-rw-rw-rw-  2.0 fat    23613 b- defN 23-Jul-13 08:56 pyhrms-0.6.1/pyhrms.egg-info/PKG-INFO
+-rw-rw-rw-  2.0 fat      157 b- defN 23-Jul-13 08:56 pyhrms-0.6.1/pyhrms.egg-info/requires.txt
+-rw-rw-rw-  2.0 fat      216 b- defN 23-Jul-13 08:56 pyhrms-0.6.1/pyhrms.egg-info/SOURCES.txt
+-rw-rw-rw-  2.0 fat        7 b- defN 23-Jul-13 08:56 pyhrms-0.6.1/pyhrms.egg-info/top_level.txt
+15 files, 295605 bytes uncompressed, 75097 bytes compressed:  74.6%
```

## zipnote {}

```diff
@@ -1,46 +1,46 @@
-Filename: pyhrms-0.6.0/
+Filename: pyhrms-0.6.1/
 Comment: 
 
-Filename: pyhrms-0.6.0/pyhrms/
+Filename: pyhrms-0.6.1/pyhrms/
 Comment: 
 
-Filename: pyhrms-0.6.0/pyhrms.egg-info/
+Filename: pyhrms-0.6.1/pyhrms.egg-info/
 Comment: 
 
-Filename: pyhrms-0.6.0/License.txt
+Filename: pyhrms-0.6.1/License.txt
 Comment: 
 
-Filename: pyhrms-0.6.0/PKG-INFO
+Filename: pyhrms-0.6.1/PKG-INFO
 Comment: 
 
-Filename: pyhrms-0.6.0/README.rst
+Filename: pyhrms-0.6.1/README.rst
 Comment: 
 
-Filename: pyhrms-0.6.0/setup.cfg
+Filename: pyhrms-0.6.1/setup.cfg
 Comment: 
 
-Filename: pyhrms-0.6.0/setup.py
+Filename: pyhrms-0.6.1/setup.py
 Comment: 
 
-Filename: pyhrms-0.6.0/pyhrms/pyhrms.py
+Filename: pyhrms-0.6.1/pyhrms/pyhrms.py
 Comment: 
 
-Filename: pyhrms-0.6.0/pyhrms/__init__.py
+Filename: pyhrms-0.6.1/pyhrms/__init__.py
 Comment: 
 
-Filename: pyhrms-0.6.0/pyhrms.egg-info/dependency_links.txt
+Filename: pyhrms-0.6.1/pyhrms.egg-info/dependency_links.txt
 Comment: 
 
-Filename: pyhrms-0.6.0/pyhrms.egg-info/PKG-INFO
+Filename: pyhrms-0.6.1/pyhrms.egg-info/PKG-INFO
 Comment: 
 
-Filename: pyhrms-0.6.0/pyhrms.egg-info/requires.txt
+Filename: pyhrms-0.6.1/pyhrms.egg-info/requires.txt
 Comment: 
 
-Filename: pyhrms-0.6.0/pyhrms.egg-info/SOURCES.txt
+Filename: pyhrms-0.6.1/pyhrms.egg-info/SOURCES.txt
 Comment: 
 
-Filename: pyhrms-0.6.0/pyhrms.egg-info/top_level.txt
+Filename: pyhrms-0.6.1/pyhrms.egg-info/top_level.txt
 Comment: 
 
 Zip file comment:
```

## Comparing `pyhrms-0.6.0/License.txt` & `pyhrms-0.6.1/License.txt`

 * *Files identical despite different names*

## Comparing `pyhrms-0.6.0/PKG-INFO` & `pyhrms-0.6.1/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pyhrms
-Version: 0.6.0
+Version: 0.6.1
 Summary: A powerful GC/LC-HRMS data analysis tool
 Home-page: https://github.com/WangRui5/PyHRMS.git
 Author: Wang Rui
 Author-email: wtrt7009@gmail.com
 License: UNKNOWN
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
@@ -22,19 +22,19 @@
 
 Contributer: Rui Wang
 ======================
 First release date: Nov.15.2021
 
 Update
 ======
-July.3.2023: pyhrms 0.6.0 new features:
-
-    * Fixed bugs for processing centroid data.
-
+July.13.2023: pyhrms 0.6.1 new features:
 
+    * rewrite fold_chagne_filter function
+    * added AIF_multi_ce function to process AIF data
+    * added pubchem_search and draw_pie_chart
 
 
 pyhrms can be installed and import as following:
 
 .. code-block:: python
 
     pip install pyhrms
@@ -103,16 +103,14 @@
 
 Licensing
 =========
 
 The package is open source and can be utilized under MIT license. Please find the detail in licence file.
 
 
-
-
 PyHRMS documentation
 ===========================
 
 
 **I want starting using PyHRMS**
 
 
@@ -206,14 +204,18 @@
   |- get_ms2_from_DDA
   |- extract_tic
   |- ms_bg_removal
   |- JsonToExcel
   |- suspect_list_matching
   |- rename_files
   |- Calibration
+  |- get_frag_DIA
+  |- AIF_multi_ce
+  |- pubchem_search
+  |- draw_pie_chart
 
 
 
 Table of Content
 ~~~~~~~~~~~~~~~~~~~
 
 1. Quick start
@@ -241,31 +243,35 @@
 ***************************************
   This function primarily includes peak picking, peak alignment, and blank comparison to prioritize features that are unique to the sample compared to the blank.To ensure that the program distinguishes between the sample set and the control set, include the strings 'methanol', 'blank', and 'control' in your control set files, and exclude these strings from your sample set files.
 
 .. code-block:: python
 
     path = '../Users/Desktop/my_HRMS_files'
     company = 'Waters'
-    pms.multi_process(path, company, profile=True, processors=1, p_value=1, ms2_analysis=True, fold_change=1,
-                  area_threshold=200, filter_type=3)
+    pms.multi_process(path, company, profile=True, control_group=['lab_blank', 'methanol'], processors=1, ms2_analysis=True,
+                  area_threshold=200, filter_type=2)
 
 
 .. note::
 
    Parameters explanation:
 
    - path: The file path for the mzML files that will be processed. For example, '../Users/Desktop/my_HRMS_files'.
    - company: The type of mass spectrometer used to acquire the data. Valid options are 'Waters', 'Thermo', 'Sciex', and 'Agilent'.
    - profile: A Boolean value that indicates whether the data is in profile or centroid mode. True for profile mode, False for centroid mode.
    - processors: This setting determines the number of processors that will be used for data processing in parallel running. If the memory usage exceeds 90%, please note that some Excel files may not be generated.
-   - p_value: The maximum p-value threshold for peak identification. Peaks with a p-value above this threshold will be excluded from analysis.
+   - control_group (List[str]): A list of labels representing the control group.These labels are used in the search for relevant file names.
+   - filter_type (int): Determines the mode of operation.
+                           Set to 1 for data without triplicates; fold change is computed
+                           as the ratio of the sample area to the maximum control area.
+                           Set to 2 for data with triplicates; the function will calculate p-values,
+                           and fold change is computed as the ratio of the mean sample area
+                           to the mean control area.
    - ms2_analysis: A Boolean value that indicates whether to perform DIA fragment analysis. Set to True to enable DIA fragment analysis, or False to disable it.
-   - fold_change: The fold change threshold for peak area comparison. Peaks with a fold change below this threshold will be excluded from analysis.
    - area_threshold: The minimum peak area threshold. Peaks with an area below this threshold will be excluded from analysis.
-   - filter_type: The type of peak filtering to perform. Set to 1 to only filter for peak area change (based on the maximum of the control group), 2 to filter for both p-value and fold change (treating all controls as a single group), or 3 to filter for both p-value and fold change (treating each set of solvent_blank, field blank, and lab blank as separate groups).
 
 
 
 **The output file will have the suffix '_unique_cmps.xlsx' and will be structured as follows:**
 
 +--------------+-------+----------+-----------+--------+-------+----+
 | new_index    | rt    | mz       | intensity | S/N    | area  |... |
```

## Comparing `pyhrms-0.6.0/README.rst` & `pyhrms-0.6.1/README.rst`

 * *Files 4% similar despite different names*

```diff
@@ -7,19 +7,19 @@
 
 Contributer: Rui Wang
 ======================
 First release date: Nov.15.2021
 
 Update
 ======
-July.3.2023: pyhrms 0.6.0 new features:
-
-    * Fixed bugs for processing centroid data.
-
+July.13.2023: pyhrms 0.6.1 new features:
 
+    * rewrite fold_chagne_filter function
+    * added AIF_multi_ce function to process AIF data
+    * added pubchem_search and draw_pie_chart
 
 
 pyhrms can be installed and import as following:
 
 .. code-block:: python
 
     pip install pyhrms
@@ -88,16 +88,14 @@
 
 Licensing
 =========
 
 The package is open source and can be utilized under MIT license. Please find the detail in licence file.
 
 
-
-
 PyHRMS documentation
 ===========================
 
 
 **I want starting using PyHRMS**
 
 
@@ -191,14 +189,18 @@
   |- get_ms2_from_DDA
   |- extract_tic
   |- ms_bg_removal
   |- JsonToExcel
   |- suspect_list_matching
   |- rename_files
   |- Calibration
+  |- get_frag_DIA
+  |- AIF_multi_ce
+  |- pubchem_search
+  |- draw_pie_chart
 
 
 
 Table of Content
 ~~~~~~~~~~~~~~~~~~~
 
 1. Quick start
@@ -226,31 +228,35 @@
 ***************************************
   This function primarily includes peak picking, peak alignment, and blank comparison to prioritize features that are unique to the sample compared to the blank.To ensure that the program distinguishes between the sample set and the control set, include the strings 'methanol', 'blank', and 'control' in your control set files, and exclude these strings from your sample set files.
 
 .. code-block:: python
 
     path = '../Users/Desktop/my_HRMS_files'
     company = 'Waters'
-    pms.multi_process(path, company, profile=True, processors=1, p_value=1, ms2_analysis=True, fold_change=1,
-                  area_threshold=200, filter_type=3)
+    pms.multi_process(path, company, profile=True, control_group=['lab_blank', 'methanol'], processors=1, ms2_analysis=True,
+                  area_threshold=200, filter_type=2)
 
 
 .. note::
 
    Parameters explanation:
 
    - path: The file path for the mzML files that will be processed. For example, '../Users/Desktop/my_HRMS_files'.
    - company: The type of mass spectrometer used to acquire the data. Valid options are 'Waters', 'Thermo', 'Sciex', and 'Agilent'.
    - profile: A Boolean value that indicates whether the data is in profile or centroid mode. True for profile mode, False for centroid mode.
    - processors: This setting determines the number of processors that will be used for data processing in parallel running. If the memory usage exceeds 90%, please note that some Excel files may not be generated.
-   - p_value: The maximum p-value threshold for peak identification. Peaks with a p-value above this threshold will be excluded from analysis.
+   - control_group (List[str]): A list of labels representing the control group.These labels are used in the search for relevant file names.
+   - filter_type (int): Determines the mode of operation.
+                           Set to 1 for data without triplicates; fold change is computed
+                           as the ratio of the sample area to the maximum control area.
+                           Set to 2 for data with triplicates; the function will calculate p-values,
+                           and fold change is computed as the ratio of the mean sample area
+                           to the mean control area.
    - ms2_analysis: A Boolean value that indicates whether to perform DIA fragment analysis. Set to True to enable DIA fragment analysis, or False to disable it.
-   - fold_change: The fold change threshold for peak area comparison. Peaks with a fold change below this threshold will be excluded from analysis.
    - area_threshold: The minimum peak area threshold. Peaks with an area below this threshold will be excluded from analysis.
-   - filter_type: The type of peak filtering to perform. Set to 1 to only filter for peak area change (based on the maximum of the control group), 2 to filter for both p-value and fold change (treating all controls as a single group), or 3 to filter for both p-value and fold change (treating each set of solvent_blank, field blank, and lab blank as separate groups).
 
 
 
 **The output file will have the suffix '_unique_cmps.xlsx' and will be structured as follows:**
 
 +--------------+-------+----------+-----------+--------+-------+----+
 | new_index    | rt    | mz       | intensity | S/N    | area  |... |
```

## Comparing `pyhrms-0.6.0/setup.py` & `pyhrms-0.6.1/setup.py`

 * *Files 1% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 
 def readme_file():
     with open('README.rst') as rf:
         return rf.read()
 
 setuptools.setup(
     name = 'pyhrms',
-    version = '0.6.0',
+    version = '0.6.1',
     author = 'Wang Rui',
     author_email = 'wtrt7009@gmail.com',
     url = 'https://github.com/WangRui5/PyHRMS.git',
     description = 'A powerful GC/LC-HRMS data analysis tool',
     long_description = readme_file(),
     packages = setuptools.find_packages(),
     install_requires = ['numpy>=1.19.2','pandas>=1.3.3'
```

## Comparing `pyhrms-0.6.0/pyhrms/pyhrms.py` & `pyhrms-0.6.1/pyhrms/pyhrms.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,23 +1,24 @@
 import pandas as pd
 from pandas import ExcelWriter
 import matplotlib.pyplot as plt
+import matplotlib.patches as mpatches
 import time
 import pymzml
 import scipy.signal
 from scipy.interpolate import interp1d
 from numpy import where, argmin, zeros
 from glob import glob
 import numpy as np
 from molmass import Formula
 import os
 import json
 from tqdm import tqdm
 from multiprocessing import Pool, cpu_count
-from scipy.stats import ttest_ind_from_stats
+from scipy.stats import ttest_ind
 from sklearn.decomposition import PCA
 from sklearn import preprocessing
 import scipy.stats as st
 from scipy import integrate
 import itertools
 import bisect
 import re
@@ -33,27 +34,32 @@
           'Oiso': 17.999159, 'F': 18.998403, 'K': 38.963708, 'P': 30.973763, 'Cl': 34.968853,
           'Cliso': 36.965903, 'S': 31.972072, 'Siso': 33.967868, 'Br': 78.918336, 'Na': 22.989770,
           'Si': 27.976928, 'Fe': 55.934939, 'Se': 79.916521, 'As': 74.921596, 'I': 126.904477, 'D': 2.014102,
           'Co': 58.933198, 'Au': 196.966560, 'B': 11.009305, 'e': 0.0005486
           })
 
 
-def one_step_process(path, company, profile=True, p_value=1, ms2_analysis=True, fold_change=0,
-                     area_threshold=200, filter_type=3, split_n=20, sat_intensity=False, long_rt_split_n=1):
+def one_step_process(path, company, profile=True, control_group=['lab_blank', 'methanol'], ms2_analysis=True,
+                     filter_type=1, split_n=20, sat_intensity=False, long_rt_split_n=1):
     """
     This function using one processor to process mzML data and perform a comparison between the sample set and the control set. The resulting data will be used to generate an Excel file that summarizes the differences between the two sets.
     Args:
        - path: The file path for the mzML files that will be processed. For example, '../Users/Desktop/my_HRMS_files'.
        - company: The type of mass spectrometer used to acquire the data. Valid options are 'Waters', 'Thermo', 'Sciex', and 'Agilent'.
        - profile: A Boolean value that indicates whether the data is in profile or centroid mode. True for profile mode, False for centroid mode.
-       - p_value: The maximum p-value threshold for peak identification. Peaks with a p-value above this threshold will be excluded from analysis.
+       - control_group (List[str]): A list of labels representing the control group.These labels are used in the search for relevant file names.
+       - filter_type (int): Determines the mode of operation.
+                           Set to 1 for data without triplicates; fold change is computed
+                           as the ratio of the sample area to the maximum control area.
+                           Set to 2 for data with triplicates; the function will calculate p-values,
+                           and fold change is computed as the ratio of the mean sample area
+                           to the mean control area.
        - ms2_analysis: A Boolean value that indicates whether to perform DIA fragment analysis. Set to True to enable DIA fragment analysis, or False to disable it.
        - fold_change: The fold change threshold for peak area comparison. Peaks with a fold change below this threshold will be excluded from analysis.
        - area_threshold: The minimum peak area threshold. Peaks with an area below this threshold will be excluded from analysis.
-       - filter_type: The type of peak filtering to perform. Set to 1 to only filter for peak area change (based on the maximum of the control group), 2 to filter for both p-value and fold change (treating all controls as a single group), or 3 to filter for both p-value and fold change (treating each set of solvent_blank, field blank, and lab blank as separate groups).
        - split_n (int): The number of pieces to split the large dataframe.
        - sat_intensity: The saturation intensity refers to the point where the intensity of an m/z value becomes so high that it may no longer be accurate. In such cases, the retention time can be adjusted to bring the intensity below the saturation intensity, thereby ensuring accurate measurement of the m/z value.
 
     returns:
         None.Generate Excel files that summarizes the differences between the control sets and sample sets.
 
     """
@@ -63,15 +69,15 @@
     print('============================================================================')
     print('                                                                            ')
 
     files_mzml = glob(os.path.join(path, '*.mzML'))
     files_mzml_DDA = [file for file in files_mzml if 'DDA' in os.path.basename(file)]
     files_mzml = [file for file in files_mzml if 'DDA' not in os.path.basename(file)]
     for file in files_mzml:
-        first_process(file, company=company, profile=profile, ms2_analysis=ms2_analysis,
+        first_process(file, company=company, profile=profile, control_group=control_group, ms2_analysis=ms2_analysis,
                       split_n=split_n, sat_intensity=sat_intensity, long_rt_split_n=long_rt_split_n)
 
     # 检查是否有遗漏的
     files_excel_temp = glob(os.path.join(path, '*.xlsx'))
     files_excel_names = [os.path.basename(i)[:-5] for i in files_excel_temp]
     path_omitted = []
     if len(files_mzml) > len(files_excel_names):
@@ -81,15 +87,16 @@
                 pass
             else:
                 path_omitted.append(path1)
     if len(path_omitted) == 0:
         pass
     else:
         for file in path_omitted:
-            first_process(file, company=company, profile=profile, ms2_analysis=ms2_analysis,
+            first_process(file, company=company, profile=profile, control_group=control_group,
+                          ms2_analysis=ms2_analysis,
                           split_n=split_n, sat_intensity=sat_intensity, long_rt_split_n=long_rt_split_n)
 
     # 中间过程
     files_excel = glob(os.path.join(path, '*.xlsx'))
     peak_alignment(files_excel)
     ref_all = pd.read_excel(os.path.join(path, 'peak_ref.xlsx'), index_col='Unnamed: 0')
 
@@ -104,20 +111,16 @@
 
     # 第三个过程, 做fold change filter
     print('                                                                            ')
     print('============================================================================')
     print('Third process started...')
     print('============================================================================')
     print('                                                                            ')
-    if filter_type == 1:
-        fold_change_filter(path, fold_change=fold_change, area_threshold=area_threshold)
-    elif filter_type == 2:
-        fold_change_filter2(path, fold_change=fold_change, p_value=p_value, area_threshold=area_threshold)
-    elif filter_type == 3:
-        fold_change_filter3(path, fold_change=fold_change, p_value=p_value, area_threshold=area_threshold)
+
+    fold_change_filter(path, control_group=control_group, filter_type=filter_type)
 
     # 如果有DDA，将DDA数据加入到excel里
     files_excel = glob(os.path.join(path, '*.xlsx'))
     unique_cmps = [file for file in files_excel if 'unique_cmps' in os.path.basename(file)]
 
     for file in files_mzml_DDA:
         df2 = gen_DDA_ms2_df(file, company, profile=profile, opt=False)
@@ -214,15 +217,16 @@
             peak_all = peak_all[(peak_all['rt'] > ranges[n][0]) & (peak_all['rt'] <= ranges[n][1])]
             peak_list_all.append(peak_all)
 
         peak_all = pd.concat(peak_list_all).reset_index(drop=True)
     return peak_all
 
 
-def first_process(file, company, profile=True, i_threshold=200, SN_threshold=3,
+def first_process(file, company, profile=True, control_group=['methanol_blank', 'control', 'lab_blank'],
+                  i_threshold=200, SN_threshold=3,
                   ms2_analysis=True, frag_rt_error=0.02, split_n=20, sat_intensity=False, long_rt_split_n=1):
     """
     Processes HRMS data by performing peak picking and generating a result file.
 
     Args:
         file (str): Path to the input file to be processed.
         company (str): The manufacturer of the instrument used to generate the data (e.g., 'Waters', 'Agilent', etc.).
@@ -247,17 +251,15 @@
 
     # 是否分析ms2
     if len(ms2) == 0:
         pass
     else:
         if ms2_analysis is True:
             basename_file = os.path.basename(file)
-            if ('control' in basename_file.lower()) | ('blank' in basename_file.lower()) | (
-                    'methanol' in basename_file.lower()) | (
-                    'qaqc' in basename_file.lower()):
+            if any(item.lower() in basename_file.lower() for item in control_group):
                 pass
             else:
                 print('----------------------------')
                 print('Starting DIA ms2 analysis...')
                 print('----------------------------')
                 peak_all2 = ultimate_peak_picking(ms2, profile=profile, split_n=split_n, i_threshold=i_threshold,
                                                   SN_threshold=SN_threshold, sat_intensity=sat_intensity,
@@ -315,28 +317,34 @@
     print('                                                              ')
     print(f'-------------------------------------------------------------')
     print(f'Result generated! File name:{file_name}')
     print('--------------------------------------------------------------')
     print('                                                              ')
 
 
-def multi_process(path, company, profile=True, processors=1, p_value=1, ms2_analysis=True, fold_change=0,
-                  area_threshold=200, filter_type=3, split_n=20, sat_intensity=False, long_rt_split_n=1):
+def multi_process(path, company, profile=True, control_group=['lab_blank', 'methanol'], processors=1, i_threshold=200,
+                  SN_threshold=3, ms2_analysis=True,
+                  filter_type=1, split_n=20, sat_intensity=False, long_rt_split_n=1):
     """
     This function is to process mzML data and perform a comparison between the sample set and the control set. The resulting data will be used to generate an Excel file that summarizes the differences between the two sets.
     Args:
        - path: The file path for the mzML files that will be processed. For example, '../Users/Desktop/my_HRMS_files'.
        - company: The type of mass spectrometer used to acquire the data. Valid options are 'Waters', 'Thermo', 'Sciex', and 'Agilent'.
        - profile: A Boolean value that indicates whether the data is in profile or centroid mode. True for profile mode, False for centroid mode.
        - processors: This setting determines the number of processors that will be used for data processing in parallel running. If the memory usage exceeds 90%, please note that some Excel files may not be generated.
-       - p_value: The maximum p-value threshold for peak identification. Peaks with a p-value above this threshold will be excluded from analysis.
+       - control_group (List[str]): A list of labels representing the control group.These labels are used in the search for relevant file names.
+       - filter_type (int): Determines the mode of operation.
+                           Set to 1 for data without triplicates; fold change is computed
+                           as the ratio of the sample area to the maximum control area.
+                           Set to 2 for data with triplicates; the function will calculate p-values,
+                           and fold change is computed as the ratio of the mean sample area
+                           to the mean control area.
        - ms2_analysis: A Boolean value that indicates whether to perform DIA fragment analysis. Set to True to enable DIA fragment analysis, or False to disable it.
        - fold_change: The fold change threshold for peak area comparison. Peaks with a fold change below this threshold will be excluded from analysis.
        - area_threshold: The minimum peak area threshold. Peaks with an area below this threshold will be excluded from analysis.
-       - filter_type: The type of peak filtering to perform. Set to 1 to only filter for peak area change (based on the maximum of the control group), 2 to filter for both p-value and fold change (treating all controls as a single group), or 3 to filter for both p-value and fold change (treating each set of solvent_blank, field blank, and lab blank as separate groups).
        - split_n (int): The number of pieces to split the large dataframe.
        - sat_intensity: The saturation intensity refers to the point where the intensity of an m/z value becomes so high that it may no longer be accurate. In such cases, the retention time can be adjusted to bring the intensity below the saturation intensity, thereby ensuring accurate measurement of the m/z value.
 
     returns:
         None.Generate Excel files that summarizes the differences between the control sets and sample sets.
 
     """
@@ -346,15 +354,15 @@
     # 第一个过程
     i_threshold = 200
     SN_threshold = 3,
     frag_rt_error = 0.02
     pool = Pool(processes=processors)
     for file in files_mzml:
         print(file)
-        pool.apply_async(first_process, args=(file, company, profile, i_threshold, SN_threshold,
+        pool.apply_async(first_process, args=(file, company, profile, control_group, i_threshold, SN_threshold,
                                               ms2_analysis, frag_rt_error, split_n, sat_intensity, long_rt_split_n))
 
     print('                                                                            ')
     print('============================================================================')
     print('First process started...')
     print('============================================================================')
     print('                                                                            ')
@@ -375,15 +383,15 @@
     if len(path_omitted) == 0:
         pass
     else:
         pool = Pool(processes=processors)
         for file in path_omitted:
             print('Omitted files')
             print(file)
-            pool.apply_async(first_process, args=(file, company, profile, i_threshold, SN_threshold,
+            pool.apply_async(first_process, args=(file, company, profile, control_group, i_threshold, SN_threshold,
                                                   ms2_analysis, frag_rt_error, split_n, sat_intensity, long_rt_split_n))
         pool.close()
         pool.join()
 
     # 中间过程
     files_excel = glob(os.path.join(path, '*.xlsx'))
     peak_alignment(files_excel)
@@ -404,20 +412,16 @@
 
     # 第三个过程, 做fold change filter
     print('                                                                            ')
     print('============================================================================')
     print('Third process started...')
     print('============================================================================')
     print('                                                                            ')
-    if filter_type == 1:
-        fold_change_filter(path, fold_change=fold_change, area_threshold=area_threshold)
-    elif filter_type == 2:
-        fold_change_filter2(path, fold_change=fold_change, p_value=p_value, area_threshold=area_threshold)
-    elif filter_type == 3:
-        fold_change_filter3(path, fold_change=fold_change, p_value=p_value, area_threshold=area_threshold)
+
+    fold_change_filter(path, control_group=control_group, filter_type=filter_type)
 
     # 如果有DDA，将DDA数据加入到excel里
     files_excel = glob(os.path.join(path, '*.xlsx'))
     unique_cmps = [file for file in files_excel if 'unique_cmps' in os.path.basename(file)]
 
     for file in files_mzml_DDA:
         df2 = gen_DDA_ms2_df(file, company, profile=profile, opt=False)
@@ -1359,15 +1363,18 @@
         if len(index[0]) == 0:
             omitted_pairs.append([rt, mz])
 
     # Generate reference pairs for omitted pairs
     omitted_reference_pairs = align_reference_pairs(np.array(omitted_pairs))
 
     # Combine reference pairs
-    final_reference_pairs = np.vstack([reference_pairs, omitted_reference_pairs])
+    if len(omitted_reference_pairs) != 0:
+        final_reference_pairs = np.vstack([reference_pairs, omitted_reference_pairs])
+    else:
+        final_reference_pairs = reference_pairs
 
     return final_reference_pairs
 
 
 def second_process(file, ref_all, company, profile=True, long_rt_split_n=1):
     """
     This function will use the reference rt&mz pair, and obtain the peak area at specific rt & mz
@@ -1474,15 +1481,15 @@
     left_locator = find_locators(df_mz_list, mzs - 0.01)
     right_locator = find_locators(df_mz_list, mzs + 0.01)
     mz_locators = np.array([left_locator, right_locator]).T
 
     # 3. find the locators of rt
     df_rt = df1.columns.values
     rt_locators = [[argmin(abs(df_rt - (rt - 0.2))), argmin(abs(df_rt - (rt + 0.2)))] for rt in rts]
-
+    rt_locators = [[x[0], x[1] if x[0] != x[1] else x[1] + 1] for x in rt_locators]  # 有时候locators是一样的[616:616] 加个保护机制
     # 4. obtain the peak areas
     area_all = [round(scipy.integrate.simps(
         df1.iloc[mz_locators[i][0]:mz_locators[i][1], rt_locators[i][0]:rt_locators[i][1]].values.sum(axis=0) - min(
             df1.iloc[mz_locators[i][0]:mz_locators[i][1], rt_locators[i][0]:rt_locators[i][1]].values.sum(axis=0)),
         df_rt[rt_locators[i][0]:rt_locators[i][1]]), 0) * 4 for i in range(len(mz_locators))]
     # Uses the locators found in stepas 2 and 3 to calculate the peak areas for each rt&mz pair in `df1`, using the `scipy.integrate.simps` function.
     # The result is multiplied by 4, then rounded to 0 decimal places.
@@ -1578,292 +1585,136 @@
         name = 'data' + str(i)
         data[name] = pd.read_excel(align[i], index_col='Unnamed: 0')
         data_to_concat.append(data[name])
     final_data = pd.concat(data_to_concat, axis=1)
     return final_data
 
 
-def fold_change_filter(path, fold_change=0, area_threshold=500):
+def fold_change_filter(path, control_group=['lab_blank'], filter_type=1):
     """
-    To only filter for peak area change (based on the maximum of the control group).
-
-    Args:
-        path:The file path for the mzML files that will be processed.
-        fold_change:The fold change threshold for peak area comparison. Peaks with a fold change below this threshold will be excluded from analysis.
-        area_threshold:The minimum peak area threshold. Peaks with an area below this threshold will be excluded from analysis.
-
-    Returns:
-        None. The output file will have the suffix '_unique_cmps.xlsx'.
-    """
-
-    # 整合blank数据，获的最大值
-    print('\r Organizing blank data...         ', end='')
-    excel_path = os.path.join(path, '*.xlsx')
-    files_excel = glob(excel_path)
-    alignment = [file for file in files_excel if 'alignment' in file]
-    area_files = [file for file in files_excel if 'final_area' in file]
-    blk_files = [file for file in area_files if 'blank' in file.lower() or
-                 'control' in file.lower() or 'qaqc' in file.lower() or 'methanol' in file.lower()]
-    blk_df = concat_alignment(blk_files)  # 生成所有blank的dataframe表
-    blk_s = blk_df.max(axis=1)  # 找到blanks中每个峰的最大值
-    final_blk = blk_s.to_frame(name='blk')
-    print('\r Start to process fold change         ', end='')
-    # 整合每个area_file与blank的对比结果，输出fold change 大于fold_change倍的值
-    area_files_sample = [file for file in area_files if 'blank' not in file.lower() and
-                         'control' not in file.lower() and 'qaqc' not in file.lower()
-                         and 'methanol' not in file.lower()]
-    for i in tqdm(range(len(area_files_sample)), desc='Fold change processing'):
-        # 基于峰面积的对比拿到比较数据
-        sample = pd.read_excel(area_files_sample[i], index_col='Unnamed: 0')
-        compare = pd.concat((sample, final_blk), axis=1)
-        compare['fold_change'] = (compare.iloc[:, 0] / compare.iloc[:, 1]).round(2)
-        compare_result1 = compare[compare['fold_change']
-                                  > fold_change].sort_values(by=compare.columns[0], ascending=False)
-        compare_result = compare_result1[compare_result1[compare_result1.columns[0]] > area_threshold]
-        # 开始处理alignment文件
-        name = os.path.basename(area_files_sample[i]).replace('_final_area.xlsx', '')  # 拿到名字
-        alignment_path = [file for file in alignment if name in file][0]
-        alignment_df = pd.read_excel(alignment_path, index_col='new_index').sort_values(by='intensity')
-        alignment_df1 = alignment_df[~alignment_df.index.duplicated(keep='last')]  # 去掉重复索引
-
-        final_index = np.intersect1d(alignment_df1.index.values, compare_result.index.values)
-        final_alignment = alignment_df1.loc[final_index, :].sort_values(by='intensity', ascending=False)
-        final_alignment['fold_change'] = compare_result.loc[final_index, ['fold_change']]
-        new_name = area_files_sample[i].replace('_final_area', '_unique_cmps')  # 文件输出名称
-        final_alignment.to_excel(new_name)
+    This function calculates the fold change and optionally p-values
+    by comparing a set of samples to a control group.
 
-
-def fold_change_filter2(path, fold_change=0, p_value=1, area_threshold=500):
-    """
-    To filter for both p-value and fold change (treating all controls as a single group).
+    The computation of fold change differs based on whether the data
+    consists of triplicates or not, which is determined by the 'filter_type' parameter.
 
     Args:
-        path:The file path for the mzML files that will be processed.
-        fold_change:The fold change threshold for peak area comparison. Peaks with a fold change below this threshold will be excluded from analysis.
-        area_threshold:The minimum peak area threshold. Peaks with an area below this threshold will be excluded from analysis.
+        path (str): The file path for the mzML files to be processed.
+        control_group (List[str]): A list of labels representing the control group.
+                                   These labels are used in the search for relevant file names.
+        filter_type (int): Determines the mode of operation.
+                           Set to 1 for data without triplicates; fold change is computed
+                           as the ratio of the sample area to the maximum control area.
+                           Set to 2 for data with triplicates; the function will calculate p-values,
+                           and fold change is computed as the ratio of the mean sample area
+                           to the mean control area.
 
     Returns:
-        None. The output file will have the suffix '_unique_cmps.xlsx'.
+        None. The output, saved as an Excel file with the suffix '_unique_cmps.xlsx',
+        contains the computed fold change (and p-values if 'filter_type' is set to 2).
     """
 
-    def get_p_value(mean1, std1, nobs1, mean2, std2, nobs2):
-        result = ttest_ind_from_stats(mean1, std1, nobs1, mean2, std2, nobs2)
-        return result.pvalue
+    # Assume df1 and df2 are your dataframes
 
-    # 把 excel files分类
+    def calculate_p_value(row1, row2):
+        t_stat, p_value = ttest_ind(row1, row2)
+        return p_value
+
+    # locate files data with alignment and final_area.
+    print('\r Organizing blank data...         ', end='')
     excel_path = os.path.join(path, '*.xlsx')
     files_excel = glob(excel_path)
     alignment = [file for file in files_excel if 'alignment' in file]
     area_files = [file for file in files_excel if 'final_area' in file]
-    blk_files = [file for file in area_files if 'blank' in file.lower() or
-                 'control' in file.lower() or 'qaqc' in file.lower() or 'methanol' in file.lower()]
 
-    # 开始处理blank 统计数据
-    blk_df = concat_alignment(blk_files)  # 生成所有blank的dataframe表
+    # Use control_group variable in file checks
+    blk_files = [file for file in area_files if any(group.lower() in file.lower() for group in control_group)]
 
-    area_files_sample = [file for file in area_files if 'blank' not in file and
-                         'control' not in file.lower() and 'qaqc' not in file.lower() and
-                         'methanol' not in file.lower()]
-    all_names = list(
-        set([os.path.basename(x).replace('_final_area.xlsx', '')[:-1] for x in area_files_sample]))  # 拿到所有样品名称
-    # 获得blank的统计信息
-    blk_df_info = pd.concat([blk_df.mean(axis=1), blk_df.std(axis=1), blk_df.apply(len, axis=1)], axis=1)
-    blk_df_info.columns = ['mean1', 'std1', 'nobs1']
-
-    # 开始处理sample
-    for name in all_names:
-        print(name)
-        samples1 = [x for x in area_files_sample if name in x]
-        sample_df = concat_alignment(samples1)
-        sample_df_info = pd.concat([sample_df.mean(axis=1), sample_df.std(axis=1), sample_df.apply(len, axis=1)],
-                                   axis=1)
-        sample_df_info.columns = ['mean2', 'std2', 'nobs2']
-        all_info = pd.concat([blk_df_info, sample_df_info], axis=1)
-        pvalues_s = all_info.apply(
-            lambda row: get_p_value(row['mean1'], row['std1'], row['nobs1'], row['mean2'], row['std2'], row['nobs2']),
-            axis=1)
-        fold_change_s = (all_info['mean2'] / all_info['mean1']).round(2)
-        area_sample_mean = all_info['mean2'].round(0)
-        area_sample_std = all_info['std2'].round(0)
-        # 将数值付给每一个alignment变量
-
-        samples1_alignment = [x for x in alignment if name in x]
-        for alignment_path in samples1_alignment:
-            alignment_file = pd.read_excel(alignment_path, index_col='new_index')
-            alignment_file = alignment_file[~alignment_file.index.duplicated(keep='last')]  # 去掉重复索引
-            alignment_file['area_mean'] = area_sample_mean.loc[alignment_file.index.values]
-            alignment_file['area_std'] = area_sample_std.loc[alignment_file.index.values]
-            alignment_file['fold_change'] = fold_change_s.loc[alignment_file.index.values]
-            alignment_file['p_values'] = pvalues_s.loc[alignment_file.index.values]
-            alignment_file['Control set number'] = len(blk_files)
-            alignment_file['Sample set number'] = len(samples1_alignment)
-
-            unique_cmp = alignment_file[(alignment_file['fold_change'] > fold_change)
-                                        & (alignment_file['p_values'] < p_value)
-                                        & (alignment_file['area'] > area_threshold)].sort_values(by='intensity',
-                                                                                                 ascending=False)
-            columns = unique_cmp.columns.values
-            new_columns = sort_columns_name(columns)
-            unique_cmp = unique_cmp.loc[:, new_columns]
-
-            unique_cmp.to_excel(alignment_path.replace('_alignment', '_unique_cmps'))
-
-
-def fold_change_filter3(path, fold_change=0, p_value=1, area_threshold=500):
-    """
-    To filter for both p-value and fold change (treating each set of solvent_blank, field blank, and lab blank as separate groups).
-
-    Args:
-        path:The file path for the mzML files that will be processed.
-        fold_change:The fold change threshold for peak area comparison. Peaks with a fold change below this threshold will be excluded from analysis.
-        p_value:The maximum p-value threshold for peak identification. Peaks with a p-value above this threshold will be excluded from analysis.
-        area_threshold:The minimum peak area threshold. Peaks with an area below this threshold will be excluded from analysis.
-
-    Returns:
-        None. The output file will have the suffix '_unique_cmps.xlsx'.
-    """
-
-    def get_p_value(mean1, std1, nobs1, mean2, std2, nobs2):
-        result = ttest_ind_from_stats(mean1, std1, nobs1, mean2, std2, nobs2)
-        return result.pvalue
-
-    # 把 excel files分类
-    excel_path = os.path.join(path, '*.xlsx')
-    files_excel = glob(excel_path)
-    alignment = [file for file in files_excel if 'alignment' in file]
-    area_files = [file for file in files_excel if 'final_area' in file]
-    methanol = [file for file in area_files if 'methanol' in file.lower()]  # blank1
-    ISTD_blank = [file for file in area_files if 'istd_blank' in file.lower()]  # blank2
-    field_blank = [file for file in area_files if 'field_blank' in file.lower()]  # blank3
-    lab_blank = [file for file in area_files if 'lab_blank' in file.lower()]  # blank4
-    sampler_blank = [file for file in area_files if 'sampler_blank' in file.lower()]  # blank5
-    control = [file for file in area_files if 'control' in file.lower()]  # blank6
-    # 开始处理blank 统计数据
-    # 1.甲醇空白
-    if len(methanol) != 0:
-        methanol_df = concat_alignment(methanol)  # 生成所有blank的dataframe表
-        methanol_df_info = pd.concat([methanol_df.mean(axis=1), methanol_df.std(axis=1),
-                                      methanol_df.apply(len, axis=1)], axis=1).fillna(1)
-        methanol_df_info.columns = ['mean1', 'std1', 'nobs1']
-    else:
-        methanol_df_info = []
-    # 2. 内标空白
-    if len(ISTD_blank) != 0:
-        ISTD_blank_df = concat_alignment(ISTD_blank)
-        ISTD_blank_df_info = pd.concat([ISTD_blank_df.mean(axis=1), ISTD_blank_df.std(axis=1),
-                                        ISTD_blank_df.apply(len, axis=1)], axis=1).fillna(1)
-        ISTD_blank_df_info.columns = ['mean1', 'std1', 'nobs1']
-    else:
-        ISTD_blank_df_info = []
-
-    # 3. 现场空白
-    if len(field_blank) != 0:
-        field_blank_df = concat_alignment(field_blank)
-        field_blank_df_info = pd.concat([field_blank_df.mean(axis=1), field_blank_df.std(axis=1),
-                                         field_blank_df.apply(len, axis=1)], axis=1).fillna(1)
-        field_blank_df_info.columns = ['mean1', 'std1', 'nobs1']
-    else:
-        field_blank_df_info = []
-
-    # 4. 实验室空白
-    if len(lab_blank) != 0:
-        lab_blank_df = concat_alignment(lab_blank)
-        lab_blank_df_info = pd.concat([lab_blank_df.mean(axis=1), lab_blank_df.std(axis=1),
-                                       lab_blank_df.apply(len, axis=1)], axis=1).fillna(1)
-        lab_blank_df_info.columns = ['mean1', 'std1', 'nobs1']
-    else:
-        lab_blank_df_info = []
-
-    # 5. 采样器空白
-    if len(sampler_blank) != 0:
-        sampler_blank_df = concat_alignment(sampler_blank)
-        sampler_blank_df_info = pd.concat([sampler_blank_df.mean(axis=1), sampler_blank_df.std(axis=1),
-                                           sampler_blank_df.apply(len, axis=1)], axis=1).fillna(1)
-        sampler_blank_df_info.columns = ['mean1', 'std1', 'nobs1']
-    else:
-        sampler_blank_df_info = []
-
-    # 其他对照
-    if len(control) != 0:
-        control_df = concat_alignment(control)
-        control_df_info = pd.concat([control_df.mean(axis=1), control_df.std(axis=1),
-                                     control_df.apply(len, axis=1)], axis=1).fillna(1)
-        control_df_info.columns = ['mean1', 'std1', 'nobs1']
-    else:
-        control_df_info = []
-
-    blank_all = [methanol_df_info, ISTD_blank_df_info, field_blank_df_info, lab_blank_df_info, sampler_blank_df_info,
-                 control_df_info]
-    blank_name = ['methanol', 'ISTD_blank', 'field_blank', 'lab_blank', 'sampler_blank', 'control']
-
-    print('--------------------------------')
-    print('Start processing samples...')
-    print('--------------------------------')
+    blk_df = concat_alignment(blk_files)  # 生成所有blank的dataframe表
 
+    print('\r Start to process fold change         ', end='')
+    # 整合每个area_file与blank的对比结果，输出fold change 大于fold_change倍的值
     area_files_sample = [file for file in area_files if
-                         'methanol' not in file.lower() and 'blank' not in file.lower()
-                         and 'control' not in file.lower()]
+                         not any(group.lower() in file.lower() for group in control_group)]
     all_names = list(
         set([os.path.basename(x).replace('_final_area.xlsx', '')[:-1] for x in area_files_sample]))  # 拿到所有样品名称
 
-    for name in all_names:
-        print(name)
-        samples1 = [x for x in area_files_sample if name in x]
-        sample_df = concat_alignment(samples1)
-        sample_df_info = pd.concat([sample_df.mean(axis=1), sample_df.std(axis=1), sample_df.apply(len, axis=1)],
-                                   axis=1)
-        sample_df_info.columns = ['mean2', 'std2', 'nobs2']
-
-        # 计算每个样品组之间的p值
-        all_p_mean_std = []
-        all_p_mean_std_name = []
-
-        area_sample_mean = sample_df_info['mean2'].round(0)
-        all_p_mean_std.append(area_sample_mean)
-        all_p_mean_std_name.append('Sample_area_mean')
-
-        area_sample_std = sample_df_info['std2'].round(0)
-        all_p_mean_std.append(area_sample_std)
-        all_p_mean_std_name.append('Sample_area_std')
-
-        for j in tqdm(range(len(blank_all)), desc='Calculating p values'):
-            blk_df_info = blank_all[j]
-            if len(blk_df_info) != 0:  # 必须不等于0才能进行合并
-                all_info = pd.concat([blk_df_info, sample_df_info], axis=1)
-                pvalues_s = all_info.apply(
-                    lambda row: get_p_value(row['mean1'], row['std1'], row['nobs1'], row['mean2'], row['std2'],
-                                            row['nobs2']),
-                    axis=1)
-                all_p_mean_std.append(pvalues_s)
-                all_p_mean_std_name.append(blank_name[j] + '_p_value')
-
-                fold_change_s = (all_info['mean2'] / all_info['mean1']).round(2)
-                all_p_mean_std.append(fold_change_s)
-                all_p_mean_std_name.append(blank_name[j] + '_fold_change')
-
-        addition = pd.concat(all_p_mean_std, axis=1)
-        addition.columns = all_p_mean_std_name
-
-        # 开始生成样品
-        samples1_alignment = [x for x in alignment if name in x]
-        for alignment_path in samples1_alignment:
-            alignment_file = pd.read_excel(alignment_path, index_col='new_index')
-            alignment_file = alignment_file[~alignment_file.index.duplicated(keep='last')]  # 去掉重复索引
-            unique_cmp = pd.concat([alignment_file, addition.loc[alignment_file.index]], axis=1)
-            columns = unique_cmp.columns.values
-            new_columns = sort_columns_name(columns)
-            unique_cmp = unique_cmp.loc[:, new_columns]
-            fold_change_columns = [column for column in unique_cmp.columns if 'fold_change' in column]
-            p_values_columns = [column for column in unique_cmp.columns if 'p_value' in column]
-            for column in fold_change_columns:
-                unique_cmp = unique_cmp[unique_cmp[column] > fold_change]
-            for column in p_values_columns:
-                unique_cmp = unique_cmp[unique_cmp[column] < p_value]
-            unique_cmp = unique_cmp[unique_cmp['area'] > area_threshold].sort_values(by='intensity', ascending=False)
-            unique_cmp.to_excel(alignment_path.replace('_alignment', '_unique_cmps'))
+    if filter_type == 1:
+        for i in tqdm(range(len(area_files_sample)), desc='Fold change processing'):
+            # 基于峰面积的对比拿到比较数据
+            sample = pd.read_excel(area_files_sample[i], index_col='Unnamed: 0')
+            # 开始处理alignment文件，不能有重复的index
+            name = os.path.basename(area_files_sample[i]).replace('_final_area.xlsx', '')  # 拿到名字
+            alignment_path = [file for file in alignment if name in file][0]
+            alignment_df = pd.read_excel(alignment_path, index_col='new_index').sort_values(by='intensity')
+            alignment_df1 = alignment_df[~alignment_df.index.duplicated(keep='last')]  # 去掉重复索引
+            # 找到共有的new_index
+            final_index = np.intersect1d(alignment_df1.index.values, sample.index.values)
+
+            for control in control_group:
+                final_blk = blk_df.loc[:, [i for i in blk_df.columns if control in i]]
+                if len(final_blk.columns) > 1:
+                    final_blk = final_blk.max(axis=1)
+                elif len(final_blk.columns) < 1:
+                    print(f'"{control}" is not in the files!')
+                compare = pd.concat((sample, final_blk), axis=1)
+                compare[f'{control}_fold_change'] = (compare.iloc[:, 0] / compare.iloc[:, 1]).round(2)
+                compare1 = compare.loc[final_index]
+                compare2 = compare1.iloc[:, -1:]
+                alignment_df1 = pd.concat([alignment_df1, compare2], axis=1)
+            alignment_df1 = alignment_df1.sort_values(by='intensity', ascending=False)
+            alignment_df1.index.name = 'new_index'
+            alignment_df1.to_excel(alignment_path.replace('_alignment', '_unique_cmps'))
+    elif filter_type == 2:
+        # 根据样品名称一个个处理
+        for name in tqdm(all_names, desc='Processing triplicate samples'):
+            # 获得该名称下的文件
+            sample_files = [file for file in area_files_sample if name in os.path.basename(file)]
+            # 获得所有final_area
+            sample_df_all = []
+            for sample_file in sample_files:
+                df = pd.read_excel(sample_file, index_col='Unnamed: 0')
+                sample_df_all.append(df)
+            sample_final_area = pd.concat(sample_df_all, axis=1)
+            # 计算样品平均值和方差
+            sample_mean = round(sample_final_area.mean(axis=1), 0)
+            sample_std = round(sample_final_area.std(axis=1) / sample_final_area.mean(axis=1), 2)
+            sample_area_info = pd.concat([sample_mean, sample_std], axis=1)
+            sample_area_info.columns = ['Sample_area_mean', 'Sample_area_std']
+            # 获得所有control的结果
+            all_result = []
+            for control in control_group:
+                control_column = [column for column in blk_df.columns if control in column]
+                if len(control_column) == 0:
+                    pass
+                else:
+                    control_df = blk_df.loc[:, control_column]
+                    p_values = sample_final_area.apply(lambda row: calculate_p_value(row, control_df.loc[row.name]),
+                                                       axis=1)
+                    p_values.name = f'{control}_p_values'
+                    all_result.append(p_values)
+                    fold_change = round(sample_final_area.mean(axis=1) / control_df.mean(axis=1), 2)
+                    fold_change.name = f'{control}_fold_change'
+                    all_result.append(fold_change)
+            all_result_df = pd.concat(all_result, axis=1)
+            # 开始写入每一个alignment文件
+            alignment_path = [file for file in alignment if name in file]
+            for alignment_file in alignment_path:
+                alignment_df = pd.read_excel(alignment_file, index_col='new_index').sort_values(by='intensity')
+                alignment_df1 = alignment_df[~alignment_df.index.duplicated(keep='last')]  # 去掉重复索引
+                # 找到共有的new_index
+                final_index = np.intersect1d(alignment_df1.index.values, sample_final_area.index.values)
+                # 根据索引填充数据
+                alignment_df1 = pd.concat([alignment_df1, sample_area_info.loc[final_index, :]], axis=1)
+                alignment_df2 = pd.concat([alignment_df1, all_result_df.loc[final_index, :]], axis=1)
+                alignment_df2 = alignment_df2.sort_values(by='intensity', ascending=False)
+                alignment_df2.index.name = 'new_index'
+                alignment_df2.to_excel(alignment_file.replace('_alignment', '_unique_cmps'))
+    else:
+        print(f'filter_type = {filter_type} is not Supported, please use 1 or 2.')
 
 
 def ms_to_centroid(profile_data):
     """
     Transform profile data to centroid data.
 
     Args:
@@ -4613,10 +4464,284 @@
             if 'REACH' in str(result.loc[index, 'new_category']):  # 看一下原来的分类里，是否包含REACH
                 new_category.append('REACH')
             result.loc[index, 'new_category'] = str(new_category)
             result.loc[index, 'usage'] = str(usage)
     return result
 
 
+def draw_pie_chart(category_data_series, path=None, show=True, fraction=False, drop_list=None):
+    """
+    Draw a pie chart to represent the distribution of categories.
+
+    Args:
+        category_data_series (pandas Series): The data series containing the category data.
+        path (str, optional): The file path to save the chart image. Default is None.
+        show (bool, optional): Whether to display the chart. Default is True.
+        fraction (bool, optional): Whether to display the percentage values as fractions. Default is False.
+        drop_list (list, optional): A list of categories to remove from the chart. Default is None.
+
+    Returns:
+        None
+    """
+    data = category_data_series
+    if isinstance(drop_list, list):
+        drop_list = [category for category in drop_list if category in data.index]
+        data = data.drop(drop_list)
+    plt.rcParams['font.sans-serif'] = 'Times New Roman'
+    plt.rcParams['font.size'] = 14
+
+    # Define explode values - explode if value is less than 5%
+    explode = (data / sum(data) < 0.05).map({True: 0.1, False: 0})
+
+    # Plot
+    fig1, ax1 = plt.subplots(figsize=(15, 6))
+
+    # Define color palette
+    colors = plt.cm.Set3(np.linspace(0, 1, len(data)))
+
+    wedges, texts, autotexts = ax1.pie(data, explode=explode, labels=None, startangle=140, autopct='')
+
+    # Equal aspect ratio ensures that pie is drawn as a circle
+    ax1.axis('equal')
+
+    # Create custom legend handles
+    legend_handles = [mpatches.Patch(color=colors[i], label=f'{data.index[i]} ({data.iloc[i]})') for i in
+                      range(len(data))]
+
+    # Set edge color for each legend handle
+    for handle in legend_handles:
+        handle.set_edgecolor('black')
+
+    # Add legend with custom handles
+    legend = plt.legend(handles=legend_handles,
+                        loc="center left",
+                        bbox_to_anchor=(0.7, 0.5),
+                        frameon=False)
+
+    # Place the percentage values outside the pie chart
+    def autopct_format(value):
+        return f'{value:.1f}%'
+
+    if fraction is False:
+        ax1.pie(data, explode=explode, labels=None, startangle=140, pctdistance=0.85, colors=colors, shadow=False)
+    else:
+        ax1.pie(data, explode=explode, labels=None, startangle=140, autopct=autopct_format, pctdistance=0.85,
+                colors=colors, shadow=False)
+
+    # Apply additional styling
+    plt.setp(autotexts, size=10, weight="bold", color='white')
+    plt.setp(texts, size=10)
+
+    # Add a title
+    plt.title("Distribution of Categories")
+
+    if show is True:
+        plt.show()
+
+    if path is not None:
+        bbox_extra_artists = [legend]  # Include legend in the bounding box calculation
+        plt.savefig(path, dpi=500, bbox_inches='tight', bbox_extra_artists=bbox_extra_artists)
+        plt.close('all')
+
+
+def pubchem_search(file):
+    """
+    Searches PubChem database to fill missing information in a given Excel file.
+
+    Args:
+        file (str): The path to the Excel file.
+
+    Returns:
+        pandas.DataFrame: The updated DataFrame with missing information filled.
+
+    The function reads an Excel file and searches PubChem using the InChIKey (ik) column
+    to retrieve missing information such as compound name, formula, CAS number, and SMILES.
+
+    If a compound's name, formula, or CAS number is missing, the function queries PubChem using
+    the provided InChIKey and retrieves the information if available.
+
+    Example:
+        updated_data = pubchem_search('data.xlsx')
+        print(updated_data.head())
+    """
+    import pubchempy as pcp
+    import re
+    df = pd.read_excel(file)
+    for i in tqdm(range(len(df)), desc='Searching in pubchem'):
+        name = df.loc[i, 'name']
+        formula = df.loc[i, 'formula']
+        cas = df.loc[i, 'CAS']
+        smile = df.loc[i, 'Smile']
+        ik = df.loc[i, 'ik']
+        if (name is np.nan) | (name is np.nan) | (name is np.nan):
+            cmp_all = pcp.get_compounds(ik, namespace='inchikey')
+            cmp = cmp_all[0] if len(cmp_all) != 0 else None
+            if cmp is not None:
+                synonyms = cmp.synonyms
+                new_smi = cmp.isomeric_smiles
+                new_formula = cmp.molecular_formula
+                cas_number = None
+                first_name = None
+                for item in synonyms:
+                    if re.match(r'^\d{2,}-\d{2,}-\d{1,}$', item) and not cas_number:
+                        cas_number = item
+                    elif not re.match(r'^\d{2,}-\d{2,}-\d{1,}$', item) and not first_name:
+                        first_name = item
+                    if cas_number and first_name:
+                        break
+                # start to fill the result
+                df.loc[i, 'name'] = first_name
+                df.loc[i, 'formula'] = new_formula
+                df.loc[i, 'Smile'] = new_smi
+                df.loc[i, 'CAS'] = cas_number
+    return df
+
+
+def AIF_multi_ce(path, company='Agilent', profile=False, control_group=['Methanol'], collision_energies=[10, 20, 40],
+                 filter_type=1, frag_rt_error=0.02, split_n=20,
+                 sat_intensity=False, long_rt_split_n=1, threshold=15, i_threshold=500, SN_threshold=3):
+    """
+    Processes AIF data when multiple collision energies are present.
+
+    Args:
+        path (str): The file path for the mzML files to be processed. Example, 'C:/Users/Desktop/my_HRMS_files'.
+        company (str): The brand of the mass spectrometer used to gather the data. Acceptable options are 'Waters', 'Thermo', 'Sciex', 'Agilent'.
+        profile (bool): Specifies whether the data is in profile or centroid mode. Set as True for profile mode, False for centroid mode.
+        control_group (List[str]): List of labels that designate the control group, used when searching for relevant filenames.
+        collision_energies (List[int]): List of different collision energies. Exclude 0 as it corresponds to MS1.
+        filter_type (int): Set to 1 for data without triplicates where fold change is calculated as the ratio of sample area to the maximum control area.
+                           Set to 2 for data with triplicates where the function computes p-values, and fold change as the ratio of the mean sample area to the mean control area.
+        frag_rt_error (float): The retention time (RT) error for matching fragment peak to the precursor's peak.
+        split_n (int): The mass range will be divided into n parts for easier processing.
+        sat_intensity (bool): Specifies if the mass spectrometer has a saturated intensity. Default is False.
+        long_rt_split_n (int): The retention time will be divided into n parts for easier processing.
+        threshold (int): The noise level threshold for a peak. Default is 15.
+        i_threshold (int): The intensity threshold for the peak picking step. Default is 500.
+        SN_threshold (int): The signal to noise ratio threshold for the peak picking step. Default is 3.
+
+    Returns:
+        None. The processed data is saved to an output file.
+    """
+
+    collision_energies = sorted(collision_energies)
+
+    files = glob(os.path.join(path, '*.mzML'))
+    # 1. first step peak picking
+    for file in files:
+        if any(control.lower() in file.lower() for control in control_group):
+            first_process(file, company, profile=profile, i_threshold=200,
+                          SN_threshold=3, ms2_analysis=False, frag_rt_error=frag_rt_error, split_n=split_n,
+                          sat_intensity=sat_intensity, long_rt_split_n=long_rt_split_n)
+        else:
+            ms_scans = [[] for _ in range(len(collision_energies) + 1)]
+            run = pymzml.run.Reader(file)
+            for scan in tqdm(run, desc='Separating Scans'):
+                if scan.ms_level == 1:
+                    ms_scans[0].append(scan)
+                elif 'collision energy' in scan:
+                    collision_energy = scan['collision energy']
+                    if collision_energy in collision_energies:
+                        index = collision_energies.index(collision_energy) + 1
+                        ms_scans[index].append(scan)
+            peak_alls = []
+            for ms in ms_scans:
+                peak_all = ultimate_peak_picking(ms, profile=profile, split_n=split_n, threshold=threshold,
+                                                 i_threshold=i_threshold, SN_threshold=SN_threshold,
+                                                 rt_error_alignment=0.05, mz_error_alignment=0.015, mz_overlap=1,
+                                                 sat_intensity=sat_intensity, long_rt_split_n=long_rt_split_n,
+                                                 rt_overlap=1)
+                peak_alls.append(peak_all)
+            peak_all_ms1 = peak_alls[0]
+            peak_alls_ms2 = peak_alls[1:]
+            for j, each_ms2_peak_all in enumerate(peak_alls_ms2):
+                frag_all, spec_all = get_frag_DIA(peak_all_ms1, each_ms2_peak_all, frag_rt_error=frag_rt_error)
+                column_name1 = 'frag_DIA_' + str(collision_energies[j]) + 'V'
+                column_name2 = 'Spec_DIA' + str(collision_energies[j]) + 'V'
+                peak_all_ms1.loc[:, column_name1] = frag_all
+                peak_all_ms1.loc[:, column_name2] = spec_all
+            peak_all_ms1.to_excel(file.replace('.mzML', '.xlsx'))
+
+    # 中间过程
+    files_excel = glob(os.path.join(path, '*.xlsx'))
+    peak_alignment(files_excel)
+    ref_all = pd.read_excel(os.path.join(path, 'peak_ref.xlsx'), index_col='Unnamed: 0')
+
+    # 第二部分
+    for file in files:
+        second_process(file, ref_all, company=company, profile=profile, long_rt_split_n=long_rt_split_n)
+
+    # 第三部份，差异性分析
+    # 第三个过程, 做fold change filter
+    print('                                                                            ')
+    print('============================================================================')
+    print('Third process started...')
+    print('============================================================================')
+    print('                                                                            ')
+    fold_change_filter(path, control_group=control_group, filter_type=filter_type)
+
+
+def get_frag_DIA(peak_all, peak_all2, frag_rt_error=0.02):
+    """
+    This function assigns possible fragments from MS2 data (peak_all2) to the corresponding MS1 peaks in peak_all
+    based on the retention time (rt) range defined by frag_rt_error.
+
+    Args:
+        peak_all (pd.DataFrame): A DataFrame of peaks generated by peak picking from MS1 data.
+        peak_all2 (pd.DataFrame): A DataFrame of peaks generated by peak picking from MS2 data.
+        frag_rt_error (float, optional): The retention time error tolerance for matching MS1 peaks to MS2 fragments.
+                                         The default value is 0.02.
+
+    Returns:
+        tuple: Two lists containing fragment ions and spectra information, respectively. The fragment ions list
+               consists of strings representing lists of m/z values. The spectra list contains strings representing
+               Series of MS2 spectra with their intensity values, where each series is sorted by descending intensity
+               and trimmed to the top 20 values.
+
+    Note:
+        The peak_all and peak_all2 dataframes should contain 'rt', 'mz', and 'intensity' columns.
+    """
+
+    frag_all = []
+    spec_all = []
+    for i in tqdm(range(len(peak_all)), desc='Assign DIA MS2 spectrum'):
+        rt = peak_all.loc[i, 'rt']
+        df_DIA = peak_all2[(peak_all2['rt'] > rt - frag_rt_error)
+                           & (peak_all2['rt'] < rt + frag_rt_error)].sort_values(
+            by='intensity', ascending=False)
+        # append fragments
+        frag = str(list(df_DIA['mz'].values))
+        frag_all.append(frag)
+        # append ms2 spectra
+
+        s = pd.Series(data=df_DIA['intensity'].values, index=df_DIA['mz'])
+        # Convert the series to a DataFrame
+        df = pd.DataFrame(s).reset_index()
+        df.columns = ['m/z', 'intensity']
+
+        # Sort the dataframe by 'm/z'
+        df = df.sort_values(by='m/z')
+
+        # Create a new column 'group' for data grouping
+        df['group'] = (df['m/z'].diff() > 0.5).cumsum()
+
+        # Keep the row with max 'intensity' from each group
+        df = df.loc[df.groupby('group')['intensity'].idxmax()]
+
+        # Drop the 'group' column as it's no longer needed
+        df = df.drop(columns=['group'])
+
+        # Convert the DataFrame back to a Series
+        result = pd.Series(df['intensity'].values, index=df['m/z']).astype(int)
+
+        # Remove the name of the index
+        result.index.name = None
+
+        result = result.sort_values(ascending=False).iloc[:20]
+
+        spec_all.append(str(result))
+
+    return frag_all, spec_all
+
+
 if __name__ == '__main__':
     pass  # %config InlineBackendlineBackend.figure_format ='retina'
 #  plt.rcParams['font.sans-serif'] = 'Times New Roman'  # 设置全局字体
```

## Comparing `pyhrms-0.6.0/pyhrms/__init__.py` & `pyhrms-0.6.1/pyhrms/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 from . import pyhrms
 
-__version__ = "0.6.0"
+__version__ = "0.6.1"
 
 __all__ = [
     "sep_scans",
     "gen_df",
     "peak_picking",
     "split_peak_picking",
     "remove_unnamed_columns",
@@ -50,9 +50,16 @@
     "JsonToExcel",
     "suspect_list_matching",
     "calibration",
     "rename_files",
     "peak_checking_plot",
     "Calibration",
     "final_result_filter",
-    "update_category"
+    "update_category"，
+    "fold_change_filter"，
+    "get_frag_DIA"，
+    "AIF_multi_ce"，
+    "pubchem_search"，
+    "draw_pie_chart"
+    
+    
 ]
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## Comparing `pyhrms-0.6.0/pyhrms.egg-info/PKG-INFO` & `pyhrms-0.6.1/pyhrms.egg-info/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pyhrms
-Version: 0.6.0
+Version: 0.6.1
 Summary: A powerful GC/LC-HRMS data analysis tool
 Home-page: https://github.com/WangRui5/PyHRMS.git
 Author: Wang Rui
 Author-email: wtrt7009@gmail.com
 License: UNKNOWN
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
@@ -22,19 +22,19 @@
 
 Contributer: Rui Wang
 ======================
 First release date: Nov.15.2021
 
 Update
 ======
-July.3.2023: pyhrms 0.6.0 new features:
-
-    * Fixed bugs for processing centroid data.
-
+July.13.2023: pyhrms 0.6.1 new features:
 
+    * rewrite fold_chagne_filter function
+    * added AIF_multi_ce function to process AIF data
+    * added pubchem_search and draw_pie_chart
 
 
 pyhrms can be installed and import as following:
 
 .. code-block:: python
 
     pip install pyhrms
@@ -103,16 +103,14 @@
 
 Licensing
 =========
 
 The package is open source and can be utilized under MIT license. Please find the detail in licence file.
 
 
-
-
 PyHRMS documentation
 ===========================
 
 
 **I want starting using PyHRMS**
 
 
@@ -206,14 +204,18 @@
   |- get_ms2_from_DDA
   |- extract_tic
   |- ms_bg_removal
   |- JsonToExcel
   |- suspect_list_matching
   |- rename_files
   |- Calibration
+  |- get_frag_DIA
+  |- AIF_multi_ce
+  |- pubchem_search
+  |- draw_pie_chart
 
 
 
 Table of Content
 ~~~~~~~~~~~~~~~~~~~
 
 1. Quick start
@@ -241,31 +243,35 @@
 ***************************************
   This function primarily includes peak picking, peak alignment, and blank comparison to prioritize features that are unique to the sample compared to the blank.To ensure that the program distinguishes between the sample set and the control set, include the strings 'methanol', 'blank', and 'control' in your control set files, and exclude these strings from your sample set files.
 
 .. code-block:: python
 
     path = '../Users/Desktop/my_HRMS_files'
     company = 'Waters'
-    pms.multi_process(path, company, profile=True, processors=1, p_value=1, ms2_analysis=True, fold_change=1,
-                  area_threshold=200, filter_type=3)
+    pms.multi_process(path, company, profile=True, control_group=['lab_blank', 'methanol'], processors=1, ms2_analysis=True,
+                  area_threshold=200, filter_type=2)
 
 
 .. note::
 
    Parameters explanation:
 
    - path: The file path for the mzML files that will be processed. For example, '../Users/Desktop/my_HRMS_files'.
    - company: The type of mass spectrometer used to acquire the data. Valid options are 'Waters', 'Thermo', 'Sciex', and 'Agilent'.
    - profile: A Boolean value that indicates whether the data is in profile or centroid mode. True for profile mode, False for centroid mode.
    - processors: This setting determines the number of processors that will be used for data processing in parallel running. If the memory usage exceeds 90%, please note that some Excel files may not be generated.
-   - p_value: The maximum p-value threshold for peak identification. Peaks with a p-value above this threshold will be excluded from analysis.
+   - control_group (List[str]): A list of labels representing the control group.These labels are used in the search for relevant file names.
+   - filter_type (int): Determines the mode of operation.
+                           Set to 1 for data without triplicates; fold change is computed
+                           as the ratio of the sample area to the maximum control area.
+                           Set to 2 for data with triplicates; the function will calculate p-values,
+                           and fold change is computed as the ratio of the mean sample area
+                           to the mean control area.
    - ms2_analysis: A Boolean value that indicates whether to perform DIA fragment analysis. Set to True to enable DIA fragment analysis, or False to disable it.
-   - fold_change: The fold change threshold for peak area comparison. Peaks with a fold change below this threshold will be excluded from analysis.
    - area_threshold: The minimum peak area threshold. Peaks with an area below this threshold will be excluded from analysis.
-   - filter_type: The type of peak filtering to perform. Set to 1 to only filter for peak area change (based on the maximum of the control group), 2 to filter for both p-value and fold change (treating all controls as a single group), or 3 to filter for both p-value and fold change (treating each set of solvent_blank, field blank, and lab blank as separate groups).
 
 
 
 **The output file will have the suffix '_unique_cmps.xlsx' and will be structured as follows:**
 
 +--------------+-------+----------+-----------+--------+-------+----+
 | new_index    | rt    | mz       | intensity | S/N    | area  |... |
```

