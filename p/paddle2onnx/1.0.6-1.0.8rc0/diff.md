# Comparing `tmp/paddle2onnx-1.0.6-cp39-cp39-win_amd64.whl.zip` & `tmp/paddle2onnx-1.0.8rc0-cp310-cp310-macosx_11_0_arm64.whl.zip`

## zipinfo {}

```diff
@@ -1,58 +1,58 @@
-Zip file size: 1672277 bytes, number of entries: 56
--rw-rw-rw-  2.0 fat     2595 b- defN 23-Mar-15 20:58 paddle2onnx/__init__.py
--rw-rw-rw-  2.0 fat    10500 b- defN 23-Mar-15 20:58 paddle2onnx/command.py
--rw-rw-rw-  2.0 fat     3494 b- defN 23-Mar-15 20:58 paddle2onnx/convert.py
--rw-rw-rw-  2.0 fat     1386 b- defN 23-Mar-15 20:58 paddle2onnx/convert_to_fp16.py
--rw-rw-rw-  2.0 fat     1686 b- defN 23-Mar-15 20:58 paddle2onnx/optimize.py
--rw-rw-rw-  2.0 fat  4212224 b- defN 23-Mar-15 21:10 paddle2onnx/paddle2onnx_cpp2py_export.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat     4473 b- defN 23-Mar-15 20:58 paddle2onnx/utils.py
--rw-rw-rw-  2.0 fat      282 b- defN 23-Mar-15 21:08 paddle2onnx/version.py
--rw-rw-rw-  2.0 fat     6233 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/__init__.py
--rw-rw-rw-  2.0 fat    10919 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/command.py
--rw-rw-rw-  2.0 fat     8532 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/convert.py
--rw-rw-rw-  2.0 fat      691 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/constant/__init__.py
--rw-rw-rw-  2.0 fat      794 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/constant/constant.py
--rw-rw-rw-  2.0 fat     3132 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/constant/dtypes.py
--rw-rw-rw-  2.0 fat      756 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/constant/op_mapping_status.py
--rw-rw-rw-  2.0 fat      755 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/graph/__init__.py
--rw-rw-rw-  2.0 fat    11288 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/graph/dygraph_helper.py
--rw-rw-rw-  2.0 fat     9813 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/graph/graph.py
--rw-rw-rw-  2.0 fat     3243 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/graph/graph_helper.py
--rw-rw-rw-  2.0 fat    12768 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/graph/onnx_graph.py
--rw-rw-rw-  2.0 fat    11975 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/graph/paddle_graph.py
--rw-rw-rw-  2.0 fat     1455 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/op_mapper/__init__.py
--rw-rw-rw-  2.0 fat     9517 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/op_mapper/activation.py
--rw-rw-rw-  2.0 fat    10273 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/op_mapper/logic.py
--rw-rw-rw-  2.0 fat    15799 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/op_mapper/mapper_helper.py
--rw-rw-rw-  2.0 fat    47055 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/op_mapper/math.py
--rw-rw-rw-  2.0 fat    39833 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/op_mapper/nn.py
--rw-rw-rw-  2.0 fat    13214 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/op_mapper/op_mapper.py
--rw-rw-rw-  2.0 fat     8523 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/op_mapper/search.py
--rw-rw-rw-  2.0 fat    83058 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/op_mapper/tensor.py
--rw-rw-rw-  2.0 fat      625 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/op_mapper/custom_paddle_op/__init__.py
--rw-rw-rw-  2.0 fat     4514 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/op_mapper/custom_paddle_op/anchor_generator.py
--rw-rw-rw-  2.0 fat     2376 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/op_mapper/custom_paddle_op/box_clip.py
--rw-rw-rw-  2.0 fat     2376 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/op_mapper/custom_paddle_op/collect_fpn_proposals.py
--rw-rw-rw-  2.0 fat    13342 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/op_mapper/custom_paddle_op/deformable_conv.py
--rw-rw-rw-  2.0 fat     4126 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/op_mapper/custom_paddle_op/distribute_fpn_proposals.py
--rw-rw-rw-  2.0 fat    10088 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/op_mapper/custom_paddle_op/generate_proposals.py
--rw-rw-rw-  2.0 fat     6352 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/op_mapper/custom_paddle_op/grid_sampler.py
--rw-rw-rw-  2.0 fat      625 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/op_mapper/detection/__init__.py
--rw-rw-rw-  2.0 fat    15869 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/op_mapper/detection/box_coder.py
--rw-rw-rw-  2.0 fat     5159 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/op_mapper/detection/density_prior_box.py
--rw-rw-rw-  2.0 fat    14882 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/op_mapper/detection/multiclass_nms.py
--rw-rw-rw-  2.0 fat     7642 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/op_mapper/detection/prior_box.py
--rw-rw-rw-  2.0 fat    19708 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/op_mapper/detection/yolo_box.py
--rw-rw-rw-  2.0 fat      625 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/op_mapper/sequence/__init__.py
--rw-rw-rw-  2.0 fat     3322 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/op_mapper/sequence/im2sequence.py
--rw-rw-rw-  2.0 fat      768 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/passes/__init__.py
--rw-rw-rw-  2.0 fat     3544 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/passes/dumplicate_names_pass.py
--rw-rw-rw-  2.0 fat     2180 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/passes/inplace_node_pass.py
--rw-rw-rw-  2.0 fat     1364 b- defN 23-Mar-15 20:58 paddle2onnx/legacy/passes/pass_manager.py
--rw-rw-rw-  2.0 fat    11640 b- defN 23-Mar-15 21:14 paddle2onnx-1.0.6.dist-info/LICENSE
--rw-rw-rw-  2.0 fat      544 b- defN 23-Mar-15 21:14 paddle2onnx-1.0.6.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 23-Mar-15 21:14 paddle2onnx-1.0.6.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       58 b- defN 23-Mar-15 21:14 paddle2onnx-1.0.6.dist-info/entry_points.txt
--rw-rw-rw-  2.0 fat       12 b- defN 23-Mar-15 21:14 paddle2onnx-1.0.6.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     5521 b- defN 23-Mar-15 21:14 paddle2onnx-1.0.6.dist-info/RECORD
-56 files, 4683628 bytes uncompressed, 1663243 bytes compressed:  64.5%
+Zip file size: 2366806 bytes, number of entries: 56
+-rw-r--r--  2.0 unx     2530 b- defN 23-Jul-13 03:20 paddle2onnx/__init__.py
+-rw-r--r--  2.0 unx    10589 b- defN 23-Jul-13 03:20 paddle2onnx/command.py
+-rw-r--r--  2.0 unx     3411 b- defN 23-Jul-13 03:20 paddle2onnx/convert.py
+-rw-r--r--  2.0 unx     1348 b- defN 23-Jul-13 03:20 paddle2onnx/convert_to_fp16.py
+-rw-r--r--  2.0 unx     1640 b- defN 23-Jul-13 03:20 paddle2onnx/optimize.py
+-rwxr-xr-x  2.0 unx  7643244 b- defN 23-Jul-13 03:59 paddle2onnx/paddle2onnx_cpp2py_export.cpython-310-darwin.so
+-rw-r--r--  2.0 unx     4342 b- defN 23-Jul-13 03:20 paddle2onnx/utils.py
+-rw-r--r--  2.0 unx      277 b- defN 23-Jul-13 03:57 paddle2onnx/version.py
+-rw-r--r--  2.0 unx     6105 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/__init__.py
+-rw-r--r--  2.0 unx    10632 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/command.py
+-rw-r--r--  2.0 unx     8326 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/convert.py
+-rw-r--r--  2.0 unx      676 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/constant/__init__.py
+-rw-r--r--  2.0 unx      770 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/constant/constant.py
+-rw-r--r--  2.0 unx     3048 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/constant/dtypes.py
+-rw-r--r--  2.0 unx      737 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/constant/op_mapping_status.py
+-rw-r--r--  2.0 unx      738 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/graph/__init__.py
+-rw-r--r--  2.0 unx    11017 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/graph/dygraph_helper.py
+-rw-r--r--  2.0 unx     9526 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/graph/graph.py
+-rw-r--r--  2.0 unx     3160 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/graph/graph_helper.py
+-rw-r--r--  2.0 unx    12435 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/graph/onnx_graph.py
+-rw-r--r--  2.0 unx    11672 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/graph/paddle_graph.py
+-rw-r--r--  2.0 unx     1416 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/op_mapper/__init__.py
+-rw-r--r--  2.0 unx     9248 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/op_mapper/activation.py
+-rw-r--r--  2.0 unx     9993 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/op_mapper/logic.py
+-rw-r--r--  2.0 unx    15367 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/op_mapper/mapper_helper.py
+-rw-r--r--  2.0 unx    45782 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/op_mapper/math.py
+-rw-r--r--  2.0 unx    38881 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/op_mapper/nn.py
+-rw-r--r--  2.0 unx    12909 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/op_mapper/op_mapper.py
+-rw-r--r--  2.0 unx     8290 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/op_mapper/search.py
+-rw-r--r--  2.0 unx    80903 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/op_mapper/tensor.py
+-rw-r--r--  2.0 unx      612 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/op_mapper/custom_paddle_op/__init__.py
+-rw-r--r--  2.0 unx     4417 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/op_mapper/custom_paddle_op/anchor_generator.py
+-rw-r--r--  2.0 unx     2320 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/op_mapper/custom_paddle_op/box_clip.py
+-rw-r--r--  2.0 unx     2321 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/op_mapper/custom_paddle_op/collect_fpn_proposals.py
+-rw-r--r--  2.0 unx    13046 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/op_mapper/custom_paddle_op/deformable_conv.py
+-rw-r--r--  2.0 unx     4026 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/op_mapper/custom_paddle_op/distribute_fpn_proposals.py
+-rw-r--r--  2.0 unx     9865 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/op_mapper/custom_paddle_op/generate_proposals.py
+-rw-r--r--  2.0 unx     6201 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/op_mapper/custom_paddle_op/grid_sampler.py
+-rw-r--r--  2.0 unx      612 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/op_mapper/detection/__init__.py
+-rw-r--r--  2.0 unx    15506 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/op_mapper/detection/box_coder.py
+-rw-r--r--  2.0 unx     5034 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/op_mapper/detection/density_prior_box.py
+-rw-r--r--  2.0 unx    14539 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/op_mapper/detection/multiclass_nms.py
+-rw-r--r--  2.0 unx     7466 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/op_mapper/detection/prior_box.py
+-rw-r--r--  2.0 unx    19202 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/op_mapper/detection/yolo_box.py
+-rw-r--r--  2.0 unx      612 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/op_mapper/sequence/__init__.py
+-rw-r--r--  2.0 unx     3244 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/op_mapper/sequence/im2sequence.py
+-rw-r--r--  2.0 unx      752 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/passes/__init__.py
+-rw-r--r--  2.0 unx     3453 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/passes/dumplicate_names_pass.py
+-rw-r--r--  2.0 unx     2118 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/passes/inplace_node_pass.py
+-rw-r--r--  2.0 unx     1325 b- defN 23-Jul-13 03:20 paddle2onnx/legacy/passes/pass_manager.py
+-rw-r--r--  2.0 unx    11437 b- defN 23-Jul-13 03:59 paddle2onnx-1.0.8rc0.dist-info/LICENSE
+-rw-r--r--  2.0 unx      482 b- defN 23-Jul-13 03:59 paddle2onnx-1.0.8rc0.dist-info/METADATA
+-rw-r--r--  2.0 unx      110 b- defN 23-Jul-13 03:59 paddle2onnx-1.0.8rc0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       57 b- defN 23-Jul-13 03:59 paddle2onnx-1.0.8rc0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       12 b- defN 23-Jul-13 03:59 paddle2onnx-1.0.8rc0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     5540 b- defN 23-Jul-13 03:59 paddle2onnx-1.0.8rc0.dist-info/RECORD
+56 files, 8103321 bytes uncompressed, 2357730 bytes compressed:  70.9%
```

## zipnote {}

```diff
@@ -9,15 +9,15 @@
 
 Filename: paddle2onnx/convert_to_fp16.py
 Comment: 
 
 Filename: paddle2onnx/optimize.py
 Comment: 
 
-Filename: paddle2onnx/paddle2onnx_cpp2py_export.cp39-win_amd64.pyd
+Filename: paddle2onnx/paddle2onnx_cpp2py_export.cpython-310-darwin.so
 Comment: 
 
 Filename: paddle2onnx/utils.py
 Comment: 
 
 Filename: paddle2onnx/version.py
 Comment: 
@@ -144,26 +144,26 @@
 
 Filename: paddle2onnx/legacy/passes/inplace_node_pass.py
 Comment: 
 
 Filename: paddle2onnx/legacy/passes/pass_manager.py
 Comment: 
 
-Filename: paddle2onnx-1.0.6.dist-info/LICENSE
+Filename: paddle2onnx-1.0.8rc0.dist-info/LICENSE
 Comment: 
 
-Filename: paddle2onnx-1.0.6.dist-info/METADATA
+Filename: paddle2onnx-1.0.8rc0.dist-info/METADATA
 Comment: 
 
-Filename: paddle2onnx-1.0.6.dist-info/WHEEL
+Filename: paddle2onnx-1.0.8rc0.dist-info/WHEEL
 Comment: 
 
-Filename: paddle2onnx-1.0.6.dist-info/entry_points.txt
+Filename: paddle2onnx-1.0.8rc0.dist-info/entry_points.txt
 Comment: 
 
-Filename: paddle2onnx-1.0.6.dist-info/top_level.txt
+Filename: paddle2onnx-1.0.8rc0.dist-info/top_level.txt
 Comment: 
 
-Filename: paddle2onnx-1.0.6.dist-info/RECORD
+Filename: paddle2onnx-1.0.8rc0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## paddle2onnx/__init__.py

 * *Ordering differences only*

```diff
@@ -1,65 +1,65 @@
-# Copyright (c) 2022  PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from paddle2onnx.utils import logging
-from . import command
-from .convert import dygraph2onnx
-from .convert import program2onnx
-from .version import version
-from .version import git_version
-
-__version__ = version
-__commit_id__ = git_version
-
-
-def run_convert(model, input_shape_dict=None, scope=None, opset_version=9):
-    logging.warning(
-        "[Deprecated] `paddle2onnx.run_convert` will be deprecated in the future version, the recommended usage is `paddle2onnx.export`"
-    )
-    from paddle2onnx.legacy import run_convert
-    return run_convert(model, input_shape_dict, scope, opset_version)
-
-
-def export(model_file,
-           params_file="",
-           save_file=None,
-           opset_version=11,
-           auto_upgrade_opset=True,
-           verbose=True,
-           enable_onnx_checker=True,
-           enable_experimental_op=True,
-           enable_optimize=True,
-           custom_op_info=None,
-           deploy_backend="onnxruntime",
-           calibration_file="",
-           external_file="",
-           export_fp16_model=False):
-    import paddle2onnx.paddle2onnx_cpp2py_export as c_p2o
-    deploy_backend = deploy_backend.lower()
-    if custom_op_info is None:
-        onnx_model_str = c_p2o.export(
-            model_file, params_file, opset_version, auto_upgrade_opset, verbose,
-            enable_onnx_checker, enable_experimental_op, enable_optimize, {},
-            deploy_backend, calibration_file, external_file, export_fp16_model)
-    else:
-        onnx_model_str = c_p2o.export(
-            model_file, params_file, opset_version, auto_upgrade_opset, verbose,
-            enable_onnx_checker, enable_experimental_op, enable_optimize,
-            custom_op_info, deploy_backend, calibration_file, external_file,
-            export_fp16_model)
-    if save_file is not None:
-        with open(save_file, "wb") as f:
-            f.write(onnx_model_str)
-    else:
-        return onnx_model_str
+# Copyright (c) 2022  PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from paddle2onnx.utils import logging
+from . import command
+from .convert import dygraph2onnx
+from .convert import program2onnx
+from .version import version
+from .version import git_version
+
+__version__ = version
+__commit_id__ = git_version
+
+
+def run_convert(model, input_shape_dict=None, scope=None, opset_version=9):
+    logging.warning(
+        "[Deprecated] `paddle2onnx.run_convert` will be deprecated in the future version, the recommended usage is `paddle2onnx.export`"
+    )
+    from paddle2onnx.legacy import run_convert
+    return run_convert(model, input_shape_dict, scope, opset_version)
+
+
+def export(model_file,
+           params_file="",
+           save_file=None,
+           opset_version=11,
+           auto_upgrade_opset=True,
+           verbose=True,
+           enable_onnx_checker=True,
+           enable_experimental_op=True,
+           enable_optimize=True,
+           custom_op_info=None,
+           deploy_backend="onnxruntime",
+           calibration_file="",
+           external_file="",
+           export_fp16_model=False):
+    import paddle2onnx.paddle2onnx_cpp2py_export as c_p2o
+    deploy_backend = deploy_backend.lower()
+    if custom_op_info is None:
+        onnx_model_str = c_p2o.export(
+            model_file, params_file, opset_version, auto_upgrade_opset, verbose,
+            enable_onnx_checker, enable_experimental_op, enable_optimize, {},
+            deploy_backend, calibration_file, external_file, export_fp16_model)
+    else:
+        onnx_model_str = c_p2o.export(
+            model_file, params_file, opset_version, auto_upgrade_opset, verbose,
+            enable_onnx_checker, enable_experimental_op, enable_optimize,
+            custom_op_info, deploy_backend, calibration_file, external_file,
+            export_fp16_model)
+    if save_file is not None:
+        with open(save_file, "wb") as f:
+            f.write(onnx_model_str)
+    else:
+        return onnx_model_str
```

## paddle2onnx/command.py

```diff
@@ -1,273 +1,284 @@
-# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-from six import text_type as _text_type
-import argparse
-import ast
-import sys
-import os
-from paddle2onnx.utils import logging
-
-
-def str2list(v):
-    if len(v) == 0:
-        return None
-    v = v.replace(" ", "")
-    v = eval(v)
-    return v
-
-
-def arg_parser():
-    parser = argparse.ArgumentParser()
-    parser.add_argument(
-        "--model_dir",
-        "-m",
-        type=_text_type,
-        default=None,
-        help="PaddlePaddle model directory, if params stored in single file, you need define '--model_filename' and 'params_filename'."
-    )
-    parser.add_argument(
-        "--model_filename",
-        "-mf",
-        type=_text_type,
-        default=None,
-        help="PaddlePaddle model's network file name, which under directory seted by --model_dir"
-    )
-    parser.add_argument(
-        "--params_filename",
-        "-pf",
-        type=_text_type,
-        default=None,
-        help="PaddlePaddle model's param file name(param files combined in single file), which under directory seted by --model_dir."
-    )
-    parser.add_argument(
-        "--save_file",
-        "-s",
-        type=_text_type,
-        default=None,
-        help="file path to save onnx model")
-    parser.add_argument(
-        "--opset_version",
-        "-ov",
-        type=int,
-        default=9,
-        help="set onnx opset version to export")
-    parser.add_argument(
-       "--input_shape_dict",
-       "-isd",
-       type=_text_type,
-       default="None",
-       help="define input shapes, e.g --input_shape_dict=\"{'image':[1, 3, 608, 608]}\" or" \
-       "--input_shape_dict=\"{'image':[1, 3, 608, 608], 'im_shape': [1, 2], 'scale_factor': [1, 2]}\"")
-    parser.add_argument(
-        "--enable_dev_version",
-        type=ast.literal_eval,
-        default=True,
-        help="whether to use new version of Paddle2ONNX which is under developing, default True"
-    )
-    parser.add_argument(
-        "--deploy_backend",
-        "-d",
-        type=_text_type,
-        default="onnxruntime",
-        choices=["onnxruntime", "tensorrt", "rknn", "others"],
-        help="Quantize model deploy backend, default onnxruntime.")
-    parser.add_argument(
-        "--save_calibration_file",
-        type=_text_type,
-        default="calibration.cache",
-        help="The calibration cache for TensorRT deploy, default calibration.cache."
-    )
-    parser.add_argument(
-        "--enable_onnx_checker",
-        type=ast.literal_eval,
-        default=True,
-        help="whether check onnx model validity, default True")
-    parser.add_argument(
-        "--enable_paddle_fallback",
-        type=ast.literal_eval,
-        default=False,
-        help="whether use PaddleFallback for custom op, default is False")
-    parser.add_argument(
-        "--version",
-        "-v",
-        action="store_true",
-        default=False,
-        help="get version of paddle2onnx")
-    parser.add_argument(
-        "--output_names",
-        "-on",
-        type=str2list,
-        default=None,
-        help="define output names, e.g --output_names=\"[\"output1\"]\" or \
-       --output_names=\"[\"output1\", \"output2\", \"output3\"]\" or \
-       --output_names=\"{\"Paddleoutput\":\"Onnxoutput\"}\"")
-    parser.add_argument(
-        "--enable_auto_update_opset",
-        type=ast.literal_eval,
-        default=True,
-        help="whether enable auto_update_opset, default is True")
-    parser.add_argument(
-        "--external_filename",
-        type=_text_type,
-        default=None,
-        help="The filename of external_data when the model is bigger than 2G.")
-    parser.add_argument(
-        "--export_fp16_model",
-        type=ast.literal_eval,
-        default=False,
-        help="Whether export FP16 model for ORT-GPU, default False")
-    return parser
-
-
-def c_paddle_to_onnx(model_file,
-                     params_file="",
-                     save_file=None,
-                     opset_version=7,
-                     auto_upgrade_opset=True,
-                     verbose=True,
-                     enable_onnx_checker=True,
-                     enable_experimental_op=True,
-                     enable_optimize=True,
-                     deploy_backend="onnxruntime",
-                     calibration_file="",
-                     external_file="",
-                     export_fp16_model=False):
-    import paddle2onnx.paddle2onnx_cpp2py_export as c_p2o
-    onnx_model_str = c_p2o.export(
-        model_file, params_file, opset_version, auto_upgrade_opset, verbose,
-        enable_onnx_checker, enable_experimental_op, enable_optimize, {},
-        deploy_backend, calibration_file, external_file, export_fp16_model)
-    if save_file is not None:
-        with open(save_file, "wb") as f:
-            f.write(onnx_model_str)
-    else:
-        return onnx_model_str
-
-
-def program2onnx(model_dir,
-                 save_file,
-                 model_filename=None,
-                 params_filename=None,
-                 opset_version=9,
-                 enable_onnx_checker=False,
-                 operator_export_type="ONNX",
-                 input_shape_dict=None,
-                 output_names=None,
-                 auto_update_opset=True):
-    logging.warning(
-        "[Deprecated] `paddle2onnx.command.program2onnx` will be deprecated in the future version, the recommended usage is `paddle2onnx.export`"
-    )
-    from paddle2onnx.legacy.command import program2onnx
-    return program2onnx(model_dir, save_file, model_filename, params_filename,
-                        opset_version, enable_onnx_checker,
-                        operator_export_type, input_shape_dict, output_names,
-                        auto_update_opset)
-
-
-def main():
-    if len(sys.argv) < 2:
-        logging.info("Use \"paddle2onnx -h\" to print the help information")
-        logging.info(
-            "For more information, please follow our github repo below:")
-        logging.info("Github: https://github.com/PaddlePaddle/paddle2onnx.git")
-        return
-
-    parser = arg_parser()
-    args = parser.parse_args()
-
-    if args.version:
-        import paddle2onnx
-        logging.info("paddle2onnx-{} with python>=3.6, paddlepaddle>=2.0.0".
-                     format(paddle2onnx.__version__))
-        return
-
-    assert args.model_dir is not None, "--model_dir should be defined while translating paddle model to onnx"
-    assert args.save_file is not None, "--save_file should be defined while translating paddle model to onnx"
-
-    input_shape_dict = eval(args.input_shape_dict)
-
-    operator_export_type = "ONNX"
-    if args.enable_paddle_fallback:
-        logging.warning(
-            "[Deprecated] The flag `--enable_paddle_fallback` will be deprecated, and only works while `--enable_dev_version False` now."
-        )
-        operator_export_type = "PaddleFallback"
-
-    if args.output_names is not None and args.enable_dev_version:
-        logging.warning(
-            "[Deprecated] The flag `--output_names` is deprecated, if you need to modify the output name, please refer to this tool https://github.com/jiangjiajun/PaddleUtils/tree/main/onnx "
-        )
-        if not isinstance(args.output_names, (list, dict)):
-            raise TypeError(
-                "The output_names should be 'list' or 'dict', but received type is %s."
-                % type(args.output_names))
-
-    if input_shape_dict is not None and args.enable_dev_version:
-        logging.warning(
-            "[Deprecated] The flag `--input_shape_dict` is deprecated, if you need to modify the input shape of PaddlePaddle model, please refer to this tool https://github.com/jiangjiajun/PaddleUtils/tree/main/paddle "
-        )
-
-    if args.enable_dev_version:
-        model_file = os.path.join(args.model_dir, args.model_filename)
-        if args.params_filename is None:
-            params_file = ""
-        else:
-            params_file = os.path.join(args.model_dir, args.params_filename)
-
-        if args.external_filename is None:
-            args.external_filename = "external_data"
-
-        base_path = os.path.dirname(args.save_file)
-        if base_path and not os.path.exists(base_path):
-            os.mkdir(base_path)
-        external_file = os.path.join(base_path, args.external_filename)
-
-        calibration_file = args.save_calibration_file
-        c_paddle_to_onnx(
-            model_file=model_file,
-            params_file=params_file,
-            save_file=args.save_file,
-            opset_version=args.opset_version,
-            auto_upgrade_opset=args.enable_auto_update_opset,
-            verbose=True,
-            enable_onnx_checker=args.enable_onnx_checker,
-            enable_experimental_op=True,
-            enable_optimize=True,
-            deploy_backend=args.deploy_backend,
-            calibration_file=calibration_file,
-            external_file=external_file,
-            export_fp16_model=args.export_fp16_model)
-        logging.info("===============Make PaddlePaddle Better!================")
-        logging.info("A little survey: https://iwenjuan.baidu.com/?code=r8hu2s")
-        return
-
-    program2onnx(
-        args.model_dir,
-        args.save_file,
-        args.model_filename,
-        args.params_filename,
-        opset_version=args.opset_version,
-        enable_onnx_checker=args.enable_onnx_checker,
-        operator_export_type=operator_export_type,
-        input_shape_dict=input_shape_dict,
-        output_names=args.output_names,
-        auto_update_opset=args.enable_auto_update_opset)
-    logging.info("===============Make PaddlePaddle Better!================")
-    logging.info("A little survey: https://iwenjuan.baidu.com/?code=r8hu2s")
-
-
-if __name__ == "__main__":
-    main()
+# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from six import text_type as _text_type
+import argparse
+import ast
+import sys
+import os
+from paddle2onnx.utils import logging
+
+
+def str2list(v):
+    if len(v) == 0:
+        return None
+    v = v.replace(" ", "")
+    v = eval(v)
+    return v
+
+
+def arg_parser():
+    parser = argparse.ArgumentParser()
+    parser.add_argument(
+        "--model_dir",
+        "-m",
+        type=_text_type,
+        default=None,
+        help="PaddlePaddle model directory, if params stored in single file, you need define '--model_filename' and 'params_filename'."
+    )
+    parser.add_argument(
+        "--model_filename",
+        "-mf",
+        type=_text_type,
+        default=None,
+        help="PaddlePaddle model's network file name, which under directory seted by --model_dir"
+    )
+    parser.add_argument(
+        "--params_filename",
+        "-pf",
+        type=_text_type,
+        default=None,
+        help="PaddlePaddle model's param file name(param files combined in single file), which under directory seted by --model_dir."
+    )
+    parser.add_argument(
+        "--save_file",
+        "-s",
+        type=_text_type,
+        default=None,
+        help="file path to save onnx model")
+    parser.add_argument(
+        "--opset_version",
+        "-ov",
+        type=int,
+        default=9,
+        help="set onnx opset version to export")
+    parser.add_argument(
+       "--input_shape_dict",
+       "-isd",
+       type=_text_type,
+       default="None",
+       help="define input shapes, e.g --input_shape_dict=\"{'image':[1, 3, 608, 608]}\" or" \
+       "--input_shape_dict=\"{'image':[1, 3, 608, 608], 'im_shape': [1, 2], 'scale_factor': [1, 2]}\"")
+    parser.add_argument(
+        "--enable_dev_version",
+        type=ast.literal_eval,
+        default=True,
+        help="whether to use new version of Paddle2ONNX which is under developing, default True"
+    )
+    parser.add_argument(
+        "--deploy_backend",
+        "-d",
+        type=_text_type,
+        default="onnxruntime",
+        choices=["onnxruntime", "tensorrt", "rknn", "others"],
+        help="Quantize model deploy backend, default onnxruntime.")
+    parser.add_argument(
+        "--save_calibration_file",
+        type=_text_type,
+        default="calibration.cache",
+        help="The calibration cache for TensorRT deploy, default calibration.cache."
+    )
+    parser.add_argument(
+        "--enable_onnx_checker",
+        type=ast.literal_eval,
+        default=True,
+        help="whether check onnx model validity, default True")
+    parser.add_argument(
+        "--enable_paddle_fallback",
+        type=ast.literal_eval,
+        default=False,
+        help="whether use PaddleFallback for custom op, default is False")
+    parser.add_argument(
+        "--version",
+        "-v",
+        action="store_true",
+        default=False,
+        help="get version of paddle2onnx")
+    parser.add_argument(
+        "--output_names",
+        "-on",
+        type=str2list,
+        default=None,
+        help="define output names, e.g --output_names=\"[\"output1\"]\" or \
+       --output_names=\"[\"output1\", \"output2\", \"output3\"]\" or \
+       --output_names=\"{\"Paddleoutput\":\"Onnxoutput\"}\"")
+    parser.add_argument(
+        "--enable_auto_update_opset",
+        type=ast.literal_eval,
+        default=True,
+        help="whether enable auto_update_opset, default is True")
+    parser.add_argument(
+        "--external_filename",
+        type=_text_type,
+        default=None,
+        help="The filename of external_data when the model is bigger than 2G.")
+    parser.add_argument(
+        "--export_fp16_model",
+        type=ast.literal_eval,
+        default=False,
+        help="Whether export FP16 model for ORT-GPU, default False")
+    parser.add_argument(
+        "--custom_ops",
+        type=_text_type,
+        default="{}",
+        help="Ops that needs to be converted to custom op, e.g --custom_ops '{\"paddle_op\":\"onnx_op\"}', default {}"
+    )
+    return parser
+
+
+def c_paddle_to_onnx(model_file,
+                     params_file="",
+                     save_file=None,
+                     opset_version=7,
+                     auto_upgrade_opset=True,
+                     verbose=True,
+                     enable_onnx_checker=True,
+                     enable_experimental_op=True,
+                     enable_optimize=True,
+                     deploy_backend="onnxruntime",
+                     calibration_file="",
+                     external_file="",
+                     export_fp16_model=False,
+                     custom_ops={}):
+    import paddle2onnx.paddle2onnx_cpp2py_export as c_p2o
+    onnx_model_str = c_p2o.export(
+        model_file, params_file, opset_version, auto_upgrade_opset, verbose,
+        enable_onnx_checker, enable_experimental_op, enable_optimize,
+        custom_ops, deploy_backend, calibration_file, external_file,
+        export_fp16_model)
+    if save_file is not None:
+        with open(save_file, "wb") as f:
+            f.write(onnx_model_str)
+    else:
+        return onnx_model_str
+
+
+def program2onnx(model_dir,
+                 save_file,
+                 model_filename=None,
+                 params_filename=None,
+                 opset_version=9,
+                 enable_onnx_checker=False,
+                 operator_export_type="ONNX",
+                 input_shape_dict=None,
+                 output_names=None,
+                 auto_update_opset=True):
+    logging.warning(
+        "[Deprecated] `paddle2onnx.command.program2onnx` will be deprecated in the future version, the recommended usage is `paddle2onnx.export`"
+    )
+    from paddle2onnx.legacy.command import program2onnx
+    return program2onnx(model_dir, save_file, model_filename, params_filename,
+                        opset_version, enable_onnx_checker,
+                        operator_export_type, input_shape_dict, output_names,
+                        auto_update_opset)
+
+
+def main():
+    if len(sys.argv) < 2:
+        logging.info("Use \"paddle2onnx -h\" to print the help information")
+        logging.info(
+            "For more information, please follow our github repo below:")
+        logging.info("Github: https://github.com/PaddlePaddle/paddle2onnx.git")
+        return
+
+    parser = arg_parser()
+    args = parser.parse_args()
+
+    if args.version:
+        import paddle2onnx
+        logging.info("paddle2onnx-{} with python>=3.6, paddlepaddle>=2.0.0".
+                     format(paddle2onnx.__version__))
+        return
+
+    assert args.model_dir is not None, "--model_dir should be defined while translating paddle model to onnx"
+    assert args.save_file is not None, "--save_file should be defined while translating paddle model to onnx"
+
+    input_shape_dict = eval(args.input_shape_dict)
+
+    operator_export_type = "ONNX"
+    if args.enable_paddle_fallback:
+        logging.warning(
+            "[Deprecated] The flag `--enable_paddle_fallback` will be deprecated, and only works while `--enable_dev_version False` now."
+        )
+        operator_export_type = "PaddleFallback"
+
+    if args.output_names is not None and args.enable_dev_version:
+        logging.warning(
+            "[Deprecated] The flag `--output_names` is deprecated, if you need to modify the output name, please refer to this tool https://github.com/jiangjiajun/PaddleUtils/tree/main/onnx "
+        )
+        if not isinstance(args.output_names, (list, dict)):
+            raise TypeError(
+                "The output_names should be 'list' or 'dict', but received type is %s."
+                % type(args.output_names))
+
+    if input_shape_dict is not None and args.enable_dev_version:
+        logging.warning(
+            "[Deprecated] The flag `--input_shape_dict` is deprecated, if you need to modify the input shape of PaddlePaddle model, please refer to this tool https://github.com/jiangjiajun/PaddleUtils/tree/main/paddle "
+        )
+
+    if args.enable_dev_version:
+        model_file = os.path.join(args.model_dir, args.model_filename)
+        if args.params_filename is None:
+            params_file = ""
+        else:
+            params_file = os.path.join(args.model_dir, args.params_filename)
+
+        if args.external_filename is None:
+            args.external_filename = "external_data"
+
+        base_path = os.path.dirname(args.save_file)
+        if base_path and not os.path.exists(base_path):
+            os.mkdir(base_path)
+        external_file = os.path.join(base_path, args.external_filename)
+
+        custom_ops_dict = eval(args.custom_ops)
+
+        calibration_file = args.save_calibration_file
+        c_paddle_to_onnx(
+            model_file=model_file,
+            params_file=params_file,
+            save_file=args.save_file,
+            opset_version=args.opset_version,
+            auto_upgrade_opset=args.enable_auto_update_opset,
+            verbose=True,
+            enable_onnx_checker=args.enable_onnx_checker,
+            enable_experimental_op=True,
+            enable_optimize=True,
+            deploy_backend=args.deploy_backend,
+            calibration_file=calibration_file,
+            external_file=external_file,
+            export_fp16_model=args.export_fp16_model,
+            custom_ops=custom_ops_dict)
+        logging.info("===============Make PaddlePaddle Better!================")
+        logging.info("A little survey: https://iwenjuan.baidu.com/?code=r8hu2s")
+        return
+
+    program2onnx(
+        args.model_dir,
+        args.save_file,
+        args.model_filename,
+        args.params_filename,
+        opset_version=args.opset_version,
+        enable_onnx_checker=args.enable_onnx_checker,
+        operator_export_type=operator_export_type,
+        input_shape_dict=input_shape_dict,
+        output_names=args.output_names,
+        auto_update_opset=args.enable_auto_update_opset)
+    logging.info("===============Make PaddlePaddle Better!================")
+    logging.info("A little survey: https://iwenjuan.baidu.com/?code=r8hu2s")
+
+
+if __name__ == "__main__":
+    main()
```

## paddle2onnx/convert.py

 * *Ordering differences only*

```diff
@@ -1,83 +1,83 @@
-# Copyright (c) 2022  PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from paddle2onnx.utils import logging
-
-
-def export_onnx(paddle_graph,
-                save_file,
-                opset_version=9,
-                enable_onnx_checker=False,
-                operator_export_type="ONNX",
-                verbose=False,
-                auto_update_opset=True,
-                output_names=None):
-    from paddle2onnx.legacy.convert import export_onnx
-    return export_onnx(paddle_graph, save_file, opset_version, opset_version,
-                       enable_onnx_checker, operator_export_type, verbose,
-                       auto_update_opset, output_names)
-
-
-def dygraph2onnx(layer, save_file, input_spec=None, opset_version=9, **configs):
-    if "enable_dev_version" in configs and not configs["enable_dev_version"]:
-        from paddle2onnx.legacy.convert import dygraph2onnx
-        return dygraph2onnx(layer, save_file, input_spec, opset_version,
-                            **configs)
-
-    import os
-    import paddle2onnx
-    import paddle
-    dirname = os.path.split(save_file)[0]
-    paddle_model_dir = os.path.join(dirname,
-                                    "paddle_model_static_onnx_temp_dir")
-    model_file = os.path.join(paddle_model_dir, "model.pdmodel")
-    params_file = os.path.join(paddle_model_dir, "model.pdiparams")
-
-    if os.path.exists(paddle_model_dir):
-        if os.path.isfile(paddle_model_dir):
-            logging.info("File {} exists, will remove it.".format(
-                paddle_model_dir))
-            os.remove(paddle_model_dir)
-        if os.path.isfile(model_file):
-            os.remove(model_file)
-        if os.path.isfile(params_file):
-            os.remove(params_file)
-    paddle.jit.save(layer, os.path.join(paddle_model_dir, "model"), input_spec)
-    logging.info("Static PaddlePaddle model saved in {}.".format(
-        paddle_model_dir))
-    if not os.path.isfile(params_file):
-        params_file = ""
-
-    if save_file is None:
-        return paddle2onnx.export(model_file, params_file, save_file,
-                                  opset_version)
-    else:
-        paddle2onnx.export(model_file, params_file, save_file, opset_version)
-    logging.info("ONNX model saved in {}.".format(save_file))
-
-
-def program2onnx(program,
-                 scope,
-                 save_file,
-                 feed_var_names=None,
-                 target_vars=None,
-                 opset_version=9,
-                 enable_onnx_checker=False,
-                 operator_export_type="ONNX",
-                 auto_update_opset=True,
-                 **configs):
-    from paddle2onnx.legacy.convert import program2onnx
-    return program2onnx(program, scope, save_file, feed_var_names, target_vars,
-                        opset_version, enable_onnx_checker,
-                        operator_export_type, auto_update_opset, **configs)
+# Copyright (c) 2022  PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from paddle2onnx.utils import logging
+
+
+def export_onnx(paddle_graph,
+                save_file,
+                opset_version=9,
+                enable_onnx_checker=False,
+                operator_export_type="ONNX",
+                verbose=False,
+                auto_update_opset=True,
+                output_names=None):
+    from paddle2onnx.legacy.convert import export_onnx
+    return export_onnx(paddle_graph, save_file, opset_version, opset_version,
+                       enable_onnx_checker, operator_export_type, verbose,
+                       auto_update_opset, output_names)
+
+
+def dygraph2onnx(layer, save_file, input_spec=None, opset_version=9, **configs):
+    if "enable_dev_version" in configs and not configs["enable_dev_version"]:
+        from paddle2onnx.legacy.convert import dygraph2onnx
+        return dygraph2onnx(layer, save_file, input_spec, opset_version,
+                            **configs)
+
+    import os
+    import paddle2onnx
+    import paddle
+    dirname = os.path.split(save_file)[0]
+    paddle_model_dir = os.path.join(dirname,
+                                    "paddle_model_static_onnx_temp_dir")
+    model_file = os.path.join(paddle_model_dir, "model.pdmodel")
+    params_file = os.path.join(paddle_model_dir, "model.pdiparams")
+
+    if os.path.exists(paddle_model_dir):
+        if os.path.isfile(paddle_model_dir):
+            logging.info("File {} exists, will remove it.".format(
+                paddle_model_dir))
+            os.remove(paddle_model_dir)
+        if os.path.isfile(model_file):
+            os.remove(model_file)
+        if os.path.isfile(params_file):
+            os.remove(params_file)
+    paddle.jit.save(layer, os.path.join(paddle_model_dir, "model"), input_spec)
+    logging.info("Static PaddlePaddle model saved in {}.".format(
+        paddle_model_dir))
+    if not os.path.isfile(params_file):
+        params_file = ""
+
+    if save_file is None:
+        return paddle2onnx.export(model_file, params_file, save_file,
+                                  opset_version)
+    else:
+        paddle2onnx.export(model_file, params_file, save_file, opset_version)
+    logging.info("ONNX model saved in {}.".format(save_file))
+
+
+def program2onnx(program,
+                 scope,
+                 save_file,
+                 feed_var_names=None,
+                 target_vars=None,
+                 opset_version=9,
+                 enable_onnx_checker=False,
+                 operator_export_type="ONNX",
+                 auto_update_opset=True,
+                 **configs):
+    from paddle2onnx.legacy.convert import program2onnx
+    return program2onnx(program, scope, save_file, feed_var_names, target_vars,
+                        opset_version, enable_onnx_checker,
+                        operator_export_type, auto_update_opset, **configs)
```

## paddle2onnx/convert_to_fp16.py

 * *Ordering differences only*

```diff
@@ -1,38 +1,38 @@
-# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-from __future__ import absolute_import
-
-import argparse
-import sys
-from paddle2onnx.utils import logging
-
-
-def parse_arguments():
-    parser = argparse.ArgumentParser()
-    parser.add_argument(
-        '--input_model_path',
-        required=True,
-        help='The path of input onnx model file.')
-    parser.add_argument(
-        '--output_model_path',
-        required=True,
-        help='The file path to write optimized onnx model file.')
-    return parser.parse_args()
-
-
-if __name__ == '__main__':
-    args = parse_arguments()
-    import paddle2onnx.paddle2onnx_cpp2py_export as c_p2o
-    c_p2o.convert_to_fp16(args.input_model_path, args.output_model_path)
-    logging.info("FP16 model saved in {}.".format(args.output_model_path))
+# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from __future__ import absolute_import
+
+import argparse
+import sys
+from paddle2onnx.utils import logging
+
+
+def parse_arguments():
+    parser = argparse.ArgumentParser()
+    parser.add_argument(
+        '--input_model_path',
+        required=True,
+        help='The path of input onnx model file.')
+    parser.add_argument(
+        '--output_model_path',
+        required=True,
+        help='The file path to write optimized onnx model file.')
+    return parser.parse_args()
+
+
+if __name__ == '__main__':
+    args = parse_arguments()
+    import paddle2onnx.paddle2onnx_cpp2py_export as c_p2o
+    c_p2o.convert_to_fp16(args.input_model_path, args.output_model_path)
+    logging.info("FP16 model saved in {}.".format(args.output_model_path))
```

## paddle2onnx/optimize.py

 * *Ordering differences only*

```diff
@@ -1,46 +1,46 @@
-# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-from __future__ import absolute_import
-
-import argparse
-import sys
-from paddle2onnx.utils import logging
-
-
-def parse_arguments():
-    parser = argparse.ArgumentParser()
-    parser.add_argument(
-        '--input_model',
-        required=True,
-        help='The path of input onnx model file.')
-    parser.add_argument(
-        '--output_model',
-        required=True,
-        help='The file path to write optimized onnx model file.')
-    parser.add_argument(
-        '--input_shape_dict',
-        default="",
-        help="The shape infos of inputs, e.g --input_shape_dict=\"{'image': [1, 3, 608, 608], 'scale_factor': [1, 2]}\""
-    )
-    return parser.parse_args()
-
-
-if __name__ == '__main__':
-    args = parse_arguments()
-    import paddle2onnx.paddle2onnx_cpp2py_export as c_p2o
-    shape_dict = {}
-    if args.input_shape_dict != "":
-        shape_dict = eval(args.input_shape_dict)
-    c_p2o.optimize(args.input_model, args.output_model, shape_dict)
-    logging.info("Model optmized, saved in {}.".format(args.output_model))
+# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from __future__ import absolute_import
+
+import argparse
+import sys
+from paddle2onnx.utils import logging
+
+
+def parse_arguments():
+    parser = argparse.ArgumentParser()
+    parser.add_argument(
+        '--input_model',
+        required=True,
+        help='The path of input onnx model file.')
+    parser.add_argument(
+        '--output_model',
+        required=True,
+        help='The file path to write optimized onnx model file.')
+    parser.add_argument(
+        '--input_shape_dict',
+        default="",
+        help="The shape infos of inputs, e.g --input_shape_dict=\"{'image': [1, 3, 608, 608], 'scale_factor': [1, 2]}\""
+    )
+    return parser.parse_args()
+
+
+if __name__ == '__main__':
+    args = parse_arguments()
+    import paddle2onnx.paddle2onnx_cpp2py_export as c_p2o
+    shape_dict = {}
+    if args.input_shape_dict != "":
+        shape_dict = eval(args.input_shape_dict)
+    c_p2o.optimize(args.input_model, args.output_model, shape_dict)
+    logging.info("Model optmized, saved in {}.".format(args.output_model))
```

## paddle2onnx/utils.py

 * *Ordering differences only*

```diff
@@ -1,131 +1,131 @@
-#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import importlib
-import collections
-import time
-import os
-import sys
-
-
-def try_import(module_name):
-    """Try importing a module, with an informative error message on failure."""
-    install_name = module_name
-    try:
-        mod = importlib.import_module(module_name)
-        return mod
-    except ImportError:
-        err_msg = (
-            "Failed importing {}. This likely means that some modules "
-            "requires additional dependencies that have to be "
-            "manually installed (usually with `pip install {}`). ").format(
-                module_name, install_name)
-        raise ImportError(err_msg)
-
-
-def check_model(onnx_model):
-    onnx = try_import('onnx')
-    try:
-        onnx.checker.check_model(onnx_model)
-    except Exception:
-        raise Exception('ONNX model is not valid.')
-    finally:
-        logging.info('ONNX model generated is valid.')
-
-
-levels = {0: 'ERROR', 1: 'WARNING', 2: 'INFO', 3: 'DEBUG'}
-
-
-class logging():
-    log_level = 2
-
-    @staticmethod
-    def log(level=2, message="", use_color=False):
-        current_time = time.time()
-        time_array = time.localtime(current_time)
-        current_time = time.strftime("%Y-%m-%d %H:%M:%S", time_array)
-        if logging.log_level >= level:
-            if use_color:
-                print("\033[1;31;40m{} [{}]\t{}\033[0m".format(
-                    current_time, levels[level], message).encode("utf-8")
-                      .decode("latin1"))
-            else:
-                print("{} [{}]\t{}".format(current_time, levels[level], message)
-                      .encode("utf-8").decode("latin1"))
-            sys.stdout.flush()
-
-    @staticmethod
-    def debug(message="", use_color=False):
-        logging.log(level=3, message=message, use_color=use_color)
-
-    @staticmethod
-    def info(message="", use_color=False):
-        logging.log(level=2, message=message, use_color=use_color)
-
-    @staticmethod
-    def warning(message="", use_color=True):
-        logging.log(level=1, message=message, use_color=use_color)
-
-    @staticmethod
-    def error(message="", use_color=True, exit=True):
-        logging.log(level=0, message=message, use_color=use_color)
-        if exit:
-            sys.exit(-1)
-
-
-def compare_value(a, b, cond):
-    if cond == 'equal':
-        if a != b:
-            return False
-        return True
-    if cond == 'greater_than':
-        if a <= b:
-            return False
-        return True
-    if cond == 'greater_equal':
-        if a < b:
-            return False
-        return True
-    if cond == 'less_equal':
-        if a > b:
-            return False
-        return True
-    if cond == 'less_than':
-        if a >= b:
-            return False
-        return True
-
-
-def compare_attr(actual_value, target_value, attr_name, cond='equal'):
-    if not compare_value(actual_value, target_value, cond):
-        raise ValueError('Support {} {} {}, actually got {}=={}.'.format(
-            attr_name, cond, target_value, attr_name, actual_value))
-
-
-def compare_attr_between_dims(attr, dims, attr_name, cond='equal'):
-    if not compare_value(attr[dims[0]], attr[dims[1]], cond):
-        expect_info = 'Support {}[{}] {} {}[{}], '.format(
-            attr_name, dims[0], cond, attr_name, dims[1])
-        actual_info = 'actually got {}[{}]=={}, not {} {}[{}]=={}.'.format(
-            attr_name, dims[0], attr[dims[0]], cond, attr_name, dims[1],
-            attr[dims[1]])
-        raise ValueError(expect_info + actual_info)
-
-
-def require_fixed_shape(op_name=None):
-    logging.error(
-        "[{}]Fixed shape is required, refer this doc for more information: https://github.com/PaddlePaddle/Paddle2ONNX/blob/develop/docs/zh/FAQ.md".
-        format(op_name))
+#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import importlib
+import collections
+import time
+import os
+import sys
+
+
+def try_import(module_name):
+    """Try importing a module, with an informative error message on failure."""
+    install_name = module_name
+    try:
+        mod = importlib.import_module(module_name)
+        return mod
+    except ImportError:
+        err_msg = (
+            "Failed importing {}. This likely means that some modules "
+            "requires additional dependencies that have to be "
+            "manually installed (usually with `pip install {}`). ").format(
+                module_name, install_name)
+        raise ImportError(err_msg)
+
+
+def check_model(onnx_model):
+    onnx = try_import('onnx')
+    try:
+        onnx.checker.check_model(onnx_model)
+    except Exception:
+        raise Exception('ONNX model is not valid.')
+    finally:
+        logging.info('ONNX model generated is valid.')
+
+
+levels = {0: 'ERROR', 1: 'WARNING', 2: 'INFO', 3: 'DEBUG'}
+
+
+class logging():
+    log_level = 2
+
+    @staticmethod
+    def log(level=2, message="", use_color=False):
+        current_time = time.time()
+        time_array = time.localtime(current_time)
+        current_time = time.strftime("%Y-%m-%d %H:%M:%S", time_array)
+        if logging.log_level >= level:
+            if use_color:
+                print("\033[1;31;40m{} [{}]\t{}\033[0m".format(
+                    current_time, levels[level], message).encode("utf-8")
+                      .decode("latin1"))
+            else:
+                print("{} [{}]\t{}".format(current_time, levels[level], message)
+                      .encode("utf-8").decode("latin1"))
+            sys.stdout.flush()
+
+    @staticmethod
+    def debug(message="", use_color=False):
+        logging.log(level=3, message=message, use_color=use_color)
+
+    @staticmethod
+    def info(message="", use_color=False):
+        logging.log(level=2, message=message, use_color=use_color)
+
+    @staticmethod
+    def warning(message="", use_color=True):
+        logging.log(level=1, message=message, use_color=use_color)
+
+    @staticmethod
+    def error(message="", use_color=True, exit=True):
+        logging.log(level=0, message=message, use_color=use_color)
+        if exit:
+            sys.exit(-1)
+
+
+def compare_value(a, b, cond):
+    if cond == 'equal':
+        if a != b:
+            return False
+        return True
+    if cond == 'greater_than':
+        if a <= b:
+            return False
+        return True
+    if cond == 'greater_equal':
+        if a < b:
+            return False
+        return True
+    if cond == 'less_equal':
+        if a > b:
+            return False
+        return True
+    if cond == 'less_than':
+        if a >= b:
+            return False
+        return True
+
+
+def compare_attr(actual_value, target_value, attr_name, cond='equal'):
+    if not compare_value(actual_value, target_value, cond):
+        raise ValueError('Support {} {} {}, actually got {}=={}.'.format(
+            attr_name, cond, target_value, attr_name, actual_value))
+
+
+def compare_attr_between_dims(attr, dims, attr_name, cond='equal'):
+    if not compare_value(attr[dims[0]], attr[dims[1]], cond):
+        expect_info = 'Support {}[{}] {} {}[{}], '.format(
+            attr_name, dims[0], cond, attr_name, dims[1])
+        actual_info = 'actually got {}[{}]=={}, not {} {}[{}]=={}.'.format(
+            attr_name, dims[0], attr[dims[0]], cond, attr_name, dims[1],
+            attr[dims[1]])
+        raise ValueError(expect_info + actual_info)
+
+
+def require_fixed_shape(op_name=None):
+    logging.error(
+        "[{}]Fixed shape is required, refer this doc for more information: https://github.com/PaddlePaddle/Paddle2ONNX/blob/develop/docs/zh/FAQ.md".
+        format(op_name))
```

## paddle2onnx/version.py

```diff
@@ -1,7 +1,7 @@
-# This file is generated by setup.py. DO NOT EDIT!
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-from __future__ import unicode_literals
-version = '1.0.6'
-git_version = 'ed9ed99b835e3e629a135a0179803ad47be96f65'
+# This file is generated by setup.py. DO NOT EDIT!
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+from __future__ import unicode_literals
+version = '1.0.8rc'
+git_version = '354a04058ad9e93e4caf78a994608c15b048c7ec'
```

## paddle2onnx/legacy/__init__.py

 * *Ordering differences only*

```diff
@@ -1,128 +1,128 @@
-#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-from __future__ import absolute_import
-
-__version__ = "0.9.6"
-
-import paddle
-from .convert import dygraph2onnx, program2onnx
-from .op_mapper import register_op_mapper
-from typing import TypeVar
-from paddle2onnx.utils import logging
-from paddle2onnx.legacy.op_mapper import OpMapper
-from . import command
-
-OP_WITHOUT_KERNEL_SET = {
-    'feed', 'fetch', 'recurrent', 'go', 'rnn_memory_helper_grad',
-    'conditional_block', 'while', 'send', 'recv', 'listen_and_serv',
-    'fl_listen_and_serv', 'ncclInit', 'select', 'checkpoint_notify',
-    'gen_bkcl_id', 'c_gen_bkcl_id', 'gen_nccl_id', 'c_gen_nccl_id',
-    'c_comm_init', 'c_sync_calc_stream', 'c_sync_comm_stream',
-    'queue_generator', 'dequeue', 'enqueue', 'heter_listen_and_serv',
-    'c_wait_comm', 'c_wait_compute', 'c_gen_hccl_id', 'c_comm_init_hccl',
-    'copy_cross_scope'
-}
-
-
-def process_old_ops_desc(model):
-    for i in range(len(model.blocks[0].ops)):
-        if model.blocks[0].ops[i].type == "matmul":
-            if not model.blocks[0].ops[i].has_attr("head_number"):
-                model.blocks[0].ops[i]._set_attr("head_number", 1)
-        elif model.blocks[0].ops[i].type == "yolo_box":
-            if not model.blocks[0].ops[i].has_attr("iou_aware"):
-                model.blocks[0].ops[i]._set_attr("iou_aware", False)
-            if not model.blocks[0].ops[i].has_attr("iou_aware_factor"):
-                model.blocks[0].ops[i]._set_attr("iou_aware_factor", 0.5)
-
-
-def get_all_registered_ops(save_file=None):
-    ops = list(OpMapper.OPSETS.keys())
-    logging.warning("The number of all registered OPs is: {}".format(len(ops)))
-    if save_file is None:
-        return
-    with open(save_file, "w") as f:
-        logging.warning("All registered OPs will be written to the file: {}".
-                        format(save_file))
-        f.write("Total OPs num: {} \n".format(len(ops)))
-        for index in range(len(ops)):
-            op = ops[index]
-            f.write(str(index + 1) + ". " + op + "\n")
-        return
-
-
-def run_convert(model, input_shape_dict=None, scope=None, opset_version=9):
-    paddle_version = paddle.__version__
-    if isinstance(model, paddle.static.Program):
-        process_old_ops_desc(model)
-        if input_shape_dict is not None:
-            model_version = model.desc._version()
-            major_ver = model_version // 1000000
-            minor_ver = (model_version - major_ver * 1000000) // 1000
-            patch_ver = model_version - major_ver * 1000000 - minor_ver * 1000
-            model_version = "{}.{}.{}".format(major_ver, minor_ver, patch_ver)
-            if model_version != paddle_version:
-                logging.warning(
-                    "The model is saved by paddlepaddle v{}, but now your paddlepaddle is version of {}, this difference may cause error, it is recommend you reinstall a same version of paddlepaddle for this model".
-                    format(model_version, paddle_version))
-            for k, v in input_shape_dict.items():
-                model.blocks[0].var(k).desc.set_shape(v)
-            for i in range(len(model.blocks[0].ops)):
-                if model.blocks[0].ops[i].type in OP_WITHOUT_KERNEL_SET:
-                    continue
-                model.blocks[0].ops[i].desc.infer_shape(model.blocks[0].desc)
-        if scope is None:
-            scope = paddle.static.global_scope()
-        input_names = list()
-        output_vars = list()
-        for i in range(len(model.blocks[0].ops)):
-            if model.blocks[0].ops[i].type == "feed":
-                input_names.append(model.blocks[0].ops[i].output("Out")[0])
-            if model.blocks[0].ops[i].type == "fetch":
-                output_vars.append(model.blocks[0].var(model.blocks[0].ops[i]
-                                                       .input("X")[0]))
-        return program2onnx(
-            model,
-            scope,
-            save_file=None,
-            feed_var_names=input_names,
-            target_vars=output_vars,
-            opset_version=opset_version,
-            enable_onnx_checker=True)
-    elif isinstance(model, paddle.jit.TranslatedLayer):
-        process_old_ops_desc(model.program())
-        model_version = model.program().desc._version()
-        major_ver = model_version // 1000000
-        minor_ver = (model_version - major_ver * 1000000) // 1000
-        patch_ver = model_version - major_ver * 1000000 - minor_ver * 1000
-        model_version = "{}.{}.{}".format(major_ver, minor_ver, patch_ver)
-        if model_version != paddle_version:
-            logging.warning(
-                "The model is saved by paddlepaddle v{}, but now your paddlepaddle is version of {}, this difference may cause error, it is recommend you reinstall a same version of paddlepaddle for this model".
-                format(model_version, paddle_version))
-
-        if input_shape_dict is not None:
-            for k, v in input_shape_dict.items():
-                model.program().blocks[0].var(k).desc.set_shape(v)
-            for i in range(len(model.program().blocks[0].ops)):
-                if model.program().blocks[0].ops[
-                        i].type in OP_WITHOUT_KERNEL_SET:
-                    continue
-                model.program().blocks[0].ops[i].desc.infer_shape(model.program(
-                ).blocks[0].desc)
-        return dygraph2onnx(model, save_file=None, opset_version=opset_version)
-    else:
-        raise Exception(
-            "Only support model loaded from paddle.static.load_inference_model() or paddle.jit.load()"
-        )
+#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from __future__ import absolute_import
+
+__version__ = "0.9.6"
+
+import paddle
+from .convert import dygraph2onnx, program2onnx
+from .op_mapper import register_op_mapper
+from typing import TypeVar
+from paddle2onnx.utils import logging
+from paddle2onnx.legacy.op_mapper import OpMapper
+from . import command
+
+OP_WITHOUT_KERNEL_SET = {
+    'feed', 'fetch', 'recurrent', 'go', 'rnn_memory_helper_grad',
+    'conditional_block', 'while', 'send', 'recv', 'listen_and_serv',
+    'fl_listen_and_serv', 'ncclInit', 'select', 'checkpoint_notify',
+    'gen_bkcl_id', 'c_gen_bkcl_id', 'gen_nccl_id', 'c_gen_nccl_id',
+    'c_comm_init', 'c_sync_calc_stream', 'c_sync_comm_stream',
+    'queue_generator', 'dequeue', 'enqueue', 'heter_listen_and_serv',
+    'c_wait_comm', 'c_wait_compute', 'c_gen_hccl_id', 'c_comm_init_hccl',
+    'copy_cross_scope'
+}
+
+
+def process_old_ops_desc(model):
+    for i in range(len(model.blocks[0].ops)):
+        if model.blocks[0].ops[i].type == "matmul":
+            if not model.blocks[0].ops[i].has_attr("head_number"):
+                model.blocks[0].ops[i]._set_attr("head_number", 1)
+        elif model.blocks[0].ops[i].type == "yolo_box":
+            if not model.blocks[0].ops[i].has_attr("iou_aware"):
+                model.blocks[0].ops[i]._set_attr("iou_aware", False)
+            if not model.blocks[0].ops[i].has_attr("iou_aware_factor"):
+                model.blocks[0].ops[i]._set_attr("iou_aware_factor", 0.5)
+
+
+def get_all_registered_ops(save_file=None):
+    ops = list(OpMapper.OPSETS.keys())
+    logging.warning("The number of all registered OPs is: {}".format(len(ops)))
+    if save_file is None:
+        return
+    with open(save_file, "w") as f:
+        logging.warning("All registered OPs will be written to the file: {}".
+                        format(save_file))
+        f.write("Total OPs num: {} \n".format(len(ops)))
+        for index in range(len(ops)):
+            op = ops[index]
+            f.write(str(index + 1) + ". " + op + "\n")
+        return
+
+
+def run_convert(model, input_shape_dict=None, scope=None, opset_version=9):
+    paddle_version = paddle.__version__
+    if isinstance(model, paddle.static.Program):
+        process_old_ops_desc(model)
+        if input_shape_dict is not None:
+            model_version = model.desc._version()
+            major_ver = model_version // 1000000
+            minor_ver = (model_version - major_ver * 1000000) // 1000
+            patch_ver = model_version - major_ver * 1000000 - minor_ver * 1000
+            model_version = "{}.{}.{}".format(major_ver, minor_ver, patch_ver)
+            if model_version != paddle_version:
+                logging.warning(
+                    "The model is saved by paddlepaddle v{}, but now your paddlepaddle is version of {}, this difference may cause error, it is recommend you reinstall a same version of paddlepaddle for this model".
+                    format(model_version, paddle_version))
+            for k, v in input_shape_dict.items():
+                model.blocks[0].var(k).desc.set_shape(v)
+            for i in range(len(model.blocks[0].ops)):
+                if model.blocks[0].ops[i].type in OP_WITHOUT_KERNEL_SET:
+                    continue
+                model.blocks[0].ops[i].desc.infer_shape(model.blocks[0].desc)
+        if scope is None:
+            scope = paddle.static.global_scope()
+        input_names = list()
+        output_vars = list()
+        for i in range(len(model.blocks[0].ops)):
+            if model.blocks[0].ops[i].type == "feed":
+                input_names.append(model.blocks[0].ops[i].output("Out")[0])
+            if model.blocks[0].ops[i].type == "fetch":
+                output_vars.append(model.blocks[0].var(model.blocks[0].ops[i]
+                                                       .input("X")[0]))
+        return program2onnx(
+            model,
+            scope,
+            save_file=None,
+            feed_var_names=input_names,
+            target_vars=output_vars,
+            opset_version=opset_version,
+            enable_onnx_checker=True)
+    elif isinstance(model, paddle.jit.TranslatedLayer):
+        process_old_ops_desc(model.program())
+        model_version = model.program().desc._version()
+        major_ver = model_version // 1000000
+        minor_ver = (model_version - major_ver * 1000000) // 1000
+        patch_ver = model_version - major_ver * 1000000 - minor_ver * 1000
+        model_version = "{}.{}.{}".format(major_ver, minor_ver, patch_ver)
+        if model_version != paddle_version:
+            logging.warning(
+                "The model is saved by paddlepaddle v{}, but now your paddlepaddle is version of {}, this difference may cause error, it is recommend you reinstall a same version of paddlepaddle for this model".
+                format(model_version, paddle_version))
+
+        if input_shape_dict is not None:
+            for k, v in input_shape_dict.items():
+                model.program().blocks[0].var(k).desc.set_shape(v)
+            for i in range(len(model.program().blocks[0].ops)):
+                if model.program().blocks[0].ops[
+                        i].type in OP_WITHOUT_KERNEL_SET:
+                    continue
+                model.program().blocks[0].ops[i].desc.infer_shape(model.program(
+                ).blocks[0].desc)
+        return dygraph2onnx(model, save_file=None, opset_version=opset_version)
+    else:
+        raise Exception(
+            "Only support model loaded from paddle.static.load_inference_model() or paddle.jit.load()"
+        )
```

## paddle2onnx/legacy/command.py

 * *Ordering differences only*

```diff
@@ -1,287 +1,287 @@
-# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-from six import text_type as _text_type
-import argparse
-import ast
-import sys
-import os
-import paddle.fluid as fluid
-from paddle2onnx.utils import logging
-
-
-def str2list(v):
-    if len(v) == 0:
-        return None
-    v = v.replace(" ", "")
-    v = eval(v)
-    return v
-
-
-def arg_parser():
-    parser = argparse.ArgumentParser()
-    parser.add_argument(
-        "--model_dir",
-        "-m",
-        type=_text_type,
-        default=None,
-        help="PaddlePaddle model directory, if params stored in single file, you need define '--model_filename' and 'params_filename'."
-    )
-    parser.add_argument(
-        "--model_filename",
-        "-mf",
-        type=_text_type,
-        default=None,
-        help="PaddlePaddle model's network file name, which under directory seted by --model_dir"
-    )
-    parser.add_argument(
-        "--params_filename",
-        "-pf",
-        type=_text_type,
-        default=None,
-        help="PaddlePaddle model's param file name(param files combined in single file), which under directory seted by --model_dir."
-    )
-    parser.add_argument(
-        "--save_file",
-        "-s",
-        type=_text_type,
-        default=None,
-        help="file path to save onnx model")
-    parser.add_argument(
-        "--opset_version",
-        "-ov",
-        type=int,
-        default=9,
-        help="set onnx opset version to export")
-    parser.add_argument(
-       "--input_shape_dict",
-       "-isd",
-       type=_text_type,
-       default="None",
-       help="define input shapes, e.g --input_shape_dict=\"{'image':[1, 3, 608, 608]}\" or" \
-       "--input_shape_dict=\"{'image':[1, 3, 608, 608], 'im_shape': [1, 2], 'scale_factor': [1, 2]}\"")
-    parser.add_argument(
-        "--enable_dev_version",
-        type=ast.literal_eval,
-        default=False,
-        help="whether to use new version of Paddle2ONNX which is under developing, default False"
-    )
-    parser.add_argument(
-        "--enable_onnx_checker",
-        type=ast.literal_eval,
-        default=True,
-        help="whether check onnx model validity, default True")
-    parser.add_argument(
-        "--enable_paddle_fallback",
-        type=ast.literal_eval,
-        default=False,
-        help="whether use PaddleFallback for custom op, default is False")
-    parser.add_argument(
-        "--version",
-        "-v",
-        action="store_true",
-        default=False,
-        help="get version of paddle2onnx")
-    parser.add_argument(
-        "--output_names",
-        "-on",
-        type=str2list,
-        default=None,
-        help="define output names, e.g --output_names=\"[\"output1\"]\" or \
-       --output_names=\"[\"output1\", \"output2\", \"output3\"]\" or \
-       --output_names=\"{\"Paddleoutput\":\"Onnxoutput\"}\"")
-    parser.add_argument(
-        "--enable_auto_update_opset",
-        type=ast.literal_eval,
-        default=True,
-        help="whether enable auto_update_opset, default is True")
-    return parser
-
-
-def c_paddle_to_onnx(model_file,
-                     params_file="",
-                     save_file=None,
-                     opset_version=7,
-                     auto_upgrade_opset=True,
-                     verbose=True,
-                     enable_onnx_checker=True,
-                     enable_experimental_op=True,
-                     enable_optimize=True):
-    import paddle2onnx.paddle2onnx_cpp2py_export as c_p2o
-    onnx_model_str = c_p2o.export(
-        model_file, params_file, opset_version, auto_upgrade_opset, verbose,
-        enable_onnx_checker, enable_experimental_op, enable_optimize)
-    if save_file is not None:
-        with open(save_file, "wb") as f:
-            f.write(onnx_model_str)
-    else:
-        return onnx_model_str
-
-
-def program2onnx(model_dir,
-                 save_file,
-                 model_filename=None,
-                 params_filename=None,
-                 opset_version=9,
-                 enable_onnx_checker=False,
-                 operator_export_type="ONNX",
-                 input_shape_dict=None,
-                 output_names=None,
-                 auto_update_opset=True):
-    try:
-        import paddle
-    except:
-        logging.error(
-            "paddlepaddle not installed, use \"pip install paddlepaddle\"")
-
-    v0, v1, v2 = paddle.__version__.split('.')
-    if v0 == '0' and v1 == '0' and v2 == '0':
-        logging.warning("You are use develop version of paddlepaddle")
-    elif int(v0) <= 1 and int(v1) < 8:
-        raise ImportError("paddlepaddle>=1.8.0 is required")
-
-    import paddle2onnx as p2o
-    # convert model save with 'paddle.fluid.io.save_inference_model'
-    if hasattr(paddle, 'enable_static'):
-        paddle.enable_static()
-    exe = fluid.Executor(fluid.CPUPlace())
-    if model_filename is None and params_filename is None:
-        [program, feed_var_names, fetch_vars] = fluid.io.load_inference_model(
-            model_dir, exe)
-    else:
-        [program, feed_var_names, fetch_vars] = fluid.io.load_inference_model(
-            model_dir,
-            exe,
-            model_filename=model_filename,
-            params_filename=params_filename)
-
-    OP_WITHOUT_KERNEL_SET = {
-        'feed', 'fetch', 'recurrent', 'go', 'rnn_memory_helper_grad',
-        'conditional_block', 'while', 'send', 'recv', 'listen_and_serv',
-        'fl_listen_and_serv', 'ncclInit', 'select', 'checkpoint_notify',
-        'gen_bkcl_id', 'c_gen_bkcl_id', 'gen_nccl_id', 'c_gen_nccl_id',
-        'c_comm_init', 'c_sync_calc_stream', 'c_sync_comm_stream',
-        'queue_generator', 'dequeue', 'enqueue', 'heter_listen_and_serv',
-        'c_wait_comm', 'c_wait_compute', 'c_gen_hccl_id', 'c_comm_init_hccl',
-        'copy_cross_scope'
-    }
-    if input_shape_dict is not None:
-        import paddle2onnx
-        paddle2onnx.legacy.process_old_ops_desc(program)
-        paddle_version = paddle.__version__
-        model_version = program.desc._version()
-        major_ver = model_version // 1000000
-        minor_ver = (model_version - major_ver * 1000000) // 1000
-        patch_ver = model_version - major_ver * 1000000 - minor_ver * 1000
-        model_version = "{}.{}.{}".format(major_ver, minor_ver, patch_ver)
-        if model_version != paddle_version:
-            logging.warning(
-                "The model is saved by paddlepaddle v{}, but now your paddlepaddle is version of {}, this difference may cause error, it is recommend you reinstall a same version of paddlepaddle for this model".
-                format(model_version, paddle_version))
-
-        for k, v in input_shape_dict.items():
-            program.blocks[0].var(k).desc.set_shape(v)
-        for i in range(len(program.blocks[0].ops)):
-            if program.blocks[0].ops[i].type in OP_WITHOUT_KERNEL_SET:
-                continue
-            program.blocks[0].ops[i].desc.infer_shape(program.blocks[0].desc)
-    p2o.program2onnx(
-        program,
-        fluid.global_scope(),
-        save_file,
-        feed_var_names=feed_var_names,
-        target_vars=fetch_vars,
-        opset_version=opset_version,
-        enable_onnx_checker=enable_onnx_checker,
-        operator_export_type=operator_export_type,
-        auto_update_opset=auto_update_opset,
-        output_names=output_names)
-
-
-def main():
-    if len(sys.argv) < 2:
-        logging.info("Use \"paddle2onnx -h\" to print the help information")
-        logging.info(
-            "For more information, please follow our github repo below:")
-        logging.info("Github: https://github.com/PaddlePaddle/paddle2onnx.git")
-        return
-
-    parser = arg_parser()
-    args = parser.parse_args()
-
-    if args.version:
-        import paddle2onnx
-        logging.info("paddle2onnx-{} with python>=2.7, paddlepaddle>=1.8.0".
-                     format(paddle2onnx.__version__))
-        return
-
-    assert args.model_dir is not None, "--model_dir should be defined while translating paddle model to onnx"
-    assert args.save_file is not None, "--save_file should be defined while translating paddle model to onnx"
-
-    input_shape_dict = eval(args.input_shape_dict)
-
-    operator_export_type = "ONNX"
-    if args.enable_paddle_fallback:
-        operator_export_type = "PaddleFallback"
-
-    if args.output_names is not None:
-        if not isinstance(args.output_names, (list, dict)):
-            raise TypeError(
-                "The output_names should be 'list' or 'dict', but received type is %s."
-                % type(args.output_names))
-
-    if args.enable_dev_version:
-        if args.enable_paddle_fallback:
-            logging.warn(
-                "--enable_paddle_fallback is deprecated while --enable_dev_version=True."
-            )
-        if args.output_names is not None:
-            logging.warn(
-                "--output_names is deprecated while --enable_dev_version=True.")
-        if input_shape_dict is not None:
-            logging.warn(
-                "--input_shape_dict is deprecated while --enable_dev_version=True."
-            )
-        model_file = os.path.join(args.model_dir, args.model_filename)
-        if args.params_filename is None:
-            params_file = ""
-        else:
-            params_file = os.path.join(args.model_dir, args.params_filename)
-        return c_paddle_to_onnx(
-            model_file=model_file,
-            params_file=params_file,
-            save_file=args.save_file,
-            opset_version=args.opset_version,
-            auto_upgrade_opset=args.enable_auto_update_opset,
-            verbose=True,
-            enable_onnx_checker=args.enable_onnx_checker,
-            enable_experimental_op=True,
-            enable_optimize=True)
-
-    program2onnx(
-        args.model_dir,
-        args.save_file,
-        args.model_filename,
-        args.params_filename,
-        opset_version=args.opset_version,
-        enable_onnx_checker=args.enable_onnx_checker,
-        operator_export_type=operator_export_type,
-        input_shape_dict=input_shape_dict,
-        output_names=args.output_names,
-        auto_update_opset=args.enable_auto_update_opset)
-
-
-if __name__ == "__main__":
-    main()
+# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from six import text_type as _text_type
+import argparse
+import ast
+import sys
+import os
+import paddle.fluid as fluid
+from paddle2onnx.utils import logging
+
+
+def str2list(v):
+    if len(v) == 0:
+        return None
+    v = v.replace(" ", "")
+    v = eval(v)
+    return v
+
+
+def arg_parser():
+    parser = argparse.ArgumentParser()
+    parser.add_argument(
+        "--model_dir",
+        "-m",
+        type=_text_type,
+        default=None,
+        help="PaddlePaddle model directory, if params stored in single file, you need define '--model_filename' and 'params_filename'."
+    )
+    parser.add_argument(
+        "--model_filename",
+        "-mf",
+        type=_text_type,
+        default=None,
+        help="PaddlePaddle model's network file name, which under directory seted by --model_dir"
+    )
+    parser.add_argument(
+        "--params_filename",
+        "-pf",
+        type=_text_type,
+        default=None,
+        help="PaddlePaddle model's param file name(param files combined in single file), which under directory seted by --model_dir."
+    )
+    parser.add_argument(
+        "--save_file",
+        "-s",
+        type=_text_type,
+        default=None,
+        help="file path to save onnx model")
+    parser.add_argument(
+        "--opset_version",
+        "-ov",
+        type=int,
+        default=9,
+        help="set onnx opset version to export")
+    parser.add_argument(
+       "--input_shape_dict",
+       "-isd",
+       type=_text_type,
+       default="None",
+       help="define input shapes, e.g --input_shape_dict=\"{'image':[1, 3, 608, 608]}\" or" \
+       "--input_shape_dict=\"{'image':[1, 3, 608, 608], 'im_shape': [1, 2], 'scale_factor': [1, 2]}\"")
+    parser.add_argument(
+        "--enable_dev_version",
+        type=ast.literal_eval,
+        default=False,
+        help="whether to use new version of Paddle2ONNX which is under developing, default False"
+    )
+    parser.add_argument(
+        "--enable_onnx_checker",
+        type=ast.literal_eval,
+        default=True,
+        help="whether check onnx model validity, default True")
+    parser.add_argument(
+        "--enable_paddle_fallback",
+        type=ast.literal_eval,
+        default=False,
+        help="whether use PaddleFallback for custom op, default is False")
+    parser.add_argument(
+        "--version",
+        "-v",
+        action="store_true",
+        default=False,
+        help="get version of paddle2onnx")
+    parser.add_argument(
+        "--output_names",
+        "-on",
+        type=str2list,
+        default=None,
+        help="define output names, e.g --output_names=\"[\"output1\"]\" or \
+       --output_names=\"[\"output1\", \"output2\", \"output3\"]\" or \
+       --output_names=\"{\"Paddleoutput\":\"Onnxoutput\"}\"")
+    parser.add_argument(
+        "--enable_auto_update_opset",
+        type=ast.literal_eval,
+        default=True,
+        help="whether enable auto_update_opset, default is True")
+    return parser
+
+
+def c_paddle_to_onnx(model_file,
+                     params_file="",
+                     save_file=None,
+                     opset_version=7,
+                     auto_upgrade_opset=True,
+                     verbose=True,
+                     enable_onnx_checker=True,
+                     enable_experimental_op=True,
+                     enable_optimize=True):
+    import paddle2onnx.paddle2onnx_cpp2py_export as c_p2o
+    onnx_model_str = c_p2o.export(
+        model_file, params_file, opset_version, auto_upgrade_opset, verbose,
+        enable_onnx_checker, enable_experimental_op, enable_optimize)
+    if save_file is not None:
+        with open(save_file, "wb") as f:
+            f.write(onnx_model_str)
+    else:
+        return onnx_model_str
+
+
+def program2onnx(model_dir,
+                 save_file,
+                 model_filename=None,
+                 params_filename=None,
+                 opset_version=9,
+                 enable_onnx_checker=False,
+                 operator_export_type="ONNX",
+                 input_shape_dict=None,
+                 output_names=None,
+                 auto_update_opset=True):
+    try:
+        import paddle
+    except:
+        logging.error(
+            "paddlepaddle not installed, use \"pip install paddlepaddle\"")
+
+    v0, v1, v2 = paddle.__version__.split('.')
+    if v0 == '0' and v1 == '0' and v2 == '0':
+        logging.warning("You are use develop version of paddlepaddle")
+    elif int(v0) <= 1 and int(v1) < 8:
+        raise ImportError("paddlepaddle>=1.8.0 is required")
+
+    import paddle2onnx as p2o
+    # convert model save with 'paddle.fluid.io.save_inference_model'
+    if hasattr(paddle, 'enable_static'):
+        paddle.enable_static()
+    exe = fluid.Executor(fluid.CPUPlace())
+    if model_filename is None and params_filename is None:
+        [program, feed_var_names, fetch_vars] = fluid.io.load_inference_model(
+            model_dir, exe)
+    else:
+        [program, feed_var_names, fetch_vars] = fluid.io.load_inference_model(
+            model_dir,
+            exe,
+            model_filename=model_filename,
+            params_filename=params_filename)
+
+    OP_WITHOUT_KERNEL_SET = {
+        'feed', 'fetch', 'recurrent', 'go', 'rnn_memory_helper_grad',
+        'conditional_block', 'while', 'send', 'recv', 'listen_and_serv',
+        'fl_listen_and_serv', 'ncclInit', 'select', 'checkpoint_notify',
+        'gen_bkcl_id', 'c_gen_bkcl_id', 'gen_nccl_id', 'c_gen_nccl_id',
+        'c_comm_init', 'c_sync_calc_stream', 'c_sync_comm_stream',
+        'queue_generator', 'dequeue', 'enqueue', 'heter_listen_and_serv',
+        'c_wait_comm', 'c_wait_compute', 'c_gen_hccl_id', 'c_comm_init_hccl',
+        'copy_cross_scope'
+    }
+    if input_shape_dict is not None:
+        import paddle2onnx
+        paddle2onnx.legacy.process_old_ops_desc(program)
+        paddle_version = paddle.__version__
+        model_version = program.desc._version()
+        major_ver = model_version // 1000000
+        minor_ver = (model_version - major_ver * 1000000) // 1000
+        patch_ver = model_version - major_ver * 1000000 - minor_ver * 1000
+        model_version = "{}.{}.{}".format(major_ver, minor_ver, patch_ver)
+        if model_version != paddle_version:
+            logging.warning(
+                "The model is saved by paddlepaddle v{}, but now your paddlepaddle is version of {}, this difference may cause error, it is recommend you reinstall a same version of paddlepaddle for this model".
+                format(model_version, paddle_version))
+
+        for k, v in input_shape_dict.items():
+            program.blocks[0].var(k).desc.set_shape(v)
+        for i in range(len(program.blocks[0].ops)):
+            if program.blocks[0].ops[i].type in OP_WITHOUT_KERNEL_SET:
+                continue
+            program.blocks[0].ops[i].desc.infer_shape(program.blocks[0].desc)
+    p2o.program2onnx(
+        program,
+        fluid.global_scope(),
+        save_file,
+        feed_var_names=feed_var_names,
+        target_vars=fetch_vars,
+        opset_version=opset_version,
+        enable_onnx_checker=enable_onnx_checker,
+        operator_export_type=operator_export_type,
+        auto_update_opset=auto_update_opset,
+        output_names=output_names)
+
+
+def main():
+    if len(sys.argv) < 2:
+        logging.info("Use \"paddle2onnx -h\" to print the help information")
+        logging.info(
+            "For more information, please follow our github repo below:")
+        logging.info("Github: https://github.com/PaddlePaddle/paddle2onnx.git")
+        return
+
+    parser = arg_parser()
+    args = parser.parse_args()
+
+    if args.version:
+        import paddle2onnx
+        logging.info("paddle2onnx-{} with python>=2.7, paddlepaddle>=1.8.0".
+                     format(paddle2onnx.__version__))
+        return
+
+    assert args.model_dir is not None, "--model_dir should be defined while translating paddle model to onnx"
+    assert args.save_file is not None, "--save_file should be defined while translating paddle model to onnx"
+
+    input_shape_dict = eval(args.input_shape_dict)
+
+    operator_export_type = "ONNX"
+    if args.enable_paddle_fallback:
+        operator_export_type = "PaddleFallback"
+
+    if args.output_names is not None:
+        if not isinstance(args.output_names, (list, dict)):
+            raise TypeError(
+                "The output_names should be 'list' or 'dict', but received type is %s."
+                % type(args.output_names))
+
+    if args.enable_dev_version:
+        if args.enable_paddle_fallback:
+            logging.warn(
+                "--enable_paddle_fallback is deprecated while --enable_dev_version=True."
+            )
+        if args.output_names is not None:
+            logging.warn(
+                "--output_names is deprecated while --enable_dev_version=True.")
+        if input_shape_dict is not None:
+            logging.warn(
+                "--input_shape_dict is deprecated while --enable_dev_version=True."
+            )
+        model_file = os.path.join(args.model_dir, args.model_filename)
+        if args.params_filename is None:
+            params_file = ""
+        else:
+            params_file = os.path.join(args.model_dir, args.params_filename)
+        return c_paddle_to_onnx(
+            model_file=model_file,
+            params_file=params_file,
+            save_file=args.save_file,
+            opset_version=args.opset_version,
+            auto_upgrade_opset=args.enable_auto_update_opset,
+            verbose=True,
+            enable_onnx_checker=args.enable_onnx_checker,
+            enable_experimental_op=True,
+            enable_optimize=True)
+
+    program2onnx(
+        args.model_dir,
+        args.save_file,
+        args.model_filename,
+        args.params_filename,
+        opset_version=args.opset_version,
+        enable_onnx_checker=args.enable_onnx_checker,
+        operator_export_type=operator_export_type,
+        input_shape_dict=input_shape_dict,
+        output_names=args.output_names,
+        auto_update_opset=args.enable_auto_update_opset)
+
+
+if __name__ == "__main__":
+    main()
```

## paddle2onnx/legacy/convert.py

 * *Ordering differences only*

```diff
@@ -1,206 +1,206 @@
-#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import os
-import six
-import paddle
-import numpy as np
-from paddle.fluid.framework import Variable
-from paddle2onnx.utils import check_model, logging
-from paddle2onnx.legacy.graph import PaddleGraph, ONNXGraph
-from paddle2onnx.legacy.passes import PassManager
-
-
-def export_onnx(paddle_graph,
-                save_file,
-                opset_version=9,
-                enable_onnx_checker=False,
-                operator_export_type="ONNX",
-                verbose=False,
-                auto_update_opset=True,
-                output_names=None):
-    onnx_graph = ONNXGraph.build(paddle_graph, opset_version,
-                                 operator_export_type, verbose,
-                                 auto_update_opset)
-    onnx_graph = PassManager.run_pass(
-        onnx_graph, ['dumplicate_names_pass', 'inplace_node_pass'])
-    onnx_proto = onnx_graph.export_proto(enable_onnx_checker, output_names)
-
-    if save_file is None:
-        return onnx_proto
-
-    path, _ = os.path.split(save_file)
-    if path != '' and not os.path.isdir(path):
-        os.makedirs(path)
-    with open(save_file, 'wb') as f:
-        f.write(onnx_proto.SerializeToString())
-    logging.info("ONNX model saved in {}".format(save_file))
-
-
-def program2onnx(program,
-                 scope,
-                 save_file,
-                 feed_var_names=None,
-                 target_vars=None,
-                 opset_version=9,
-                 enable_onnx_checker=False,
-                 operator_export_type="ONNX",
-                 auto_update_opset=True,
-                 **configs):
-    from paddle import fluid
-    if hasattr(paddle, 'enable_static'):
-        paddle.enable_static()
-    if isinstance(program, paddle.fluid.framework.Program):
-        if feed_var_names is not None:
-            if isinstance(feed_var_names, six.string_types):
-                feed_var_names = [feed_var_names]
-            else:
-                if not (bool(feed_var_names) and all(
-                        isinstance(name, six.string_types)
-                        for name in feed_var_names)):
-                    raise TypeError("'feed_var_names' should be a list of str.")
-
-        if target_vars is not None:
-            if isinstance(target_vars, Variable):
-                target_vars = [target_vars]
-            else:
-                if not (bool(target_vars) and
-                        all(isinstance(var, Variable) for var in target_vars)):
-                    raise TypeError(
-                        "'target_vars' should be a list of variable.")
-
-        paddle_graph = PaddleGraph.build_from_program(program, feed_var_names,
-                                                      target_vars, scope)
-        output_names = None
-        if 'output_names' in configs:
-            output_names = configs['output_names']
-            if output_names is not None and not isinstance(output_names,
-                                                           (list, dict)):
-                raise TypeError(
-                    "The output_names should be 'list' or dict, but received type is %s."
-                    % type(output_names))
-        return export_onnx(
-            paddle_graph,
-            save_file,
-            opset_version,
-            enable_onnx_checker,
-            operator_export_type,
-            auto_update_opset=auto_update_opset,
-            output_names=output_names)
-    else:
-        raise TypeError(
-            "the input 'program' should be 'Program', but received type is %s."
-            % type(program))
-
-
-def dygraph2onnx(layer, save_file, input_spec=None, opset_version=9, **configs):
-    from paddle.nn import Layer
-    from paddle.fluid import core
-    from paddle.fluid.framework import Variable
-    from paddle.fluid.dygraph.dygraph_to_static import program_translator
-    from paddle.fluid import dygraph
-    if not isinstance(layer, Layer):
-        raise TypeError(
-            "the input 'layer' should be 'Layer', 'TranslatedLayer', but received type is %s."
-            % type(layer))
-
-    inner_input_spec = None
-    if input_spec is not None:
-        if not isinstance(input_spec, list):
-            raise TypeError(
-                "The input input_spec should be 'list', but received type is %s."
-                % type(input_spec))
-        inner_input_spec = []
-        for var in input_spec:
-            if isinstance(var, paddle.static.InputSpec):
-                inner_input_spec.append(var)
-            elif isinstance(var, (core.VarBase, Variable)):
-                inner_input_spec.append(
-                    paddle.static.InputSpec.from_tensor(var))
-            else:
-                raise TypeError(
-                    "The element in input_spec list should be 'Variable' or `paddle.static.InputSpec`, but received element's type is %s."
-                    % type(var))
-
-    output_spec = None
-    if 'output_spec' in configs:
-        output_spec = configs['output_spec']
-        if not isinstance(output_spec, list):
-            raise TypeError(
-                "The output_spec should be 'list', but received type is %s." %
-                type(output_spec))
-        for var in output_spec:
-            if not isinstance(var, (core.VarBase, Variable)):
-                raise TypeError(
-                    "The element in output_spec list should be 'Variable', but received element's type is %s."
-                    % type(var))
-
-    verbose = False
-    if 'verbose' in configs:
-        if isinstance(configs['verbose'], bool):
-            verbose = configs['verbose']
-        else:
-            raise TypeError(
-                "The verbose should be 'bool', but received type is %s." %
-                type(configs['verbose']))
-
-    enable_onnx_checker = False
-    if 'enable_onnx_checker' in configs:
-        if isinstance(configs['enable_onnx_checker'], bool):
-            enable_onnx_checker = configs['enable_onnx_checker']
-        else:
-            raise TypeError(
-                "The 'enable_onnx_checker' should be 'bool', but received type is %s."
-                % type(configs['enable_onnx_checker']))
-
-    operator_export_type = "ONNX"
-    enable_paddle_fallback = False
-    if 'enable_paddle_fallback' in configs:
-        if isinstance(configs['enable_paddle_fallback'], bool):
-            enable_paddle_fallback = configs['enable_paddle_fallback']
-            if enable_paddle_fallback:
-                operator_export_type = "PaddleFallback"
-        else:
-            raise TypeError(
-                "The 'enable_paddle_fallback' should be 'bool', but received type is %s."
-                % type(configs['enable_paddle_fallback']))
-
-    paddle_graph = PaddleGraph.build_from_dygraph(layer, inner_input_spec,
-                                                  output_spec)
-
-    if 'get_paddle_graph' in configs:
-        return paddle_graph
-
-    auto_update_opset = True
-    if 'auto_update_opset' in configs:
-        if isinstance(configs['auto_update_opset'], bool):
-            auto_update_opset = configs['auto_update_opset']
-        else:
-            raise TypeError(
-                "The auto_update_opset should be 'bool', but received type is %s."
-                % type(configs['auto_update_opset']))
-
-    output_names = None
-    if 'output_names' in configs:
-        output_names = configs['output_names']
-        if not isinstance(output_names, (list, dict)):
-            raise TypeError(
-                "The output_names should be 'list' or dict, but received type is %s."
-                % type(output_names))
-
-    return export_onnx(paddle_graph, save_file, opset_version,
-                       enable_onnx_checker, operator_export_type, verbose,
-                       auto_update_opset, output_names)
+#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import os
+import six
+import paddle
+import numpy as np
+from paddle.fluid.framework import Variable
+from paddle2onnx.utils import check_model, logging
+from paddle2onnx.legacy.graph import PaddleGraph, ONNXGraph
+from paddle2onnx.legacy.passes import PassManager
+
+
+def export_onnx(paddle_graph,
+                save_file,
+                opset_version=9,
+                enable_onnx_checker=False,
+                operator_export_type="ONNX",
+                verbose=False,
+                auto_update_opset=True,
+                output_names=None):
+    onnx_graph = ONNXGraph.build(paddle_graph, opset_version,
+                                 operator_export_type, verbose,
+                                 auto_update_opset)
+    onnx_graph = PassManager.run_pass(
+        onnx_graph, ['dumplicate_names_pass', 'inplace_node_pass'])
+    onnx_proto = onnx_graph.export_proto(enable_onnx_checker, output_names)
+
+    if save_file is None:
+        return onnx_proto
+
+    path, _ = os.path.split(save_file)
+    if path != '' and not os.path.isdir(path):
+        os.makedirs(path)
+    with open(save_file, 'wb') as f:
+        f.write(onnx_proto.SerializeToString())
+    logging.info("ONNX model saved in {}".format(save_file))
+
+
+def program2onnx(program,
+                 scope,
+                 save_file,
+                 feed_var_names=None,
+                 target_vars=None,
+                 opset_version=9,
+                 enable_onnx_checker=False,
+                 operator_export_type="ONNX",
+                 auto_update_opset=True,
+                 **configs):
+    from paddle import fluid
+    if hasattr(paddle, 'enable_static'):
+        paddle.enable_static()
+    if isinstance(program, paddle.fluid.framework.Program):
+        if feed_var_names is not None:
+            if isinstance(feed_var_names, six.string_types):
+                feed_var_names = [feed_var_names]
+            else:
+                if not (bool(feed_var_names) and all(
+                        isinstance(name, six.string_types)
+                        for name in feed_var_names)):
+                    raise TypeError("'feed_var_names' should be a list of str.")
+
+        if target_vars is not None:
+            if isinstance(target_vars, Variable):
+                target_vars = [target_vars]
+            else:
+                if not (bool(target_vars) and
+                        all(isinstance(var, Variable) for var in target_vars)):
+                    raise TypeError(
+                        "'target_vars' should be a list of variable.")
+
+        paddle_graph = PaddleGraph.build_from_program(program, feed_var_names,
+                                                      target_vars, scope)
+        output_names = None
+        if 'output_names' in configs:
+            output_names = configs['output_names']
+            if output_names is not None and not isinstance(output_names,
+                                                           (list, dict)):
+                raise TypeError(
+                    "The output_names should be 'list' or dict, but received type is %s."
+                    % type(output_names))
+        return export_onnx(
+            paddle_graph,
+            save_file,
+            opset_version,
+            enable_onnx_checker,
+            operator_export_type,
+            auto_update_opset=auto_update_opset,
+            output_names=output_names)
+    else:
+        raise TypeError(
+            "the input 'program' should be 'Program', but received type is %s."
+            % type(program))
+
+
+def dygraph2onnx(layer, save_file, input_spec=None, opset_version=9, **configs):
+    from paddle.nn import Layer
+    from paddle.fluid import core
+    from paddle.fluid.framework import Variable
+    from paddle.fluid.dygraph.dygraph_to_static import program_translator
+    from paddle.fluid import dygraph
+    if not isinstance(layer, Layer):
+        raise TypeError(
+            "the input 'layer' should be 'Layer', 'TranslatedLayer', but received type is %s."
+            % type(layer))
+
+    inner_input_spec = None
+    if input_spec is not None:
+        if not isinstance(input_spec, list):
+            raise TypeError(
+                "The input input_spec should be 'list', but received type is %s."
+                % type(input_spec))
+        inner_input_spec = []
+        for var in input_spec:
+            if isinstance(var, paddle.static.InputSpec):
+                inner_input_spec.append(var)
+            elif isinstance(var, (core.VarBase, Variable)):
+                inner_input_spec.append(
+                    paddle.static.InputSpec.from_tensor(var))
+            else:
+                raise TypeError(
+                    "The element in input_spec list should be 'Variable' or `paddle.static.InputSpec`, but received element's type is %s."
+                    % type(var))
+
+    output_spec = None
+    if 'output_spec' in configs:
+        output_spec = configs['output_spec']
+        if not isinstance(output_spec, list):
+            raise TypeError(
+                "The output_spec should be 'list', but received type is %s." %
+                type(output_spec))
+        for var in output_spec:
+            if not isinstance(var, (core.VarBase, Variable)):
+                raise TypeError(
+                    "The element in output_spec list should be 'Variable', but received element's type is %s."
+                    % type(var))
+
+    verbose = False
+    if 'verbose' in configs:
+        if isinstance(configs['verbose'], bool):
+            verbose = configs['verbose']
+        else:
+            raise TypeError(
+                "The verbose should be 'bool', but received type is %s." %
+                type(configs['verbose']))
+
+    enable_onnx_checker = False
+    if 'enable_onnx_checker' in configs:
+        if isinstance(configs['enable_onnx_checker'], bool):
+            enable_onnx_checker = configs['enable_onnx_checker']
+        else:
+            raise TypeError(
+                "The 'enable_onnx_checker' should be 'bool', but received type is %s."
+                % type(configs['enable_onnx_checker']))
+
+    operator_export_type = "ONNX"
+    enable_paddle_fallback = False
+    if 'enable_paddle_fallback' in configs:
+        if isinstance(configs['enable_paddle_fallback'], bool):
+            enable_paddle_fallback = configs['enable_paddle_fallback']
+            if enable_paddle_fallback:
+                operator_export_type = "PaddleFallback"
+        else:
+            raise TypeError(
+                "The 'enable_paddle_fallback' should be 'bool', but received type is %s."
+                % type(configs['enable_paddle_fallback']))
+
+    paddle_graph = PaddleGraph.build_from_dygraph(layer, inner_input_spec,
+                                                  output_spec)
+
+    if 'get_paddle_graph' in configs:
+        return paddle_graph
+
+    auto_update_opset = True
+    if 'auto_update_opset' in configs:
+        if isinstance(configs['auto_update_opset'], bool):
+            auto_update_opset = configs['auto_update_opset']
+        else:
+            raise TypeError(
+                "The auto_update_opset should be 'bool', but received type is %s."
+                % type(configs['auto_update_opset']))
+
+    output_names = None
+    if 'output_names' in configs:
+        output_names = configs['output_names']
+        if not isinstance(output_names, (list, dict)):
+            raise TypeError(
+                "The output_names should be 'list' or dict, but received type is %s."
+                % type(output_names))
+
+    return export_onnx(paddle_graph, save_file, opset_version,
+                       enable_onnx_checker, operator_export_type, verbose,
+                       auto_update_opset, output_names)
```

## paddle2onnx/legacy/constant/__init__.py

 * *Ordering differences only*

```diff
@@ -1,15 +1,15 @@
-#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-from .constant import PRODUCER
-from .constant import NodeDomain
+#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from .constant import PRODUCER
+from .constant import NodeDomain
```

## paddle2onnx/legacy/constant/constant.py

 * *Ordering differences only*

```diff
@@ -1,24 +1,24 @@
-#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-PRODUCER = 'PaddlePaddle'
-
-ONNX_HELPER_VERSION = '1.7.0'
-
-
-class NodeDomain():
-    ONNX = 'onnx'
-    PADDLE = 'paddle'
-    CUSTOM = 'custom'
-    RAW = 'raw'
+#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+PRODUCER = 'PaddlePaddle'
+
+ONNX_HELPER_VERSION = '1.7.0'
+
+
+class NodeDomain():
+    ONNX = 'onnx'
+    PADDLE = 'paddle'
+    CUSTOM = 'custom'
+    RAW = 'raw'
```

## paddle2onnx/legacy/constant/dtypes.py

 * *Ordering differences only*

```diff
@@ -1,84 +1,84 @@
-#   Copyright (c) 2019  PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-import numpy as np
-import paddle.fluid.core as core
-from onnx import helper
-from onnx import TensorProto
-
-ONNX = TensorProto
-
-DTYPE_PADDLE_ONNX_MAP = {
-    TensorProto.FLOAT16: core.VarDesc.VarType.FP16,
-    TensorProto.FLOAT: core.VarDesc.VarType.FP32,
-    TensorProto.DOUBLE: core.VarDesc.VarType.FP64,
-    TensorProto.INT16: core.VarDesc.VarType.INT16,
-    TensorProto.INT32: core.VarDesc.VarType.INT32,
-    TensorProto.INT64: core.VarDesc.VarType.INT64,
-    TensorProto.BOOL: core.VarDesc.VarType.BOOL,
-    TensorProto.UINT8: core.VarDesc.VarType.UINT8,
-    core.VarDesc.VarType.FP16: TensorProto.FLOAT16,
-    core.VarDesc.VarType.FP32: TensorProto.FLOAT,
-    core.VarDesc.VarType.FP64: TensorProto.DOUBLE,
-    core.VarDesc.VarType.INT16: TensorProto.INT16,
-    core.VarDesc.VarType.INT32: TensorProto.INT32,
-    core.VarDesc.VarType.INT64: TensorProto.INT64,
-    core.VarDesc.VarType.BOOL: TensorProto.BOOL,
-    core.VarDesc.VarType.UINT8: TensorProto.UINT8,
-}
-
-DTYPE_PADDLE_NUMPY_MAP = {
-    np.float32: core.VarDesc.VarType.FP32,
-    np.float64: core.VarDesc.VarType.FP64,
-    np.int16: core.VarDesc.VarType.INT16,
-    np.int32: core.VarDesc.VarType.INT32,
-    np.int64: core.VarDesc.VarType.INT64,
-    np.bool_: core.VarDesc.VarType.BOOL,
-    core.VarDesc.VarType.FP32: np.float32,
-    core.VarDesc.VarType.FP64: np.float64,
-    core.VarDesc.VarType.INT16: np.int16,
-    core.VarDesc.VarType.INT32: np.int32,
-    core.VarDesc.VarType.INT64: np.int64,
-    core.VarDesc.VarType.BOOL: np.bool_
-}
-
-DTYPE_PADDLE_STR_MAP = {
-    core.VarDesc.VarType.FP32: 'float32',
-    core.VarDesc.VarType.FP64: 'float64',
-    core.VarDesc.VarType.INT16: 'int16',
-    core.VarDesc.VarType.INT32: 'int32',
-    core.VarDesc.VarType.INT64: 'int64',
-    core.VarDesc.VarType.BOOL: 'bool',
-    'float32': core.VarDesc.VarType.FP32,
-    'float64': core.VarDesc.VarType.FP64,
-    'int16': core.VarDesc.VarType.INT16,
-    'int32': core.VarDesc.VarType.INT32,
-    'int64': core.VarDesc.VarType.INT64,
-    'bool': core.VarDesc.VarType.BOOL
-}
-
-DTYPE_ONNX_STR_MAP = {
-    TensorProto.FLOAT: 'float32',
-    TensorProto.DOUBLE: 'float64',
-    TensorProto.INT16: 'int16',
-    TensorProto.INT32: 'int32',
-    TensorProto.INT64: 'int64',
-    TensorProto.BOOL: 'bool',
-    'float32': TensorProto.FLOAT,
-    'float64': TensorProto.DOUBLE,
-    'int16': TensorProto.INT16,
-    'int32': TensorProto.INT32,
-    'int64': TensorProto.INT64,
-    'bool': TensorProto.BOOL,
-}
+#   Copyright (c) 2019  PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import numpy as np
+import paddle.fluid.core as core
+from onnx import helper
+from onnx import TensorProto
+
+ONNX = TensorProto
+
+DTYPE_PADDLE_ONNX_MAP = {
+    TensorProto.FLOAT16: core.VarDesc.VarType.FP16,
+    TensorProto.FLOAT: core.VarDesc.VarType.FP32,
+    TensorProto.DOUBLE: core.VarDesc.VarType.FP64,
+    TensorProto.INT16: core.VarDesc.VarType.INT16,
+    TensorProto.INT32: core.VarDesc.VarType.INT32,
+    TensorProto.INT64: core.VarDesc.VarType.INT64,
+    TensorProto.BOOL: core.VarDesc.VarType.BOOL,
+    TensorProto.UINT8: core.VarDesc.VarType.UINT8,
+    core.VarDesc.VarType.FP16: TensorProto.FLOAT16,
+    core.VarDesc.VarType.FP32: TensorProto.FLOAT,
+    core.VarDesc.VarType.FP64: TensorProto.DOUBLE,
+    core.VarDesc.VarType.INT16: TensorProto.INT16,
+    core.VarDesc.VarType.INT32: TensorProto.INT32,
+    core.VarDesc.VarType.INT64: TensorProto.INT64,
+    core.VarDesc.VarType.BOOL: TensorProto.BOOL,
+    core.VarDesc.VarType.UINT8: TensorProto.UINT8,
+}
+
+DTYPE_PADDLE_NUMPY_MAP = {
+    np.float32: core.VarDesc.VarType.FP32,
+    np.float64: core.VarDesc.VarType.FP64,
+    np.int16: core.VarDesc.VarType.INT16,
+    np.int32: core.VarDesc.VarType.INT32,
+    np.int64: core.VarDesc.VarType.INT64,
+    np.bool_: core.VarDesc.VarType.BOOL,
+    core.VarDesc.VarType.FP32: np.float32,
+    core.VarDesc.VarType.FP64: np.float64,
+    core.VarDesc.VarType.INT16: np.int16,
+    core.VarDesc.VarType.INT32: np.int32,
+    core.VarDesc.VarType.INT64: np.int64,
+    core.VarDesc.VarType.BOOL: np.bool_
+}
+
+DTYPE_PADDLE_STR_MAP = {
+    core.VarDesc.VarType.FP32: 'float32',
+    core.VarDesc.VarType.FP64: 'float64',
+    core.VarDesc.VarType.INT16: 'int16',
+    core.VarDesc.VarType.INT32: 'int32',
+    core.VarDesc.VarType.INT64: 'int64',
+    core.VarDesc.VarType.BOOL: 'bool',
+    'float32': core.VarDesc.VarType.FP32,
+    'float64': core.VarDesc.VarType.FP64,
+    'int16': core.VarDesc.VarType.INT16,
+    'int32': core.VarDesc.VarType.INT32,
+    'int64': core.VarDesc.VarType.INT64,
+    'bool': core.VarDesc.VarType.BOOL
+}
+
+DTYPE_ONNX_STR_MAP = {
+    TensorProto.FLOAT: 'float32',
+    TensorProto.DOUBLE: 'float64',
+    TensorProto.INT16: 'int16',
+    TensorProto.INT32: 'int32',
+    TensorProto.INT64: 'int64',
+    TensorProto.BOOL: 'bool',
+    'float32': TensorProto.FLOAT,
+    'float64': TensorProto.DOUBLE,
+    'int16': TensorProto.INT16,
+    'int32': TensorProto.INT32,
+    'int64': TensorProto.INT64,
+    'bool': TensorProto.BOOL,
+}
```

## paddle2onnx/legacy/constant/op_mapping_status.py

 * *Ordering differences only*

```diff
@@ -1,19 +1,19 @@
-#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-OP_MAPPING_WAITTING = 0
-OP_MAPPING_NO_REGISTER = 1
-OP_MAPPING_NO_VERSION = 2
-OP_MAPPING_SUCCESSED = 3
-OP_MAPPING_FAILED = 4
+#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+OP_MAPPING_WAITTING = 0
+OP_MAPPING_NO_REGISTER = 1
+OP_MAPPING_NO_VERSION = 2
+OP_MAPPING_SUCCESSED = 3
+OP_MAPPING_FAILED = 4
```

## paddle2onnx/legacy/graph/__init__.py

 * *Ordering differences only*

```diff
@@ -1,17 +1,17 @@
-#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from .graph import Graph, Node
-from .paddle_graph import PaddleGraph, PaddleNode
-from .onnx_graph import ONNXGraph, ONNXNode
+#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from .graph import Graph, Node
+from .paddle_graph import PaddleGraph, PaddleNode
+from .onnx_graph import ONNXGraph, ONNXNode
```

## paddle2onnx/legacy/graph/dygraph_helper.py

 * *Ordering differences only*

```diff
@@ -1,271 +1,271 @@
-#   Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import os
-import numpy as np
-import inspect
-import six
-import paddle
-from paddle.fluid.io import _get_valid_program
-from paddle.fluid.dygraph.dygraph_to_static.program_translator import ProgramTranslator, StaticFunction
-from paddle.fluid.layers.utils import flatten, pack_sequence_as
-from collections import OrderedDict
-from paddle.fluid import dygraph
-from paddle.fluid.dygraph.jit import declarative
-from paddle.fluid import core
-from paddle.fluid import layers
-from paddle.nn import Layer
-from paddle.fluid.framework import Block, ParamBase, Program, Variable, Parameter, program_guard
-from paddle.fluid.dygraph.layers import Layer
-
-from paddle2onnx.utils import logging
-from paddle2onnx.legacy.graph.graph_helper import prepend_feed_ops, append_fetch_ops
-
-
-def _get_input_var_names(inputs, input_spec):
-    name_none_error = "The %s's name is None. " \
-        "When using jit.save, please set InputSepc's name in " \
-        "to_static(input_spec=[]) and jit.save(input_spec=[]) " \
-        "and make sure they are consistent."
-    name_no_exists_error = "The tensor `%s` does not exists. " \
-        "Please make sure the name of InputSpec or example Tensor " \
-        "in input_spec is the same as the name of InputSpec in " \
-        "`to_static` decorated on the Layer.forward method."
-    result_list = []
-    input_var_names = [
-        var.name for var in flatten(inputs) if isinstance(var, Variable)
-    ]
-    if input_spec is None:
-        # no prune
-        return input_var_names
-    else:
-        # fileter out non-tensor type spec infos.
-        input_spec = [
-            spec for spec in input_spec
-            if isinstance(spec, paddle.static.InputSpec)
-        ]
-
-    if len(input_spec) == len(input_var_names):
-        # no prune
-        result_list = input_var_names
-        # if input spec name not in input_var_names, only raise warning
-        for spec in input_spec:
-            if spec.name is None:
-                warnings.warn(name_none_error % spec)
-            elif spec.name not in input_var_names:
-                warnings.warn(name_no_exists_error % spec.name)
-            else:
-                # do nothing
-                pass
-    else:
-        # prune
-        for spec in input_spec:
-            if spec.name is None:
-                # name is None, the input_spec only can be InputSpec
-                raise ValueError(name_none_error % spec)
-            elif spec.name not in input_var_names:
-                # the input_spec can be `InputSpec` or `VarBase`
-                raise ValueError(name_no_exists_error % spec.name)
-            else:
-                result_list.append(spec.name)
-
-    return result_list
-
-
-def _get_output_vars(outputs, output_spec):
-    name_no_exists_error = "The tensor `%s` does not exists. " \
-        "Please make sure the name of example Tensor " \
-        "in configs.output_spec is the output tensor of " \
-        "Layer.forward method."
-    result_list = []
-    output_vars_dict = OrderedDict()
-    for var in flatten(outputs):
-        if isinstance(var, Variable):
-            output_vars_dict[var.name] = var
-    if output_spec is None:
-        result_list = output_vars_dict.values()
-    elif output_spec is not None and len(output_spec) == len(output_vars_dict):
-        result_list = output_vars_dict.values()
-        for var in output_spec:
-            if var.name not in output_vars_dict:
-                warnings.warn(name_no_exists_error % var.name)
-    else:
-        for var in output_spec:
-            if var.name not in output_vars_dict:
-                raise ValueError(name_no_exists_error % var.name)
-            else:
-                result_list.append(output_vars_dict[var.name])
-    return result_list
-
-
-@dygraph.base.switch_to_static_graph
-def get_program(layer, input_spec, output_spec, **configs):
-    paddle.jit.set_verbosity(0)
-    prog_translator = ProgramTranslator()
-    if not prog_translator.enable_to_static:
-        raise RuntimeError(
-            "The Paddle2onnx doesn't work when setting ProgramTranslator.enable to False."
-        )
-
-    if not isinstance(layer, Layer):
-        raise TypeError(
-            "The input of paddle2onnx should be 'Layer', but received input type is %s."
-            % type(layer))
-
-    if isinstance(layer, paddle.DataParallel):
-        inner_layer = layer._layers
-    else:
-        inner_layer = layer
-
-    # avoid change user given input_spec
-    inner_input_spec = None
-    if input_spec is not None:
-        for attr_func in dir(inner_layer):
-            static_func = getattr(inner_layer, attr_func, None)
-            if isinstance(static_func,
-                          StaticFunction) and 'forward' != attr_func:
-                raise ValueError(
-                    "If there are static functions other than 'forward' that need to be saved, the input 'input_spec' should be None, but received the type of 'input_spec' is %s."
-                    % type(input_spec))
-
-        if not isinstance(input_spec, (list, tuple)):
-            raise TypeError(
-                "The input input_spec should be 'list', but received input_spec's type is %s."
-                % type(input_spec))
-        inner_input_spec = []
-        for var in flatten(input_spec):
-            if isinstance(var, paddle.static.InputSpec):
-                inner_input_spec.append(var)
-            elif isinstance(var, (core.VarBase, core.eager.Tensor, Variable)):
-                inner_input_spec.append(
-                    paddle.static.InputSpec.from_tensor(var))
-            else:
-                # NOTE(Aurelius84): Support non-Tensor type in `input_spec`.
-                inner_input_spec.append(var)
-
-    extra_var_info = dict()
-    functions = dir(inner_layer)
-    for attr_func in functions:
-        static_func = getattr(inner_layer, attr_func, None)
-        if isinstance(static_func, StaticFunction):
-            concrete_program = static_func.concrete_program_specify_input_spec(
-                inner_input_spec)
-        elif 'forward' == attr_func:
-            # transform in jit.save, if input_spec is incomplete, declarative will throw error
-            # inner_input_spec is list[InputSpec], it should be packed with same structure
-            # as original input_spec here.
-            if inner_input_spec:
-                inner_input_spec = pack_sequence_as(input_spec,
-                                                    inner_input_spec)
-            static_forward = declarative(
-                inner_layer.forward, input_spec=inner_input_spec)
-            concrete_program = static_forward.concrete_program
-            # the input_spec has been used in declarative, which is equal to
-            # @declarative with input_spec and jit.save without input_spec,
-            # avoid needless warning
-            inner_input_spec = None
-        else:
-            continue
-
-        input_var_names = _get_input_var_names(concrete_program.inputs,
-                                               inner_input_spec)
-
-        # NOTE(chenweihang): [ Get output variables ]
-        # the rule is like [ Get input variables name ]. For output var,
-        # we only support VarBase spec, and actually, we only need the
-        # var name of output, and we don't recommended to use output_spec
-        output_vars = _get_output_vars(concrete_program.outputs, output_spec)
-
-    feeded_var_names = input_var_names
-    target_vars = output_vars
-    main_program = concrete_program.main_program.clone()
-    export_for_deployment = True
-
-    if isinstance(feeded_var_names, six.string_types):
-        feeded_var_names = [feeded_var_names]
-    elif export_for_deployment:
-        if len(feeded_var_names) > 0:
-            # TODO(paddle-dev): polish these code blocks
-            if not (bool(feeded_var_names) and all(
-                    isinstance(name, six.string_types)
-                    for name in feeded_var_names)):
-                raise ValueError("'feed_var_names' should be a list of str.")
-
-    if isinstance(target_vars, Variable):
-        target_vars = [target_vars]
-    elif export_for_deployment:
-        if not (bool(target_vars) and
-                all(isinstance(var, Variable) for var in target_vars)):
-            raise ValueError("'target_vars' should be a list of Variable.")
-
-    main_program = _get_valid_program(main_program)
-
-    # remind user to set auc_states to zeros if the program contains auc op
-    all_ops = main_program.global_block().ops
-    for op in all_ops:
-        # clear device of Op
-        device_attr_name = core.op_proto_and_checker_maker.kOpDeviceAttrName()
-        op._set_attr(device_attr_name, "")
-        if op.type == 'auc':
-            warnings.warn(
-                "please ensure that you have set the auc states to zeros before saving inference model"
-            )
-            break
-
-    with program_guard(main_program):
-        uniq_target_vars = []
-        for i, var in enumerate(target_vars):
-            uniq_target_vars.append(var)
-        target_vars = uniq_target_vars
-    target_var_name_list = [var.name for var in target_vars]
-
-    origin_program = main_program.clone()
-
-    main_program = main_program.clone()
-    global_block = main_program.global_block()
-    need_to_remove_op_index = []
-    for i, op in enumerate(global_block.ops):
-        op.desc.set_is_target(False)
-        if op.type == "feed" or op.type == "fetch":
-            need_to_remove_op_index.append(i)
-
-    for index in need_to_remove_op_index[::-1]:
-        global_block._remove_op(index)
-
-    main_program.desc.flush()
-
-    main_program = main_program._prune_with_input(
-        feeded_var_names=feeded_var_names, targets=target_vars)
-    main_program = main_program._inference_optimize(prune_read_op=True)
-    fetch_var_names = [v.name for v in target_vars]
-
-    for target_v in target_vars:
-        if not main_program.global_block().has_var(target_v.name):
-            main_program.global_block().create_var(
-                name=target_v.name,
-                shape=target_v.shape,
-                dtype=target_v.dtype,
-                persistable=target_v.persistable)
-
-    prepend_feed_ops(main_program, feeded_var_names)
-    append_fetch_ops(main_program, fetch_var_names)
-
-    main_program.desc._set_version()
-    paddle.fluid.core.save_op_version_info(main_program.desc)
-
-    main_program._copy_dist_param_info_from(origin_program)
-
-    return main_program, feeded_var_names, target_vars
+#   Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import os
+import numpy as np
+import inspect
+import six
+import paddle
+from paddle.fluid.io import _get_valid_program
+from paddle.fluid.dygraph.dygraph_to_static.program_translator import ProgramTranslator, StaticFunction
+from paddle.fluid.layers.utils import flatten, pack_sequence_as
+from collections import OrderedDict
+from paddle.fluid import dygraph
+from paddle.fluid.dygraph.jit import declarative
+from paddle.fluid import core
+from paddle.fluid import layers
+from paddle.nn import Layer
+from paddle.fluid.framework import Block, ParamBase, Program, Variable, Parameter, program_guard
+from paddle.fluid.dygraph.layers import Layer
+
+from paddle2onnx.utils import logging
+from paddle2onnx.legacy.graph.graph_helper import prepend_feed_ops, append_fetch_ops
+
+
+def _get_input_var_names(inputs, input_spec):
+    name_none_error = "The %s's name is None. " \
+        "When using jit.save, please set InputSepc's name in " \
+        "to_static(input_spec=[]) and jit.save(input_spec=[]) " \
+        "and make sure they are consistent."
+    name_no_exists_error = "The tensor `%s` does not exists. " \
+        "Please make sure the name of InputSpec or example Tensor " \
+        "in input_spec is the same as the name of InputSpec in " \
+        "`to_static` decorated on the Layer.forward method."
+    result_list = []
+    input_var_names = [
+        var.name for var in flatten(inputs) if isinstance(var, Variable)
+    ]
+    if input_spec is None:
+        # no prune
+        return input_var_names
+    else:
+        # fileter out non-tensor type spec infos.
+        input_spec = [
+            spec for spec in input_spec
+            if isinstance(spec, paddle.static.InputSpec)
+        ]
+
+    if len(input_spec) == len(input_var_names):
+        # no prune
+        result_list = input_var_names
+        # if input spec name not in input_var_names, only raise warning
+        for spec in input_spec:
+            if spec.name is None:
+                warnings.warn(name_none_error % spec)
+            elif spec.name not in input_var_names:
+                warnings.warn(name_no_exists_error % spec.name)
+            else:
+                # do nothing
+                pass
+    else:
+        # prune
+        for spec in input_spec:
+            if spec.name is None:
+                # name is None, the input_spec only can be InputSpec
+                raise ValueError(name_none_error % spec)
+            elif spec.name not in input_var_names:
+                # the input_spec can be `InputSpec` or `VarBase`
+                raise ValueError(name_no_exists_error % spec.name)
+            else:
+                result_list.append(spec.name)
+
+    return result_list
+
+
+def _get_output_vars(outputs, output_spec):
+    name_no_exists_error = "The tensor `%s` does not exists. " \
+        "Please make sure the name of example Tensor " \
+        "in configs.output_spec is the output tensor of " \
+        "Layer.forward method."
+    result_list = []
+    output_vars_dict = OrderedDict()
+    for var in flatten(outputs):
+        if isinstance(var, Variable):
+            output_vars_dict[var.name] = var
+    if output_spec is None:
+        result_list = output_vars_dict.values()
+    elif output_spec is not None and len(output_spec) == len(output_vars_dict):
+        result_list = output_vars_dict.values()
+        for var in output_spec:
+            if var.name not in output_vars_dict:
+                warnings.warn(name_no_exists_error % var.name)
+    else:
+        for var in output_spec:
+            if var.name not in output_vars_dict:
+                raise ValueError(name_no_exists_error % var.name)
+            else:
+                result_list.append(output_vars_dict[var.name])
+    return result_list
+
+
+@dygraph.base.switch_to_static_graph
+def get_program(layer, input_spec, output_spec, **configs):
+    paddle.jit.set_verbosity(0)
+    prog_translator = ProgramTranslator()
+    if not prog_translator.enable_to_static:
+        raise RuntimeError(
+            "The Paddle2onnx doesn't work when setting ProgramTranslator.enable to False."
+        )
+
+    if not isinstance(layer, Layer):
+        raise TypeError(
+            "The input of paddle2onnx should be 'Layer', but received input type is %s."
+            % type(layer))
+
+    if isinstance(layer, paddle.DataParallel):
+        inner_layer = layer._layers
+    else:
+        inner_layer = layer
+
+    # avoid change user given input_spec
+    inner_input_spec = None
+    if input_spec is not None:
+        for attr_func in dir(inner_layer):
+            static_func = getattr(inner_layer, attr_func, None)
+            if isinstance(static_func,
+                          StaticFunction) and 'forward' != attr_func:
+                raise ValueError(
+                    "If there are static functions other than 'forward' that need to be saved, the input 'input_spec' should be None, but received the type of 'input_spec' is %s."
+                    % type(input_spec))
+
+        if not isinstance(input_spec, (list, tuple)):
+            raise TypeError(
+                "The input input_spec should be 'list', but received input_spec's type is %s."
+                % type(input_spec))
+        inner_input_spec = []
+        for var in flatten(input_spec):
+            if isinstance(var, paddle.static.InputSpec):
+                inner_input_spec.append(var)
+            elif isinstance(var, (core.VarBase, core.eager.Tensor, Variable)):
+                inner_input_spec.append(
+                    paddle.static.InputSpec.from_tensor(var))
+            else:
+                # NOTE(Aurelius84): Support non-Tensor type in `input_spec`.
+                inner_input_spec.append(var)
+
+    extra_var_info = dict()
+    functions = dir(inner_layer)
+    for attr_func in functions:
+        static_func = getattr(inner_layer, attr_func, None)
+        if isinstance(static_func, StaticFunction):
+            concrete_program = static_func.concrete_program_specify_input_spec(
+                inner_input_spec)
+        elif 'forward' == attr_func:
+            # transform in jit.save, if input_spec is incomplete, declarative will throw error
+            # inner_input_spec is list[InputSpec], it should be packed with same structure
+            # as original input_spec here.
+            if inner_input_spec:
+                inner_input_spec = pack_sequence_as(input_spec,
+                                                    inner_input_spec)
+            static_forward = declarative(
+                inner_layer.forward, input_spec=inner_input_spec)
+            concrete_program = static_forward.concrete_program
+            # the input_spec has been used in declarative, which is equal to
+            # @declarative with input_spec and jit.save without input_spec,
+            # avoid needless warning
+            inner_input_spec = None
+        else:
+            continue
+
+        input_var_names = _get_input_var_names(concrete_program.inputs,
+                                               inner_input_spec)
+
+        # NOTE(chenweihang): [ Get output variables ]
+        # the rule is like [ Get input variables name ]. For output var,
+        # we only support VarBase spec, and actually, we only need the
+        # var name of output, and we don't recommended to use output_spec
+        output_vars = _get_output_vars(concrete_program.outputs, output_spec)
+
+    feeded_var_names = input_var_names
+    target_vars = output_vars
+    main_program = concrete_program.main_program.clone()
+    export_for_deployment = True
+
+    if isinstance(feeded_var_names, six.string_types):
+        feeded_var_names = [feeded_var_names]
+    elif export_for_deployment:
+        if len(feeded_var_names) > 0:
+            # TODO(paddle-dev): polish these code blocks
+            if not (bool(feeded_var_names) and all(
+                    isinstance(name, six.string_types)
+                    for name in feeded_var_names)):
+                raise ValueError("'feed_var_names' should be a list of str.")
+
+    if isinstance(target_vars, Variable):
+        target_vars = [target_vars]
+    elif export_for_deployment:
+        if not (bool(target_vars) and
+                all(isinstance(var, Variable) for var in target_vars)):
+            raise ValueError("'target_vars' should be a list of Variable.")
+
+    main_program = _get_valid_program(main_program)
+
+    # remind user to set auc_states to zeros if the program contains auc op
+    all_ops = main_program.global_block().ops
+    for op in all_ops:
+        # clear device of Op
+        device_attr_name = core.op_proto_and_checker_maker.kOpDeviceAttrName()
+        op._set_attr(device_attr_name, "")
+        if op.type == 'auc':
+            warnings.warn(
+                "please ensure that you have set the auc states to zeros before saving inference model"
+            )
+            break
+
+    with program_guard(main_program):
+        uniq_target_vars = []
+        for i, var in enumerate(target_vars):
+            uniq_target_vars.append(var)
+        target_vars = uniq_target_vars
+    target_var_name_list = [var.name for var in target_vars]
+
+    origin_program = main_program.clone()
+
+    main_program = main_program.clone()
+    global_block = main_program.global_block()
+    need_to_remove_op_index = []
+    for i, op in enumerate(global_block.ops):
+        op.desc.set_is_target(False)
+        if op.type == "feed" or op.type == "fetch":
+            need_to_remove_op_index.append(i)
+
+    for index in need_to_remove_op_index[::-1]:
+        global_block._remove_op(index)
+
+    main_program.desc.flush()
+
+    main_program = main_program._prune_with_input(
+        feeded_var_names=feeded_var_names, targets=target_vars)
+    main_program = main_program._inference_optimize(prune_read_op=True)
+    fetch_var_names = [v.name for v in target_vars]
+
+    for target_v in target_vars:
+        if not main_program.global_block().has_var(target_v.name):
+            main_program.global_block().create_var(
+                name=target_v.name,
+                shape=target_v.shape,
+                dtype=target_v.dtype,
+                persistable=target_v.persistable)
+
+    prepend_feed_ops(main_program, feeded_var_names)
+    append_fetch_ops(main_program, fetch_var_names)
+
+    main_program.desc._set_version()
+    paddle.fluid.core.save_op_version_info(main_program.desc)
+
+    main_program._copy_dist_param_info_from(origin_program)
+
+    return main_program, feeded_var_names, target_vars
```

## paddle2onnx/legacy/graph/graph.py

 * *Ordering differences only*

```diff
@@ -1,287 +1,287 @@
-#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import os
-import copy
-import six
-import collections
-from paddle2onnx.legacy.constant import NodeDomain
-
-
-class Node(object):
-    def __init__(self,
-                 op_type,
-                 inputs,
-                 outputs,
-                 attrs,
-                 layer_name,
-                 domain=NodeDomain.RAW):
-        self.domain = domain
-        self.type = op_type
-        self.attrs = attrs
-        self.layer_name = layer_name
-        self.set_inputs(inputs)
-        self.set_outputs(outputs)
-
-    def __hash__(self):
-        return hash(self.layer_name)
-
-    def __eq__(self, other):
-        if self.layer_name == other.layer_name:
-            return True
-        return False
-
-    def __str__(self):
-        node_str = ''
-        attrs = ''
-        for key, value in self.attrs.items():
-            attrs += ', ' + key + '=' + str(value)
-        node_str += "  {} = {}::{}(inputs={}{}) \n".format(
-            self.outputs, self.domain, self.type, self.inputs, attrs)
-        return node_str
-
-    def input(self, idx=None):
-        if idx is None:
-            return self.inputs
-        return self.inputs[idx]
-
-    def output(self, idx=None):
-        if idx is None:
-            return self.outputs
-        return self.outputs[idx]
-
-    def attr(self, name):
-        if name in self.attrs:
-            return self.attrs[name]
-        return None
-
-    def set_inputs(self, inputs):
-        if isinstance(inputs, list):
-            self.inputs = [
-                ipt.layer_name if isinstance(ipt, Node) else ipt
-                for ipt in inputs
-            ]
-        elif isinstance(inputs, six.string_types):
-            self.inputs = [inputs]
-        elif isinstance(inputs, Node):
-            self.inputs = [inputs.layer_name]
-        else:
-            raise TypeError(
-                'Inputs of node must be type: list, Node, or String but got {}'.
-                format(type(inputs)))
-
-    def set_outputs(self, outputs):
-        if isinstance(outputs, list):
-            self.outputs = [
-                opt.layer_name if isinstance(opt, Node) else opt
-                for opt in outputs
-            ]
-        elif isinstance(outputs, six.string_types):
-            self.outputs = [outputs]
-        elif isinstance(outputs, Node):
-            self.outputs = [outputs.layer_name]
-        else:
-            raise TypeError(
-                'Outputs of node must be type: list, Node, or String but got {}'.
-                format(type(outputs)))
-
-
-class Graph(object):
-    def __init__(self):
-        self.parameters = {}
-        self.node_map = collections.OrderedDict()
-        self.input_nodes = list()
-        self.output_nodes = list()
-        self.op_type_count = dict()
-
-    def __hash__(self):
-        return hash(self.id)
-
-    def __eq__(self, other):
-        if self.id == other.id:
-            return True
-        return False
-
-    def __str__(self):
-        graph_str = 'graph { \n'
-        for node in self.input_nodes:
-            graph_str += " input: {} \n".format(node.layer_name)
-        for node in self.output_nodes:
-            graph_str += " output: {} \n \n".format(node.layer_name)
-        for name, node in self.node_map.items():
-            graph_str += node.__str__()
-        graph_str += ' }'
-        return graph_str
-
-    def set_output_nodes(self, node_list):
-        if isinstance(node_list, list):
-            self.output_nodes = node_list
-        else:
-            raise TypeError(
-                'output_nodes of Graph must be type: list, but got {}'.format(
-                    type(node_list)))
-
-    def set_node_map(self, node_map):
-        if isinstance(node_map, dict):
-            self.node_map = node_map
-            self.generate_topo_sort()
-        else:
-            raise TypeError('node_map of Graph must be type: list, but got {}'.
-                            format(type(node_map)))
-
-    def set_input_nodes(self, node_list):
-        if isinstance(node_list, list):
-            self.input_nodes = node_list
-        else:
-            raise TypeError(
-                'input_nodes of Graph must be type: list, but got {}'.format(
-                    type(node_list)))
-
-    def set_parameters(self, parameters):
-        if isinstance(parameters, dict):
-            self.parameters = parameters
-        else:
-            raise TypeError(
-                'parameters of Graph must be type: dict, but got {}'.format(
-                    type(parameters)))
-
-    def generate_node_name(self, op_type):
-        if op_type in self.op_type_count:
-            self.op_type_count[op_type] += 1
-        else:
-            self.op_type_count[op_type] = 1
-        # layer_name need follow https://github.com/onnx/onnx/blob/master/docs/OpConventions.md
-        layer_name = op_type + '_' + str(self.op_type_count[op_type] - 1)
-        return layer_name
-
-    def insert_node(self, node):
-        if node.type not in ['feed', 'fetch']:
-            self.node_map[node.layer_name] = node
-
-    def make_node(self,
-                  op_type,
-                  inputs=None,
-                  outputs=None,
-                  attrs=None,
-                  layer_name=None,
-                  domain=None,
-                  **kw):
-        if layer_name is None:
-            layer_name = self.generate_node_name(op_type)
-
-        if attrs is None:
-            attrs = kw
-        attrs.update(kw)
-
-        if inputs is None:
-            inputs = []
-        if outputs is None:
-            outputs = [layer_name]
-        node = Node(op_type, layer_name, inputs, outputs, attrs, domain)
-        self.insert_node(node)
-        return node
-
-    def update_node(self,
-                    node,
-                    op_type=None,
-                    inputs=None,
-                    outputs=None,
-                    attrs=None,
-                    block=None,
-                    move_to_end=True,
-                    domain=None,
-                    **kw):
-        if op_type is not None:
-            node.type = op_type
-        if inputs is not None:
-            node.set_inputs(inputs)
-        if outputs is not None:
-            node.set_outputs(outputs)
-        if attrs is None:
-            attrs = kw
-        attrs.update(kw)
-        node.attrs = attrs
-        if domain is not None:
-            node.domain = domain
-        if move_to_end:
-            self.node_map.pop(node.layer_name)
-        self.node_map[node.layer_name] = node
-        return node
-
-    def get_node(self, name, copy=False):
-        if name not in self.node_map:
-            raise TypeError('Node with name:{} not in graph'.format(name))
-        if copy:
-            node = copy.copy(self.node_map[name])
-        else:
-            node = self.node_map[name]
-        return node
-
-    def remove_node_by_name(self, name):
-        if name in self.node_map:
-            node = self.node_map.pop(name)
-            return node
-        raise TypeError('Node with name:{} not in graph'.format(name))
-
-    def remove_node(self, node):
-        if isinstance(node, Node):
-            node = self.remove_node_by_name(node.layer_name)
-            return node
-        else:
-            node = self.remove_node_by_name(node)
-            return node
-
-    def get_output_nodes_of_node(self, node):
-        if node in self.edge_map:
-            return self.edge_map[node]
-        elif self.get_node(node.layer_name, copy=False):
-            return []
-        else:
-            raise KeyError('Node with layer_name {} not in graph.egde_map'.
-                           format(node.layer_name))
-
-    def get_adjacency_map(self):
-        adjacency_map = {}
-        for layer_name, current_node in self.node_map.items():
-            inputs = current_node.inputs
-            for ipt in inputs:
-                for layer_name, node in self.node_map.items():
-                    if current_node == node:
-                        continue
-                    outputs = node.outputs
-                    if ipt in outputs:
-                        if node not in adjacency_map:
-                            adjacency_map[node] = set([current_node])
-                        else:
-                            adjacency_map[node].add(current_node)
-        return adjacency_map
-
-    def get_topo_sort_list(self):
-        topo_sort_list = list()
-        adjacency_map = self.get_adjacency_map()
-        for layer_name, node in self.node_map.items():
-            if node not in adjacency_map:
-                topo_sort_list.append(node)
-        idx = 0
-        while idx < len(topo_sort_list):
-            current_node = topo_sort_list[idx]
-            for input_node, output_nodes in adjacency_map.items():
-                if current_node in output_nodes:
-                    adjacency_map[input_node].remove(current_node)
-                    if len(adjacency_map[input_node]) == 0:
-                        topo_sort_list.append(input_node)
-            idx += 1
-        return topo_sort_list[::-1]
+#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import os
+import copy
+import six
+import collections
+from paddle2onnx.legacy.constant import NodeDomain
+
+
+class Node(object):
+    def __init__(self,
+                 op_type,
+                 inputs,
+                 outputs,
+                 attrs,
+                 layer_name,
+                 domain=NodeDomain.RAW):
+        self.domain = domain
+        self.type = op_type
+        self.attrs = attrs
+        self.layer_name = layer_name
+        self.set_inputs(inputs)
+        self.set_outputs(outputs)
+
+    def __hash__(self):
+        return hash(self.layer_name)
+
+    def __eq__(self, other):
+        if self.layer_name == other.layer_name:
+            return True
+        return False
+
+    def __str__(self):
+        node_str = ''
+        attrs = ''
+        for key, value in self.attrs.items():
+            attrs += ', ' + key + '=' + str(value)
+        node_str += "  {} = {}::{}(inputs={}{}) \n".format(
+            self.outputs, self.domain, self.type, self.inputs, attrs)
+        return node_str
+
+    def input(self, idx=None):
+        if idx is None:
+            return self.inputs
+        return self.inputs[idx]
+
+    def output(self, idx=None):
+        if idx is None:
+            return self.outputs
+        return self.outputs[idx]
+
+    def attr(self, name):
+        if name in self.attrs:
+            return self.attrs[name]
+        return None
+
+    def set_inputs(self, inputs):
+        if isinstance(inputs, list):
+            self.inputs = [
+                ipt.layer_name if isinstance(ipt, Node) else ipt
+                for ipt in inputs
+            ]
+        elif isinstance(inputs, six.string_types):
+            self.inputs = [inputs]
+        elif isinstance(inputs, Node):
+            self.inputs = [inputs.layer_name]
+        else:
+            raise TypeError(
+                'Inputs of node must be type: list, Node, or String but got {}'.
+                format(type(inputs)))
+
+    def set_outputs(self, outputs):
+        if isinstance(outputs, list):
+            self.outputs = [
+                opt.layer_name if isinstance(opt, Node) else opt
+                for opt in outputs
+            ]
+        elif isinstance(outputs, six.string_types):
+            self.outputs = [outputs]
+        elif isinstance(outputs, Node):
+            self.outputs = [outputs.layer_name]
+        else:
+            raise TypeError(
+                'Outputs of node must be type: list, Node, or String but got {}'.
+                format(type(outputs)))
+
+
+class Graph(object):
+    def __init__(self):
+        self.parameters = {}
+        self.node_map = collections.OrderedDict()
+        self.input_nodes = list()
+        self.output_nodes = list()
+        self.op_type_count = dict()
+
+    def __hash__(self):
+        return hash(self.id)
+
+    def __eq__(self, other):
+        if self.id == other.id:
+            return True
+        return False
+
+    def __str__(self):
+        graph_str = 'graph { \n'
+        for node in self.input_nodes:
+            graph_str += " input: {} \n".format(node.layer_name)
+        for node in self.output_nodes:
+            graph_str += " output: {} \n \n".format(node.layer_name)
+        for name, node in self.node_map.items():
+            graph_str += node.__str__()
+        graph_str += ' }'
+        return graph_str
+
+    def set_output_nodes(self, node_list):
+        if isinstance(node_list, list):
+            self.output_nodes = node_list
+        else:
+            raise TypeError(
+                'output_nodes of Graph must be type: list, but got {}'.format(
+                    type(node_list)))
+
+    def set_node_map(self, node_map):
+        if isinstance(node_map, dict):
+            self.node_map = node_map
+            self.generate_topo_sort()
+        else:
+            raise TypeError('node_map of Graph must be type: list, but got {}'.
+                            format(type(node_map)))
+
+    def set_input_nodes(self, node_list):
+        if isinstance(node_list, list):
+            self.input_nodes = node_list
+        else:
+            raise TypeError(
+                'input_nodes of Graph must be type: list, but got {}'.format(
+                    type(node_list)))
+
+    def set_parameters(self, parameters):
+        if isinstance(parameters, dict):
+            self.parameters = parameters
+        else:
+            raise TypeError(
+                'parameters of Graph must be type: dict, but got {}'.format(
+                    type(parameters)))
+
+    def generate_node_name(self, op_type):
+        if op_type in self.op_type_count:
+            self.op_type_count[op_type] += 1
+        else:
+            self.op_type_count[op_type] = 1
+        # layer_name need follow https://github.com/onnx/onnx/blob/master/docs/OpConventions.md
+        layer_name = op_type + '_' + str(self.op_type_count[op_type] - 1)
+        return layer_name
+
+    def insert_node(self, node):
+        if node.type not in ['feed', 'fetch']:
+            self.node_map[node.layer_name] = node
+
+    def make_node(self,
+                  op_type,
+                  inputs=None,
+                  outputs=None,
+                  attrs=None,
+                  layer_name=None,
+                  domain=None,
+                  **kw):
+        if layer_name is None:
+            layer_name = self.generate_node_name(op_type)
+
+        if attrs is None:
+            attrs = kw
+        attrs.update(kw)
+
+        if inputs is None:
+            inputs = []
+        if outputs is None:
+            outputs = [layer_name]
+        node = Node(op_type, layer_name, inputs, outputs, attrs, domain)
+        self.insert_node(node)
+        return node
+
+    def update_node(self,
+                    node,
+                    op_type=None,
+                    inputs=None,
+                    outputs=None,
+                    attrs=None,
+                    block=None,
+                    move_to_end=True,
+                    domain=None,
+                    **kw):
+        if op_type is not None:
+            node.type = op_type
+        if inputs is not None:
+            node.set_inputs(inputs)
+        if outputs is not None:
+            node.set_outputs(outputs)
+        if attrs is None:
+            attrs = kw
+        attrs.update(kw)
+        node.attrs = attrs
+        if domain is not None:
+            node.domain = domain
+        if move_to_end:
+            self.node_map.pop(node.layer_name)
+        self.node_map[node.layer_name] = node
+        return node
+
+    def get_node(self, name, copy=False):
+        if name not in self.node_map:
+            raise TypeError('Node with name:{} not in graph'.format(name))
+        if copy:
+            node = copy.copy(self.node_map[name])
+        else:
+            node = self.node_map[name]
+        return node
+
+    def remove_node_by_name(self, name):
+        if name in self.node_map:
+            node = self.node_map.pop(name)
+            return node
+        raise TypeError('Node with name:{} not in graph'.format(name))
+
+    def remove_node(self, node):
+        if isinstance(node, Node):
+            node = self.remove_node_by_name(node.layer_name)
+            return node
+        else:
+            node = self.remove_node_by_name(node)
+            return node
+
+    def get_output_nodes_of_node(self, node):
+        if node in self.edge_map:
+            return self.edge_map[node]
+        elif self.get_node(node.layer_name, copy=False):
+            return []
+        else:
+            raise KeyError('Node with layer_name {} not in graph.egde_map'.
+                           format(node.layer_name))
+
+    def get_adjacency_map(self):
+        adjacency_map = {}
+        for layer_name, current_node in self.node_map.items():
+            inputs = current_node.inputs
+            for ipt in inputs:
+                for layer_name, node in self.node_map.items():
+                    if current_node == node:
+                        continue
+                    outputs = node.outputs
+                    if ipt in outputs:
+                        if node not in adjacency_map:
+                            adjacency_map[node] = set([current_node])
+                        else:
+                            adjacency_map[node].add(current_node)
+        return adjacency_map
+
+    def get_topo_sort_list(self):
+        topo_sort_list = list()
+        adjacency_map = self.get_adjacency_map()
+        for layer_name, node in self.node_map.items():
+            if node not in adjacency_map:
+                topo_sort_list.append(node)
+        idx = 0
+        while idx < len(topo_sort_list):
+            current_node = topo_sort_list[idx]
+            for input_node, output_nodes in adjacency_map.items():
+                if current_node in output_nodes:
+                    adjacency_map[input_node].remove(current_node)
+                    if len(adjacency_map[input_node]) == 0:
+                        topo_sort_list.append(input_node)
+            idx += 1
+        return topo_sort_list[::-1]
```

## paddle2onnx/legacy/graph/graph_helper.py

 * *Ordering differences only*

```diff
@@ -1,83 +1,83 @@
-#   Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import os
-import paddle
-import numpy as np
-from paddle.fluid import core
-from paddle.fluid.framework import Variable, program_guard
-from paddle2onnx.utils import logging
-
-
-def prepend_feed_ops(inference_program,
-                     feed_target_names,
-                     feed_holder_name='feed'):
-    if len(feed_target_names) == 0:
-        return
-    global_block = inference_program.global_block()
-    feed_var = global_block.create_var(
-        name=feed_holder_name,
-        type=core.VarDesc.VarType.FEED_MINIBATCH,
-        persistable=True)
-    for i, name in enumerate(feed_target_names):
-        if not global_block.has_var(name):
-            raise ValueError(
-                "The feed_var_names[{i}]: '{name}' doesn't exist in pruned inference program. "
-                "Please check whether '{name}' is a valid feed_var name, or remove it from feed_var_names "
-                "if '{name}' is not involved in the fetch_vars calculation.".
-                format(
-                    i=i, name=name))
-        out = global_block.var(name)
-        global_block._prepend_op(
-            type='feed',
-            inputs={'X': [feed_var]},
-            outputs={'Out': [out]},
-            attrs={'col': i})
-
-
-def append_fetch_ops(inference_program,
-                     fetch_target_names,
-                     fetch_holder_name='fetch'):
-    global_block = inference_program.global_block()
-    fetch_var = global_block.create_var(
-        name=fetch_holder_name,
-        type=core.VarDesc.VarType.FETCH_LIST,
-        persistable=True)
-    for i, name in enumerate(fetch_target_names):
-        global_block.append_op(
-            type='fetch',
-            inputs={'X': [name]},
-            outputs={'Out': [fetch_var]},
-            attrs={'col': i})
-
-
-def get_program(program, feed_var_names, fetch_vars):
-    global_block = program.global_block()
-    need_to_remove_op_index = []
-    for i, op in enumerate(global_block.ops):
-        op.desc.set_is_target(False)
-        if op.type == "feed" or op.type == "fetch":
-            need_to_remove_op_index.append(i)
-    for index in need_to_remove_op_index[::-1]:
-        global_block._remove_op(index)
-    program.desc.flush()
-    program = program._prune_with_input(
-        feeded_var_names=feed_var_names, targets=fetch_vars)
-    program = program._inference_optimize(prune_read_op=True)
-    fetch_var_names = [v.name for v in fetch_vars]
-    prepend_feed_ops(program, feed_var_names)
-    append_fetch_ops(program, fetch_var_names)
-    return program
+#   Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import os
+import paddle
+import numpy as np
+from paddle.fluid import core
+from paddle.fluid.framework import Variable, program_guard
+from paddle2onnx.utils import logging
+
+
+def prepend_feed_ops(inference_program,
+                     feed_target_names,
+                     feed_holder_name='feed'):
+    if len(feed_target_names) == 0:
+        return
+    global_block = inference_program.global_block()
+    feed_var = global_block.create_var(
+        name=feed_holder_name,
+        type=core.VarDesc.VarType.FEED_MINIBATCH,
+        persistable=True)
+    for i, name in enumerate(feed_target_names):
+        if not global_block.has_var(name):
+            raise ValueError(
+                "The feed_var_names[{i}]: '{name}' doesn't exist in pruned inference program. "
+                "Please check whether '{name}' is a valid feed_var name, or remove it from feed_var_names "
+                "if '{name}' is not involved in the fetch_vars calculation.".
+                format(
+                    i=i, name=name))
+        out = global_block.var(name)
+        global_block._prepend_op(
+            type='feed',
+            inputs={'X': [feed_var]},
+            outputs={'Out': [out]},
+            attrs={'col': i})
+
+
+def append_fetch_ops(inference_program,
+                     fetch_target_names,
+                     fetch_holder_name='fetch'):
+    global_block = inference_program.global_block()
+    fetch_var = global_block.create_var(
+        name=fetch_holder_name,
+        type=core.VarDesc.VarType.FETCH_LIST,
+        persistable=True)
+    for i, name in enumerate(fetch_target_names):
+        global_block.append_op(
+            type='fetch',
+            inputs={'X': [name]},
+            outputs={'Out': [fetch_var]},
+            attrs={'col': i})
+
+
+def get_program(program, feed_var_names, fetch_vars):
+    global_block = program.global_block()
+    need_to_remove_op_index = []
+    for i, op in enumerate(global_block.ops):
+        op.desc.set_is_target(False)
+        if op.type == "feed" or op.type == "fetch":
+            need_to_remove_op_index.append(i)
+    for index in need_to_remove_op_index[::-1]:
+        global_block._remove_op(index)
+    program.desc.flush()
+    program = program._prune_with_input(
+        feeded_var_names=feed_var_names, targets=fetch_vars)
+    program = program._inference_optimize(prune_read_op=True)
+    fetch_var_names = [v.name for v in fetch_vars]
+    prepend_feed_ops(program, feed_var_names)
+    append_fetch_ops(program, fetch_var_names)
+    return program
```

## paddle2onnx/legacy/graph/onnx_graph.py

 * *Ordering differences only*

```diff
@@ -1,333 +1,333 @@
-#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import os
-import copy
-import collections
-import numpy as np
-from paddle2onnx.legacy.graph import Node, Graph
-from paddle2onnx.legacy.constant import NodeDomain, PRODUCER, dtypes
-from paddle2onnx.legacy.op_mapper import OpMapper
-from onnx import helper
-from paddle2onnx.utils import check_model, logging
-
-
-class ONNXNode(Node):
-    def __init__(self, op_type, inputs, outputs, attrs, layer_name, domain):
-        super(ONNXNode, self).__init__(op_type, inputs, outputs, attrs,
-                                       layer_name, domain)
-        self.domain = domain
-        self.onnx_node = self.make_onnx_node()
-
-    def make_onnx_constant_node(self):
-        dtype = self.attr('dtype')
-        value = self.attr('value')
-        if isinstance(value, list):
-            dims = (len(value), )
-        elif value is None:
-            dims = ()
-            value = []
-        else:
-            dims = ()
-            value = [value]
-
-        if 'dims' in self.attrs:
-            dims = self.attrs['dims']
-
-        tensor = helper.make_tensor(
-            name=self.layer_name, data_type=dtype, dims=dims, vals=value)
-
-        onnx_node = helper.make_node(
-            self.type, inputs=self.inputs, outputs=self.outputs, value=tensor)
-
-        return onnx_node
-
-    def make_onnx_node(self):
-        if self.type in ['Constant', 'ConstantOfShape']:
-            onnx_node = self.make_onnx_constant_node()
-        else:
-            onnx_node = helper.make_node(
-                self.type,
-                inputs=self.inputs,
-                outputs=self.outputs,
-                name=self.layer_name,
-                domain=self.domain,
-                **self.attrs)
-        return onnx_node
-
-
-class ONNXGraph(Graph):
-    def __init__(self,
-                 paddle_graph,
-                 opset_version,
-                 operator_export_type="ONNX",
-                 block=None,
-                 auto_update_opset=True):
-        super(ONNXGraph, self).__init__()
-        self.opset_version = opset_version
-        self.operator_export_type = operator_export_type
-        self.ctx = paddle_graph
-        self.custom = []
-        if auto_update_opset:
-            self.update_opset_version()
-
-    def __str__(self):
-        graph_str = 'graph { \n'
-        for node in self.input_nodes:
-            graph_str += " input: {} \n".format(node)
-        for node in self.output_nodes:
-            graph_str += " output: {} \n \n".format(node)
-        for name, node in self.node_map.items():
-            graph_str += node.__str__()
-        graph_str += ' }'
-        return graph_str
-
-    def make_node(self,
-                  op_type,
-                  inputs=[],
-                  outputs=[],
-                  attrs=None,
-                  layer_name=None,
-                  domain=None,
-                  **kw):
-        if layer_name is None:
-            layer_name = self.generate_node_name(op_type)
-
-        if domain is not None:
-            if domain not in self.custom:
-                self.custom.append(domain)
-
-        if attrs is None:
-            attrs = kw
-        attrs.update(kw)
-
-        if inputs is None:
-            inputs = []
-
-        real_outputs = None
-        if outputs is None:
-            real_outputs = [layer_name]
-        elif isinstance(outputs, int):
-            real_outputs = []
-            for i in range(outputs):
-                real_outputs.append(self.generate_node_name(op_type))
-        elif isinstance(outputs, list):
-            real_outputs = []
-            if len(outputs) == 0:
-                real_outputs = [layer_name]
-            else:
-                for opt in outputs:
-                    if isinstance(opt, Node):
-                        real_outputs.append(opt.layer_name)
-                    elif isinstance(opt, int):
-                        real_outputs.append(self.generate_node_name(op_type))
-                    else:
-                        real_outputs.append(opt)
-        else:
-            real_outputs = outputs
-
-        node = ONNXNode(op_type, inputs, real_outputs, attrs, layer_name,
-                        domain)
-
-        self.insert_node(node)
-        if len(node.outputs) == 1:
-            return node.outputs[0]
-        else:
-            return node.outputs
-
-    def update_node(self,
-                    node,
-                    op_type=None,
-                    inputs=None,
-                    outputs=None,
-                    attrs=None,
-                    **kw):
-        if op_type is None:
-            op_type = node.type
-        if inputs is None:
-            inputs = node.inputs
-        if outputs is None:
-            outputs = node.outputs
-        if attrs is None:
-            attrs = node.attrs
-        attrs.update(kw)
-
-        node = ONNXNode(op_type, inputs, outputs, attrs, node.layer_name,
-                        node.domain)
-        self.insert_node(node)
-        return node
-
-    def build_parameters(self, parameters):
-        # build weight nodes
-        for name, param in parameters.items():
-            weight = param['data']
-            if weight is not np.ndarray:
-                weight = np.array(weight)
-            tensor = helper.make_tensor(
-                name=name,
-                dims=param['shape'],
-                data_type=dtypes.DTYPE_PADDLE_ONNX_MAP[param['dtype']],
-                vals=weight.flatten().tolist())
-            node = helper.make_node(
-                'Constant', inputs=[], outputs=[name], value=tensor)
-            self.parameters[name] = node
-
-    def build_input_nodes(self, input_nodes):
-        # build input nodes
-        for ipt in input_nodes:
-            self.add_input_node(ipt.layer_name,
-                                ipt.attr('shape'), ipt.attr('dtype'))
-
-    def build_output_nodes(self, output_nodes):
-        # build output nodes
-        for opt in output_nodes:
-            self.add_output_node(opt.layer_name,
-                                 opt.attr('shape'), opt.attr('dtype'))
-
-    def update_opset_version(self):
-        node_map = self.ctx.node_map
-        self.opset_version = OpMapper.get_recommend_opset_version(
-            node_map, self.opset_version)
-
-    def build_op_nodes(self, node_map):
-        OpMapper.check_support_status(node_map, self.opset_version)
-        # build op nodes
-        for name, node in list(node_map.items()):
-            OpMapper.mapping(self, node, self.operator_export_type)
-
-    def make_value_info(self, name, shape, dtype):
-        tensor_info = helper.make_tensor_value_info(
-            name=name,
-            shape=shape,
-            elem_type=dtypes.DTYPE_PADDLE_ONNX_MAP[dtype])
-        return tensor_info
-
-    def add_input_node(self, name, shape, dtype):
-        vi = self.make_value_info(name, shape, dtype)
-        self.input_nodes.append(vi)
-
-    def add_output_node(self, name, shape, dtype):
-        vi = self.make_value_info(name, shape, dtype)
-        self.output_nodes.append(vi)
-
-    def find_index(self, node_inout, name):
-        for i in range(len(node_inout)):
-            if node_inout[i] == name:
-                return i
-        return -1
-
-    def change_output_names(self, onnx_proto, output_names):
-        logging.info("The output of the ONNX model is set to: {}".format(
-            output_names))
-        if isinstance(output_names, list):
-            assert len(output_names) == len(
-                onnx_proto.graph.output
-            ), "The provided output names are inconsistent with the output number of the onnx model when output_names is list"
-            origin_output_names = []
-            for i in range(len(onnx_proto.graph.output)):
-                origin_output_names.append(onnx_proto.graph.output[i].name)
-                onnx_proto.graph.output[i].name = output_names[i]
-
-            for i in range(len(onnx_proto.graph.node)):
-                node = onnx_proto.graph.node[i]
-                # Prevent changed names from being changed again
-                output_visited_node = []
-                input_visited_node = []
-                for j in range(len(origin_output_names)):
-                    if origin_output_names[j] in node.output:
-                        index = self.find_index(node.output,
-                                                origin_output_names[j])
-                        if index in output_visited_node:
-                            continue
-                        output_visited_node.append(index)
-                        onnx_proto.graph.node[i].output[index] = output_names[j]
-                    if origin_output_names[j] in node.input:
-                        index = self.find_index(node.input,
-                                                origin_output_names[j])
-                        if index in input_visited_node:
-                            continue
-                        input_visited_node.append(index)
-                        onnx_proto.graph.node[i].input[index] = output_names[j]
-        if isinstance(output_names, dict):
-            for i in range(len(onnx_proto.graph.output)):
-                for key, value in output_names.items():
-                    if onnx_proto.graph.output[i].name == key:
-                        onnx_proto.graph.output[i].name = value
-                        break
-
-            for i in range(len(onnx_proto.graph.node)):
-                node = onnx_proto.graph.node[i]
-                # Prevent changed names from being changed again
-                output_visited_node = []
-                input_visited_node = []
-                for key, value in output_names.items():
-                    if key in node.output:
-                        index = self.find_index(node.output, key)
-                        if index in output_visited_node:
-                            continue
-                        output_visited_node.append(index)
-                        onnx_proto.graph.node[i].output[index] = value
-                    if key in node.input:
-                        index = self.find_index(node.input, key)
-                        if index in input_visited_node:
-                            continue
-                        input_visited_node.append(index)
-                        onnx_proto.graph.node[i].input[index] = value
-
-        return onnx_proto
-
-    def export_proto(self, enable_onnx_checker=False, output_names=None):
-
-        op_nodes = [node.onnx_node for node in self.node_map.values()]
-        weight_nodes = [node for node in self.parameters.values()]
-
-        onnx_graph = helper.make_graph(
-            nodes=weight_nodes + op_nodes,
-            name='paddle-onnx',
-            initializer=[],
-            inputs=self.input_nodes,
-            outputs=self.output_nodes)
-
-        opset_imports = [helper.make_opsetid("", self.opset_version)]
-        for custom_domain in self.custom:
-            opset_imports.append(helper.make_opsetid(custom_domain, 1))
-        onnx_proto = helper.make_model(
-            onnx_graph, producer_name=PRODUCER, opset_imports=opset_imports)
-        if output_names is not None:
-            onnx_proto = self.change_output_names(onnx_proto, output_names)
-
-        if enable_onnx_checker:
-            check_model(onnx_proto)
-
-        return onnx_proto
-
-    @staticmethod
-    def build(paddle_graph,
-              opset_version,
-              operator_export_type="ONNX",
-              verbose=False,
-              auto_update_opset=True):
-        onnx_graph = ONNXGraph(
-            paddle_graph,
-            opset_version=opset_version,
-            operator_export_type=operator_export_type,
-            auto_update_opset=auto_update_opset)
-        onnx_graph.build_parameters(paddle_graph.parameters)
-        onnx_graph.build_input_nodes(paddle_graph.input_nodes)
-        onnx_graph.build_output_nodes(paddle_graph.output_nodes)
-        onnx_graph.build_op_nodes(paddle_graph.node_map)
-
-        return onnx_graph
+#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import os
+import copy
+import collections
+import numpy as np
+from paddle2onnx.legacy.graph import Node, Graph
+from paddle2onnx.legacy.constant import NodeDomain, PRODUCER, dtypes
+from paddle2onnx.legacy.op_mapper import OpMapper
+from onnx import helper
+from paddle2onnx.utils import check_model, logging
+
+
+class ONNXNode(Node):
+    def __init__(self, op_type, inputs, outputs, attrs, layer_name, domain):
+        super(ONNXNode, self).__init__(op_type, inputs, outputs, attrs,
+                                       layer_name, domain)
+        self.domain = domain
+        self.onnx_node = self.make_onnx_node()
+
+    def make_onnx_constant_node(self):
+        dtype = self.attr('dtype')
+        value = self.attr('value')
+        if isinstance(value, list):
+            dims = (len(value), )
+        elif value is None:
+            dims = ()
+            value = []
+        else:
+            dims = ()
+            value = [value]
+
+        if 'dims' in self.attrs:
+            dims = self.attrs['dims']
+
+        tensor = helper.make_tensor(
+            name=self.layer_name, data_type=dtype, dims=dims, vals=value)
+
+        onnx_node = helper.make_node(
+            self.type, inputs=self.inputs, outputs=self.outputs, value=tensor)
+
+        return onnx_node
+
+    def make_onnx_node(self):
+        if self.type in ['Constant', 'ConstantOfShape']:
+            onnx_node = self.make_onnx_constant_node()
+        else:
+            onnx_node = helper.make_node(
+                self.type,
+                inputs=self.inputs,
+                outputs=self.outputs,
+                name=self.layer_name,
+                domain=self.domain,
+                **self.attrs)
+        return onnx_node
+
+
+class ONNXGraph(Graph):
+    def __init__(self,
+                 paddle_graph,
+                 opset_version,
+                 operator_export_type="ONNX",
+                 block=None,
+                 auto_update_opset=True):
+        super(ONNXGraph, self).__init__()
+        self.opset_version = opset_version
+        self.operator_export_type = operator_export_type
+        self.ctx = paddle_graph
+        self.custom = []
+        if auto_update_opset:
+            self.update_opset_version()
+
+    def __str__(self):
+        graph_str = 'graph { \n'
+        for node in self.input_nodes:
+            graph_str += " input: {} \n".format(node)
+        for node in self.output_nodes:
+            graph_str += " output: {} \n \n".format(node)
+        for name, node in self.node_map.items():
+            graph_str += node.__str__()
+        graph_str += ' }'
+        return graph_str
+
+    def make_node(self,
+                  op_type,
+                  inputs=[],
+                  outputs=[],
+                  attrs=None,
+                  layer_name=None,
+                  domain=None,
+                  **kw):
+        if layer_name is None:
+            layer_name = self.generate_node_name(op_type)
+
+        if domain is not None:
+            if domain not in self.custom:
+                self.custom.append(domain)
+
+        if attrs is None:
+            attrs = kw
+        attrs.update(kw)
+
+        if inputs is None:
+            inputs = []
+
+        real_outputs = None
+        if outputs is None:
+            real_outputs = [layer_name]
+        elif isinstance(outputs, int):
+            real_outputs = []
+            for i in range(outputs):
+                real_outputs.append(self.generate_node_name(op_type))
+        elif isinstance(outputs, list):
+            real_outputs = []
+            if len(outputs) == 0:
+                real_outputs = [layer_name]
+            else:
+                for opt in outputs:
+                    if isinstance(opt, Node):
+                        real_outputs.append(opt.layer_name)
+                    elif isinstance(opt, int):
+                        real_outputs.append(self.generate_node_name(op_type))
+                    else:
+                        real_outputs.append(opt)
+        else:
+            real_outputs = outputs
+
+        node = ONNXNode(op_type, inputs, real_outputs, attrs, layer_name,
+                        domain)
+
+        self.insert_node(node)
+        if len(node.outputs) == 1:
+            return node.outputs[0]
+        else:
+            return node.outputs
+
+    def update_node(self,
+                    node,
+                    op_type=None,
+                    inputs=None,
+                    outputs=None,
+                    attrs=None,
+                    **kw):
+        if op_type is None:
+            op_type = node.type
+        if inputs is None:
+            inputs = node.inputs
+        if outputs is None:
+            outputs = node.outputs
+        if attrs is None:
+            attrs = node.attrs
+        attrs.update(kw)
+
+        node = ONNXNode(op_type, inputs, outputs, attrs, node.layer_name,
+                        node.domain)
+        self.insert_node(node)
+        return node
+
+    def build_parameters(self, parameters):
+        # build weight nodes
+        for name, param in parameters.items():
+            weight = param['data']
+            if weight is not np.ndarray:
+                weight = np.array(weight)
+            tensor = helper.make_tensor(
+                name=name,
+                dims=param['shape'],
+                data_type=dtypes.DTYPE_PADDLE_ONNX_MAP[param['dtype']],
+                vals=weight.flatten().tolist())
+            node = helper.make_node(
+                'Constant', inputs=[], outputs=[name], value=tensor)
+            self.parameters[name] = node
+
+    def build_input_nodes(self, input_nodes):
+        # build input nodes
+        for ipt in input_nodes:
+            self.add_input_node(ipt.layer_name,
+                                ipt.attr('shape'), ipt.attr('dtype'))
+
+    def build_output_nodes(self, output_nodes):
+        # build output nodes
+        for opt in output_nodes:
+            self.add_output_node(opt.layer_name,
+                                 opt.attr('shape'), opt.attr('dtype'))
+
+    def update_opset_version(self):
+        node_map = self.ctx.node_map
+        self.opset_version = OpMapper.get_recommend_opset_version(
+            node_map, self.opset_version)
+
+    def build_op_nodes(self, node_map):
+        OpMapper.check_support_status(node_map, self.opset_version)
+        # build op nodes
+        for name, node in list(node_map.items()):
+            OpMapper.mapping(self, node, self.operator_export_type)
+
+    def make_value_info(self, name, shape, dtype):
+        tensor_info = helper.make_tensor_value_info(
+            name=name,
+            shape=shape,
+            elem_type=dtypes.DTYPE_PADDLE_ONNX_MAP[dtype])
+        return tensor_info
+
+    def add_input_node(self, name, shape, dtype):
+        vi = self.make_value_info(name, shape, dtype)
+        self.input_nodes.append(vi)
+
+    def add_output_node(self, name, shape, dtype):
+        vi = self.make_value_info(name, shape, dtype)
+        self.output_nodes.append(vi)
+
+    def find_index(self, node_inout, name):
+        for i in range(len(node_inout)):
+            if node_inout[i] == name:
+                return i
+        return -1
+
+    def change_output_names(self, onnx_proto, output_names):
+        logging.info("The output of the ONNX model is set to: {}".format(
+            output_names))
+        if isinstance(output_names, list):
+            assert len(output_names) == len(
+                onnx_proto.graph.output
+            ), "The provided output names are inconsistent with the output number of the onnx model when output_names is list"
+            origin_output_names = []
+            for i in range(len(onnx_proto.graph.output)):
+                origin_output_names.append(onnx_proto.graph.output[i].name)
+                onnx_proto.graph.output[i].name = output_names[i]
+
+            for i in range(len(onnx_proto.graph.node)):
+                node = onnx_proto.graph.node[i]
+                # Prevent changed names from being changed again
+                output_visited_node = []
+                input_visited_node = []
+                for j in range(len(origin_output_names)):
+                    if origin_output_names[j] in node.output:
+                        index = self.find_index(node.output,
+                                                origin_output_names[j])
+                        if index in output_visited_node:
+                            continue
+                        output_visited_node.append(index)
+                        onnx_proto.graph.node[i].output[index] = output_names[j]
+                    if origin_output_names[j] in node.input:
+                        index = self.find_index(node.input,
+                                                origin_output_names[j])
+                        if index in input_visited_node:
+                            continue
+                        input_visited_node.append(index)
+                        onnx_proto.graph.node[i].input[index] = output_names[j]
+        if isinstance(output_names, dict):
+            for i in range(len(onnx_proto.graph.output)):
+                for key, value in output_names.items():
+                    if onnx_proto.graph.output[i].name == key:
+                        onnx_proto.graph.output[i].name = value
+                        break
+
+            for i in range(len(onnx_proto.graph.node)):
+                node = onnx_proto.graph.node[i]
+                # Prevent changed names from being changed again
+                output_visited_node = []
+                input_visited_node = []
+                for key, value in output_names.items():
+                    if key in node.output:
+                        index = self.find_index(node.output, key)
+                        if index in output_visited_node:
+                            continue
+                        output_visited_node.append(index)
+                        onnx_proto.graph.node[i].output[index] = value
+                    if key in node.input:
+                        index = self.find_index(node.input, key)
+                        if index in input_visited_node:
+                            continue
+                        input_visited_node.append(index)
+                        onnx_proto.graph.node[i].input[index] = value
+
+        return onnx_proto
+
+    def export_proto(self, enable_onnx_checker=False, output_names=None):
+
+        op_nodes = [node.onnx_node for node in self.node_map.values()]
+        weight_nodes = [node for node in self.parameters.values()]
+
+        onnx_graph = helper.make_graph(
+            nodes=weight_nodes + op_nodes,
+            name='paddle-onnx',
+            initializer=[],
+            inputs=self.input_nodes,
+            outputs=self.output_nodes)
+
+        opset_imports = [helper.make_opsetid("", self.opset_version)]
+        for custom_domain in self.custom:
+            opset_imports.append(helper.make_opsetid(custom_domain, 1))
+        onnx_proto = helper.make_model(
+            onnx_graph, producer_name=PRODUCER, opset_imports=opset_imports)
+        if output_names is not None:
+            onnx_proto = self.change_output_names(onnx_proto, output_names)
+
+        if enable_onnx_checker:
+            check_model(onnx_proto)
+
+        return onnx_proto
+
+    @staticmethod
+    def build(paddle_graph,
+              opset_version,
+              operator_export_type="ONNX",
+              verbose=False,
+              auto_update_opset=True):
+        onnx_graph = ONNXGraph(
+            paddle_graph,
+            opset_version=opset_version,
+            operator_export_type=operator_export_type,
+            auto_update_opset=auto_update_opset)
+        onnx_graph.build_parameters(paddle_graph.parameters)
+        onnx_graph.build_input_nodes(paddle_graph.input_nodes)
+        onnx_graph.build_output_nodes(paddle_graph.output_nodes)
+        onnx_graph.build_op_nodes(paddle_graph.node_map)
+
+        return onnx_graph
```

## paddle2onnx/legacy/graph/paddle_graph.py

 * *Ordering differences only*

```diff
@@ -1,303 +1,303 @@
-#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import os
-import copy
-import collections
-import numpy as np
-import paddle
-from paddle import fluid
-from paddle.fluid import dygraph
-from paddle.fluid.framework import Operator
-from paddle2onnx.legacy.graph import Node, Graph
-from paddle2onnx.legacy.constant import NodeDomain
-from paddle2onnx.utils import logging
-
-
-class PaddleNode(Node):
-    def __init__(self, paddle_op, inputs, outputs, attrs, layer_name, block):
-        super(PaddleNode, self).__init__(paddle_op.type, inputs, outputs, attrs,
-                                         layer_name, NodeDomain.PADDLE)
-        self.paddle_op = paddle_op
-        self.block = block
-
-    def __str__(self):
-        node_str = ''
-        attrs = ''
-        for key, value in self.attrs.items():
-            if key == 'op_callstack':
-                continue
-            attrs += ', ' + key + '=' + str(value)
-        node_str += "  {} = {}::{}(inputs={}{}) \n".format(
-            self.outputs, self.domain, self.type, self.inputs, attrs)
-        return node_str
-
-    @property
-    def input_names(self):
-        return [name for name in self.inputs.keys()]
-
-    @property
-    def output_names(self):
-        return [name for name in self.outputs.keys()]
-
-    def input(self, name, idx=None):
-        if name not in self.inputs:
-            return None
-        if idx is None:
-            return self.inputs[name]
-        if len(self.inputs[name]) <= idx:
-            return None
-        return self.inputs[name][idx]
-
-    def output(self, name, idx=None):
-        if idx is None:
-            return self.outputs[name]
-        return self.outputs[name][idx]
-
-    def output_shape(self, name, idx):
-        return self.block.var(self.output(name, idx)).shape
-
-    def input_shape(self, name, idx):
-        return self.block.var(self.input(name, idx)).shape
-
-    def input_var(self, name, idx):
-        return self.block.var(self.input(name, idx))
-
-    def input_dtype(self, name, idx):
-        return self.block.var(self.input(name, idx)).dtype
-
-    def output_dtype(self, name, idx):
-        return self.block.var(self.output(name, idx)).dtype
-
-    def attr(self, name, default=None):
-        if name in self.attrs:
-            return self.attrs[name]
-        return default
-
-    def set_inputs(self, inputs):
-        if isinstance(inputs, dict):
-            # input of node in paddle, which stored by dict 
-            self.inputs = inputs
-        else:
-            raise TypeError('Inputs of node must be type: dict, but got {}'.
-                            format(type(inputs)))
-
-    def set_outputs(self, outputs):
-        if isinstance(outputs, dict):
-            # output of node in paddle, which stored by dict 
-            self.outputs = outputs
-        else:
-            raise TypeError('Outputs of node must be type: dict, but got {}'.
-                            format(type(outputs)))
-
-
-class PaddleGraph(Graph):
-    def __init__(self, program, parameters, feed_var_names, fetch_vars):
-        super(PaddleGraph, self).__init__()
-        self.build_graph(program, parameters, feed_var_names, fetch_vars)
-
-    def make_node(self,
-                  op,
-                  inputs=None,
-                  outputs=None,
-                  attrs=None,
-                  block=None,
-                  layer_name=None,
-                  **kw):
-        if layer_name is None:
-            layer_name = self.generate_node_name(op.type)
-
-        if attrs is None:
-            attrs = kw
-        attrs.update(kw)
-
-        if inputs is None:
-            inputs = {}
-        if outputs is None:
-            outputs = {'Out': layer_name}
-        node = PaddleNode(op, inputs, outputs, attrs, layer_name, block)
-        self.insert_node(node)
-        return node
-
-    def add_input_node(self, inputs, block=None):
-        for ipt in inputs:
-            # parse feed_names
-            layer_name = ipt
-            var = block.var(ipt)
-            attrs = {}
-            attrs['shape'] = var.shape
-            attrs['dtype'] = var.dtype
-            node = Node('feed', [], [layer_name], attrs, layer_name)
-            self.input_nodes.append(node)
-
-    def add_output_node(self, outputs, block=None):
-        from paddle.fluid.framework import Variable
-        for opt in outputs:
-            # parse fetch_target_vars 
-            layer_name = opt.name
-            attrs = {}
-            attrs['shape'] = opt.shape
-            attrs['dtype'] = opt.dtype
-            node = Node('fetch', [layer_name], [], attrs, layer_name)
-            self.output_nodes.append(node)
-
-    def get_adjacency_map(self):
-        adjacency_map = {}
-        for layer_name, current_node in self.node_map.items():
-            inputs = current_node.inputs.values()
-            inputs = [x for j in inputs for x in j]
-            for ipt in inputs:
-                for layer_name, node in self.node_map.items():
-                    if current_node == node:
-                        continue
-                    outputs = node.outputs.values()
-                    outputs = [x for j in outputs for x in j]
-                    if ipt in outputs:
-                        if node not in adjacency_map:
-                            adjacency_map[node] = set([current_node])
-                        else:
-                            adjacency_map[node].add(current_node)
-        return adjacency_map
-
-    def build_graph(self,
-                    program,
-                    parameters,
-                    feed_var_names=None,
-                    target_vars=None):
-        self.program = program
-        self.set_parameters(parameters)
-        self.add_input_node(feed_var_names, program.global_block())
-        self.add_output_node(target_vars, program.global_block())
-        for block in program.blocks:
-            for i, op in enumerate(block.ops):
-                if op.type in ['feed', 'fetch']:
-                    continue
-                else:
-                    inputs = {}
-                    outputs = {}
-                    for ipt in op.input_names:
-                        inputs[ipt] = op.input(ipt)
-                    for opt in op.output_names:
-                        outputs[opt] = op.output(opt)
-                    node = self.make_node(op, inputs, outputs,
-                                          op.all_attrs(), block)
-
-    @staticmethod
-    def build_from_program(program,
-                           feed_var_names=None,
-                           fetch_vars=None,
-                           scope=None):
-        parameters_dict = {}
-        vars = program.global_block().vars
-        for name in vars:
-            var = program.global_block().var(name)
-            if name.endswith('feed') or name.endswith('fetch'):
-                continue
-            if not var.persistable:
-                continue
-            parameters_dict[name] = {
-                'data': np.array(scope.var(name).get_tensor()),
-                'dtype': var.dtype,
-                'shape': var.shape
-            }
-
-        graph = PaddleGraph(program, parameters_dict, feed_var_names,
-                            fetch_vars)
-        return graph
-
-    @staticmethod
-    def build_from_dygraph(layer, input_spec=None, output_spec=None):
-        from paddle.nn import Layer
-        from paddle.fluid import core
-        from paddle.fluid.framework import Variable
-        from paddle2onnx.legacy.graph import dygraph_helper as dg_helper
-        if isinstance(layer, dygraph.TranslatedLayer):
-            program = layer.program()
-            parameters_dict = {}
-            pruned_vars = program.global_block().vars
-            for param in layer.parameters():
-                if param.name.endswith('feed') or param.name.endswith('fetch'):
-                    continue
-                if not param.persistable:
-                    continue
-                if param.name in pruned_vars:
-                    parameters_dict[param.name] = {
-                        'data': np.array(param.value().get_tensor()),
-                        'dtype': param.dtype,
-                        'shape': param.shape
-                    }
-            for param in layer.buffers():
-                if param.name.endswith('feed') or param.name.endswith('fetch'):
-                    continue
-                if not param.value().get_tensor()._is_initialized():
-                    continue
-                if param.name in pruned_vars:
-                    parameters_dict[param.name] = {
-                        'data': np.array(param.value().get_tensor()),
-                        'dtype': param.dtype,
-                        'shape': param.shape
-                    }
-            if input_spec is not None:
-                logging.warning(
-                    "Although input_spec is specified, TranslatedLayer is not support prune. An Complete network will be exported."
-                )
-                input_spec = layer._input_spec()
-            if output_spec is not None:
-                logging.warning(
-                    "Although output_spec is specified, TranslatedLayer is not support prune. An Complete network will be exported."
-                )
-            feed_var_names = [ipt.name for ipt in layer._input_spec()]
-            fetch_vars = [
-                program.global_block().var(opt.name)
-                for opt in layer._output_spec()
-            ]
-            graph = PaddleGraph(program, parameters_dict, feed_var_names,
-                                fetch_vars)
-            return graph
-        elif isinstance(layer, Layer):
-            program, feed_var_names, fetch_vars = dg_helper.get_program(
-                layer, input_spec, output_spec)
-            parameters_dict = {}
-            pruned_vars = program.global_block().vars
-            for param in layer.parameters():
-                if param.name.endswith('feed') or param.name.endswith('fetch'):
-                    continue
-                if not param.persistable:
-                    continue
-                if param.name in pruned_vars:
-                    parameters_dict[param.name] = {
-                        'data': np.array(param.value().get_tensor()),
-                        'dtype': param.dtype,
-                        'shape': param.shape
-                    }
-            for param in layer.buffers():
-                if param.name.endswith('feed') or param.name.endswith('fetch'):
-                    continue
-                if not param.value().get_tensor()._is_initialized():
-                    continue
-                if param.name in pruned_vars:
-                    parameters_dict[param.name] = {
-                        'data': np.array(param.value().get_tensor()),
-                        'dtype': param.dtype,
-                        'shape': param.shape
-                    }
-            graph = PaddleGraph(program, parameters_dict, feed_var_names,
-                                fetch_vars)
-            return graph
-        else:
-            raise TypeError(
-                "The input Layer should be 'Layer' or 'TranslatedLayer', but received  type is %s."
-                % type(layer))
+#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import os
+import copy
+import collections
+import numpy as np
+import paddle
+from paddle import fluid
+from paddle.fluid import dygraph
+from paddle.fluid.framework import Operator
+from paddle2onnx.legacy.graph import Node, Graph
+from paddle2onnx.legacy.constant import NodeDomain
+from paddle2onnx.utils import logging
+
+
+class PaddleNode(Node):
+    def __init__(self, paddle_op, inputs, outputs, attrs, layer_name, block):
+        super(PaddleNode, self).__init__(paddle_op.type, inputs, outputs, attrs,
+                                         layer_name, NodeDomain.PADDLE)
+        self.paddle_op = paddle_op
+        self.block = block
+
+    def __str__(self):
+        node_str = ''
+        attrs = ''
+        for key, value in self.attrs.items():
+            if key == 'op_callstack':
+                continue
+            attrs += ', ' + key + '=' + str(value)
+        node_str += "  {} = {}::{}(inputs={}{}) \n".format(
+            self.outputs, self.domain, self.type, self.inputs, attrs)
+        return node_str
+
+    @property
+    def input_names(self):
+        return [name for name in self.inputs.keys()]
+
+    @property
+    def output_names(self):
+        return [name for name in self.outputs.keys()]
+
+    def input(self, name, idx=None):
+        if name not in self.inputs:
+            return None
+        if idx is None:
+            return self.inputs[name]
+        if len(self.inputs[name]) <= idx:
+            return None
+        return self.inputs[name][idx]
+
+    def output(self, name, idx=None):
+        if idx is None:
+            return self.outputs[name]
+        return self.outputs[name][idx]
+
+    def output_shape(self, name, idx):
+        return self.block.var(self.output(name, idx)).shape
+
+    def input_shape(self, name, idx):
+        return self.block.var(self.input(name, idx)).shape
+
+    def input_var(self, name, idx):
+        return self.block.var(self.input(name, idx))
+
+    def input_dtype(self, name, idx):
+        return self.block.var(self.input(name, idx)).dtype
+
+    def output_dtype(self, name, idx):
+        return self.block.var(self.output(name, idx)).dtype
+
+    def attr(self, name, default=None):
+        if name in self.attrs:
+            return self.attrs[name]
+        return default
+
+    def set_inputs(self, inputs):
+        if isinstance(inputs, dict):
+            # input of node in paddle, which stored by dict 
+            self.inputs = inputs
+        else:
+            raise TypeError('Inputs of node must be type: dict, but got {}'.
+                            format(type(inputs)))
+
+    def set_outputs(self, outputs):
+        if isinstance(outputs, dict):
+            # output of node in paddle, which stored by dict 
+            self.outputs = outputs
+        else:
+            raise TypeError('Outputs of node must be type: dict, but got {}'.
+                            format(type(outputs)))
+
+
+class PaddleGraph(Graph):
+    def __init__(self, program, parameters, feed_var_names, fetch_vars):
+        super(PaddleGraph, self).__init__()
+        self.build_graph(program, parameters, feed_var_names, fetch_vars)
+
+    def make_node(self,
+                  op,
+                  inputs=None,
+                  outputs=None,
+                  attrs=None,
+                  block=None,
+                  layer_name=None,
+                  **kw):
+        if layer_name is None:
+            layer_name = self.generate_node_name(op.type)
+
+        if attrs is None:
+            attrs = kw
+        attrs.update(kw)
+
+        if inputs is None:
+            inputs = {}
+        if outputs is None:
+            outputs = {'Out': layer_name}
+        node = PaddleNode(op, inputs, outputs, attrs, layer_name, block)
+        self.insert_node(node)
+        return node
+
+    def add_input_node(self, inputs, block=None):
+        for ipt in inputs:
+            # parse feed_names
+            layer_name = ipt
+            var = block.var(ipt)
+            attrs = {}
+            attrs['shape'] = var.shape
+            attrs['dtype'] = var.dtype
+            node = Node('feed', [], [layer_name], attrs, layer_name)
+            self.input_nodes.append(node)
+
+    def add_output_node(self, outputs, block=None):
+        from paddle.fluid.framework import Variable
+        for opt in outputs:
+            # parse fetch_target_vars 
+            layer_name = opt.name
+            attrs = {}
+            attrs['shape'] = opt.shape
+            attrs['dtype'] = opt.dtype
+            node = Node('fetch', [layer_name], [], attrs, layer_name)
+            self.output_nodes.append(node)
+
+    def get_adjacency_map(self):
+        adjacency_map = {}
+        for layer_name, current_node in self.node_map.items():
+            inputs = current_node.inputs.values()
+            inputs = [x for j in inputs for x in j]
+            for ipt in inputs:
+                for layer_name, node in self.node_map.items():
+                    if current_node == node:
+                        continue
+                    outputs = node.outputs.values()
+                    outputs = [x for j in outputs for x in j]
+                    if ipt in outputs:
+                        if node not in adjacency_map:
+                            adjacency_map[node] = set([current_node])
+                        else:
+                            adjacency_map[node].add(current_node)
+        return adjacency_map
+
+    def build_graph(self,
+                    program,
+                    parameters,
+                    feed_var_names=None,
+                    target_vars=None):
+        self.program = program
+        self.set_parameters(parameters)
+        self.add_input_node(feed_var_names, program.global_block())
+        self.add_output_node(target_vars, program.global_block())
+        for block in program.blocks:
+            for i, op in enumerate(block.ops):
+                if op.type in ['feed', 'fetch']:
+                    continue
+                else:
+                    inputs = {}
+                    outputs = {}
+                    for ipt in op.input_names:
+                        inputs[ipt] = op.input(ipt)
+                    for opt in op.output_names:
+                        outputs[opt] = op.output(opt)
+                    node = self.make_node(op, inputs, outputs,
+                                          op.all_attrs(), block)
+
+    @staticmethod
+    def build_from_program(program,
+                           feed_var_names=None,
+                           fetch_vars=None,
+                           scope=None):
+        parameters_dict = {}
+        vars = program.global_block().vars
+        for name in vars:
+            var = program.global_block().var(name)
+            if name.endswith('feed') or name.endswith('fetch'):
+                continue
+            if not var.persistable:
+                continue
+            parameters_dict[name] = {
+                'data': np.array(scope.var(name).get_tensor()),
+                'dtype': var.dtype,
+                'shape': var.shape
+            }
+
+        graph = PaddleGraph(program, parameters_dict, feed_var_names,
+                            fetch_vars)
+        return graph
+
+    @staticmethod
+    def build_from_dygraph(layer, input_spec=None, output_spec=None):
+        from paddle.nn import Layer
+        from paddle.fluid import core
+        from paddle.fluid.framework import Variable
+        from paddle2onnx.legacy.graph import dygraph_helper as dg_helper
+        if isinstance(layer, dygraph.TranslatedLayer):
+            program = layer.program()
+            parameters_dict = {}
+            pruned_vars = program.global_block().vars
+            for param in layer.parameters():
+                if param.name.endswith('feed') or param.name.endswith('fetch'):
+                    continue
+                if not param.persistable:
+                    continue
+                if param.name in pruned_vars:
+                    parameters_dict[param.name] = {
+                        'data': np.array(param.value().get_tensor()),
+                        'dtype': param.dtype,
+                        'shape': param.shape
+                    }
+            for param in layer.buffers():
+                if param.name.endswith('feed') or param.name.endswith('fetch'):
+                    continue
+                if not param.value().get_tensor()._is_initialized():
+                    continue
+                if param.name in pruned_vars:
+                    parameters_dict[param.name] = {
+                        'data': np.array(param.value().get_tensor()),
+                        'dtype': param.dtype,
+                        'shape': param.shape
+                    }
+            if input_spec is not None:
+                logging.warning(
+                    "Although input_spec is specified, TranslatedLayer is not support prune. An Complete network will be exported."
+                )
+                input_spec = layer._input_spec()
+            if output_spec is not None:
+                logging.warning(
+                    "Although output_spec is specified, TranslatedLayer is not support prune. An Complete network will be exported."
+                )
+            feed_var_names = [ipt.name for ipt in layer._input_spec()]
+            fetch_vars = [
+                program.global_block().var(opt.name)
+                for opt in layer._output_spec()
+            ]
+            graph = PaddleGraph(program, parameters_dict, feed_var_names,
+                                fetch_vars)
+            return graph
+        elif isinstance(layer, Layer):
+            program, feed_var_names, fetch_vars = dg_helper.get_program(
+                layer, input_spec, output_spec)
+            parameters_dict = {}
+            pruned_vars = program.global_block().vars
+            for param in layer.parameters():
+                if param.name.endswith('feed') or param.name.endswith('fetch'):
+                    continue
+                if not param.persistable:
+                    continue
+                if param.name in pruned_vars:
+                    parameters_dict[param.name] = {
+                        'data': np.array(param.value().get_tensor()),
+                        'dtype': param.dtype,
+                        'shape': param.shape
+                    }
+            for param in layer.buffers():
+                if param.name.endswith('feed') or param.name.endswith('fetch'):
+                    continue
+                if not param.value().get_tensor()._is_initialized():
+                    continue
+                if param.name in pruned_vars:
+                    parameters_dict[param.name] = {
+                        'data': np.array(param.value().get_tensor()),
+                        'dtype': param.dtype,
+                        'shape': param.shape
+                    }
+            graph = PaddleGraph(program, parameters_dict, feed_var_names,
+                                fetch_vars)
+            return graph
+        else:
+            raise TypeError(
+                "The input Layer should be 'Layer' or 'TranslatedLayer', but received  type is %s."
+                % type(layer))
```

## paddle2onnx/legacy/op_mapper/__init__.py

 * *Ordering differences only*

```diff
@@ -1,39 +1,39 @@
-#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-from .op_mapper import OpMapper, register_op_mapper, CustomPaddleOp, register_custom_paddle_op
-
-from . import nn
-from . import math
-from . import activation
-from . import tensor
-from . import logic
-from . import search
-
-from .detection import yolo_box
-from .detection import multiclass_nms
-from .detection import prior_box
-from .detection import density_prior_box
-from .detection import box_coder
-from .sequence import im2sequence
-
-from .custom_paddle_op import deformable_conv
-from .custom_paddle_op import anchor_generator
-from .custom_paddle_op import generate_proposals
-from .custom_paddle_op import collect_fpn_proposals
-from .custom_paddle_op import distribute_fpn_proposals
-from .custom_paddle_op import box_clip
-from .custom_paddle_op import grid_sampler
+#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+from .op_mapper import OpMapper, register_op_mapper, CustomPaddleOp, register_custom_paddle_op
+
+from . import nn
+from . import math
+from . import activation
+from . import tensor
+from . import logic
+from . import search
+
+from .detection import yolo_box
+from .detection import multiclass_nms
+from .detection import prior_box
+from .detection import density_prior_box
+from .detection import box_coder
+from .sequence import im2sequence
+
+from .custom_paddle_op import deformable_conv
+from .custom_paddle_op import anchor_generator
+from .custom_paddle_op import generate_proposals
+from .custom_paddle_op import collect_fpn_proposals
+from .custom_paddle_op import distribute_fpn_proposals
+from .custom_paddle_op import box_clip
+from .custom_paddle_op import grid_sampler
```

## paddle2onnx/legacy/op_mapper/activation.py

 * *Ordering differences only*

```diff
@@ -1,269 +1,269 @@
-#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import numpy as np
-import math
-from paddle2onnx.legacy.constant import dtypes
-from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
-from paddle2onnx.legacy.op_mapper import mapper_helper
-import paddle
-
-
-@op_mapper(
-    ['relu', 'tanh', 'log', 'sigmoid', 'sqrt'],
-    mapper_dict={
-        'relu': 'Relu',
-        'tanh': 'Tanh',
-        'log': 'Log',
-        'sigmoid': 'Sigmoid',
-        'sqrt': 'Sqrt',
-    })
-class ActivationOps():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        onnx_type = kw['mapper_dict'][node.type]
-        onnx_node = graph.make_node(
-            onnx_type, inputs=node.input('X'), outputs=node.output('Out'))
-
-
-@op_mapper('silu')
-class Silu():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        x = node.input('X')[0]
-        out = graph.make_node('Sigmoid', inputs=[x])
-        graph.make_node('Mul', inputs=[x, out], outputs=node.output('Out'))
-
-@op_mapper('leaky_relu')
-class LeakyRelu():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        onnx_node = graph.make_node(
-            'LeakyRelu',
-            inputs=[node.input('X')[0]],
-            outputs=node.output('Out'),
-            alpha=node.attr('alpha'))
-
-
-@op_mapper('softplus')
-class Softplus():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        beta = node.attr('beta')
-        threshold = node.attr('threshold')
-        if np.isclose(beta, 1.0, 1e-06, 1e-06) and \
-            np.isclose(threshold, 20.0, 1e-06, 1e-06):
-            onnx_node = graph.make_node(
-                'Softplus',
-                inputs=[node.input('X')[0]],
-                outputs=node.output('Out'))
-        else:
-            raise Exception("[ERROR] Operator softplus " \
-            "only supported while beta==1.0 and threshold==20.0")
-
-
-@op_mapper('prelu')
-class PRelu():
-    support_opset_version_range = (9, 15)
-
-    @classmethod
-    def opset_9(cls, graph, node, **kw):
-        slope_shape = node.input_shape('Alpha', 0)
-        input_shape = node.input_shape('X', 0)
-
-        slope_node = node.input('Alpha')[0]
-        if len(input_shape) != len(slope_shape):
-            assert len(
-                slope_shape) == 1, "Slope shape is not expected for prelu"
-            broadcast_shape = [-1] + [1] * (len(input_shape) - 2)
-            broadcast_shape = graph.make_node(
-                'Constant', dtype=dtypes.ONNX.INT64, value=broadcast_shape)
-            slope_node = graph.make_node(
-                'Reshape', inputs=[node.input('Alpha')[0], broadcast_shape])
-        x = node.input('X')[0]
-        x_dtype = node.input_dtype('X', 0)
-        slope_dtype = node.input_dtype('Alpha', 0)
-        if slope_dtype != paddle.float32:
-            slope_node = graph.make_node(
-                'Cast', inputs=[slope_node], to=dtypes.ONNX.FLOAT)
-        if x_dtype != paddle.float32:
-            x = graph.make_node('Cast', inputs=[x], to=dtypes.ONNX.FLOAT)
-            onnx_node = graph.make_node('PRelu', inputs=[x, slope_node])
-            graph.make_node(
-                'Cast',
-                inputs=[onnx_node],
-                outputs=node.output('Out'),
-                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])
-        else:
-            onnx_node = graph.make_node(
-                'PRelu', inputs=[x, slope_node], outputs=node.output('Out'))
-
-
-@op_mapper('relu6')
-class Relu6():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        mapper_helper.clip_helper(graph, node,
-                                  node.input('X', 0),
-                                  node.attr('threshold'), 0.0,
-                                  node.output('Out', 0))
-
-
-@op_mapper('gelu')
-class Gelu():
-    support_opset_version_range = (9, 15)
-
-    @classmethod
-    def opset_9(cls, graph, node, **kw):
-        input = node.input('X', 0)
-        x_dtype = node.input_dtype('X', 0)
-        # onnxruntime only support float32 Erf
-        if x_dtype != paddle.float32:
-            input = graph.make_node(
-                'Cast', inputs=[input], to=dtypes.ONNX.FLOAT)
-        sqrt2 = graph.make_node(
-            'Constant', dtype=dtypes.ONNX.FLOAT, value=[1.4142135623730951])
-        zero_point_five = graph.make_node(
-            'Constant', dtype=dtypes.ONNX.FLOAT, value=[0.5])
-        one = graph.make_node('Constant', dtype=dtypes.ONNX.FLOAT, value=[1])
-        x = graph.make_node('Div', inputs=[input, sqrt2])
-        x = graph.make_node('Erf', inputs=x)
-        x = graph.make_node('Add', inputs=[x, one])
-        x = graph.make_node('Mul', inputs=[input, x])
-        if x_dtype != paddle.float32:
-            mul_node = graph.make_node('Mul', inputs=[x, zero_point_five])
-            graph.make_node(
-                'Cast',
-                inputs=[mul_node],
-                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype],
-                outputs=node.output('Out'))
-        else:
-            graph.make_node(
-                'Mul', inputs=[x, zero_point_five], outputs=node.output('Out'))
-
-
-@op_mapper('selu')
-class Selu():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_6(cls, graph, node, **kw):
-        graph.make_node(
-            'Selu',
-            inputs=node.input('X'),
-            alpha=node.attr('alpha'),
-            gamma=node.attr('scale'),
-            outputs=node.output('Out'))
-
-
-@op_mapper('hard_sigmoid')
-class HardSigmoid():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        slope = node.attr('slope')
-        offset = node.attr('offset')
-        graph.make_node(
-            'HardSigmoid',
-            inputs=node.input('X'),
-            outputs=node.output('Out'),
-            alpha=slope,
-            beta=offset)
-
-
-@op_mapper('swish')
-class Swish():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        x = node.input('X')[0]
-        if math.fabs(node.attr("beta") - 1.0) > 1e-05:
-            beta_node = graph.make_node(
-                'Constant',
-                attrs={'dtype': dtypes.ONNX.FLOAT,
-                       'value': [node.attr('beta')]})
-            x = graph.make_node(
-                'Mul', inputs=[x, beta_node])
-        sigmoid_node = graph.make_node('Sigmoid', inputs=[x])
-        graph.make_node(
-            'Mul',
-            inputs=[x, sigmoid_node],
-            outputs=node.output('Out'))
-
-
-@op_mapper('mish')
-class Mish():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        inputs = node.input('X', 0)
-        dtype = node.input_dtype("X", 0)
-        if dtype != paddle.float32:
-            inputs = graph.make_node(
-                'Cast', inputs=[inputs], to=dtypes.ONNX.FLOAT)
-            dtype = paddle.float32
-        threshold = node.attr('threshold')
-        assert np.fabs(
-            threshold - 20
-        ) < 1e-4, "In mish OP, the threshold only supports 20, no other values are supported"
-        softplus_node = graph.make_node('Softplus', inputs=[inputs])
-        tanh_node = graph.make_node('Tanh', inputs=[softplus_node])
-        if node.input_dtype("X", 0) != paddle.float32:
-            mul_node = graph.make_node('Mul', inputs=[inputs, tanh_node])
-            inputs = graph.make_node(
-                'Cast',
-                inputs=[mul_node],
-                to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype("X", 0)],
-                outputs=node.output('Out'))
-        else:
-            graph.make_node(
-                'Mul', inputs=[inputs, tanh_node], outputs=node.output('Out'))
-
-
-@op_mapper('hard_swish')
-class HardSwish():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        scale_node = graph.make_node(
-            'Constant',
-            attrs={'dtype': dtypes.ONNX.FLOAT,
-                   'value': node.attr('scale')})
-        offset_node = graph.make_node(
-            'Constant',
-            attrs={'dtype': dtypes.ONNX.FLOAT,
-                   'value': node.attr('offset')})
-
-        node0 = graph.make_node('Add', inputs=[node.input('X')[0], offset_node])
-        node1 = mapper_helper.clip_helper(graph, node, node0,
-                                          node.attr('threshold'), 0.0)
-        node2 = graph.make_node('Mul', inputs=[node.input('X')[0], node1])
-        node3 = graph.make_node(
-            'Div', inputs=[node2, scale_node], outputs=node.output('Out'))
+#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import numpy as np
+import math
+from paddle2onnx.legacy.constant import dtypes
+from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
+from paddle2onnx.legacy.op_mapper import mapper_helper
+import paddle
+
+
+@op_mapper(
+    ['relu', 'tanh', 'log', 'sigmoid', 'sqrt'],
+    mapper_dict={
+        'relu': 'Relu',
+        'tanh': 'Tanh',
+        'log': 'Log',
+        'sigmoid': 'Sigmoid',
+        'sqrt': 'Sqrt',
+    })
+class ActivationOps():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        onnx_type = kw['mapper_dict'][node.type]
+        onnx_node = graph.make_node(
+            onnx_type, inputs=node.input('X'), outputs=node.output('Out'))
+
+
+@op_mapper('silu')
+class Silu():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        x = node.input('X')[0]
+        out = graph.make_node('Sigmoid', inputs=[x])
+        graph.make_node('Mul', inputs=[x, out], outputs=node.output('Out'))
+
+@op_mapper('leaky_relu')
+class LeakyRelu():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        onnx_node = graph.make_node(
+            'LeakyRelu',
+            inputs=[node.input('X')[0]],
+            outputs=node.output('Out'),
+            alpha=node.attr('alpha'))
+
+
+@op_mapper('softplus')
+class Softplus():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        beta = node.attr('beta')
+        threshold = node.attr('threshold')
+        if np.isclose(beta, 1.0, 1e-06, 1e-06) and \
+            np.isclose(threshold, 20.0, 1e-06, 1e-06):
+            onnx_node = graph.make_node(
+                'Softplus',
+                inputs=[node.input('X')[0]],
+                outputs=node.output('Out'))
+        else:
+            raise Exception("[ERROR] Operator softplus " \
+            "only supported while beta==1.0 and threshold==20.0")
+
+
+@op_mapper('prelu')
+class PRelu():
+    support_opset_version_range = (9, 15)
+
+    @classmethod
+    def opset_9(cls, graph, node, **kw):
+        slope_shape = node.input_shape('Alpha', 0)
+        input_shape = node.input_shape('X', 0)
+
+        slope_node = node.input('Alpha')[0]
+        if len(input_shape) != len(slope_shape):
+            assert len(
+                slope_shape) == 1, "Slope shape is not expected for prelu"
+            broadcast_shape = [-1] + [1] * (len(input_shape) - 2)
+            broadcast_shape = graph.make_node(
+                'Constant', dtype=dtypes.ONNX.INT64, value=broadcast_shape)
+            slope_node = graph.make_node(
+                'Reshape', inputs=[node.input('Alpha')[0], broadcast_shape])
+        x = node.input('X')[0]
+        x_dtype = node.input_dtype('X', 0)
+        slope_dtype = node.input_dtype('Alpha', 0)
+        if slope_dtype != paddle.float32:
+            slope_node = graph.make_node(
+                'Cast', inputs=[slope_node], to=dtypes.ONNX.FLOAT)
+        if x_dtype != paddle.float32:
+            x = graph.make_node('Cast', inputs=[x], to=dtypes.ONNX.FLOAT)
+            onnx_node = graph.make_node('PRelu', inputs=[x, slope_node])
+            graph.make_node(
+                'Cast',
+                inputs=[onnx_node],
+                outputs=node.output('Out'),
+                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])
+        else:
+            onnx_node = graph.make_node(
+                'PRelu', inputs=[x, slope_node], outputs=node.output('Out'))
+
+
+@op_mapper('relu6')
+class Relu6():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        mapper_helper.clip_helper(graph, node,
+                                  node.input('X', 0),
+                                  node.attr('threshold'), 0.0,
+                                  node.output('Out', 0))
+
+
+@op_mapper('gelu')
+class Gelu():
+    support_opset_version_range = (9, 15)
+
+    @classmethod
+    def opset_9(cls, graph, node, **kw):
+        input = node.input('X', 0)
+        x_dtype = node.input_dtype('X', 0)
+        # onnxruntime only support float32 Erf
+        if x_dtype != paddle.float32:
+            input = graph.make_node(
+                'Cast', inputs=[input], to=dtypes.ONNX.FLOAT)
+        sqrt2 = graph.make_node(
+            'Constant', dtype=dtypes.ONNX.FLOAT, value=[1.4142135623730951])
+        zero_point_five = graph.make_node(
+            'Constant', dtype=dtypes.ONNX.FLOAT, value=[0.5])
+        one = graph.make_node('Constant', dtype=dtypes.ONNX.FLOAT, value=[1])
+        x = graph.make_node('Div', inputs=[input, sqrt2])
+        x = graph.make_node('Erf', inputs=x)
+        x = graph.make_node('Add', inputs=[x, one])
+        x = graph.make_node('Mul', inputs=[input, x])
+        if x_dtype != paddle.float32:
+            mul_node = graph.make_node('Mul', inputs=[x, zero_point_five])
+            graph.make_node(
+                'Cast',
+                inputs=[mul_node],
+                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype],
+                outputs=node.output('Out'))
+        else:
+            graph.make_node(
+                'Mul', inputs=[x, zero_point_five], outputs=node.output('Out'))
+
+
+@op_mapper('selu')
+class Selu():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_6(cls, graph, node, **kw):
+        graph.make_node(
+            'Selu',
+            inputs=node.input('X'),
+            alpha=node.attr('alpha'),
+            gamma=node.attr('scale'),
+            outputs=node.output('Out'))
+
+
+@op_mapper('hard_sigmoid')
+class HardSigmoid():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        slope = node.attr('slope')
+        offset = node.attr('offset')
+        graph.make_node(
+            'HardSigmoid',
+            inputs=node.input('X'),
+            outputs=node.output('Out'),
+            alpha=slope,
+            beta=offset)
+
+
+@op_mapper('swish')
+class Swish():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        x = node.input('X')[0]
+        if math.fabs(node.attr("beta") - 1.0) > 1e-05:
+            beta_node = graph.make_node(
+                'Constant',
+                attrs={'dtype': dtypes.ONNX.FLOAT,
+                       'value': [node.attr('beta')]})
+            x = graph.make_node(
+                'Mul', inputs=[x, beta_node])
+        sigmoid_node = graph.make_node('Sigmoid', inputs=[x])
+        graph.make_node(
+            'Mul',
+            inputs=[x, sigmoid_node],
+            outputs=node.output('Out'))
+
+
+@op_mapper('mish')
+class Mish():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        inputs = node.input('X', 0)
+        dtype = node.input_dtype("X", 0)
+        if dtype != paddle.float32:
+            inputs = graph.make_node(
+                'Cast', inputs=[inputs], to=dtypes.ONNX.FLOAT)
+            dtype = paddle.float32
+        threshold = node.attr('threshold')
+        assert np.fabs(
+            threshold - 20
+        ) < 1e-4, "In mish OP, the threshold only supports 20, no other values are supported"
+        softplus_node = graph.make_node('Softplus', inputs=[inputs])
+        tanh_node = graph.make_node('Tanh', inputs=[softplus_node])
+        if node.input_dtype("X", 0) != paddle.float32:
+            mul_node = graph.make_node('Mul', inputs=[inputs, tanh_node])
+            inputs = graph.make_node(
+                'Cast',
+                inputs=[mul_node],
+                to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype("X", 0)],
+                outputs=node.output('Out'))
+        else:
+            graph.make_node(
+                'Mul', inputs=[inputs, tanh_node], outputs=node.output('Out'))
+
+
+@op_mapper('hard_swish')
+class HardSwish():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        scale_node = graph.make_node(
+            'Constant',
+            attrs={'dtype': dtypes.ONNX.FLOAT,
+                   'value': node.attr('scale')})
+        offset_node = graph.make_node(
+            'Constant',
+            attrs={'dtype': dtypes.ONNX.FLOAT,
+                   'value': node.attr('offset')})
+
+        node0 = graph.make_node('Add', inputs=[node.input('X')[0], offset_node])
+        node1 = mapper_helper.clip_helper(graph, node, node0,
+                                          node.attr('threshold'), 0.0)
+        node2 = graph.make_node('Mul', inputs=[node.input('X')[0], node1])
+        node3 = graph.make_node(
+            'Div', inputs=[node2, scale_node], outputs=node.output('Out'))
```

## paddle2onnx/legacy/op_mapper/logic.py

 * *Ordering differences only*

```diff
@@ -1,280 +1,280 @@
-#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import numpy as np
-from paddle2onnx.legacy.constant import dtypes
-from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
-import paddle
-from paddle2onnx.utils import logging
-from paddle2onnx.legacy.op_mapper import mapper_helper
-
-
-@op_mapper('greater_equal')
-class GreaterOrEqual():
-    support_opset_version_range = (12, 15)
-
-    @classmethod
-    def opset_12(cls, graph, node, **kw):
-        onnx_node = graph.make_node(
-            'GreaterOrEqual',
-            inputs=[node.input('X', 0), node.input('Y', 0)],
-            outputs=node.output('Out'))
-
-
-@op_mapper('equal')
-class Equal():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        if node.input_dtype('X', 0) in [paddle.float32, paddle.float64]:
-            warning_info = "Operator 'Equal' only support input with dtype of int/bool, now the dtype of input is {}, this may cause wrong results, it is more recommend converting this model with opset version >= 11.".format(
-                node.input_dtype('X', 0))
-            logging.warning(warning_info)
-            x_node = graph.make_node(
-                'Cast', inputs=node.input('X'), to=dtypes.ONNX.INT32)
-            y_node = graph.make_node(
-                'Cast', inputs=node.input('Y'), to=dtypes.ONNX.INT32)
-            onnx_node = graph.make_node(
-                'Equal', inputs=[x_node, y_node], outputs=node.output('Out'))
-        else:
-            onnx_node = graph.make_node(
-                'Equal',
-                inputs=[node.input('X', 0), node.input('Y', 0)],
-                outputs=node.output('Out'))
-
-    @classmethod
-    def opset_11(cls, graph, node, **kw):
-        onnx_node = graph.make_node(
-            'Equal',
-            inputs=[node.input('X', 0), node.input('Y', 0)],
-            outputs=node.output('Out'))
-
-
-@op_mapper('not_equal')
-class NotEqual():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        equal_val = None
-        if node.input_dtype('X', 0) in [paddle.float32, paddle.float64]:
-            warning_info = "Operator 'not_equal' only support input with dtype of int/bool, now the dtype of input is {}, this may cause wrong results, it is more recommend converting this model with opset version >= 11.".format(
-                node.input_dtype('X', 0))
-            logging.warning(warning_info)
-            x_node = graph.make_node(
-                'Cast', inputs=node.input('X'), to=dtypes.ONNX.INT32)
-            y_node = graph.make_node(
-                'Cast', inputs=node.input('Y'), to=dtypes.ONNX.INT32)
-            equal_val = graph.make_node(
-                'Equal', inputs=[x_node, y_node], outputs=node.output('Out'))
-        else:
-            equal_val = graph.make_node(
-                'Equal',
-                inputs=[node.input('X', 0), node.input('Y', 0)],
-                outputs=node.output('Out'))
-        k_node = graph.make_node(
-            'Cast', inputs=[equal_val], to=dtypes.ONNX.INT64)
-        const = graph.make_node('Constant', dtype=dtypes.ONNX.INT64, value=1)
-        sub_ = graph.make_node('Sub', inputs=[const, k_node])
-        graph.make_node(
-            'Cast',
-            inputs=[sub_],
-            outputs=node.output('Out'),
-            to=dtypes.ONNX.BOOL)
-
-    @classmethod
-    def opset_11(cls, graph, node, **kw):
-        equal_val = graph.make_node(
-            'Equal', inputs=[node.input('X', 0), node.input('Y', 0)])
-        k_node = graph.make_node(
-            'Cast', inputs=[equal_val], to=dtypes.ONNX.INT64)
-        const = graph.make_node('Constant', dtype=dtypes.ONNX.INT64, value=1)
-        sub_ = graph.make_node('Sub', inputs=[const, k_node])
-        graph.make_node(
-            'Cast',
-            inputs=[sub_],
-            outputs=node.output('Out'),
-            to=dtypes.ONNX.BOOL)
-
-
-@op_mapper('greater_than')
-class GreaterThan():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        if node.input_dtype('X', 0) in [paddle.int32, paddle.int64]:
-            warning_info = "Operator 'greater_than' only support input with dtype of float/double, now the dtype of input is {}, this may cause wrong results, it is more recommend converting this model with opset version >= 11.".format(
-                node.input_dtype('X', 0))
-            logging.warning(warning_info)
-            x_node = graph.make_node(
-                'Cast', inputs=node.input('X'), to=dtypes.ONNX.INT32)
-            y_node = graph.make_node(
-                'Cast', inputs=node.input('Y'), to=dtypes.ONNX.INT32)
-            graph.make_node(
-                'Greater',
-                inputs=[node.input('X', 0), node.input('Y', 0)],
-                outputs=node.output('Out'))
-        else:
-            graph.make_node(
-                'Greater',
-                inputs=[node.input('X', 0), node.input('Y', 0)],
-                outputs=node.output('Out'))
-
-    @classmethod
-    def opset_11(cls, graph, node, **kw):
-        onnx_node = graph.make_node(
-            'Greater',
-            inputs=[node.input('X', 0), node.input('Y', 0)],
-            outputs=node.output('Out'))
-
-
-@op_mapper('logical_and')
-class LogicalAnd():
-    support_opset_version_range = (1, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        onnx_node = graph.make_node(
-            'And',
-            inputs=[node.input('X', 0), node.input('Y', 0)],
-            outputs=node.output('Out'))
-
-
-@op_mapper('logical_not')
-class LogicalNot():
-    support_opset_version_range = (1, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        graph.make_node(
-            'Not', inputs=node.input('X'), outputs=node.output('Out'))
-
-
-@op_mapper('logical_or')
-class LogicalOr():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        graph.make_node(
-            'Or',
-            inputs=[node.input('X', 0), node.input('Y', 0)],
-            outputs=node.output('Out'))
-
-
-@op_mapper('logical_xor')
-class LogicalXOr():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        graph.make_node(
-            'Xor',
-            inputs=[node.input('X', 0), node.input('Y', 0)],
-            outputs=node.output('Out'))
-
-
-@op_mapper('less_equal')
-class LessOrEqual():
-    support_opset_version_range = (12, 15)
-
-    @classmethod
-    def opset_12(cls, graph, node, **kw):
-        onnx_node = graph.make_node(
-            'LessOrEqual',
-            inputs=[node.input('X', 0), node.input('Y', 0)],
-            outputs=node.output('Out'))
-
-
-@op_mapper('less_than')
-class Less_than():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        if node.input_dtype('X', 0) in [paddle.int32, paddle.int64]:
-            warning_info = "Operator 'less_than' only support input with dtype of float/double, now the dtype of input is {}, this may cause wrong results, it is more recommend converting this model with opset version >= 11.".format(
-                node.input_dtype('X', 0))
-            logging.warning(warning_info)
-            x_node = graph.make_node(
-                'Cast', inputs=node.input('X'), to=dtypes.ONNX.INT32)
-            y_node = graph.make_node(
-                'Cast', inputs=node.input('Y'), to=dtypes.ONNX.INT32)
-            graph.make_node(
-                'Less',
-                inputs=[node.input('X', 0), node.input('Y', 0)],
-                outputs=node.output('Out'))
-        else:
-            graph.make_node(
-                'Less',
-                inputs=[node.input('X', 0), node.input('Y', 0)],
-                outputs=node.output('Out'))
-
-    @classmethod
-    def opset_9(cls, graph, node, **kw):
-        graph.make_node(
-            'Less',
-            inputs=[node.input('X', 0), node.input('Y', 0)],
-            outputs=node.output('Out'), )
-
-
-@op_mapper('isfinite_v2')
-class Isfinite():
-    support_opset_version_range = (10, 15)
-
-    @classmethod
-    def opset_10(cls, graph, node, **kw):
-        is_inf = graph.make_node('IsInf', inputs=node.input('X', 0))
-        is_nan = graph.make_node('IsNaN', inputs=node.input('X', 0))
-        finite = graph.make_node('Or', inputs=[is_inf, is_nan])
-        graph.make_node('Not', inputs=[finite], outputs=node.output('Out'))
-
-
-@op_mapper('isinf_v2')
-class IsInf():
-    support_opset_version_range = (10, 15)
-
-    @classmethod
-    def opset_10(cls, graph, node, **kw):
-        graph.make_node(
-            'IsInf', inputs=node.input('X'), outputs=node.output('Out'))
-
-
-@op_mapper('isnan_v2')
-class IsNaN():
-    support_opset_version_range = (9, 15)
-
-    @classmethod
-    def opset_9(cls, graph, node, **kw):
-        graph.make_node(
-            'IsNaN', inputs=node.input('X'), outputs=node.output('Out'))
-
-
-@op_mapper('isnan')
-class IsNaN():
-    support_opset_version_range = (9, 15)
-
-    @classmethod
-    def opset_9(cls, graph, node, **kw):
-        isnan = graph.make_node('IsNaN', inputs=node.input('X'))
-        cast_node = graph.make_node(
-            'Cast', inputs=isnan, attrs={'to': dtypes.ONNX.FLOAT})
-        reduce_node = graph.make_node(
-            'ReduceMax', inputs=[cast_node], keepdims=False)
-        mapper_helper.unsqueeze_helper(graph, reduce_node, [0],
-                                       node.output('Out'))
+#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import numpy as np
+from paddle2onnx.legacy.constant import dtypes
+from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
+import paddle
+from paddle2onnx.utils import logging
+from paddle2onnx.legacy.op_mapper import mapper_helper
+
+
+@op_mapper('greater_equal')
+class GreaterOrEqual():
+    support_opset_version_range = (12, 15)
+
+    @classmethod
+    def opset_12(cls, graph, node, **kw):
+        onnx_node = graph.make_node(
+            'GreaterOrEqual',
+            inputs=[node.input('X', 0), node.input('Y', 0)],
+            outputs=node.output('Out'))
+
+
+@op_mapper('equal')
+class Equal():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        if node.input_dtype('X', 0) in [paddle.float32, paddle.float64]:
+            warning_info = "Operator 'Equal' only support input with dtype of int/bool, now the dtype of input is {}, this may cause wrong results, it is more recommend converting this model with opset version >= 11.".format(
+                node.input_dtype('X', 0))
+            logging.warning(warning_info)
+            x_node = graph.make_node(
+                'Cast', inputs=node.input('X'), to=dtypes.ONNX.INT32)
+            y_node = graph.make_node(
+                'Cast', inputs=node.input('Y'), to=dtypes.ONNX.INT32)
+            onnx_node = graph.make_node(
+                'Equal', inputs=[x_node, y_node], outputs=node.output('Out'))
+        else:
+            onnx_node = graph.make_node(
+                'Equal',
+                inputs=[node.input('X', 0), node.input('Y', 0)],
+                outputs=node.output('Out'))
+
+    @classmethod
+    def opset_11(cls, graph, node, **kw):
+        onnx_node = graph.make_node(
+            'Equal',
+            inputs=[node.input('X', 0), node.input('Y', 0)],
+            outputs=node.output('Out'))
+
+
+@op_mapper('not_equal')
+class NotEqual():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        equal_val = None
+        if node.input_dtype('X', 0) in [paddle.float32, paddle.float64]:
+            warning_info = "Operator 'not_equal' only support input with dtype of int/bool, now the dtype of input is {}, this may cause wrong results, it is more recommend converting this model with opset version >= 11.".format(
+                node.input_dtype('X', 0))
+            logging.warning(warning_info)
+            x_node = graph.make_node(
+                'Cast', inputs=node.input('X'), to=dtypes.ONNX.INT32)
+            y_node = graph.make_node(
+                'Cast', inputs=node.input('Y'), to=dtypes.ONNX.INT32)
+            equal_val = graph.make_node(
+                'Equal', inputs=[x_node, y_node], outputs=node.output('Out'))
+        else:
+            equal_val = graph.make_node(
+                'Equal',
+                inputs=[node.input('X', 0), node.input('Y', 0)],
+                outputs=node.output('Out'))
+        k_node = graph.make_node(
+            'Cast', inputs=[equal_val], to=dtypes.ONNX.INT64)
+        const = graph.make_node('Constant', dtype=dtypes.ONNX.INT64, value=1)
+        sub_ = graph.make_node('Sub', inputs=[const, k_node])
+        graph.make_node(
+            'Cast',
+            inputs=[sub_],
+            outputs=node.output('Out'),
+            to=dtypes.ONNX.BOOL)
+
+    @classmethod
+    def opset_11(cls, graph, node, **kw):
+        equal_val = graph.make_node(
+            'Equal', inputs=[node.input('X', 0), node.input('Y', 0)])
+        k_node = graph.make_node(
+            'Cast', inputs=[equal_val], to=dtypes.ONNX.INT64)
+        const = graph.make_node('Constant', dtype=dtypes.ONNX.INT64, value=1)
+        sub_ = graph.make_node('Sub', inputs=[const, k_node])
+        graph.make_node(
+            'Cast',
+            inputs=[sub_],
+            outputs=node.output('Out'),
+            to=dtypes.ONNX.BOOL)
+
+
+@op_mapper('greater_than')
+class GreaterThan():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        if node.input_dtype('X', 0) in [paddle.int32, paddle.int64]:
+            warning_info = "Operator 'greater_than' only support input with dtype of float/double, now the dtype of input is {}, this may cause wrong results, it is more recommend converting this model with opset version >= 11.".format(
+                node.input_dtype('X', 0))
+            logging.warning(warning_info)
+            x_node = graph.make_node(
+                'Cast', inputs=node.input('X'), to=dtypes.ONNX.INT32)
+            y_node = graph.make_node(
+                'Cast', inputs=node.input('Y'), to=dtypes.ONNX.INT32)
+            graph.make_node(
+                'Greater',
+                inputs=[node.input('X', 0), node.input('Y', 0)],
+                outputs=node.output('Out'))
+        else:
+            graph.make_node(
+                'Greater',
+                inputs=[node.input('X', 0), node.input('Y', 0)],
+                outputs=node.output('Out'))
+
+    @classmethod
+    def opset_11(cls, graph, node, **kw):
+        onnx_node = graph.make_node(
+            'Greater',
+            inputs=[node.input('X', 0), node.input('Y', 0)],
+            outputs=node.output('Out'))
+
+
+@op_mapper('logical_and')
+class LogicalAnd():
+    support_opset_version_range = (1, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        onnx_node = graph.make_node(
+            'And',
+            inputs=[node.input('X', 0), node.input('Y', 0)],
+            outputs=node.output('Out'))
+
+
+@op_mapper('logical_not')
+class LogicalNot():
+    support_opset_version_range = (1, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        graph.make_node(
+            'Not', inputs=node.input('X'), outputs=node.output('Out'))
+
+
+@op_mapper('logical_or')
+class LogicalOr():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        graph.make_node(
+            'Or',
+            inputs=[node.input('X', 0), node.input('Y', 0)],
+            outputs=node.output('Out'))
+
+
+@op_mapper('logical_xor')
+class LogicalXOr():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        graph.make_node(
+            'Xor',
+            inputs=[node.input('X', 0), node.input('Y', 0)],
+            outputs=node.output('Out'))
+
+
+@op_mapper('less_equal')
+class LessOrEqual():
+    support_opset_version_range = (12, 15)
+
+    @classmethod
+    def opset_12(cls, graph, node, **kw):
+        onnx_node = graph.make_node(
+            'LessOrEqual',
+            inputs=[node.input('X', 0), node.input('Y', 0)],
+            outputs=node.output('Out'))
+
+
+@op_mapper('less_than')
+class Less_than():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        if node.input_dtype('X', 0) in [paddle.int32, paddle.int64]:
+            warning_info = "Operator 'less_than' only support input with dtype of float/double, now the dtype of input is {}, this may cause wrong results, it is more recommend converting this model with opset version >= 11.".format(
+                node.input_dtype('X', 0))
+            logging.warning(warning_info)
+            x_node = graph.make_node(
+                'Cast', inputs=node.input('X'), to=dtypes.ONNX.INT32)
+            y_node = graph.make_node(
+                'Cast', inputs=node.input('Y'), to=dtypes.ONNX.INT32)
+            graph.make_node(
+                'Less',
+                inputs=[node.input('X', 0), node.input('Y', 0)],
+                outputs=node.output('Out'))
+        else:
+            graph.make_node(
+                'Less',
+                inputs=[node.input('X', 0), node.input('Y', 0)],
+                outputs=node.output('Out'))
+
+    @classmethod
+    def opset_9(cls, graph, node, **kw):
+        graph.make_node(
+            'Less',
+            inputs=[node.input('X', 0), node.input('Y', 0)],
+            outputs=node.output('Out'), )
+
+
+@op_mapper('isfinite_v2')
+class Isfinite():
+    support_opset_version_range = (10, 15)
+
+    @classmethod
+    def opset_10(cls, graph, node, **kw):
+        is_inf = graph.make_node('IsInf', inputs=node.input('X', 0))
+        is_nan = graph.make_node('IsNaN', inputs=node.input('X', 0))
+        finite = graph.make_node('Or', inputs=[is_inf, is_nan])
+        graph.make_node('Not', inputs=[finite], outputs=node.output('Out'))
+
+
+@op_mapper('isinf_v2')
+class IsInf():
+    support_opset_version_range = (10, 15)
+
+    @classmethod
+    def opset_10(cls, graph, node, **kw):
+        graph.make_node(
+            'IsInf', inputs=node.input('X'), outputs=node.output('Out'))
+
+
+@op_mapper('isnan_v2')
+class IsNaN():
+    support_opset_version_range = (9, 15)
+
+    @classmethod
+    def opset_9(cls, graph, node, **kw):
+        graph.make_node(
+            'IsNaN', inputs=node.input('X'), outputs=node.output('Out'))
+
+
+@op_mapper('isnan')
+class IsNaN():
+    support_opset_version_range = (9, 15)
+
+    @classmethod
+    def opset_9(cls, graph, node, **kw):
+        isnan = graph.make_node('IsNaN', inputs=node.input('X'))
+        cast_node = graph.make_node(
+            'Cast', inputs=isnan, attrs={'to': dtypes.ONNX.FLOAT})
+        reduce_node = graph.make_node(
+            'ReduceMax', inputs=[cast_node], keepdims=False)
+        mapper_helper.unsqueeze_helper(graph, reduce_node, [0],
+                                       node.output('Out'))
```

## paddle2onnx/legacy/op_mapper/mapper_helper.py

 * *Ordering differences only*

```diff
@@ -1,432 +1,432 @@
-#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-import paddle.fluid.core as core
-import six
-import copy
-from paddle2onnx.legacy.constant import dtypes
-import paddle
-from onnx import TensorProto
-
-
-def is_static_shape(shape):
-    if len(shape) > 1 and shape[1:].count(-1) > 0:
-        raise Exception(
-            "Converting this model to ONNX need with static input shape," \
-            " please fix input shape of this model, see doc Q2 in" \
-            " https://github.com/PaddlePaddle/paddle2onnx/blob/develop/docs/en/FAQ.md."
-        )
-
-
-def shape_helper(graph, input, dim=None):
-    if dim is None:
-        shape_node = graph.make_node('Shape', inputs=[input])
-        return shape_node
-    full_shape = graph.make_node('Shape', inputs=[input])
-    shape_node = slice_helper(graph, full_shape, [0], [dim], [dim + 1])
-    return shape_node
-
-
-def unsqueeze_helper(graph, input, axes, outputs=None):
-    inputs = []
-    if not isinstance(input, list):
-        input = [input]
-    inputs.append(input[0])
-    if not isinstance(axes, list):
-        axes = [axes]
-    if outputs is not None and isinstance(outputs, six.string_types):
-        outputs = [outputs]
-
-    if graph.opset_version < 13:
-        unsqueeze_node = graph.make_node(
-            "Unsqueeze", inputs=inputs, outputs=outputs, axes=axes)
-        return unsqueeze_node
-    else:
-        axes_node = graph.make_node(
-            'Constant', dtype=dtypes.ONNX.INT64, value=axes)
-        inputs = inputs + [axes_node]
-        unsqueeze_node = graph.make_node(
-            "Unsqueeze", inputs=inputs, outputs=outputs)
-        return unsqueeze_node
-
-
-def split_helper(graph, input, axis=0, split=None, outputs=None):
-    assert outputs is not None, "outputs can not be None in split_helper."
-    inputs = []
-    if not isinstance(input, list):
-        input = [input]
-    inputs.append(input[0])
-    if split is not None and not isinstance(split, list):
-        split = [split]
-    if split is None:
-        split_node = graph.make_node(
-            "Split", inputs=inputs, outputs=outputs, axis=axis)
-        return split_node
-    if graph.opset_version < 13:
-        split_node = graph.make_node(
-            "Split", inputs=inputs, outputs=outputs, axis=axis, split=split)
-        return split_node
-    else:
-        split = graph.make_node(
-            'Constant', dtype=dtypes.ONNX.INT64, value=split)
-        inputs = inputs + [split]
-        split_node = graph.make_node(
-            "Split", inputs=inputs, axis=axis, outputs=outputs)
-        return split_node
-    
-    
-def slice_helper(graph,
-                 input,
-                 axes,
-                 starts,
-                 ends,
-                 outputs=None,
-                 dtype=dtypes.ONNX.INT64):
-    inputs = []
-    if not isinstance(input, list):
-        input = [input]
-    inputs.append(input[0])
-    if axes is not None and not isinstance(axes, list):
-        axes = [axes]
-    if starts is not None and not isinstance(starts, (list, six.string_types)):
-        starts = [starts]
-    if ends is not None and not isinstance(ends, (list, six.string_types)):
-        ends = [ends]
-
-    if graph.opset_version < 10:
-        attrs = {
-            'starts': starts,
-            'ends': ends,
-        }
-        if axes not in [None, []]:
-            attrs['axes'] = axes
-        slice_node = graph.make_node(
-            "Slice", inputs=inputs, outputs=outputs, attrs=attrs)
-        return slice_node
-    else:
-        if not isinstance(starts, six.string_types):
-            starts = graph.make_node('Constant', dtype=dtype, value=starts)
-        if not isinstance(ends, six.string_types):
-            ends = graph.make_node('Constant', dtype=dtype, value=ends)
-        inputs = inputs + [starts, ends]
-        if axes not in [None, []]:
-            axes_node = graph.make_node('Constant', dtype=dtype, value=axes)
-            inputs.append(axes_node)
-        slice_node = graph.make_node("Slice", inputs=inputs, outputs=outputs)
-        return slice_node
-
-
-def squeeze_helper(graph, input, axes=None, outputs=None):
-    inputs = []
-    if not isinstance(input, list):
-        input = [input]
-    inputs.append(input[0])
-    if axes is not None and not isinstance(axes, list):
-        axes = [axes]
-    if graph.opset_version < 13:
-        squeeze_node = graph.make_node(
-            "Squeeze", inputs=inputs, axes=axes, outputs=outputs)
-        return squeeze_node
-    else:
-        if axes is not None:
-            axes_node = graph.make_node(
-                'Constant', dtype=dtypes.ONNX.INT64, value=axes)
-            inputs.append(axes_node)
-        squeeze_node = graph.make_node(
-            "Squeeze", inputs=inputs, outputs=outputs)
-        return squeeze_node
-
-
-def unsqueeze_helper(graph, input, axes, outputs=None):
-    inputs = []
-    if isinstance(input, list):
-        input = input[0]
-    inputs.append(input)
-    if not isinstance(axes, list):
-        axes = [axes]
-    if graph.opset_version < 13:
-        unsqueeze_node = graph.make_node(
-            'Unsqueeze', inputs=inputs, axes=axes, outputs=outputs)
-    else:
-        axes_node = graph.make_node(
-            'Constant', attrs={'dtype': dtypes.ONNX.INT64,
-                               'value': axes})
-        inputs.append(axes_node)
-        unsqueeze_node = graph.make_node(
-            'Unsqueeze', inputs=inputs, outputs=outputs)
-    return unsqueeze_node
-
-
-def split_helper(graph, inputs, outputs, axis, split, dtype=paddle.float32):
-    if not isinstance(inputs, (list, tuple)):
-        inputs = [inputs]
-
-    if not isinstance(outputs, int) and not isinstance(outputs, (list, tuple)):
-        outputs = [outputs]
-
-    if dtype == paddle.float64:
-        cast_inputs = []
-        for i in range(len(inputs)):
-            one = graph.make_node(
-                'Cast', inputs=[inputs[i]], to=TensorProto.FLOAT)
-            cast_inputs.append(one)
-        if graph.opset_version < 13:
-            split_node = graph.make_node(
-                "Split",
-                inputs=cast_inputs,
-                outputs=outputs,
-                axis=axis,
-                split=split)
-        else:
-            split_const = graph.make_node(
-                'Constant', dtype=dtypes.ONNX.INT64, value=split)
-            split_node = graph.make_node(
-                "Split",
-                inputs=cast_inputs + [split_const],
-                outputs=outputs,
-                axis=axis)
-        casted_output = []
-        for i in range(len(outputs)):
-            one = graph.make_node(
-                'Cast',
-                inputs=[split_node[i]],
-                outputs=[outputs[i]],
-                to=TensorProto.DOUBLE)
-            casted_output.append(one)
-        return casted_output
-    else:
-        if graph.opset_version < 13:
-            split_node = graph.make_node(
-                "Split", inputs=inputs, outputs=outputs, axis=axis, split=split)
-        else:
-            split_const = graph.make_node(
-                'Constant', dtype=dtypes.ONNX.INT64, value=split)
-            split_node = graph.make_node(
-                "Split",
-                inputs=inputs + [split_const],
-                outputs=outputs,
-                axis=axis)
-        return split_node
-
-
-def constant_helper(graph, dtype, value, shape=None, outputs=[]):
-    constant = graph.make_node(
-        'Constant',
-        inputs=[],
-        outputs=outputs,
-        attrs={
-            'dims': shape,
-            'dtype': dtypes.DTYPE_PADDLE_ONNX_MAP[dtype],
-            'value': value
-        })
-    return constant
-
-
-def clip_helper(graph, node, input, max, min, output=[]):
-    x_dtype = node.input_dtype('X', 0)
-    if (isinstance(min, six.string_types) or
-            isinstance(max, six.string_types)) and graph.opset_version < 11:
-        raise Exception(
-            "min or max of Clip is Tensor, please try with higher onnx opset_version."
-        )
-    if graph.opset_version < 11:
-        if x_dtype != paddle.float32:
-            input = graph.make_node(
-                'Cast', inputs=[input], to=dtypes.ONNX.FLOAT)
-            clip = graph.make_node('Clip', inputs=input, max=max, min=min)
-            clip = graph.make_node(
-                'Cast',
-                inputs=[clip],
-                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype],
-                outputs=output)
-        else:
-            clip = graph.make_node(
-                'Clip', inputs=input, max=max, min=min, outputs=output)
-    else:
-        if x_dtype != paddle.float32:
-            input = graph.make_node(
-                'Cast', inputs=[input], to=dtypes.ONNX.FLOAT)
-
-        if not isinstance(min, six.string_types):
-            min = graph.make_node(
-                'Constant',
-                attrs={
-                    'dtype': dtypes.DTYPE_PADDLE_ONNX_MAP[paddle.float32],
-                    'value': min
-                })
-        else:
-            if node.input_dtype('Min', 0) != paddle.float32:
-                min = graph.make_node(
-                    'Cast',
-                    inputs=min,
-                    attrs={'to': dtypes.DTYPE_PADDLE_ONNX_MAP[paddle.float32]})
-            min = graph.make_node('Squeeze', min)
-
-        if not isinstance(max, six.string_types):
-            max = graph.make_node(
-                'Constant',
-                attrs={
-                    'dtype': dtypes.DTYPE_PADDLE_ONNX_MAP[paddle.float32],
-                    'value': max
-                })
-        else:
-            if node.input_dtype('Max', 0) != paddle.float32:
-                max = graph.make_node(
-                    'Cast',
-                    inputs=max,
-                    attrs={'to': dtypes.DTYPE_PADDLE_ONNX_MAP[paddle.float32]})
-            max = graph.make_node('Squeeze', max)
-        if x_dtype != paddle.float32:
-            clip_pre = graph.make_node('Clip', inputs=[input, min, max])
-            clip = graph.make_node(
-                'Cast',
-                inputs=[clip_pre],
-                outputs=output,
-                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])
-        else:
-            clip = graph.make_node(
-                'Clip', inputs=[input, min, max], outputs=output)
-    return clip
-
-
-def dtype_alignment(graph, nodes, node_dtypes, to=None):
-    assert len(nodes) == len(
-        node_dtypes), "Length of nodes and node_dtypes should be equal."
-    dtype_order = [
-        core.VarDesc.VarType.BOOL,
-        core.VarDesc.VarType.INT16,
-        core.VarDesc.VarType.INT32,
-        core.VarDesc.VarType.INT64,
-        core.VarDesc.VarType.FP16,
-        core.VarDesc.VarType.FP32,
-        core.VarDesc.VarType.FP64,
-    ]
-    max_index = -1
-    for dtype in node_dtypes:
-        index = dtype_order.index(dtype)
-        if index > max_index:
-            max_index = index
-
-    if max_index < 0:
-        return nodes
-
-    casted_nodes = list()
-    cast_dtype = dtype_order[max_index]
-    cast_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[cast_dtype]
-    for i, dtype in enumerate(node_dtypes):
-        index = dtype_order.index(dtype)
-        if to is not None:
-            cast_dtype = to
-            condition = dtypes.DTYPE_PADDLE_ONNX_MAP[index] != cast_dtype
-        else:
-            condition = index != max_index
-        if condition:
-            cast_node = graph.make_node(
-                'Cast', inputs=[nodes[i]], to=cast_dtype)
-            casted_nodes.append(cast_node)
-        else:
-            casted_nodes.append(nodes[i])
-    return casted_nodes
-
-
-def cast(graph, input, origin_dtype, target_dtype):
-    if not isinstance(origin_dtype, six.string_types):
-        origin_dtype = dtypes.DTYPE_PADDLE_STR_MAP[origin_dtype]
-    if origin_dtype != target_dtype:
-        cast_node = graph.make_node(
-            'Cast', inputs=input, to=dtypes.DTYPE_ONNX_STR_MAP[target_dtype])
-        return cast_node
-    return input
-
-
-def shape_alignment(graph, nodes, node_shapes):
-    assert len(nodes) == len(
-        node_shapes), "Length of nodes and node_shapes should be equal."
-    max_dim = -1
-    for shape in node_shapes:
-        dim = len(shape)
-        if dim > max_dim:
-            max_dim = dim
-
-    if max_dim < 0:
-        return nodes
-
-    assert max_dim == 1 or max_dim == 0, "max_dim is only supported when max_dim is 1 or 0."
-    max_dim = 1 if max_dim == 0 else max_dim
-    unsqueeze_nodes = list()
-    for i, shape in enumerate(node_shapes):
-        dim = len(shape)
-        if dim != max_dim:
-            unsqueeze_node = nodes[i]
-            for j in range(max_dim - dim):
-                unsqueeze_node = unsqueeze_helper(graph, unsqueeze_node, [0])
-            unsqueeze_nodes.append(unsqueeze_node)
-        else:
-            unsqueeze_nodes.append(nodes[i])
-    return unsqueeze_nodes
-
-
-def get_tensor_list_node(graph, node, name, dtype=None):
-    node_list = node.input(name)
-    node_dtypes = [node.input_dtype(name, i) for i in range(len(node_list))]
-    node_list = dtype_alignment(graph, node_list, node_dtypes, dtype)
-
-    node_shapes = [node.input_shape(name, i) for i in range(len(node_list))]
-    node_list = shape_alignment(graph, node_list, node_shapes)
-    node = graph.make_node("Concat", inputs=node_list, axis=0)
-    return node
-
-
-def get_value_from_parameters(graph, input_node):
-    assert input_node in graph.parameters, "{} is not in graph.parameters".format(
-        input_node)
-    data = graph.parameters[input_node].attribute[0].t.int32_data
-    if data is None or len(data) < 1:
-        data = graph.parameters[input_node].attribute[0].t.int64_data
-    value = [val for _, val in enumerate(data)]
-    return value
-
-
-# return value
-# arg1: attr_value
-# arg2: attr_value is tensor or not
-def get_node_attr_value(graph,
-                        node,
-                        attr_name=None,
-                        attr_tensor_name=None,
-                        attr_tensor_list_name=None,
-                        return_list=False,
-                        dtype=None):
-    attr_tensor = node.input(attr_tensor_name)
-    attr_tensor_list = node.input(attr_tensor_list_name)
-    if attr_tensor is not None and len(attr_tensor) > 0:
-        value = node.input(attr_tensor_name)[0]
-        if return_list:
-            try:
-                value = get_value_from_parameters(graph, value)
-                return value, False  # value, is_tensor
-            except Exception:
-                return value, True
-        else:
-            input_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype(
-                attr_tensor_name, 0)]
-            if input_dtype != dtype:
-                value = graph.make_node('Cast', inputs=[value], to=dtype)
-            return value, True
-    elif attr_tensor_list is not None and len(attr_tensor_list) > 0:
-        value = get_tensor_list_node(graph, node, attr_tensor_list_name, dtype)
-        return value, True
-    else:
-        value = node.attr(attr_name)
-        return value, False
+#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import paddle.fluid.core as core
+import six
+import copy
+from paddle2onnx.legacy.constant import dtypes
+import paddle
+from onnx import TensorProto
+
+
+def is_static_shape(shape):
+    if len(shape) > 1 and shape[1:].count(-1) > 0:
+        raise Exception(
+            "Converting this model to ONNX need with static input shape," \
+            " please fix input shape of this model, see doc Q2 in" \
+            " https://github.com/PaddlePaddle/paddle2onnx/blob/develop/docs/en/FAQ.md."
+        )
+
+
+def shape_helper(graph, input, dim=None):
+    if dim is None:
+        shape_node = graph.make_node('Shape', inputs=[input])
+        return shape_node
+    full_shape = graph.make_node('Shape', inputs=[input])
+    shape_node = slice_helper(graph, full_shape, [0], [dim], [dim + 1])
+    return shape_node
+
+
+def unsqueeze_helper(graph, input, axes, outputs=None):
+    inputs = []
+    if not isinstance(input, list):
+        input = [input]
+    inputs.append(input[0])
+    if not isinstance(axes, list):
+        axes = [axes]
+    if outputs is not None and isinstance(outputs, six.string_types):
+        outputs = [outputs]
+
+    if graph.opset_version < 13:
+        unsqueeze_node = graph.make_node(
+            "Unsqueeze", inputs=inputs, outputs=outputs, axes=axes)
+        return unsqueeze_node
+    else:
+        axes_node = graph.make_node(
+            'Constant', dtype=dtypes.ONNX.INT64, value=axes)
+        inputs = inputs + [axes_node]
+        unsqueeze_node = graph.make_node(
+            "Unsqueeze", inputs=inputs, outputs=outputs)
+        return unsqueeze_node
+
+
+def split_helper(graph, input, axis=0, split=None, outputs=None):
+    assert outputs is not None, "outputs can not be None in split_helper."
+    inputs = []
+    if not isinstance(input, list):
+        input = [input]
+    inputs.append(input[0])
+    if split is not None and not isinstance(split, list):
+        split = [split]
+    if split is None:
+        split_node = graph.make_node(
+            "Split", inputs=inputs, outputs=outputs, axis=axis)
+        return split_node
+    if graph.opset_version < 13:
+        split_node = graph.make_node(
+            "Split", inputs=inputs, outputs=outputs, axis=axis, split=split)
+        return split_node
+    else:
+        split = graph.make_node(
+            'Constant', dtype=dtypes.ONNX.INT64, value=split)
+        inputs = inputs + [split]
+        split_node = graph.make_node(
+            "Split", inputs=inputs, axis=axis, outputs=outputs)
+        return split_node
+    
+    
+def slice_helper(graph,
+                 input,
+                 axes,
+                 starts,
+                 ends,
+                 outputs=None,
+                 dtype=dtypes.ONNX.INT64):
+    inputs = []
+    if not isinstance(input, list):
+        input = [input]
+    inputs.append(input[0])
+    if axes is not None and not isinstance(axes, list):
+        axes = [axes]
+    if starts is not None and not isinstance(starts, (list, six.string_types)):
+        starts = [starts]
+    if ends is not None and not isinstance(ends, (list, six.string_types)):
+        ends = [ends]
+
+    if graph.opset_version < 10:
+        attrs = {
+            'starts': starts,
+            'ends': ends,
+        }
+        if axes not in [None, []]:
+            attrs['axes'] = axes
+        slice_node = graph.make_node(
+            "Slice", inputs=inputs, outputs=outputs, attrs=attrs)
+        return slice_node
+    else:
+        if not isinstance(starts, six.string_types):
+            starts = graph.make_node('Constant', dtype=dtype, value=starts)
+        if not isinstance(ends, six.string_types):
+            ends = graph.make_node('Constant', dtype=dtype, value=ends)
+        inputs = inputs + [starts, ends]
+        if axes not in [None, []]:
+            axes_node = graph.make_node('Constant', dtype=dtype, value=axes)
+            inputs.append(axes_node)
+        slice_node = graph.make_node("Slice", inputs=inputs, outputs=outputs)
+        return slice_node
+
+
+def squeeze_helper(graph, input, axes=None, outputs=None):
+    inputs = []
+    if not isinstance(input, list):
+        input = [input]
+    inputs.append(input[0])
+    if axes is not None and not isinstance(axes, list):
+        axes = [axes]
+    if graph.opset_version < 13:
+        squeeze_node = graph.make_node(
+            "Squeeze", inputs=inputs, axes=axes, outputs=outputs)
+        return squeeze_node
+    else:
+        if axes is not None:
+            axes_node = graph.make_node(
+                'Constant', dtype=dtypes.ONNX.INT64, value=axes)
+            inputs.append(axes_node)
+        squeeze_node = graph.make_node(
+            "Squeeze", inputs=inputs, outputs=outputs)
+        return squeeze_node
+
+
+def unsqueeze_helper(graph, input, axes, outputs=None):
+    inputs = []
+    if isinstance(input, list):
+        input = input[0]
+    inputs.append(input)
+    if not isinstance(axes, list):
+        axes = [axes]
+    if graph.opset_version < 13:
+        unsqueeze_node = graph.make_node(
+            'Unsqueeze', inputs=inputs, axes=axes, outputs=outputs)
+    else:
+        axes_node = graph.make_node(
+            'Constant', attrs={'dtype': dtypes.ONNX.INT64,
+                               'value': axes})
+        inputs.append(axes_node)
+        unsqueeze_node = graph.make_node(
+            'Unsqueeze', inputs=inputs, outputs=outputs)
+    return unsqueeze_node
+
+
+def split_helper(graph, inputs, outputs, axis, split, dtype=paddle.float32):
+    if not isinstance(inputs, (list, tuple)):
+        inputs = [inputs]
+
+    if not isinstance(outputs, int) and not isinstance(outputs, (list, tuple)):
+        outputs = [outputs]
+
+    if dtype == paddle.float64:
+        cast_inputs = []
+        for i in range(len(inputs)):
+            one = graph.make_node(
+                'Cast', inputs=[inputs[i]], to=TensorProto.FLOAT)
+            cast_inputs.append(one)
+        if graph.opset_version < 13:
+            split_node = graph.make_node(
+                "Split",
+                inputs=cast_inputs,
+                outputs=outputs,
+                axis=axis,
+                split=split)
+        else:
+            split_const = graph.make_node(
+                'Constant', dtype=dtypes.ONNX.INT64, value=split)
+            split_node = graph.make_node(
+                "Split",
+                inputs=cast_inputs + [split_const],
+                outputs=outputs,
+                axis=axis)
+        casted_output = []
+        for i in range(len(outputs)):
+            one = graph.make_node(
+                'Cast',
+                inputs=[split_node[i]],
+                outputs=[outputs[i]],
+                to=TensorProto.DOUBLE)
+            casted_output.append(one)
+        return casted_output
+    else:
+        if graph.opset_version < 13:
+            split_node = graph.make_node(
+                "Split", inputs=inputs, outputs=outputs, axis=axis, split=split)
+        else:
+            split_const = graph.make_node(
+                'Constant', dtype=dtypes.ONNX.INT64, value=split)
+            split_node = graph.make_node(
+                "Split",
+                inputs=inputs + [split_const],
+                outputs=outputs,
+                axis=axis)
+        return split_node
+
+
+def constant_helper(graph, dtype, value, shape=None, outputs=[]):
+    constant = graph.make_node(
+        'Constant',
+        inputs=[],
+        outputs=outputs,
+        attrs={
+            'dims': shape,
+            'dtype': dtypes.DTYPE_PADDLE_ONNX_MAP[dtype],
+            'value': value
+        })
+    return constant
+
+
+def clip_helper(graph, node, input, max, min, output=[]):
+    x_dtype = node.input_dtype('X', 0)
+    if (isinstance(min, six.string_types) or
+            isinstance(max, six.string_types)) and graph.opset_version < 11:
+        raise Exception(
+            "min or max of Clip is Tensor, please try with higher onnx opset_version."
+        )
+    if graph.opset_version < 11:
+        if x_dtype != paddle.float32:
+            input = graph.make_node(
+                'Cast', inputs=[input], to=dtypes.ONNX.FLOAT)
+            clip = graph.make_node('Clip', inputs=input, max=max, min=min)
+            clip = graph.make_node(
+                'Cast',
+                inputs=[clip],
+                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype],
+                outputs=output)
+        else:
+            clip = graph.make_node(
+                'Clip', inputs=input, max=max, min=min, outputs=output)
+    else:
+        if x_dtype != paddle.float32:
+            input = graph.make_node(
+                'Cast', inputs=[input], to=dtypes.ONNX.FLOAT)
+
+        if not isinstance(min, six.string_types):
+            min = graph.make_node(
+                'Constant',
+                attrs={
+                    'dtype': dtypes.DTYPE_PADDLE_ONNX_MAP[paddle.float32],
+                    'value': min
+                })
+        else:
+            if node.input_dtype('Min', 0) != paddle.float32:
+                min = graph.make_node(
+                    'Cast',
+                    inputs=min,
+                    attrs={'to': dtypes.DTYPE_PADDLE_ONNX_MAP[paddle.float32]})
+            min = graph.make_node('Squeeze', min)
+
+        if not isinstance(max, six.string_types):
+            max = graph.make_node(
+                'Constant',
+                attrs={
+                    'dtype': dtypes.DTYPE_PADDLE_ONNX_MAP[paddle.float32],
+                    'value': max
+                })
+        else:
+            if node.input_dtype('Max', 0) != paddle.float32:
+                max = graph.make_node(
+                    'Cast',
+                    inputs=max,
+                    attrs={'to': dtypes.DTYPE_PADDLE_ONNX_MAP[paddle.float32]})
+            max = graph.make_node('Squeeze', max)
+        if x_dtype != paddle.float32:
+            clip_pre = graph.make_node('Clip', inputs=[input, min, max])
+            clip = graph.make_node(
+                'Cast',
+                inputs=[clip_pre],
+                outputs=output,
+                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])
+        else:
+            clip = graph.make_node(
+                'Clip', inputs=[input, min, max], outputs=output)
+    return clip
+
+
+def dtype_alignment(graph, nodes, node_dtypes, to=None):
+    assert len(nodes) == len(
+        node_dtypes), "Length of nodes and node_dtypes should be equal."
+    dtype_order = [
+        core.VarDesc.VarType.BOOL,
+        core.VarDesc.VarType.INT16,
+        core.VarDesc.VarType.INT32,
+        core.VarDesc.VarType.INT64,
+        core.VarDesc.VarType.FP16,
+        core.VarDesc.VarType.FP32,
+        core.VarDesc.VarType.FP64,
+    ]
+    max_index = -1
+    for dtype in node_dtypes:
+        index = dtype_order.index(dtype)
+        if index > max_index:
+            max_index = index
+
+    if max_index < 0:
+        return nodes
+
+    casted_nodes = list()
+    cast_dtype = dtype_order[max_index]
+    cast_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[cast_dtype]
+    for i, dtype in enumerate(node_dtypes):
+        index = dtype_order.index(dtype)
+        if to is not None:
+            cast_dtype = to
+            condition = dtypes.DTYPE_PADDLE_ONNX_MAP[index] != cast_dtype
+        else:
+            condition = index != max_index
+        if condition:
+            cast_node = graph.make_node(
+                'Cast', inputs=[nodes[i]], to=cast_dtype)
+            casted_nodes.append(cast_node)
+        else:
+            casted_nodes.append(nodes[i])
+    return casted_nodes
+
+
+def cast(graph, input, origin_dtype, target_dtype):
+    if not isinstance(origin_dtype, six.string_types):
+        origin_dtype = dtypes.DTYPE_PADDLE_STR_MAP[origin_dtype]
+    if origin_dtype != target_dtype:
+        cast_node = graph.make_node(
+            'Cast', inputs=input, to=dtypes.DTYPE_ONNX_STR_MAP[target_dtype])
+        return cast_node
+    return input
+
+
+def shape_alignment(graph, nodes, node_shapes):
+    assert len(nodes) == len(
+        node_shapes), "Length of nodes and node_shapes should be equal."
+    max_dim = -1
+    for shape in node_shapes:
+        dim = len(shape)
+        if dim > max_dim:
+            max_dim = dim
+
+    if max_dim < 0:
+        return nodes
+
+    assert max_dim == 1 or max_dim == 0, "max_dim is only supported when max_dim is 1 or 0."
+    max_dim = 1 if max_dim == 0 else max_dim
+    unsqueeze_nodes = list()
+    for i, shape in enumerate(node_shapes):
+        dim = len(shape)
+        if dim != max_dim:
+            unsqueeze_node = nodes[i]
+            for j in range(max_dim - dim):
+                unsqueeze_node = unsqueeze_helper(graph, unsqueeze_node, [0])
+            unsqueeze_nodes.append(unsqueeze_node)
+        else:
+            unsqueeze_nodes.append(nodes[i])
+    return unsqueeze_nodes
+
+
+def get_tensor_list_node(graph, node, name, dtype=None):
+    node_list = node.input(name)
+    node_dtypes = [node.input_dtype(name, i) for i in range(len(node_list))]
+    node_list = dtype_alignment(graph, node_list, node_dtypes, dtype)
+
+    node_shapes = [node.input_shape(name, i) for i in range(len(node_list))]
+    node_list = shape_alignment(graph, node_list, node_shapes)
+    node = graph.make_node("Concat", inputs=node_list, axis=0)
+    return node
+
+
+def get_value_from_parameters(graph, input_node):
+    assert input_node in graph.parameters, "{} is not in graph.parameters".format(
+        input_node)
+    data = graph.parameters[input_node].attribute[0].t.int32_data
+    if data is None or len(data) < 1:
+        data = graph.parameters[input_node].attribute[0].t.int64_data
+    value = [val for _, val in enumerate(data)]
+    return value
+
+
+# return value
+# arg1: attr_value
+# arg2: attr_value is tensor or not
+def get_node_attr_value(graph,
+                        node,
+                        attr_name=None,
+                        attr_tensor_name=None,
+                        attr_tensor_list_name=None,
+                        return_list=False,
+                        dtype=None):
+    attr_tensor = node.input(attr_tensor_name)
+    attr_tensor_list = node.input(attr_tensor_list_name)
+    if attr_tensor is not None and len(attr_tensor) > 0:
+        value = node.input(attr_tensor_name)[0]
+        if return_list:
+            try:
+                value = get_value_from_parameters(graph, value)
+                return value, False  # value, is_tensor
+            except Exception:
+                return value, True
+        else:
+            input_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype(
+                attr_tensor_name, 0)]
+            if input_dtype != dtype:
+                value = graph.make_node('Cast', inputs=[value], to=dtype)
+            return value, True
+    elif attr_tensor_list is not None and len(attr_tensor_list) > 0:
+        value = get_tensor_list_node(graph, node, attr_tensor_list_name, dtype)
+        return value, True
+    else:
+        value = node.attr(attr_name)
+        return value, False
```

## paddle2onnx/legacy/op_mapper/math.py

 * *Ordering differences only*

```diff
@@ -1,1273 +1,1273 @@
-#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import numpy as np
-from paddle2onnx.legacy.constant import dtypes
-from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
-from paddle2onnx.legacy.op_mapper import mapper_helper
-import paddle
-
-
-@op_mapper('matmul')
-class MatMul():
-    support_opset_version_range = (1, 12)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        x = node.input('X', idx=0)
-        y = node.input('Y', idx=0)
-        if node.attr('transpose_X'):
-            perm = list(range(len(node.input_shape('X', 0))))
-            perm[-1], perm[-2] = perm[-2], perm[-1]
-            if node.input_dtype('X', 0) == paddle.float64:
-                x = graph.make_node('Cast', inputs=x, to=dtypes.ONNX.FLOAT)
-            x = graph.make_node('Transpose', inputs=[x], perm=perm)
-        if node.attr('transpose_Y'):
-            perm = list(range(len(node.input_shape('Y', 0))))
-            perm[-1], perm[-2] = perm[-2], perm[-1]
-            if node.input_dtype('Y', 0) == paddle.float64:
-                y = graph.make_node('Cast', inputs=y, to=dtypes.ONNX.FLOAT)
-            y = graph.make_node('Transpose', inputs=[y], perm=perm)
-        if node.attr('alpha') == 1.0:
-            if node.input_dtype('X', 0) == paddle.float64:
-                output_node = graph.make_node('MatMul', inputs=[x, y])
-                graph.make_node(
-                    'Cast',
-                    inputs=output_node,
-                    to=dtypes.ONNX.DOUBLE,
-                    outputs=node.output('Out'))
-            else:
-                graph.make_node(
-                    'MatMul', inputs=[x, y], outputs=node.output('Out'))
-        else:
-            if node.input_dtype('X', 0) == paddle.float64:
-                output_node = graph.make_node('MatMul', inputs=[x, y])
-                matmul = graph.make_node(
-                    'Cast', inputs=output_node, to=dtypes.ONNX.DOUBLE)
-                scale = graph.make_node(
-                    'Constant',
-                    dtype=dtypes.ONNX.DOUBLE,
-                    value=node.attr('alpha'))
-            else:
-                matmul = graph.make_node('MatMul', inputs=[x, y])
-                scale = graph.make_node(
-                    'Constant',
-                    dtype=dtypes.ONNX.FLOAT,
-                    value=node.attr('alpha'))
-
-            onnx_node = graph.make_node(
-                'Mul', inputs=[matmul, scale], outputs=node.output('Out'))
-
-
-@op_mapper('matmul_v2')
-class MatMul():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        x = node.input('X', idx=0)
-        y = node.input('Y', idx=0)
-        out = node.output('Out')
-        ## TODO(wangjunjie06): The current addition of cast op is only for onnxruntime optimization, after onnxruntime is repaired, remove this logic
-        if node.attr('trans_x'):
-            perm = list(range(len(node.input_shape('X', 0))))
-            perm[-1], perm[-2] = perm[-2], perm[-1]
-            if node.input_dtype('X', 0) == paddle.float64:
-                x = graph.make_node('Cast', inputs=x, to=dtypes.ONNX.FLOAT)
-            x = graph.make_node('Transpose', inputs=[x], perm=perm)
-        if node.attr('trans_y'):
-            perm = list(range(len(node.input_shape('Y', 0))))
-            perm[-1], perm[-2] = perm[-2], perm[-1]
-            if node.input_dtype('Y', 0) == paddle.float64:
-                y = graph.make_node('Cast', inputs=y, to=dtypes.ONNX.FLOAT)
-            y = graph.make_node('Transpose', inputs=[y], perm=perm)
-        if node.input_dtype('X', 0) == paddle.float64:
-            output_node = graph.make_node('MatMul', inputs=[x, y])
-            graph.make_node(
-                'Cast', inputs=output_node, to=dtypes.ONNX.DOUBLE, outputs=out)
-        else:
-            graph.make_node('MatMul', inputs=[x, y], outputs=out)
-
-
-@op_mapper('exp')
-class Exp():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        graph.make_node(
-            'Exp', inputs=node.input('X'), outputs=node.output('Out'))
-
-
-@op_mapper('abs')
-class Abs:
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        graph.make_node(
-            'Abs', inputs=node.input('X'), outputs=node.output('Out'))
-
-
-@op_mapper('erf')
-class Erf():
-    support_opset_version_range = (9, 15)
-
-    @classmethod
-    def opset_9(cls, graph, node, **kw):
-        x_dtype = node.input_dtype('X', 0)
-        x = node.input('X', 0)
-        # onnxruntime only support float32 Erf
-        if x_dtype != paddle.float32:
-            x = graph.make_node('Cast', inputs=x, to=dtypes.ONNX.FLOAT)
-            erf_node = graph.make_node('Erf', inputs=[x])
-            graph.make_node(
-                'Cast',
-                inputs=[erf_node],
-                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype],
-                outputs=node.output('Out'))
-        else:
-            graph.make_node('Erf', inputs=[x], outputs=node.output('Out'))
-
-
-@op_mapper('acos')
-class Acos():
-    supports_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        graph.make_node(
-            'Acos', inputs=node.input('X'), outputs=node.output('Out'))
-
-
-@op_mapper('asin')
-class Asin():
-    supports_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        graph.make_node(
-            'Asin', inputs=node.input('X'), outputs=node.output('Out'))
-
-
-@op_mapper('sinh')
-class Sinh():
-    supports_opset_version_range = (9, 15)
-
-    @classmethod
-    def opset_9(cls, graph, node, **kw):
-        graph.make_node(
-            'Sinh', inputs=node.input('X'), outputs=node.output('Out'))
-
-
-@op_mapper('sin')
-class Sin():
-    supports_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        graph.make_node(
-            'Sin', inputs=node.input('X'), outputs=node.output('Out'))
-
-
-@op_mapper('atan')
-class Atan():
-    supports_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        graph.make_node(
-            'Atan', inputs=node.input('X'), outputs=node.output('Out'))
-
-
-@op_mapper('tan')
-class Tan():
-    supports_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        graph.make_node(
-            'Tan', inputs=node.input('X'), outputs=node.output('Out'))
-
-
-@op_mapper('ceil')
-class Ceil():
-    supports_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_6(cls, graph, node, **kw):
-        graph.make_node(
-            'Ceil', inputs=node.input('X'), outputs=node.output('Out'))
-
-
-@op_mapper('cos')
-class Cos():
-    supports_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        graph.make_node(
-            'Cos', inputs=node.input('X'), outputs=node.output('Out'))
-
-
-@op_mapper('cosh')
-class Cosh():
-    supports_opset_version_range = (9, 15)
-
-    @classmethod
-    def opset_9(cls, graph, node, **kw):
-        graph.make_node(
-            'Cosh', inputs=node.input('X'), outputs=node.output('Out'))
-
-
-@op_mapper('log2')
-class Log2():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        _ln2 = 0.693147180559945309
-        dtype = dtypes.ONNX.FLOAT
-        if node.input_dtype('X', 0) == paddle.float64:
-            dtype = dtypes.ONNX.DOUBLE
-        _ln2 = graph.make_node('Constant', dtype=dtype, value=_ln2)
-        lnx = graph.make_node('Log', inputs=node.input('X'))
-        graph.make_node('Div', inputs=[lnx, _ln2], outputs=node.output('Out'))
-
-
-@op_mapper('logsumexp')
-class LogSumExp():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-
-        if node.attr('reduce_all'):
-            if not node.attr('keepdim'):
-                reduce_node = graph.make_node(
-                    'ReduceLogSumExp',
-                    inputs=node.input('X'),
-                    keepdims=node.attr('keepdim'))
-                mapper_helper.unsqueeze_helper(graph, reduce_node, [0],
-                                               node.output('Out'))
-            else:
-                graph.make_node(
-                    'ReduceLogSumExp',
-                    inputs=node.input('X'),
-                    keepdims=node.attr('keepdim'),
-                    outputs=node.output('Out'))
-        else:
-            graph.make_node(
-                'ReduceLogSumExp',
-                inputs=node.input('X'),
-                keepdims=node.attr('keepdim'),
-                axes=node.attr('axis'),
-                outputs=node.output('Out'))
-
-
-@op_mapper(
-    [
-        'elementwise_add', 'elementwise_sub', 'elementwise_div',
-        'elementwise_mul', 'elementwise_min', 'elementwise_max',
-        'elementwise_pow'
-    ],
-    mapper_dict={
-        'elementwise_add': 'Add',
-        'elementwise_sub': 'Sub',
-        'elementwise_div': 'Div',
-        'elementwise_mul': 'Mul',
-        'elementwise_min': 'Min',
-        'elementwise_max': 'Max',
-        'elementwise_pow': 'Pow'
-    })
-class ElementwiseOps():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        x_shape = node.input_shape('X', 0)
-        y_shape = node.input_shape('Y', 0)
-        if node.type in ["elementwise_min", "elementwise_max"]:
-            assert False, "{} op is not supported when opset version < 8".format(
-                node.type)
-
-        op_type = kw['mapper_dict'][node.type]
-        axis = node.attr('axis')
-        x = node.input('X', 0)
-        y = node.input('Y', 0)
-
-        if axis == -1 or axis == (len(x_shape) - 1
-                                  ) or len(x_shape) == len(y_shape):
-            onnx_node = graph.make_node(
-                op_type, inputs=[x, y], outputs=node.output('Out'))
-        else:
-            broadcast_shape = [1] * len(x_shape)
-            broadcast_shape[axis:axis + len(y_shape)] = y_shape
-            broadcast_shape_node = graph.make_node(
-                'Constant',
-                dtype=dtypes.ONNX.INT64,
-                value=list(broadcast_shape))
-            y_node = graph.make_node(
-                'Reshape', inputs=[y, broadcast_shape_node])
-            onnx_node = graph.make_node(
-                op_type, inputs=[x, y_node], outputs=node.output('Out'))
-
-    @classmethod
-    def opset_8(cls, graph, node, **kw):
-        op_type = kw['mapper_dict'][node.type]
-        x = node.input('X', 0)
-        y = node.input('Y', 0)
-        axis = node.attr('axis')
-        x_shape = node.input_shape('X', 0)
-        y_shape = node.input_shape('Y', 0)
-        if axis == -1 or axis == (len(x_shape) - 1
-                                  ) or len(x_shape) == len(y_shape):
-            onnx_node = graph.make_node(
-                op_type, inputs=[x, y], outputs=node.output('Out'))
-        else:
-            broadcast_shape = [1] * len(x_shape)
-            broadcast_shape[axis:axis + len(y_shape)] = y_shape
-            broadcast_shape_node = graph.make_node(
-                'Constant',
-                dtype=dtypes.ONNX.INT64,
-                value=list(broadcast_shape))
-            y_node = graph.make_node(
-                'Reshape', inputs=[y, broadcast_shape_node])
-            onnx_node = graph.make_node(
-                op_type, inputs=[x, y_node], outputs=node.output('Out'))
-
-
-@op_mapper('elementwise_mod')
-class ElementWiseMod():
-    support_opset_version_range = (10, 15)
-
-    @classmethod
-    def opset_10(cls, graph, node, **kw):
-        x_shape = node.input_shape('X', 0)
-        y_shape = node.input_shape('Y', 0)
-        axis = node.attr('axis')
-        x = node.input('X', 0)
-        y = node.input('Y', 0)
-
-        if node.input_dtype('Y', 0) == paddle.int32 or node.input_dtype(
-                'Y', 0) == paddle.int64:
-            onnx_node = graph.make_node(
-                "Mod", inputs=[x, y], outputs=node.output('Out'))
-            return
-
-        fmod = 1
-
-        abs_x_node = graph.make_node("Abs", inputs=[x])
-        abs_y_node = graph.make_node("Abs", inputs=[y])
-
-        dtype = dtypes.ONNX.FLOAT
-        val_0 = [0.0]
-        val_1 = [-1.0]
-        if node.input_dtype('Y', 0) == paddle.float64:
-            dtype = dtypes.ONNX.DOUBLE
-        if node.input_dtype('Y', 0) == paddle.int32:
-            dtype = dtypes.ONNX.INT32
-            val_0 = [0]
-            val_1 = [-1]
-        if node.input_dtype('Y', 0) == paddle.int64:
-            dtype = dtypes.ONNX.INT64
-            val_0 = [0]
-            val_1 = [-1]
-        zero_node = graph.make_node('Constant', dtype=dtype, value=val_0)
-        one_node = graph.make_node('Constant', dtype=dtype, value=val_1)
-
-        mod_node = graph.make_node(
-            "Mod", inputs=[abs_x_node, abs_y_node], fmod=fmod)
-
-        minus_node = graph.make_node("Mul", inputs=[mod_node, one_node])
-
-        condition_dtype = graph.make_node("Less", inputs=[x, zero_node])
-        condition = graph.make_node(
-            'Cast', inputs=[condition_dtype], to=dtypes.ONNX.BOOL)
-
-        mod_res = graph.make_node(
-            "Where", inputs=[condition, minus_node, mod_node])
-
-        add_node = graph.make_node("Add", inputs=[mod_res, y])
-
-        mod_y_mul_node = graph.make_node("Mul", inputs=[mod_res, y])
-        condition_dtype_1 = graph.make_node(
-            "Less", inputs=[mod_y_mul_node, zero_node])
-        condition_1 = graph.make_node(
-            'Cast', inputs=[condition_dtype_1], to=dtypes.ONNX.BOOL)
-
-        graph.make_node(
-            "Where",
-            inputs=[condition_1, add_node, mod_res],
-            outputs=node.output('Out'))
-
-
-@op_mapper('elementwise_floordiv')
-class ElementWiseFloorDiv():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        x = node.input('X', 0)
-        y = node.input('Y', 0)
-        axis = node.attr('axis')
-        x_shape = node.input_shape('X', 0)
-        y_shape = node.input_shape('Y', 0)
-        x_dtype = node.input_dtype('X', 0)
-        y_dtype = node.input_dtype('Y', 0)
-        x_dtype = dtypes.DTYPE_PADDLE_STR_MAP[x_dtype]
-        y_dtype = dtypes.DTYPE_PADDLE_STR_MAP[y_dtype]
-        is_int = False
-        if x_dtype.count('int') > 0 and y_dtype.count('int') > 0:
-            is_int = True
-        if axis == -1 or axis == (len(x_shape) - 1
-                                  ) or len(x_shape) == len(y_shape):
-            if is_int:
-                graph.make_node(
-                    'Div', inputs=[x, y], outputs=node.output('Out'))
-            else:
-                div_node = graph.make_node('Div', inputs=[x, y])
-                graph.make_node(
-                    'Floor', inputs=[div_node], outputs=node.output('Out'))
-        else:
-            broadcast_shape = [1] * len(x_shape)
-            broadcast_shape[axis:axis + len(y_shape)] = y_shape
-            broadcast_shape_node = graph.make_node(
-                'Constant',
-                dtype=dtypes.ONNX.INT64,
-                value=list(broadcast_shape))
-            y_node = graph.make_node(
-                'Reshape', inputs=[y, broadcast_shape_node])
-            if is_int:
-                div_node = graph.make_node(
-                    'Div', inputs=[x, y_node], outputs=node.output('Out'))
-            else:
-                div_node = graph.make_node('Div', inputs=[x, y_node])
-                graph.make_node(
-                    'Floor', inputs=[div_node], outputs=node.output('Out'))
-
-
-@op_mapper('pow')
-class Pow():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        x = node.input('X', 0)
-        x_dtype = node.input_dtype('X', 0)
-        factor = node.attr('factor')
-        # Pow-7 Only support input type as float and double
-        if x_dtype == paddle.int32 or x_dtype == paddle.int64:
-            x = graph.make_node('Cast', inputs=[x], to=dtypes.ONNX.FLOAT)
-            factor_node = graph.make_node(
-                'Constant',
-                inputs=[],
-                dims=[1],
-                dtype=dtypes.ONNX.FLOAT,
-                value=factor)
-        else:
-            factor_node = graph.make_node(
-                'Constant',
-                inputs=[],
-                dims=[1],
-                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype],
-                value=factor)
-        if x_dtype == paddle.int32 or x_dtype == paddle.int64:
-            pow_node = graph.make_node('Pow', inputs=[x, factor_node])
-            graph.make_node(
-                'Cast',
-                inputs=[pow_node],
-                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype],
-                outputs=node.output('Out'))
-        else:
-            graph.make_node(
-                'Pow', inputs=[x, factor_node], outputs=node.output('Out'))
-
-    @classmethod
-    def opset_12(cls, graph, node, **kw):
-        x = node.input('X', 0)
-        factor = node.attr('factor')
-        factor_node = graph.make_node(
-            'Constant',
-            inputs=[],
-            dims=[1],
-            dtype=dtypes.ONNX.FLOAT,
-            value=factor)
-        pow_node = graph.make_node(
-            'Pow', inputs=[x, factor_node], outputs=node.output('Out'))
-
-
-@op_mapper('square')
-class Square():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        x = node.input('X', 0)
-        onnx_node = graph.make_node(
-            'Mul', inputs=[x, x], outputs=node.output('Out'))
-
-
-@op_mapper('cumsum')
-class CumSum():
-    support_opset_version_range = (11, 15)
-
-    @classmethod
-    def opset_11(cls, graph, node, **kw):
-
-        axis = graph.make_node(
-            'Constant', dtype=dtypes.ONNX.INT64, value=node.attr('axis'))
-        graph.make_node(
-            'CumSum',
-            inputs=[node.input('X', 0), axis],
-            outputs=node.output('Out'))
-
-
-@op_mapper('mul')
-class Mul():
-    support_opset_version_range = (5, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        x = node.input('X', 0)
-        y = node.input('Y', 0)
-        out = node.output('Out', 0)
-        x_num_col_dims = node.attr('x_num_col_dims')
-        y_num_col_dims = node.attr('y_num_col_dims')
-        flatten_x = graph.make_node(
-            'Flatten', inputs=node.input('X'), attrs={'axis': x_num_col_dims})
-        flatten_y = graph.make_node(
-            'Flatten', inputs=node.input('Y'), attrs={'axis': y_num_col_dims})
-        mul_node = graph.make_node('MatMul', inputs=[flatten_x, flatten_y])
-
-        x_shape = graph.make_node('Shape', inputs=[x])
-        l_shape = mapper_helper.slice_helper(
-            graph, x_shape, axes=[0], starts=[0], ends=[x_num_col_dims])
-        y_shape = graph.make_node('Shape', inputs=[y])
-        y_rank = len(node.input_shape('Y', 0))
-        r_shape = mapper_helper.slice_helper(
-            graph, y_shape, axes=[0], starts=[y_num_col_dims], ends=[y_rank])
-
-        out_shape = graph.make_node('Concat', inputs=[l_shape, r_shape], axis=0)
-        graph.make_node('Reshape', [mul_node, out_shape], node.output('Out'))
-
-
-@op_mapper('affine_channel')
-class AffineChannel():
-    support_opset_version_range = (1, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        if "data_layout" in node.attrs.keys():
-            assert node.attrs['data_layout'] == 'NCHW' or node.attrs['data_layout'] == "AnyLayout",  \
-                                "The affine_channel data format should be 'NCHW', but received data format " \
-                                "is %s." % node.attrs['data_layout']
-        x = node.input('X', 0)
-        bias = node.input('Bias', 0)
-        scale = node.input('Scale', 0)
-        scale = mapper_helper.unsqueeze_helper(graph, scale, [0, 2, 3])
-        bias = mapper_helper.unsqueeze_helper(graph, bias, [0, 2, 3])
-        x = graph.make_node('Mul', inputs=[x, scale])
-        x = graph.make_node('Add', inputs=[x, bias], outputs=node.output('Out'))
-
-
-@op_mapper('bmm')
-class BMM():
-    support_opset_version_range = (1, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        x = node.input('X', 0)
-        y = node.input('Y', 0)
-        mul_node = graph.make_node(
-            'MatMul', inputs=[x, y], outputs=node.output('Out'))
-
-
-@op_mapper('p_norm')
-class PNorm():
-    support_opset_version_range = (1, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        x = node.input('X', 0)
-        axis = node.attr('axis')
-        if isinstance(axis, (int, float)):
-            axis = [axis]
-        p = node.attr('porder')
-        keepdim = node.attr('keepdim')
-        dtype = dtypes.ONNX.FLOAT
-        if node.input_dtype('X', 0) == paddle.float64:
-            dtype = dtypes.ONNX.DOUBLE
-
-        pnode = graph.make_node('Constant', dtype=dtype, value=[p])
-
-        abs_node = graph.make_node('Abs', inputs=[x])
-        pow_node = graph.make_node('Pow', inputs=[abs_node, pnode])
-        reduce_sum = graph.make_node(
-            'ReduceSum', inputs=[pow_node], axes=axis, keepdims=keepdim)
-        pnode1 = graph.make_node('Constant', dtype=dtype, value=[1.0 / p])
-        graph.make_node(
-            'Pow', inputs=[reduce_sum, pnode1], outputs=node.output('Out'))
-
-    @classmethod
-    def opset_13(cls, graph, node, **kw):
-        x = node.input('X', 0)
-        axis = node.attr('axis')
-        if isinstance(axis, (int, float)):
-            axis = [axis]
-        p = node.attr('porder')
-        keepdim = node.attr('keepdim')
-        pnode = graph.make_node('Constant', dtype=dtypes.ONNX.FLOAT, value=[p])
-        abs_node = graph.make_node('Abs', inputs=[x])
-        pow_node = graph.make_node('Pow', inputs=[abs_node, pnode])
-        axes = graph.make_node('Constant', dtype=dtypes.ONNX.INT64, value=axis)
-        reduce_sum = graph.make_node(
-            'ReduceSum', inputs=[pow_node, axes], keepdims=keepdim)
-        pnode1 = graph.make_node(
-            'Constant', dtype=dtypes.ONNX.FLOAT, value=[1.0 / p])
-        graph.make_node(
-            'Pow', inputs=[reduce_sum, pnode1], outputs=node.output('Out'))
-
-
-@op_mapper('sum')
-class Sum():
-    support_opset_version_range = (1, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        graph.make_node(
-            'Sum', inputs=node.input('X'), outputs=node.output('Out'))
-
-
-@op_mapper('floor')
-class Floor():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        graph.make_node(
-            'Floor', inputs=node.input('X'), outputs=node.output('Out'))
-
-
-@op_mapper('log10')
-class Log10():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        _ln10 = 2.30258509299404568401
-        dtype = dtypes.ONNX.FLOAT
-        if node.input_dtype('X', 0) == paddle.float64:
-            dtype = dtypes.ONNX.DOUBLE
-        _ln10 = graph.make_node('Constant', dtype=dtype, value=_ln10)
-        lnx = graph.make_node('Log', inputs=node.input('X'))
-        graph.make_node('Div', inputs=[lnx, _ln10], outputs=node.output('Out'))
-
-
-@op_mapper('log1p')
-class Log1p():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        dtype = dtypes.ONNX.FLOAT
-        if node.input_dtype('X', 0) == paddle.float64:
-            dtype = dtypes.ONNX.DOUBLE
-        one = graph.make_node('Constant', attrs={'dtype': dtype, 'value': [1]})
-        add_node = graph.make_node('Add', inputs=[node.input('X', 0), one])
-        graph.make_node('Log', inputs=add_node, outputs=node.output('Out'))
-
-
-@op_mapper(
-    ['reduce_all', 'reduce_any'],
-    mapper_dict={'reduce_all': 'ReduceMin',
-                 'reduce_any': 'ReduceMax'})
-class ReduceAll():
-    support_opset_version_range = (6, 15)
-
-    @classmethod
-    def opset_6(cls, graph, node, **kw):
-        op_type = kw['mapper_dict'][node.type]
-        input_dtype = node.block.vars[node.input('X', 0)].dtype
-        input_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[input_dtype]
-        all_node = graph.make_node(
-            'Cast', inputs=[node.input('X', 0)], to=dtypes.ONNX.INT32)
-
-        attrs = {'keepdims': node.attr('keep_dim'), }
-        if not node.attr('reduce_all'):
-            attrs['axes'] = node.attr('dim')
-        output_node = graph.make_node(op_type, inputs=[all_node], attrs=attrs)
-
-        if node.attr('reduce_all') and not node.attr('keep_dim'):
-            output_node = mapper_helper.unsqueeze_helper(graph, output_node,
-                                                         [0])
-        graph.make_node(
-            'Cast',
-            inputs=[output_node],
-            to=input_dtype,
-            outputs=node.output('Out'))
-
-
-@op_mapper(
-    ['reduce_mean', 'reduce_sum', 'reduce_min', 'reduce_max', 'reduce_prod'],
-    mapper_dict={
-        'reduce_mean': 'ReduceMean',
-        'reduce_sum': 'ReduceSum',
-        'reduce_min': 'ReduceMin',
-        'reduce_max': 'ReduceMax',
-        'reduce_prod': 'ReduceProd'
-    })
-class ReduceMean():
-    support_opset_version_range = (1, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        op_type = kw['mapper_dict'][node.type]
-
-        output_shape = node.output_shape('Out', 0)
-        reduce_all = node.attr("reduce_all")
-        axes = node.attr("dim")
-        if reduce_all:
-            axes = list(range(len(node.input_shape("X", 0))))
-        if len(axes) == len(node.input_shape("X", 0)):
-            reduce_all = True
-        keepdims = node.attr('keep_dim')
-        if keepdims:
-            cls.create_reduce_node(graph, op_type,
-                                   node.input("X"), node.output("Out"), axes, 1)
-        else:
-            if reduce_all:
-                shape = graph.make_node(
-                    "Constant", dtype=dtypes.ONNX.INT64, value=[-1])
-                flatten_node = graph.make_node(
-                    "Reshape", inputs=[node.input("X")[0], shape])
-                cls.create_reduce_node(graph, op_type, [flatten_node],
-                                       node.output("Out"), [0], 1)
-            else:
-                cls.create_reduce_node(graph, op_type,
-                                       node.input("X"),
-                                       node.output("Out"), axes, 0)
-
-    @classmethod
-    def create_reduce_node(cls, graph, op_type, inputs, outputs, axes,
-                           keepdims):
-        if graph.opset_version >= 13 and op_type == "ReduceSum":
-            axes = graph.make_node(
-                "Constant", dtype=dtypes.ONNX.INT64, value=axes)
-            output = graph.make_node(
-                "ReduceSum",
-                inputs=inputs + [axes],
-                outputs=outputs,
-                keepdims=keepdims)
-        else:
-            output = graph.make_node(
-                op_type,
-                inputs=inputs,
-                outputs=outputs,
-                axes=axes,
-                keepdims=keepdims)
-
-
-@op_mapper('mean')
-class Mean():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        shape = graph.make_node("Constant", dtype=dtypes.ONNX.INT64, value=[-1])
-        flatten_node = graph.make_node(
-            "Reshape", inputs=[node.input("X")[0], shape])
-        mean_node = graph.make_node(
-            'ReduceMean',
-            inputs=flatten_node,
-            outputs=node.output("Out"),
-            keepdims=1)
-
-
-@op_mapper('arg_max')
-class ArgMax():
-    support_opset_version_range = (1, 12)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        if node.attr('dtype') and node.attr('dtype') == 2:
-            arg_node = graph.make_node(
-                'ArgMax',
-                inputs=node.input('X'),
-                attrs={
-                    'axis': node.attr('axis'),
-                    'keepdims': node.attr('keepdims')
-                })
-            graph.make_node(
-                'Cast',
-                inputs=arg_node,
-                attrs={'to': dtypes.ONNX.INT32},
-                outputs=node.output('Out'))
-        else:
-            graph.make_node(
-                'ArgMax',
-                inputs=node.input('X'),
-                outputs=node.output('Out'),
-                attrs={
-                    'axis': node.attr('axis'),
-                    'keepdims': node.attr('keepdims')
-                })
-
-
-@op_mapper('arg_min')
-class ArgMin():
-    support_opset_version_range = (1, 12)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        if node.attr('flatten'):
-            flatten = graph.make_node('Flatten', inputs=node.input('X'), axis=0)
-            squeeze_node = graph.make_node('Squeeze', inputs=flatten)
-            graph.make_node(
-                'ArgMin', inputs=squeeze_node, outputs=node.output('Out'))
-        else:
-            if node.attr('keepdims'):
-                graph.make_node(
-                    'ArgMin',
-                    inputs=node.input('X'),
-                    outputs=node.output('Out'),
-                    axis=node.attr('axis'),
-                    keepdims=1)
-            else:
-                graph.make_node(
-                    'ArgMin',
-                    inputs=node.input('X'),
-                    outputs=node.output('Out'),
-                    axis=node.attr('axis'),
-                    keepdims=0)
-
-
-@op_mapper('brelu')
-class Hardtanh():
-    support_opset_version_range = (9, 15)
-
-    @classmethod
-    def opset_6(cls, graph, node, **kw):
-        mapper_helper.clip_helper(graph, node,
-                                  node.input('X', 0),
-                                  node.attr('t_max'),
-                                  node.attr('t_min'), node.output('Out', 0))
-
-
-@op_mapper('mv')
-class Mv():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        graph.make_node(
-            'MatMul',
-            inputs=[node.input('X', 0), node.input('Vec', 0)],
-            outputs=node.output('Out'))
-
-
-@op_mapper('dot')
-class Dot():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        mul_node = graph.make_node(
-            'Mul', inputs=[node.input('X', 0), node.input('Y', 0)])
-        graph.make_node(
-            'ReduceSum',
-            inputs=[mul_node],
-            axes=[len(node.input_shape('X', 0)) - 1],
-            outputs=node.output('Out'))
-
-    @classmethod
-    def opset_13(cls, graph, node, **kw):
-        mul_node = graph.make_node(
-            'Mul', inputs=[node.input('X', 0), node.input('Y', 0)])
-        one = graph.make_node(
-            'Constant',
-            dtype=dtypes.ONNX.INT64,
-            value=[len(node.input_shape('X', 0)) - 1])
-        graph.make_node(
-            'ReduceSum', inputs=[mul_node, one], outputs=node.output('Out'))
-
-
-@op_mapper('dist')
-class Dist():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        sub_node = graph.make_node(
-            'Sub', inputs=[node.input('X', 0), node.input('Y', 0)])
-        abs_node = graph.make_node('Abs', inputs=sub_node)
-        if node.attr('p') == 0:
-            assert graph.opset_version >= 9, "When p is 0, onnx opset should be (onnx_opset>=9)."
-            sign_node = graph.make_node('Sign', inputs=abs_node)
-            sum_node = graph.make_node(
-                'ReduceSum', inputs=sign_node, keepdims=0)
-            mapper_helper.unsqueeze_helper(graph, sum_node, [0],
-                                           node.output('Out'))
-        elif node.attr('p') == float('inf'):
-            max_node = graph.make_node('ReduceMax', inputs=abs_node, keepdims=0)
-            mapper_helper.unsqueeze_helper(graph, max_node, [0],
-                                           node.output('Out'))
-        elif node.attr('p') == float('-inf'):
-            min_node = graph.make_node('ReduceMin', inputs=abs_node, keepdims=0)
-            mapper_helper.unsqueeze_helper(graph, min_node, [0],
-                                           node.output('Out'))
-        else:
-            x_dtype = node.input_dtype('X', 0)
-            p = graph.make_node(
-                'Constant',
-                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype],
-                value=node.attr('p'))
-            pow_node = graph.make_node(
-                'Pow',
-                inputs=[abs_node, p], )
-            sum_node = graph.make_node('ReduceSum', inputs=pow_node, keepdims=0)
-            sum_node = mapper_helper.unsqueeze_helper(graph, sum_node, [0])
-            p_1 = graph.make_node('Reciprocal', inputs=p)
-            graph.make_node(
-                'Pow', inputs=[sum_node, p_1], outputs=node.output('Out'))
-
-
-@op_mapper('round')
-class Round():
-    support_opset_version_range = (11, 15)
-
-    @classmethod
-    def opset_11(cls, graph, node, **kw):
-        graph.make_node(
-            'Round', inputs=node.input('X'), outputs=node.output('Out'))
-
-
-@op_mapper('rsqrt')
-class Rsqrt():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_6(cls, graph, node, **kw):
-        sqrt_node = graph.make_node('Sqrt', inputs=node.input('X'))
-        graph.make_node(
-            'Reciprocal', inputs=sqrt_node, outputs=node.output('Out'))
-
-
-@op_mapper('sign')
-class Sign():
-    support_opset_version_range = (9, 15)
-
-    @classmethod
-    def opset_9(cls, graph, node, **kw):
-        graph.make_node(
-            'Sign', inputs=node.input('X'), outputs=node.output('Out'))
-
-
-@op_mapper('scale')
-class Scale():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        scale = node.attr('scale')
-        bias = node.attr('bias')
-        if len(node.input('ScaleTensor')) == 0 and np.fabs(
-                scale - 1.0) < 1e-06 and np.fabs(bias - 0.0) < 1e-06:
-            graph.make_node(
-                'Identity', inputs=node.input('X'), outputs=node.output('Out'))
-        else:
-            input_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)]
-            if input_dtype in [
-                    dtypes.ONNX.INT16, dtypes.ONNX.INT32, dtypes.ONNX.INT64
-            ]:
-                outputs = None
-                data_type = dtypes.ONNX.FLOAT
-                cast_node = graph.make_node(
-                    'Cast', inputs=node.input('X'), attrs={'to': data_type})
-            else:
-                outputs = node.output('Out')
-                data_type = input_dtype
-                cast_node = node.input('X')[0]
-
-            if len(node.input('ScaleTensor')) > 0:
-                scale_node = node.input('ScaleTensor')[0]
-                scale_type = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype(
-                    'ScaleTensor', 0)]
-                if scale_type != data_type:
-                    scale_node = graph.make_node(
-                        'Cast', inputs=[scale_node], attrs={'to': data_type})
-            else:
-                scale_node = graph.make_node(
-                    'Constant', attrs={'dtype': data_type,
-                                       'value': [scale]})
-            bias_node = graph.make_node(
-                'Constant', attrs={'dtype': data_type,
-                                   'value': [bias]})
-
-            if node.attr('bias_after_scale'):
-                node1 = graph.make_node('Mul', inputs=[cast_node, scale_node])
-                node2 = graph.make_node(
-                    'Add', inputs=[node1, bias_node], outputs=outputs)
-            else:
-                node1 = graph.make_node('Add', inputs=[cast_node, bias_node])
-                node2 = graph.make_node(
-                    'Mul', inputs=[node1, scale_node], outputs=outputs)
-
-            if input_dtype in [
-                    dtypes.ONNX.INT16, dtypes.ONNX.INT32, dtypes.ONNX.INT64
-            ]:
-                cast_node = graph.make_node(
-                    'Cast',
-                    inputs=node2,
-                    outputs=node.output('Out'),
-                    attrs={'to': input_dtype})
-
-
-@op_mapper('softmax')
-class Softmax():
-    support_opset_version_range = (1, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        axis = node.attr('axis')
-        shape = node.output_shape('Out', 0)
-        if axis is None:
-            axis = -1
-        if axis < 0:
-            axis += len(shape)
-        if axis == len(shape) - 1:
-            node = graph.make_node(
-                'Softmax',
-                inputs=node.input('X'),
-                outputs=node.output('Out'),
-                attrs={'axis': axis})
-        else:
-            perm = [i for i in range(len(shape))]
-            perm[-1] = axis
-            perm[axis] = len(shape) - 1
-            transpose_node = graph.make_node(
-                'Transpose', inputs=node.input('X'), attrs={'perm': perm})
-            softmax_node = graph.make_node(
-                'Softmax', inputs=[transpose_node], axis=-1)
-            transpose_node1 = graph.make_node(
-                'Transpose',
-                inputs=[softmax_node],
-                outputs=node.output('Out'),
-                attrs={'perm': perm})
-
-    @classmethod
-    def opset_13(cls, graph, node, **kw):
-        graph.make_node(
-            'Softmax',
-            inputs=node.input('X'),
-            axis=node.attr('axis'),
-            outputs=node.output('Out'))
-
-
-@op_mapper('unfold')
-class Unfold():
-    support_opset_version_range = (11, 15)
-
-    @classmethod
-    def opset_11(cls, graph, node, **kw):
-
-        strides = node.attr('strides')
-        stride_h = strides[0]
-        stride_w = strides[1]
-
-        paddings = node.attr('paddings')
-        padding_h_1 = paddings[0]
-        padding_w_1 = paddings[1]
-        padding_h_2 = paddings[2]
-        padding_w_2 = paddings[3]
-
-        dilations = node.attr('dilations')
-        dilation_h = dilations[0]
-        dilation_w = dilations[1]
-
-        kernel_sizes = node.attr('kernel_sizes')
-        kernel_h = kernel_sizes[0]
-        kernel_w = kernel_sizes[1]
-
-        input_w = mapper_helper.shape_helper(graph, node.input('X', 0), 3)
-        blocks_row_indices_node = cls._get_im2col_indices_along_dim(
-            graph, node, 2, kernel_h, dilation_h, padding_h_1, padding_h_2,
-            stride_h)
-        blocks_col_indices_node = cls._get_im2col_indices_along_dim(
-            graph, node, 3, kernel_w, dilation_w, padding_w_1, padding_w_2,
-            stride_w)
-
-        output_shape = cls._get_im2col_output_shape(graph, node, kernel_h,
-                                                    kernel_w)
-        padded_input = cls._get_im2col_padded_input(
-            graph, node, padding_h_1, padding_h_2, padding_w_1, padding_w_2)
-
-        output = graph.make_node(
-            'Gather', inputs=[padded_input, blocks_row_indices_node], axis=2)
-
-        output = graph.make_node(
-            'Gather', inputs=[output, blocks_col_indices_node], axis=4)
-        output = graph.make_node(
-            'Transpose', inputs=[output], perm=[0, 1, 2, 4, 3, 5])
-
-        graph.make_node(
-            'Reshape', inputs=[output, output_shape], outputs=node.output('Y'))
-
-    @classmethod
-    def _get_im2col_indices_along_dim(cls, graph, node, index, kernel_size_d,
-                                      dilation_d, padding_d_1, padding_d_2,
-                                      stride_d):
-        input_shape = node.input_shape('X', 0)
-        if input_shape[index] == -1:
-            input_d_node = mapper_helper.shape_helper(graph,
-                                                      node.input('X', 0), index)
-
-            padding_d_node = graph.make_node(
-                'Constant',
-                dtype=dtypes.ONNX.INT64,
-                value=[padding_d_1 + padding_d_2])
-            blocks_d_node = graph.make_node(
-                'Add', inputs=[input_d_node, padding_d_node])
-
-            dilation_kernel_size_node = graph.make_node(
-                'Constant',
-                dtype=dtypes.ONNX.INT64,
-                value=[dilation_d * (kernel_size_d - 1)])
-            blocks_d_node = graph.make_node(
-                'Sub', inputs=[blocks_d_node, dilation_kernel_size_node])
-
-            zero_node = graph.make_node(
-                'Constant', dtype=dtypes.ONNX.INT64, value=[0])
-            stride_node = graph.make_node(
-                'Constant', dtype=dtypes.ONNX.INT64, value=[stride_d])
-            blocks_d_indices_node = graph.make_node(
-                'Range', inputs=[zero_node, blocks_d_node, stride_node])
-        else:
-            end = input_shape[
-                index] + padding_d_1 + padding_d_2 - dilation_d * (kernel_size_d
-                                                                   - 1)
-            stride = stride_d
-            blocks_d_indices = np.arange(0, end, stride)
-            blocks_d_indices_node = graph.make_node(
-                'Constant',
-                dtype=dtypes.ONNX.INT64,
-                value=blocks_d_indices.flatten().tolist())
-
-        kernel_grid = np.arange(0, kernel_size_d * dilation_d, dilation_d)
-        kernel_grid_node = graph.make_node(
-            'Constant',
-            dtype=dtypes.ONNX.INT64,
-            value=kernel_grid.flatten().tolist())
-
-        shape_node = graph.make_node(
-            'Constant', dtype=dtypes.ONNX.INT64, value=[-1, 1])
-        kernel_mask_node = graph.make_node(
-            'Reshape', inputs=[kernel_grid_node, shape_node])
-
-        block_mask_node = graph.make_node(
-            'Add', inputs=[blocks_d_indices_node, kernel_mask_node])
-        return block_mask_node
-
-    @classmethod
-    def _get_im2col_output_shape(cls, graph, node, kernel_h, kernel_w):
-        batch_dim = mapper_helper.shape_helper(graph, node.input('X', 0), 0)
-        channel_dim = mapper_helper.shape_helper(graph, node.input('X', 0), 1)
-
-        constant_node = graph.make_node(
-            'Constant', dtype=dtypes.ONNX.INT64, value=[kernel_h * kernel_w])
-        channel_unfolded = graph.make_node(
-            'Mul', inputs=[channel_dim, constant_node])
-
-        concat_const_node = graph.make_node(
-            'Constant', dtype=dtypes.ONNX.INT64, value=[-1])
-        result_node = graph.make_node(
-            'Concat',
-            inputs=[batch_dim, channel_unfolded, concat_const_node],
-            axis=0)
-
-        return result_node
-
-    @classmethod
-    def _get_im2col_padded_input(cls, graph, node, padding_h_1, padding_h_2,
-                                 padding_w_1, padding_w_2):
-        pad_const_node = graph.make_node(
-            'Constant',
-            dtype=dtypes.ONNX.INT64,
-            value=[
-                0, 0, padding_h_1, padding_w_1, 0, 0, padding_h_2, padding_w_2
-            ])
-        result_node = graph.make_node(
-            'Pad', inputs=[node.input('X', 0), pad_const_node])
-        return result_node
-
-
-@op_mapper('softmax_with_cross_entropy')
-class SoftmaxCrossEntropyLoss():
-    support_opset_version_range = (12, 15)
-
-    @classmethod
-    def opset_12(cls, graph, node, **kw):
-        if node.attr('soft_label'):
-            raise Exception(
-                "SoftmaxCrossEntropyLoss in onnx not support soft label.")
-        scores = node.input('Logits', 0)
-        labels = node.input('Label', 0)
-        # Whether return_softmax is True or False, the model will have two outputs
-        outputs = [node.output('Loss', 0), node.output('Softmax', 0)]
-
-        shape = node.input_shape('Logits', 0)
-        if len(shape) < 2:
-            raise Exception(
-                "SoftmaxCrossEntropyLoss in onnx not support 1D logits.")
-        axis = node.attr('axis')
-        if axis < 0:
-            axis += len(shape)
-        if axis == 1:
-            squeeze_node = mapper_helper.squeeze_helper(graph, labels, [axis])
-            loss_node, softmax_node = graph.make_node(
-                'SoftmaxCrossEntropyLoss',
-                inputs=[scores, squeeze_node],
-                outputs=2,
-                ignore_index=node.attr('ignore_index'),
-                reduction='none')
-            loss_node = mapper_helper.unsqueeze_helper(graph, loss_node,
-                                                       [axis], outputs[0])
-            # onnx output is log(softmax), but paddle output is softmax
-            graph.make_node('Exp', inputs=[softmax_node], outputs=outputs[1])
-        else:
-            perm = [i for i in range(len(shape))]
-            perm[1] = axis
-            perm[axis] = 1
-            transpose_scores = graph.make_node(
-                'Transpose', inputs=[scores], perm=perm)
-            transpose_labels = graph.make_node(
-                'Transpose', inputs=[labels], perm=perm)
-            squeeze_labels = mapper_helper.squeeze_helper(
-                graph, transpose_labels, [1])
-
-            loss_node, softmax_node = graph.make_node(
-                'SoftmaxCrossEntropyLoss',
-                inputs=[transpose_scores, squeeze_labels],
-                ignore_index=node.attr('ignore_index'),
-                outputs=2,
-                reduction='none')
-            output_node = mapper_helper.unsqueeze_helper(graph, loss_node, [1])
-            graph.make_node(
-                'Transpose', inputs=output_node, outputs=outputs[0], perm=perm)
-            softmax_node = graph.make_node(
-                'Transpose', inputs=softmax_node, perm=perm)
-            # onnx output is log(softmax), but paddle output is softmax
-            graph.make_node('Exp', inputs=[softmax_node], outputs=outputs[1])
+#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import numpy as np
+from paddle2onnx.legacy.constant import dtypes
+from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
+from paddle2onnx.legacy.op_mapper import mapper_helper
+import paddle
+
+
+@op_mapper('matmul')
+class MatMul():
+    support_opset_version_range = (1, 12)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        x = node.input('X', idx=0)
+        y = node.input('Y', idx=0)
+        if node.attr('transpose_X'):
+            perm = list(range(len(node.input_shape('X', 0))))
+            perm[-1], perm[-2] = perm[-2], perm[-1]
+            if node.input_dtype('X', 0) == paddle.float64:
+                x = graph.make_node('Cast', inputs=x, to=dtypes.ONNX.FLOAT)
+            x = graph.make_node('Transpose', inputs=[x], perm=perm)
+        if node.attr('transpose_Y'):
+            perm = list(range(len(node.input_shape('Y', 0))))
+            perm[-1], perm[-2] = perm[-2], perm[-1]
+            if node.input_dtype('Y', 0) == paddle.float64:
+                y = graph.make_node('Cast', inputs=y, to=dtypes.ONNX.FLOAT)
+            y = graph.make_node('Transpose', inputs=[y], perm=perm)
+        if node.attr('alpha') == 1.0:
+            if node.input_dtype('X', 0) == paddle.float64:
+                output_node = graph.make_node('MatMul', inputs=[x, y])
+                graph.make_node(
+                    'Cast',
+                    inputs=output_node,
+                    to=dtypes.ONNX.DOUBLE,
+                    outputs=node.output('Out'))
+            else:
+                graph.make_node(
+                    'MatMul', inputs=[x, y], outputs=node.output('Out'))
+        else:
+            if node.input_dtype('X', 0) == paddle.float64:
+                output_node = graph.make_node('MatMul', inputs=[x, y])
+                matmul = graph.make_node(
+                    'Cast', inputs=output_node, to=dtypes.ONNX.DOUBLE)
+                scale = graph.make_node(
+                    'Constant',
+                    dtype=dtypes.ONNX.DOUBLE,
+                    value=node.attr('alpha'))
+            else:
+                matmul = graph.make_node('MatMul', inputs=[x, y])
+                scale = graph.make_node(
+                    'Constant',
+                    dtype=dtypes.ONNX.FLOAT,
+                    value=node.attr('alpha'))
+
+            onnx_node = graph.make_node(
+                'Mul', inputs=[matmul, scale], outputs=node.output('Out'))
+
+
+@op_mapper('matmul_v2')
+class MatMul():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        x = node.input('X', idx=0)
+        y = node.input('Y', idx=0)
+        out = node.output('Out')
+        ## TODO(wangjunjie06): The current addition of cast op is only for onnxruntime optimization, after onnxruntime is repaired, remove this logic
+        if node.attr('trans_x'):
+            perm = list(range(len(node.input_shape('X', 0))))
+            perm[-1], perm[-2] = perm[-2], perm[-1]
+            if node.input_dtype('X', 0) == paddle.float64:
+                x = graph.make_node('Cast', inputs=x, to=dtypes.ONNX.FLOAT)
+            x = graph.make_node('Transpose', inputs=[x], perm=perm)
+        if node.attr('trans_y'):
+            perm = list(range(len(node.input_shape('Y', 0))))
+            perm[-1], perm[-2] = perm[-2], perm[-1]
+            if node.input_dtype('Y', 0) == paddle.float64:
+                y = graph.make_node('Cast', inputs=y, to=dtypes.ONNX.FLOAT)
+            y = graph.make_node('Transpose', inputs=[y], perm=perm)
+        if node.input_dtype('X', 0) == paddle.float64:
+            output_node = graph.make_node('MatMul', inputs=[x, y])
+            graph.make_node(
+                'Cast', inputs=output_node, to=dtypes.ONNX.DOUBLE, outputs=out)
+        else:
+            graph.make_node('MatMul', inputs=[x, y], outputs=out)
+
+
+@op_mapper('exp')
+class Exp():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        graph.make_node(
+            'Exp', inputs=node.input('X'), outputs=node.output('Out'))
+
+
+@op_mapper('abs')
+class Abs:
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        graph.make_node(
+            'Abs', inputs=node.input('X'), outputs=node.output('Out'))
+
+
+@op_mapper('erf')
+class Erf():
+    support_opset_version_range = (9, 15)
+
+    @classmethod
+    def opset_9(cls, graph, node, **kw):
+        x_dtype = node.input_dtype('X', 0)
+        x = node.input('X', 0)
+        # onnxruntime only support float32 Erf
+        if x_dtype != paddle.float32:
+            x = graph.make_node('Cast', inputs=x, to=dtypes.ONNX.FLOAT)
+            erf_node = graph.make_node('Erf', inputs=[x])
+            graph.make_node(
+                'Cast',
+                inputs=[erf_node],
+                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype],
+                outputs=node.output('Out'))
+        else:
+            graph.make_node('Erf', inputs=[x], outputs=node.output('Out'))
+
+
+@op_mapper('acos')
+class Acos():
+    supports_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        graph.make_node(
+            'Acos', inputs=node.input('X'), outputs=node.output('Out'))
+
+
+@op_mapper('asin')
+class Asin():
+    supports_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        graph.make_node(
+            'Asin', inputs=node.input('X'), outputs=node.output('Out'))
+
+
+@op_mapper('sinh')
+class Sinh():
+    supports_opset_version_range = (9, 15)
+
+    @classmethod
+    def opset_9(cls, graph, node, **kw):
+        graph.make_node(
+            'Sinh', inputs=node.input('X'), outputs=node.output('Out'))
+
+
+@op_mapper('sin')
+class Sin():
+    supports_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        graph.make_node(
+            'Sin', inputs=node.input('X'), outputs=node.output('Out'))
+
+
+@op_mapper('atan')
+class Atan():
+    supports_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        graph.make_node(
+            'Atan', inputs=node.input('X'), outputs=node.output('Out'))
+
+
+@op_mapper('tan')
+class Tan():
+    supports_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        graph.make_node(
+            'Tan', inputs=node.input('X'), outputs=node.output('Out'))
+
+
+@op_mapper('ceil')
+class Ceil():
+    supports_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_6(cls, graph, node, **kw):
+        graph.make_node(
+            'Ceil', inputs=node.input('X'), outputs=node.output('Out'))
+
+
+@op_mapper('cos')
+class Cos():
+    supports_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        graph.make_node(
+            'Cos', inputs=node.input('X'), outputs=node.output('Out'))
+
+
+@op_mapper('cosh')
+class Cosh():
+    supports_opset_version_range = (9, 15)
+
+    @classmethod
+    def opset_9(cls, graph, node, **kw):
+        graph.make_node(
+            'Cosh', inputs=node.input('X'), outputs=node.output('Out'))
+
+
+@op_mapper('log2')
+class Log2():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        _ln2 = 0.693147180559945309
+        dtype = dtypes.ONNX.FLOAT
+        if node.input_dtype('X', 0) == paddle.float64:
+            dtype = dtypes.ONNX.DOUBLE
+        _ln2 = graph.make_node('Constant', dtype=dtype, value=_ln2)
+        lnx = graph.make_node('Log', inputs=node.input('X'))
+        graph.make_node('Div', inputs=[lnx, _ln2], outputs=node.output('Out'))
+
+
+@op_mapper('logsumexp')
+class LogSumExp():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+
+        if node.attr('reduce_all'):
+            if not node.attr('keepdim'):
+                reduce_node = graph.make_node(
+                    'ReduceLogSumExp',
+                    inputs=node.input('X'),
+                    keepdims=node.attr('keepdim'))
+                mapper_helper.unsqueeze_helper(graph, reduce_node, [0],
+                                               node.output('Out'))
+            else:
+                graph.make_node(
+                    'ReduceLogSumExp',
+                    inputs=node.input('X'),
+                    keepdims=node.attr('keepdim'),
+                    outputs=node.output('Out'))
+        else:
+            graph.make_node(
+                'ReduceLogSumExp',
+                inputs=node.input('X'),
+                keepdims=node.attr('keepdim'),
+                axes=node.attr('axis'),
+                outputs=node.output('Out'))
+
+
+@op_mapper(
+    [
+        'elementwise_add', 'elementwise_sub', 'elementwise_div',
+        'elementwise_mul', 'elementwise_min', 'elementwise_max',
+        'elementwise_pow'
+    ],
+    mapper_dict={
+        'elementwise_add': 'Add',
+        'elementwise_sub': 'Sub',
+        'elementwise_div': 'Div',
+        'elementwise_mul': 'Mul',
+        'elementwise_min': 'Min',
+        'elementwise_max': 'Max',
+        'elementwise_pow': 'Pow'
+    })
+class ElementwiseOps():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        x_shape = node.input_shape('X', 0)
+        y_shape = node.input_shape('Y', 0)
+        if node.type in ["elementwise_min", "elementwise_max"]:
+            assert False, "{} op is not supported when opset version < 8".format(
+                node.type)
+
+        op_type = kw['mapper_dict'][node.type]
+        axis = node.attr('axis')
+        x = node.input('X', 0)
+        y = node.input('Y', 0)
+
+        if axis == -1 or axis == (len(x_shape) - 1
+                                  ) or len(x_shape) == len(y_shape):
+            onnx_node = graph.make_node(
+                op_type, inputs=[x, y], outputs=node.output('Out'))
+        else:
+            broadcast_shape = [1] * len(x_shape)
+            broadcast_shape[axis:axis + len(y_shape)] = y_shape
+            broadcast_shape_node = graph.make_node(
+                'Constant',
+                dtype=dtypes.ONNX.INT64,
+                value=list(broadcast_shape))
+            y_node = graph.make_node(
+                'Reshape', inputs=[y, broadcast_shape_node])
+            onnx_node = graph.make_node(
+                op_type, inputs=[x, y_node], outputs=node.output('Out'))
+
+    @classmethod
+    def opset_8(cls, graph, node, **kw):
+        op_type = kw['mapper_dict'][node.type]
+        x = node.input('X', 0)
+        y = node.input('Y', 0)
+        axis = node.attr('axis')
+        x_shape = node.input_shape('X', 0)
+        y_shape = node.input_shape('Y', 0)
+        if axis == -1 or axis == (len(x_shape) - 1
+                                  ) or len(x_shape) == len(y_shape):
+            onnx_node = graph.make_node(
+                op_type, inputs=[x, y], outputs=node.output('Out'))
+        else:
+            broadcast_shape = [1] * len(x_shape)
+            broadcast_shape[axis:axis + len(y_shape)] = y_shape
+            broadcast_shape_node = graph.make_node(
+                'Constant',
+                dtype=dtypes.ONNX.INT64,
+                value=list(broadcast_shape))
+            y_node = graph.make_node(
+                'Reshape', inputs=[y, broadcast_shape_node])
+            onnx_node = graph.make_node(
+                op_type, inputs=[x, y_node], outputs=node.output('Out'))
+
+
+@op_mapper('elementwise_mod')
+class ElementWiseMod():
+    support_opset_version_range = (10, 15)
+
+    @classmethod
+    def opset_10(cls, graph, node, **kw):
+        x_shape = node.input_shape('X', 0)
+        y_shape = node.input_shape('Y', 0)
+        axis = node.attr('axis')
+        x = node.input('X', 0)
+        y = node.input('Y', 0)
+
+        if node.input_dtype('Y', 0) == paddle.int32 or node.input_dtype(
+                'Y', 0) == paddle.int64:
+            onnx_node = graph.make_node(
+                "Mod", inputs=[x, y], outputs=node.output('Out'))
+            return
+
+        fmod = 1
+
+        abs_x_node = graph.make_node("Abs", inputs=[x])
+        abs_y_node = graph.make_node("Abs", inputs=[y])
+
+        dtype = dtypes.ONNX.FLOAT
+        val_0 = [0.0]
+        val_1 = [-1.0]
+        if node.input_dtype('Y', 0) == paddle.float64:
+            dtype = dtypes.ONNX.DOUBLE
+        if node.input_dtype('Y', 0) == paddle.int32:
+            dtype = dtypes.ONNX.INT32
+            val_0 = [0]
+            val_1 = [-1]
+        if node.input_dtype('Y', 0) == paddle.int64:
+            dtype = dtypes.ONNX.INT64
+            val_0 = [0]
+            val_1 = [-1]
+        zero_node = graph.make_node('Constant', dtype=dtype, value=val_0)
+        one_node = graph.make_node('Constant', dtype=dtype, value=val_1)
+
+        mod_node = graph.make_node(
+            "Mod", inputs=[abs_x_node, abs_y_node], fmod=fmod)
+
+        minus_node = graph.make_node("Mul", inputs=[mod_node, one_node])
+
+        condition_dtype = graph.make_node("Less", inputs=[x, zero_node])
+        condition = graph.make_node(
+            'Cast', inputs=[condition_dtype], to=dtypes.ONNX.BOOL)
+
+        mod_res = graph.make_node(
+            "Where", inputs=[condition, minus_node, mod_node])
+
+        add_node = graph.make_node("Add", inputs=[mod_res, y])
+
+        mod_y_mul_node = graph.make_node("Mul", inputs=[mod_res, y])
+        condition_dtype_1 = graph.make_node(
+            "Less", inputs=[mod_y_mul_node, zero_node])
+        condition_1 = graph.make_node(
+            'Cast', inputs=[condition_dtype_1], to=dtypes.ONNX.BOOL)
+
+        graph.make_node(
+            "Where",
+            inputs=[condition_1, add_node, mod_res],
+            outputs=node.output('Out'))
+
+
+@op_mapper('elementwise_floordiv')
+class ElementWiseFloorDiv():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        x = node.input('X', 0)
+        y = node.input('Y', 0)
+        axis = node.attr('axis')
+        x_shape = node.input_shape('X', 0)
+        y_shape = node.input_shape('Y', 0)
+        x_dtype = node.input_dtype('X', 0)
+        y_dtype = node.input_dtype('Y', 0)
+        x_dtype = dtypes.DTYPE_PADDLE_STR_MAP[x_dtype]
+        y_dtype = dtypes.DTYPE_PADDLE_STR_MAP[y_dtype]
+        is_int = False
+        if x_dtype.count('int') > 0 and y_dtype.count('int') > 0:
+            is_int = True
+        if axis == -1 or axis == (len(x_shape) - 1
+                                  ) or len(x_shape) == len(y_shape):
+            if is_int:
+                graph.make_node(
+                    'Div', inputs=[x, y], outputs=node.output('Out'))
+            else:
+                div_node = graph.make_node('Div', inputs=[x, y])
+                graph.make_node(
+                    'Floor', inputs=[div_node], outputs=node.output('Out'))
+        else:
+            broadcast_shape = [1] * len(x_shape)
+            broadcast_shape[axis:axis + len(y_shape)] = y_shape
+            broadcast_shape_node = graph.make_node(
+                'Constant',
+                dtype=dtypes.ONNX.INT64,
+                value=list(broadcast_shape))
+            y_node = graph.make_node(
+                'Reshape', inputs=[y, broadcast_shape_node])
+            if is_int:
+                div_node = graph.make_node(
+                    'Div', inputs=[x, y_node], outputs=node.output('Out'))
+            else:
+                div_node = graph.make_node('Div', inputs=[x, y_node])
+                graph.make_node(
+                    'Floor', inputs=[div_node], outputs=node.output('Out'))
+
+
+@op_mapper('pow')
+class Pow():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        x = node.input('X', 0)
+        x_dtype = node.input_dtype('X', 0)
+        factor = node.attr('factor')
+        # Pow-7 Only support input type as float and double
+        if x_dtype == paddle.int32 or x_dtype == paddle.int64:
+            x = graph.make_node('Cast', inputs=[x], to=dtypes.ONNX.FLOAT)
+            factor_node = graph.make_node(
+                'Constant',
+                inputs=[],
+                dims=[1],
+                dtype=dtypes.ONNX.FLOAT,
+                value=factor)
+        else:
+            factor_node = graph.make_node(
+                'Constant',
+                inputs=[],
+                dims=[1],
+                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype],
+                value=factor)
+        if x_dtype == paddle.int32 or x_dtype == paddle.int64:
+            pow_node = graph.make_node('Pow', inputs=[x, factor_node])
+            graph.make_node(
+                'Cast',
+                inputs=[pow_node],
+                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype],
+                outputs=node.output('Out'))
+        else:
+            graph.make_node(
+                'Pow', inputs=[x, factor_node], outputs=node.output('Out'))
+
+    @classmethod
+    def opset_12(cls, graph, node, **kw):
+        x = node.input('X', 0)
+        factor = node.attr('factor')
+        factor_node = graph.make_node(
+            'Constant',
+            inputs=[],
+            dims=[1],
+            dtype=dtypes.ONNX.FLOAT,
+            value=factor)
+        pow_node = graph.make_node(
+            'Pow', inputs=[x, factor_node], outputs=node.output('Out'))
+
+
+@op_mapper('square')
+class Square():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        x = node.input('X', 0)
+        onnx_node = graph.make_node(
+            'Mul', inputs=[x, x], outputs=node.output('Out'))
+
+
+@op_mapper('cumsum')
+class CumSum():
+    support_opset_version_range = (11, 15)
+
+    @classmethod
+    def opset_11(cls, graph, node, **kw):
+
+        axis = graph.make_node(
+            'Constant', dtype=dtypes.ONNX.INT64, value=node.attr('axis'))
+        graph.make_node(
+            'CumSum',
+            inputs=[node.input('X', 0), axis],
+            outputs=node.output('Out'))
+
+
+@op_mapper('mul')
+class Mul():
+    support_opset_version_range = (5, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        x = node.input('X', 0)
+        y = node.input('Y', 0)
+        out = node.output('Out', 0)
+        x_num_col_dims = node.attr('x_num_col_dims')
+        y_num_col_dims = node.attr('y_num_col_dims')
+        flatten_x = graph.make_node(
+            'Flatten', inputs=node.input('X'), attrs={'axis': x_num_col_dims})
+        flatten_y = graph.make_node(
+            'Flatten', inputs=node.input('Y'), attrs={'axis': y_num_col_dims})
+        mul_node = graph.make_node('MatMul', inputs=[flatten_x, flatten_y])
+
+        x_shape = graph.make_node('Shape', inputs=[x])
+        l_shape = mapper_helper.slice_helper(
+            graph, x_shape, axes=[0], starts=[0], ends=[x_num_col_dims])
+        y_shape = graph.make_node('Shape', inputs=[y])
+        y_rank = len(node.input_shape('Y', 0))
+        r_shape = mapper_helper.slice_helper(
+            graph, y_shape, axes=[0], starts=[y_num_col_dims], ends=[y_rank])
+
+        out_shape = graph.make_node('Concat', inputs=[l_shape, r_shape], axis=0)
+        graph.make_node('Reshape', [mul_node, out_shape], node.output('Out'))
+
+
+@op_mapper('affine_channel')
+class AffineChannel():
+    support_opset_version_range = (1, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        if "data_layout" in node.attrs.keys():
+            assert node.attrs['data_layout'] == 'NCHW' or node.attrs['data_layout'] == "AnyLayout",  \
+                                "The affine_channel data format should be 'NCHW', but received data format " \
+                                "is %s." % node.attrs['data_layout']
+        x = node.input('X', 0)
+        bias = node.input('Bias', 0)
+        scale = node.input('Scale', 0)
+        scale = mapper_helper.unsqueeze_helper(graph, scale, [0, 2, 3])
+        bias = mapper_helper.unsqueeze_helper(graph, bias, [0, 2, 3])
+        x = graph.make_node('Mul', inputs=[x, scale])
+        x = graph.make_node('Add', inputs=[x, bias], outputs=node.output('Out'))
+
+
+@op_mapper('bmm')
+class BMM():
+    support_opset_version_range = (1, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        x = node.input('X', 0)
+        y = node.input('Y', 0)
+        mul_node = graph.make_node(
+            'MatMul', inputs=[x, y], outputs=node.output('Out'))
+
+
+@op_mapper('p_norm')
+class PNorm():
+    support_opset_version_range = (1, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        x = node.input('X', 0)
+        axis = node.attr('axis')
+        if isinstance(axis, (int, float)):
+            axis = [axis]
+        p = node.attr('porder')
+        keepdim = node.attr('keepdim')
+        dtype = dtypes.ONNX.FLOAT
+        if node.input_dtype('X', 0) == paddle.float64:
+            dtype = dtypes.ONNX.DOUBLE
+
+        pnode = graph.make_node('Constant', dtype=dtype, value=[p])
+
+        abs_node = graph.make_node('Abs', inputs=[x])
+        pow_node = graph.make_node('Pow', inputs=[abs_node, pnode])
+        reduce_sum = graph.make_node(
+            'ReduceSum', inputs=[pow_node], axes=axis, keepdims=keepdim)
+        pnode1 = graph.make_node('Constant', dtype=dtype, value=[1.0 / p])
+        graph.make_node(
+            'Pow', inputs=[reduce_sum, pnode1], outputs=node.output('Out'))
+
+    @classmethod
+    def opset_13(cls, graph, node, **kw):
+        x = node.input('X', 0)
+        axis = node.attr('axis')
+        if isinstance(axis, (int, float)):
+            axis = [axis]
+        p = node.attr('porder')
+        keepdim = node.attr('keepdim')
+        pnode = graph.make_node('Constant', dtype=dtypes.ONNX.FLOAT, value=[p])
+        abs_node = graph.make_node('Abs', inputs=[x])
+        pow_node = graph.make_node('Pow', inputs=[abs_node, pnode])
+        axes = graph.make_node('Constant', dtype=dtypes.ONNX.INT64, value=axis)
+        reduce_sum = graph.make_node(
+            'ReduceSum', inputs=[pow_node, axes], keepdims=keepdim)
+        pnode1 = graph.make_node(
+            'Constant', dtype=dtypes.ONNX.FLOAT, value=[1.0 / p])
+        graph.make_node(
+            'Pow', inputs=[reduce_sum, pnode1], outputs=node.output('Out'))
+
+
+@op_mapper('sum')
+class Sum():
+    support_opset_version_range = (1, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        graph.make_node(
+            'Sum', inputs=node.input('X'), outputs=node.output('Out'))
+
+
+@op_mapper('floor')
+class Floor():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        graph.make_node(
+            'Floor', inputs=node.input('X'), outputs=node.output('Out'))
+
+
+@op_mapper('log10')
+class Log10():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        _ln10 = 2.30258509299404568401
+        dtype = dtypes.ONNX.FLOAT
+        if node.input_dtype('X', 0) == paddle.float64:
+            dtype = dtypes.ONNX.DOUBLE
+        _ln10 = graph.make_node('Constant', dtype=dtype, value=_ln10)
+        lnx = graph.make_node('Log', inputs=node.input('X'))
+        graph.make_node('Div', inputs=[lnx, _ln10], outputs=node.output('Out'))
+
+
+@op_mapper('log1p')
+class Log1p():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        dtype = dtypes.ONNX.FLOAT
+        if node.input_dtype('X', 0) == paddle.float64:
+            dtype = dtypes.ONNX.DOUBLE
+        one = graph.make_node('Constant', attrs={'dtype': dtype, 'value': [1]})
+        add_node = graph.make_node('Add', inputs=[node.input('X', 0), one])
+        graph.make_node('Log', inputs=add_node, outputs=node.output('Out'))
+
+
+@op_mapper(
+    ['reduce_all', 'reduce_any'],
+    mapper_dict={'reduce_all': 'ReduceMin',
+                 'reduce_any': 'ReduceMax'})
+class ReduceAll():
+    support_opset_version_range = (6, 15)
+
+    @classmethod
+    def opset_6(cls, graph, node, **kw):
+        op_type = kw['mapper_dict'][node.type]
+        input_dtype = node.block.vars[node.input('X', 0)].dtype
+        input_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[input_dtype]
+        all_node = graph.make_node(
+            'Cast', inputs=[node.input('X', 0)], to=dtypes.ONNX.INT32)
+
+        attrs = {'keepdims': node.attr('keep_dim'), }
+        if not node.attr('reduce_all'):
+            attrs['axes'] = node.attr('dim')
+        output_node = graph.make_node(op_type, inputs=[all_node], attrs=attrs)
+
+        if node.attr('reduce_all') and not node.attr('keep_dim'):
+            output_node = mapper_helper.unsqueeze_helper(graph, output_node,
+                                                         [0])
+        graph.make_node(
+            'Cast',
+            inputs=[output_node],
+            to=input_dtype,
+            outputs=node.output('Out'))
+
+
+@op_mapper(
+    ['reduce_mean', 'reduce_sum', 'reduce_min', 'reduce_max', 'reduce_prod'],
+    mapper_dict={
+        'reduce_mean': 'ReduceMean',
+        'reduce_sum': 'ReduceSum',
+        'reduce_min': 'ReduceMin',
+        'reduce_max': 'ReduceMax',
+        'reduce_prod': 'ReduceProd'
+    })
+class ReduceMean():
+    support_opset_version_range = (1, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        op_type = kw['mapper_dict'][node.type]
+
+        output_shape = node.output_shape('Out', 0)
+        reduce_all = node.attr("reduce_all")
+        axes = node.attr("dim")
+        if reduce_all:
+            axes = list(range(len(node.input_shape("X", 0))))
+        if len(axes) == len(node.input_shape("X", 0)):
+            reduce_all = True
+        keepdims = node.attr('keep_dim')
+        if keepdims:
+            cls.create_reduce_node(graph, op_type,
+                                   node.input("X"), node.output("Out"), axes, 1)
+        else:
+            if reduce_all:
+                shape = graph.make_node(
+                    "Constant", dtype=dtypes.ONNX.INT64, value=[-1])
+                flatten_node = graph.make_node(
+                    "Reshape", inputs=[node.input("X")[0], shape])
+                cls.create_reduce_node(graph, op_type, [flatten_node],
+                                       node.output("Out"), [0], 1)
+            else:
+                cls.create_reduce_node(graph, op_type,
+                                       node.input("X"),
+                                       node.output("Out"), axes, 0)
+
+    @classmethod
+    def create_reduce_node(cls, graph, op_type, inputs, outputs, axes,
+                           keepdims):
+        if graph.opset_version >= 13 and op_type == "ReduceSum":
+            axes = graph.make_node(
+                "Constant", dtype=dtypes.ONNX.INT64, value=axes)
+            output = graph.make_node(
+                "ReduceSum",
+                inputs=inputs + [axes],
+                outputs=outputs,
+                keepdims=keepdims)
+        else:
+            output = graph.make_node(
+                op_type,
+                inputs=inputs,
+                outputs=outputs,
+                axes=axes,
+                keepdims=keepdims)
+
+
+@op_mapper('mean')
+class Mean():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        shape = graph.make_node("Constant", dtype=dtypes.ONNX.INT64, value=[-1])
+        flatten_node = graph.make_node(
+            "Reshape", inputs=[node.input("X")[0], shape])
+        mean_node = graph.make_node(
+            'ReduceMean',
+            inputs=flatten_node,
+            outputs=node.output("Out"),
+            keepdims=1)
+
+
+@op_mapper('arg_max')
+class ArgMax():
+    support_opset_version_range = (1, 12)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        if node.attr('dtype') and node.attr('dtype') == 2:
+            arg_node = graph.make_node(
+                'ArgMax',
+                inputs=node.input('X'),
+                attrs={
+                    'axis': node.attr('axis'),
+                    'keepdims': node.attr('keepdims')
+                })
+            graph.make_node(
+                'Cast',
+                inputs=arg_node,
+                attrs={'to': dtypes.ONNX.INT32},
+                outputs=node.output('Out'))
+        else:
+            graph.make_node(
+                'ArgMax',
+                inputs=node.input('X'),
+                outputs=node.output('Out'),
+                attrs={
+                    'axis': node.attr('axis'),
+                    'keepdims': node.attr('keepdims')
+                })
+
+
+@op_mapper('arg_min')
+class ArgMin():
+    support_opset_version_range = (1, 12)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        if node.attr('flatten'):
+            flatten = graph.make_node('Flatten', inputs=node.input('X'), axis=0)
+            squeeze_node = graph.make_node('Squeeze', inputs=flatten)
+            graph.make_node(
+                'ArgMin', inputs=squeeze_node, outputs=node.output('Out'))
+        else:
+            if node.attr('keepdims'):
+                graph.make_node(
+                    'ArgMin',
+                    inputs=node.input('X'),
+                    outputs=node.output('Out'),
+                    axis=node.attr('axis'),
+                    keepdims=1)
+            else:
+                graph.make_node(
+                    'ArgMin',
+                    inputs=node.input('X'),
+                    outputs=node.output('Out'),
+                    axis=node.attr('axis'),
+                    keepdims=0)
+
+
+@op_mapper('brelu')
+class Hardtanh():
+    support_opset_version_range = (9, 15)
+
+    @classmethod
+    def opset_6(cls, graph, node, **kw):
+        mapper_helper.clip_helper(graph, node,
+                                  node.input('X', 0),
+                                  node.attr('t_max'),
+                                  node.attr('t_min'), node.output('Out', 0))
+
+
+@op_mapper('mv')
+class Mv():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        graph.make_node(
+            'MatMul',
+            inputs=[node.input('X', 0), node.input('Vec', 0)],
+            outputs=node.output('Out'))
+
+
+@op_mapper('dot')
+class Dot():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        mul_node = graph.make_node(
+            'Mul', inputs=[node.input('X', 0), node.input('Y', 0)])
+        graph.make_node(
+            'ReduceSum',
+            inputs=[mul_node],
+            axes=[len(node.input_shape('X', 0)) - 1],
+            outputs=node.output('Out'))
+
+    @classmethod
+    def opset_13(cls, graph, node, **kw):
+        mul_node = graph.make_node(
+            'Mul', inputs=[node.input('X', 0), node.input('Y', 0)])
+        one = graph.make_node(
+            'Constant',
+            dtype=dtypes.ONNX.INT64,
+            value=[len(node.input_shape('X', 0)) - 1])
+        graph.make_node(
+            'ReduceSum', inputs=[mul_node, one], outputs=node.output('Out'))
+
+
+@op_mapper('dist')
+class Dist():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        sub_node = graph.make_node(
+            'Sub', inputs=[node.input('X', 0), node.input('Y', 0)])
+        abs_node = graph.make_node('Abs', inputs=sub_node)
+        if node.attr('p') == 0:
+            assert graph.opset_version >= 9, "When p is 0, onnx opset should be (onnx_opset>=9)."
+            sign_node = graph.make_node('Sign', inputs=abs_node)
+            sum_node = graph.make_node(
+                'ReduceSum', inputs=sign_node, keepdims=0)
+            mapper_helper.unsqueeze_helper(graph, sum_node, [0],
+                                           node.output('Out'))
+        elif node.attr('p') == float('inf'):
+            max_node = graph.make_node('ReduceMax', inputs=abs_node, keepdims=0)
+            mapper_helper.unsqueeze_helper(graph, max_node, [0],
+                                           node.output('Out'))
+        elif node.attr('p') == float('-inf'):
+            min_node = graph.make_node('ReduceMin', inputs=abs_node, keepdims=0)
+            mapper_helper.unsqueeze_helper(graph, min_node, [0],
+                                           node.output('Out'))
+        else:
+            x_dtype = node.input_dtype('X', 0)
+            p = graph.make_node(
+                'Constant',
+                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype],
+                value=node.attr('p'))
+            pow_node = graph.make_node(
+                'Pow',
+                inputs=[abs_node, p], )
+            sum_node = graph.make_node('ReduceSum', inputs=pow_node, keepdims=0)
+            sum_node = mapper_helper.unsqueeze_helper(graph, sum_node, [0])
+            p_1 = graph.make_node('Reciprocal', inputs=p)
+            graph.make_node(
+                'Pow', inputs=[sum_node, p_1], outputs=node.output('Out'))
+
+
+@op_mapper('round')
+class Round():
+    support_opset_version_range = (11, 15)
+
+    @classmethod
+    def opset_11(cls, graph, node, **kw):
+        graph.make_node(
+            'Round', inputs=node.input('X'), outputs=node.output('Out'))
+
+
+@op_mapper('rsqrt')
+class Rsqrt():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_6(cls, graph, node, **kw):
+        sqrt_node = graph.make_node('Sqrt', inputs=node.input('X'))
+        graph.make_node(
+            'Reciprocal', inputs=sqrt_node, outputs=node.output('Out'))
+
+
+@op_mapper('sign')
+class Sign():
+    support_opset_version_range = (9, 15)
+
+    @classmethod
+    def opset_9(cls, graph, node, **kw):
+        graph.make_node(
+            'Sign', inputs=node.input('X'), outputs=node.output('Out'))
+
+
+@op_mapper('scale')
+class Scale():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        scale = node.attr('scale')
+        bias = node.attr('bias')
+        if len(node.input('ScaleTensor')) == 0 and np.fabs(
+                scale - 1.0) < 1e-06 and np.fabs(bias - 0.0) < 1e-06:
+            graph.make_node(
+                'Identity', inputs=node.input('X'), outputs=node.output('Out'))
+        else:
+            input_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)]
+            if input_dtype in [
+                    dtypes.ONNX.INT16, dtypes.ONNX.INT32, dtypes.ONNX.INT64
+            ]:
+                outputs = None
+                data_type = dtypes.ONNX.FLOAT
+                cast_node = graph.make_node(
+                    'Cast', inputs=node.input('X'), attrs={'to': data_type})
+            else:
+                outputs = node.output('Out')
+                data_type = input_dtype
+                cast_node = node.input('X')[0]
+
+            if len(node.input('ScaleTensor')) > 0:
+                scale_node = node.input('ScaleTensor')[0]
+                scale_type = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype(
+                    'ScaleTensor', 0)]
+                if scale_type != data_type:
+                    scale_node = graph.make_node(
+                        'Cast', inputs=[scale_node], attrs={'to': data_type})
+            else:
+                scale_node = graph.make_node(
+                    'Constant', attrs={'dtype': data_type,
+                                       'value': [scale]})
+            bias_node = graph.make_node(
+                'Constant', attrs={'dtype': data_type,
+                                   'value': [bias]})
+
+            if node.attr('bias_after_scale'):
+                node1 = graph.make_node('Mul', inputs=[cast_node, scale_node])
+                node2 = graph.make_node(
+                    'Add', inputs=[node1, bias_node], outputs=outputs)
+            else:
+                node1 = graph.make_node('Add', inputs=[cast_node, bias_node])
+                node2 = graph.make_node(
+                    'Mul', inputs=[node1, scale_node], outputs=outputs)
+
+            if input_dtype in [
+                    dtypes.ONNX.INT16, dtypes.ONNX.INT32, dtypes.ONNX.INT64
+            ]:
+                cast_node = graph.make_node(
+                    'Cast',
+                    inputs=node2,
+                    outputs=node.output('Out'),
+                    attrs={'to': input_dtype})
+
+
+@op_mapper('softmax')
+class Softmax():
+    support_opset_version_range = (1, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        axis = node.attr('axis')
+        shape = node.output_shape('Out', 0)
+        if axis is None:
+            axis = -1
+        if axis < 0:
+            axis += len(shape)
+        if axis == len(shape) - 1:
+            node = graph.make_node(
+                'Softmax',
+                inputs=node.input('X'),
+                outputs=node.output('Out'),
+                attrs={'axis': axis})
+        else:
+            perm = [i for i in range(len(shape))]
+            perm[-1] = axis
+            perm[axis] = len(shape) - 1
+            transpose_node = graph.make_node(
+                'Transpose', inputs=node.input('X'), attrs={'perm': perm})
+            softmax_node = graph.make_node(
+                'Softmax', inputs=[transpose_node], axis=-1)
+            transpose_node1 = graph.make_node(
+                'Transpose',
+                inputs=[softmax_node],
+                outputs=node.output('Out'),
+                attrs={'perm': perm})
+
+    @classmethod
+    def opset_13(cls, graph, node, **kw):
+        graph.make_node(
+            'Softmax',
+            inputs=node.input('X'),
+            axis=node.attr('axis'),
+            outputs=node.output('Out'))
+
+
+@op_mapper('unfold')
+class Unfold():
+    support_opset_version_range = (11, 15)
+
+    @classmethod
+    def opset_11(cls, graph, node, **kw):
+
+        strides = node.attr('strides')
+        stride_h = strides[0]
+        stride_w = strides[1]
+
+        paddings = node.attr('paddings')
+        padding_h_1 = paddings[0]
+        padding_w_1 = paddings[1]
+        padding_h_2 = paddings[2]
+        padding_w_2 = paddings[3]
+
+        dilations = node.attr('dilations')
+        dilation_h = dilations[0]
+        dilation_w = dilations[1]
+
+        kernel_sizes = node.attr('kernel_sizes')
+        kernel_h = kernel_sizes[0]
+        kernel_w = kernel_sizes[1]
+
+        input_w = mapper_helper.shape_helper(graph, node.input('X', 0), 3)
+        blocks_row_indices_node = cls._get_im2col_indices_along_dim(
+            graph, node, 2, kernel_h, dilation_h, padding_h_1, padding_h_2,
+            stride_h)
+        blocks_col_indices_node = cls._get_im2col_indices_along_dim(
+            graph, node, 3, kernel_w, dilation_w, padding_w_1, padding_w_2,
+            stride_w)
+
+        output_shape = cls._get_im2col_output_shape(graph, node, kernel_h,
+                                                    kernel_w)
+        padded_input = cls._get_im2col_padded_input(
+            graph, node, padding_h_1, padding_h_2, padding_w_1, padding_w_2)
+
+        output = graph.make_node(
+            'Gather', inputs=[padded_input, blocks_row_indices_node], axis=2)
+
+        output = graph.make_node(
+            'Gather', inputs=[output, blocks_col_indices_node], axis=4)
+        output = graph.make_node(
+            'Transpose', inputs=[output], perm=[0, 1, 2, 4, 3, 5])
+
+        graph.make_node(
+            'Reshape', inputs=[output, output_shape], outputs=node.output('Y'))
+
+    @classmethod
+    def _get_im2col_indices_along_dim(cls, graph, node, index, kernel_size_d,
+                                      dilation_d, padding_d_1, padding_d_2,
+                                      stride_d):
+        input_shape = node.input_shape('X', 0)
+        if input_shape[index] == -1:
+            input_d_node = mapper_helper.shape_helper(graph,
+                                                      node.input('X', 0), index)
+
+            padding_d_node = graph.make_node(
+                'Constant',
+                dtype=dtypes.ONNX.INT64,
+                value=[padding_d_1 + padding_d_2])
+            blocks_d_node = graph.make_node(
+                'Add', inputs=[input_d_node, padding_d_node])
+
+            dilation_kernel_size_node = graph.make_node(
+                'Constant',
+                dtype=dtypes.ONNX.INT64,
+                value=[dilation_d * (kernel_size_d - 1)])
+            blocks_d_node = graph.make_node(
+                'Sub', inputs=[blocks_d_node, dilation_kernel_size_node])
+
+            zero_node = graph.make_node(
+                'Constant', dtype=dtypes.ONNX.INT64, value=[0])
+            stride_node = graph.make_node(
+                'Constant', dtype=dtypes.ONNX.INT64, value=[stride_d])
+            blocks_d_indices_node = graph.make_node(
+                'Range', inputs=[zero_node, blocks_d_node, stride_node])
+        else:
+            end = input_shape[
+                index] + padding_d_1 + padding_d_2 - dilation_d * (kernel_size_d
+                                                                   - 1)
+            stride = stride_d
+            blocks_d_indices = np.arange(0, end, stride)
+            blocks_d_indices_node = graph.make_node(
+                'Constant',
+                dtype=dtypes.ONNX.INT64,
+                value=blocks_d_indices.flatten().tolist())
+
+        kernel_grid = np.arange(0, kernel_size_d * dilation_d, dilation_d)
+        kernel_grid_node = graph.make_node(
+            'Constant',
+            dtype=dtypes.ONNX.INT64,
+            value=kernel_grid.flatten().tolist())
+
+        shape_node = graph.make_node(
+            'Constant', dtype=dtypes.ONNX.INT64, value=[-1, 1])
+        kernel_mask_node = graph.make_node(
+            'Reshape', inputs=[kernel_grid_node, shape_node])
+
+        block_mask_node = graph.make_node(
+            'Add', inputs=[blocks_d_indices_node, kernel_mask_node])
+        return block_mask_node
+
+    @classmethod
+    def _get_im2col_output_shape(cls, graph, node, kernel_h, kernel_w):
+        batch_dim = mapper_helper.shape_helper(graph, node.input('X', 0), 0)
+        channel_dim = mapper_helper.shape_helper(graph, node.input('X', 0), 1)
+
+        constant_node = graph.make_node(
+            'Constant', dtype=dtypes.ONNX.INT64, value=[kernel_h * kernel_w])
+        channel_unfolded = graph.make_node(
+            'Mul', inputs=[channel_dim, constant_node])
+
+        concat_const_node = graph.make_node(
+            'Constant', dtype=dtypes.ONNX.INT64, value=[-1])
+        result_node = graph.make_node(
+            'Concat',
+            inputs=[batch_dim, channel_unfolded, concat_const_node],
+            axis=0)
+
+        return result_node
+
+    @classmethod
+    def _get_im2col_padded_input(cls, graph, node, padding_h_1, padding_h_2,
+                                 padding_w_1, padding_w_2):
+        pad_const_node = graph.make_node(
+            'Constant',
+            dtype=dtypes.ONNX.INT64,
+            value=[
+                0, 0, padding_h_1, padding_w_1, 0, 0, padding_h_2, padding_w_2
+            ])
+        result_node = graph.make_node(
+            'Pad', inputs=[node.input('X', 0), pad_const_node])
+        return result_node
+
+
+@op_mapper('softmax_with_cross_entropy')
+class SoftmaxCrossEntropyLoss():
+    support_opset_version_range = (12, 15)
+
+    @classmethod
+    def opset_12(cls, graph, node, **kw):
+        if node.attr('soft_label'):
+            raise Exception(
+                "SoftmaxCrossEntropyLoss in onnx not support soft label.")
+        scores = node.input('Logits', 0)
+        labels = node.input('Label', 0)
+        # Whether return_softmax is True or False, the model will have two outputs
+        outputs = [node.output('Loss', 0), node.output('Softmax', 0)]
+
+        shape = node.input_shape('Logits', 0)
+        if len(shape) < 2:
+            raise Exception(
+                "SoftmaxCrossEntropyLoss in onnx not support 1D logits.")
+        axis = node.attr('axis')
+        if axis < 0:
+            axis += len(shape)
+        if axis == 1:
+            squeeze_node = mapper_helper.squeeze_helper(graph, labels, [axis])
+            loss_node, softmax_node = graph.make_node(
+                'SoftmaxCrossEntropyLoss',
+                inputs=[scores, squeeze_node],
+                outputs=2,
+                ignore_index=node.attr('ignore_index'),
+                reduction='none')
+            loss_node = mapper_helper.unsqueeze_helper(graph, loss_node,
+                                                       [axis], outputs[0])
+            # onnx output is log(softmax), but paddle output is softmax
+            graph.make_node('Exp', inputs=[softmax_node], outputs=outputs[1])
+        else:
+            perm = [i for i in range(len(shape))]
+            perm[1] = axis
+            perm[axis] = 1
+            transpose_scores = graph.make_node(
+                'Transpose', inputs=[scores], perm=perm)
+            transpose_labels = graph.make_node(
+                'Transpose', inputs=[labels], perm=perm)
+            squeeze_labels = mapper_helper.squeeze_helper(
+                graph, transpose_labels, [1])
+
+            loss_node, softmax_node = graph.make_node(
+                'SoftmaxCrossEntropyLoss',
+                inputs=[transpose_scores, squeeze_labels],
+                ignore_index=node.attr('ignore_index'),
+                outputs=2,
+                reduction='none')
+            output_node = mapper_helper.unsqueeze_helper(graph, loss_node, [1])
+            graph.make_node(
+                'Transpose', inputs=output_node, outputs=outputs[0], perm=perm)
+            softmax_node = graph.make_node(
+                'Transpose', inputs=softmax_node, perm=perm)
+            # onnx output is log(softmax), but paddle output is softmax
+            graph.make_node('Exp', inputs=[softmax_node], outputs=outputs[1])
```

## paddle2onnx/legacy/op_mapper/nn.py

 * *Ordering differences only*

```diff
@@ -1,952 +1,952 @@
-#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import numpy as np
-import math
-import collections
-from paddle2onnx.legacy.constant import dtypes
-from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
-from paddle2onnx.legacy.op_mapper import mapper_helper
-from paddle2onnx import utils
-import paddle
-
-
-@op_mapper(['conv2d', 'depthwise_conv2d', 'conv3d'])
-class Conv():
-    support_opset_version_range = (1, 12)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        kernel_shape = node.input_shape('Filter', 0)
-        dilations = node.attr('dilations')
-        kernel_shape = kernel_shape[2:]
-        strides = node.attr('strides')
-        group = node.attr('groups')
-        pads = node.attr('paddings')
-        assert node.attrs['data_format'] == 'NCHW' or node.attrs['data_format'] == 'NCDHW' or node.attrs['data_format'] == "AnyLayout",  \
-                            "The conv data format should be 'NCHW' or 'NCDHW', but received data format " \
-                            "is %s." % node.attrs['data_format']
-        # onnx padding is [x1_begin, x2_begin...x1_end, x2_end, ...]
-        if len(pads) == 2 or len(pads) == 3:
-            pads = pads + pads
-        elif len(pads) == 4:
-            pads = [pads[i] for i in [0, 2, 1, 3]]
-        elif len(pads) == 6:
-            pads = [pads[i] for i in [0, 2, 4, 1, 3, 5]]
-        attrs = {
-            'dilations': dilations,
-            'kernel_shape': kernel_shape,
-            'strides': strides,
-            'group': group
-        }
-        auto_pad = node.attr('padding_algorithm')
-        if auto_pad == 'SAME':
-            attrs['auto_pad'] = 'SAME_UPPER'
-        elif auto_pad == 'VALID':
-            attrs['auto_pad'] = 'VALID'
-        else:
-            attrs['pads'] = pads
-        graph.make_node(
-            'Conv',
-            inputs=node.input('Input') + node.input('Filter'),
-            outputs=node.output('Output'),
-            attrs=attrs)
-
-
-@op_mapper(
-    ['conv2d_transpose', 'depthwise_conv2d_transpose', 'conv3d_transpose'])
-class ConvTranspose():
-    support_opset_version_range = (1, 12)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        output_padding = node.attr('output_padding')
-        kernel_shape = node.input_shape('Filter', 0)
-        dilations = node.attr('dilations')
-        kernel_shape = kernel_shape[2:]
-        strides = node.attr('strides')
-        group = node.attr('groups')
-        pads = node.attr('paddings')
-        assert node.attrs['data_format'] == 'NCHW' or node.attrs['data_format'] == 'NCDHW', \
-            "The conv data format should be 'NCHW' or 'NCDHW', but received data format " \
-            "is %s." % node.attrs['data_format']
-
-        if len(pads) == 2 or len(pads) == 3:
-            pads = pads + pads
-        elif len(pads) == 4:
-            pads = [pads[i] for i in [0, 2, 1, 3]]
-        elif len(pads) == 6:
-            pads = [pads[i] for i in [0, 2, 4, 1, 3, 5]]
-
-        attrs = {
-            'dilations': dilations,
-            'kernel_shape': kernel_shape,
-            'strides': strides,
-            'group': group
-        }
-        auto_pad = node.attr('padding_algorithm')
-        if auto_pad == 'SAME':
-            attrs['auto_pad'] = 'SAME_UPPER'
-        elif auto_pad == 'VALID':
-            attrs['auto_pad'] = 'VALID'
-        else:
-            attrs['pads'] = pads
-        if output_padding and len(output_padding) > 0:
-            attrs['output_padding'] = output_padding
-        graph.make_node(
-            'ConvTranspose',
-            inputs=node.input('Input') + node.input('Filter'),
-            outputs=node.output('Output'),
-            attrs=attrs)
-
-
-@op_mapper('pool2d')
-class Pool():
-    support_opset_version_range = (1, 12)
-    pool_type = {
-        'max': ('MaxPool', 'GlobalMaxPool'),
-        'avg': ('AveragePool', 'GlobalAveragePool')
-    }
-
-    @classmethod
-    def is_same_span(cls, in_size, out_size):
-        spans = []
-        for i in range(out_size):
-            start = math.floor(i * (in_size / out_size))
-            end = math.ceil((i + 1) * (in_size / out_size))
-            spans.append(end - start)
-        if len(set(spans)) == 1:
-            return True
-        return False
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        assert node.attrs['data_format'] == 'NCHW' or node.attrs['data_format'] == "AnyLayout",  \
-                            "The conv data format should be 'NCHW', but received data format " \
-                            "is %s." % node.attrs['data_format']
-        x_dtype = node.input_dtype('X', 0)
-        need_dtype_convert = False
-        input_name = node.input('X', 0)
-        if x_dtype != paddle.float32:
-            need_dtype_convert = True
-            input_name = graph.make_node(
-                'Cast', inputs=node.input('X'), to=dtypes.ONNX.FLOAT)
-
-        if node.attr('global_pooling') or (node.attr('adaptive') and
-                                           node.attr('ksize') == [1, 1]):
-            if need_dtype_convert:
-                onnx_node = graph.make_node(
-                    cls.pool_type[node.attr('pooling_type')][1],
-                    inputs=[input_name])
-                graph.make_node(
-                    'Cast',
-                    inputs=[onnx_node],
-                    outputs=node.output('Out'),
-                    to=dtypes.ONNX.DOUBLE)
-            else:
-                onnx_node = graph.make_node(
-                    cls.pool_type[node.attr('pooling_type')][1],
-                    inputs=[input_name],
-                    outputs=node.output('Out'))
-        elif node.attr('adaptive'):
-            # if pool is adaptive, check if input shape of pool is fixed.
-            if node.input_shape('X', 0)[2:].count(-1) > 0:
-                raise Exception(
-                    "Converting this model to ONNX need with static input shape," \
-                    " please fix input shape of this model, see doc Q2 in" \
-                    " https://github.com/PaddlePaddle/paddle2onnx/blob/develop/docs/en/FAQ.md."
-                )
-            input_h, input_w = node.input_shape('X', 0)[2:]
-            output_h, output_w = node.output_shape('Out', 0)[2:]
-            stride_h = int(input_h / output_h)
-            stride_w = int(input_w / output_w)
-
-            kernel_h = input_h - (output_h - 1) * stride_h
-            kernel_w = input_w - (output_w - 1) * stride_w
-
-            #check if kernel_size is fixed.
-            if not cls.is_same_span(input_h, output_h) or not cls.is_same_span(
-                    input_w, output_w):
-                raise Exception(
-                    "Cannot convert adaptive pool with input_size: {}, output_size: {}"
-                    .format(
-                        node.input_shape('X', 0), node.output_shape('Out', 0)))
-            else:
-                attrs = {
-                    'kernel_shape': (kernel_h, kernel_w),
-                    'strides': (stride_h, stride_w),
-                }
-                if node.attr('ceil_mode') and graph.opset_version < 10:
-                    raise Exception(
-                        "Cannot convert pool with ceil_model == True to ONNX Opset version < 10."
-                    )
-                elif graph.opset_version > 10:
-                    attrs['ceil_mode'] = node.attr('ceil_mode')
-                auto_pad = node.attr('padding_algorithm')
-                if auto_pad == 'SAME':
-                    attrs['auto_pad'] = 'SAME_UPPER'
-                elif auto_pad == 'VALID':
-                    attrs['auto_pad'] = 'VALID'
-                if node.attr('pooling_type') == 'avg':
-                    attrs['count_include_pad'] = not node.attr('exclusive')
-                if need_dtype_convert:
-                    onnx_node = graph.make_node(
-                        cls.pool_type[node.attr('pooling_type')][0],
-                        inputs=[input_name],
-                        attrs=attrs)
-                    graph.make_node(
-                        'Cast',
-                        inputs=[onnx_node],
-                        outputs=node.output('Out'),
-                        to=dtypes.ONNX.DOUBLE)
-                else:
-                    onnx_node = graph.make_node(
-                        cls.pool_type[node.attr('pooling_type')][0],
-                        inputs=[input_name],
-                        outputs=node.output('Out'),
-                        attrs=attrs)
-        else:
-            input_shape = node.input_shape('X', 0)
-            k_size = node.attr('ksize')
-            pads = node.attr('paddings')
-            strides = node.attr('strides')
-
-            if len(pads) == 2:
-                pads = pads + pads
-            elif len(pads) == 4:
-                pads = [pads[i] for i in [0, 2, 1, 3]]
-
-            if input_shape[2] > 0 and input_shape[2] + pads[0] < k_size[0]:
-                k_size[0] = input_shape[2] + pads[0]
-            if input_shape[3] > 0 and input_shape[3] + pads[1] < k_size[1]:
-                k_size[1] = input_shape[3] + pads[1]
-
-            input_x = [input_name]
-            if max(k_size) <= max(pads):
-                onnx_paddings = [0, 0, pads[0], pads[1], 0, 0, pads[2], pads[3]]
-                attrs_pad = {'mode': 'constant', }
-                if graph.opset_version >= 11:
-                    pads_node = graph.make_node(
-                        'Constant',
-                        attrs={
-                            'dtype': dtypes.ONNX.INT64,
-                            'value': onnx_paddings
-                        })
-                    value_node = graph.make_node(
-                        'Constant',
-                        attrs={'dtype': dtypes.ONNX.FLOAT,
-                               'value': 0.0})
-                    input_x = input_x + [pads_node, value_node]
-                else:
-                    attrs_pad['pads'] = onnx_paddings
-                    attrs_pad['value'] = 0.0
-                input_x = graph.make_node(
-                    'Pad', inputs=input_x, attrs=attrs_pad)
-                pads = [0, 0, 0, 0]
-
-            attrs = {
-                'kernel_shape': k_size,
-                'strides': strides,
-            }
-            auto_pad = node.attr('padding_algorithm')
-            if auto_pad == 'SAME':
-                attrs['auto_pad'] = 'SAME_UPPER'
-            elif auto_pad == 'VALID':
-                attrs['auto_pad'] = 'VALID'
-            else:
-                attrs['pads'] = pads
-            if node.attr('ceil_mode') and graph.opset_version < 10:
-                raise Exception(
-                    "Cannot convert pool with ceil_model == True to ONNX Opset version < 10"
-                )
-            elif graph.opset_version >= 10:
-                attrs['ceil_mode'] = node.attr('ceil_mode')
-
-            if node.attr('pooling_type') == 'avg':
-                attrs['count_include_pad'] = not node.attr('exclusive')
-            if need_dtype_convert:
-                onnx_node = graph.make_node(
-                    cls.pool_type[node.attr('pooling_type')][0],
-                    inputs=input_x,
-                    attrs=attrs)
-                graph.make_node(
-                    'Cast',
-                    inputs=[onnx_node],
-                    outputs=node.output('Out'),
-                    to=dtypes.ONNX.DOUBLE)
-            else:
-                onnx_node = graph.make_node(
-                    cls.pool_type[node.attr('pooling_type')][0],
-                    inputs=input_x,
-                    outputs=node.output('Out'),
-                    attrs=attrs)
-
-
-@op_mapper('pool3d')
-class Pool3D():
-    support_opset_version_range = (1, 12)
-    pool_type = {
-        'max': ('MaxPool', 'GlobalMaxPool'),
-        'avg': ('AveragePool', 'GlobalAveragePool')
-    }
-
-    @classmethod
-    def is_same_span(cls, in_size, out_size):
-        spans = []
-        for i in range(out_size):
-            start = math.floor(i * (in_size / out_size))
-            end = math.ceil((i + 1) * (in_size / out_size))
-            spans.append(end - start)
-        if len(set(spans)) == 1:
-            return True
-        return False
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        assert node.attrs['data_format'] == 'NCDHW' or node.attrs['data_format'] == "AnyLayout",  \
-                            "The conv data format should be 'NCDHW', but received data format " \
-                            "is %s." % node.attrs['data_format']
-
-        if node.attr('global_pooling') or (node.attr('adaptive') and
-                                           node.attr('ksize') == [1, 1, 1]):
-            onnx_node = graph.make_node(
-                cls.pool_type[node.attr('pooling_type')][1],
-                inputs=node.input('X'),
-                outputs=node.output('Out'))
-        elif node.attr('adaptive'):
-            # if pool is adaptive, check if input shape of pool is fixed.
-            if node.input_shape('X', 0)[2:].count(-1) > 0:
-                raise Exception(
-                    "Converting this model to ONNX need with static input shape," \
-                    " please fix input shape of this model, see doc Q2 in" \
-                    " https://github.com/PaddlePaddle/paddle2onnx/blob/develop/docs/en/FAQ.md."
-                )
-            input_d, input_h, input_w = node.input_shape('X', 0)[2:]
-            output_d, output_h, output_w = node.output_shape('Out', 0)[2:]
-            stride_d = int(input_d / output_d)
-            stride_h = int(input_h / output_h)
-            stride_w = int(input_w / output_w)
-
-            kernel_d = input_d - (output_d - 1) * stride_d
-            kernel_h = input_h - (output_h - 1) * stride_h
-            kernel_w = input_w - (output_w - 1) * stride_w
-
-            #check if kernel_size is fixed.
-            if not cls.is_same_span(input_h, output_h) or not cls.is_same_span(
-                    input_w, output_w) or not cls.is_same_span(input_d,
-                                                               output_d):
-                raise Exception(
-                    "Cannot convert adaptive pool with input_size: {}, output_size: {}"
-                    .format(
-                        node.input_shape('X', 0), node.output_shape('Out', 0)))
-            else:
-                attrs = {
-                    'kernel_shape': (kernel_d, kernel_h, kernel_w),
-                    'strides': (stride_d, stride_h, stride_w),
-                }
-                if node.attr('ceil_mode') and graph.opset_version < 10:
-                    raise Exception(
-                        "Cannot convert pool with ceil_model == True to ONNX Opset version < 10."
-                    )
-                elif graph.opset_version > 10:
-                    attrs['ceil_mode'] = node.attr('ceil_mode')
-                auto_pad = node.attr('padding_algorithm')
-                if auto_pad == 'SAME':
-                    attrs['auto_pad'] = 'SAME_UPPER'
-                elif auto_pad == 'VALID':
-                    attrs['auto_pad'] = 'VALID'
-                if node.attr('pooling_type') == 'avg':
-                    attrs['count_include_pad'] = not node.attr('exclusive')
-                onnx_node = graph.make_node(
-                    cls.pool_type[node.attr('pooling_type')][0],
-                    inputs=node.input('X'),
-                    outputs=node.output('Out'),
-                    attrs=attrs)
-        else:
-            input_shape = node.input_shape('X', 0)
-            k_size = node.attr('ksize')
-            paddings = node.attr('paddings')
-            if input_shape[2] > 0 and input_shape[2] + paddings[0] < k_size[0]:
-                k_size[0] = input_shape[2] + paddings[0]
-            if input_shape[3] > 0 and input_shape[3] + paddings[1] < k_size[1]:
-                k_size[1] = input_shape[3] + paddings[1]
-            if input_shape[4] > 0 and input_shape[4] + paddings[2] < k_size[2]:
-                k_size[2] = input_shape[4] + paddings[2]
-            attrs = {
-                'kernel_shape': k_size,
-                'strides': node.attr('strides'),
-                'pads': node.attr('paddings') + node.attr('paddings'),
-            }
-            if node.attr('ceil_mode') and graph.opset_version < 10:
-                raise Exception(
-                    "Cannot convert pool with ceil_model == True to ONNX Opset version < 10"
-                )
-            elif graph.opset_version >= 10:
-                attrs['ceil_mode'] = node.attr('ceil_mode')
-
-            if node.attr('pooling_type') == 'avg':
-                attrs['count_include_pad'] = not node.attr('exclusive')
-            onnx_node = graph.make_node(
-                cls.pool_type[node.attr('pooling_type')][0],
-                inputs=node.input('X'),
-                outputs=node.output('Out'),
-                attrs=attrs)
-
-
-@op_mapper('elu')
-class ELU():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        node = graph.make_node(
-            'Elu',
-            inputs=node.input('X'),
-            outputs=node.output('Out'),
-            alpha=node.attr('alpha'))
-
-
-@op_mapper('softsign')
-class SoftSign():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        graph.make_node(
-            'Softsign', inputs=node.input('X'), outputs=node.output('Out'))
-
-
-@op_mapper('hard_shrink')
-class Hardshrink():
-    support_opset_version_range = (9, 15)
-
-    @classmethod
-    def opset_9(cls, graph, node, **kw):
-        node = graph.make_node(
-            'Shrink',
-            inputs=node.input('X'),
-            outputs=node.output('Out'),
-            lambd=node.attr('threshold'))
-
-
-@op_mapper('logsigmoid')
-class LogSigmoid():
-    support_opset_version_range = (1, 12)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        sigmoid_node = graph.make_node('Sigmoid', inputs=node.input('X'))
-        graph.make_node('Log', inputs=sigmoid_node, outputs=node.output('Out'))
-
-
-@op_mapper('norm')
-class Norm():
-    support_opset_version_range = (1, 12)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        node = graph.make_node(
-            'LpNormalization',
-            inputs=node.input('X'),
-            outputs=node.output('Out'),
-            axis=node.attr('axis'))
-
-
-@op_mapper('softshrink')
-class SoftShrink():
-    support_opset_version_range = (9, 15)
-
-    @classmethod
-    def opset_9(cls, graph, node, **kw):
-        graph.make_node(
-            'Shrink',
-            inputs=node.input('X'),
-            bias=node.attr('lambda'),
-            lambd=node.attr('lambda'),
-            outputs=node.output('Out'))
-
-
-@op_mapper('tanh_shrink')
-class TanhShrink():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        tanh_node = graph.make_node(
-            'Tanh',
-            inputs=node.input('X', 0), )
-        graph.make_node(
-            'Sub',
-            inputs=[node.input('X', 0), tanh_node],
-            outputs=node.output('Out'))
-
-
-@op_mapper('log_softmax')
-class LogSoftmax():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        axis = node.attr('axis')
-        shape = node.output_shape('Out', 0)
-        if axis is None:
-            axis = -1
-        if axis < 0:
-            axis += len(shape)
-        if axis == len(shape) - 1:
-            node = graph.make_node(
-                'LogSoftmax',
-                inputs=node.input('X'),
-                outputs=node.output('Out'),
-                attrs={'axis': axis})
-        else:
-            perm = [i for i in range(len(shape))]
-            perm[-1] = axis
-            perm[axis] = len(shape) - 1
-            transpose_node = graph.make_node(
-                'Transpose', inputs=node.input('X'), attrs={'perm': perm})
-            softmax_node = graph.make_node(
-                'LogSoftmax', inputs=[transpose_node], axis=-1)
-            transpose_node1 = graph.make_node(
-                'Transpose',
-                inputs=[softmax_node],
-                outputs=node.output('Out'),
-                attrs={'perm': perm})
-
-    @classmethod
-    def opset_13(cls, graph, node, **kw):
-        graph.make_node(
-            'LogSoftmax',
-            inputs=node.input('X'),
-            axis=node.attr('axis'),
-            outputs=node.output('Out'))
-
-
-@op_mapper('layer_norm')
-class LayerNorm():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        ipt = node.input('X', 0)
-        ipt_dims = len(node.input_shape('X', 0))
-        normalized_shape = node.attr('begin_norm_axis')
-        axes = None
-        if isinstance(normalized_shape, collections.Iterable):
-            axes = [-i for i in range(len(normalized_shape), 0, -1)]
-        else:
-            axes = [i for i in range(normalized_shape, ipt_dims)]
-        dtype = node.block.vars[node.input('X', 0)].dtype
-        dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype]
-        epsilon = graph.make_node(
-            'Constant', dtype=dtype, value=node.attr('epsilon'))
-        two = graph.make_node('Constant', dtype=dtype, value=2.0)
-        mean = graph.make_node("ReduceMean", inputs=[ipt], axes=axes)
-        numerator = graph.make_node("Sub", inputs=[ipt, mean])
-        pow_num = graph.make_node("Pow", inputs=[numerator, two])
-        variance = graph.make_node("ReduceMean", inputs=[pow_num], axes=axes)
-        add_eps = graph.make_node("Add", inputs=[variance, epsilon])
-        denominator = graph.make_node("Sqrt", inputs=[add_eps])
-
-        ipt_shape = graph.make_node("Shape", inputs=[ipt])
-        weight_shape = mapper_helper.slice_helper(
-            graph, ipt_shape, [0], [ipt_dims - len(axes)], [ipt_dims])
-        if 'Bias' in node.inputs and 'Scale' in node.inputs and len(
-                node.input('Scale')) > 0 and len(node.input('Bias')) > 0:
-            if normalized_shape == ipt_dims - 1:
-                shape_const = graph.make_node(
-                    'Constant', dtype=dtypes.ONNX.INT64, value=[-1])
-                scale = graph.make_node(
-                    "Reshape", inputs=[node.input('Scale', 0), shape_const])
-                bias = graph.make_node(
-                    "Reshape", inputs=[node.input('Bias', 0), shape_const])
-            else:
-                scale = graph.make_node(
-                    "Reshape", inputs=[node.input('Scale', 0), weight_shape])
-                bias = graph.make_node(
-                    "Reshape", inputs=[node.input('Bias', 0), weight_shape])
-            layer_norm = graph.make_node("Div", inputs=[numerator, denominator])
-            layer_norm = graph.make_node("Mul", inputs=[layer_norm, scale])
-            graph.make_node(
-                "Add", inputs=[layer_norm, bias], outputs=node.output('Y'))
-        elif 'Bias' in node.inputs and len(node.input('Bias')) > 0:
-            if normalized_shape == ipt_dims - 1:
-                shape_const = graph.make_node(
-                    'Constant', dtype=dtypes.ONNX.INT64, value=[-1])
-                bias = graph.make_node(
-                    "Reshape", inputs=[node.input('Bias', 0), shape_const])
-            else:
-                bias = graph.make_node(
-                    "Reshape", inputs=[node.input('Bias', 0), weight_shape])
-            layer_norm = graph.make_node("Div", inputs=[numerator, denominator])
-            graph.make_node(
-                "Add", inputs=[layer_norm, bias], outputs=node.output('Y'))
-        elif 'Scale' in node.inputs and len(node.input('Scale')) > 0:
-            if normalized_shape == ipt_dims - 1:
-                shape_const = graph.make_node(
-                    'Constant', dtype=dtypes.ONNX.INT64, value=[-1])
-                scale = graph.make_node(
-                    "Reshape", inputs=[node.input('Scale', 0), shape_const])
-            else:
-                scale = graph.make_node(
-                    "Reshape", inputs=[node.input('Scale', 0), weight_shape])
-            layer_norm = graph.make_node("Div", inputs=[numerator, denominator])
-            graph.make_node(
-                "Mul", inputs=[layer_norm, scale], outputs=node.output('Y'))
-        else:
-            layer_norm = graph.make_node(
-                "Div",
-                inputs=[numerator, denominator],
-                outputs=node.output('Y'))
-
-
-@op_mapper('batch_norm')
-class BatchNorm():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def make_attrs_and_inputs(cls, graph, node, **kw):
-        onnx_attr = {
-            'epsilon': node.attr('epsilon'),
-            'momentum': node.attr('momentum')
-        }
-        inputs = node.input('X') + node.input('Scale') + node.input(
-            'Bias') + node.input('Mean') + node.input('Variance')
-        return onnx_attr, inputs
-
-    @classmethod
-    def opset_9(cls, graph, node, **kw):
-        onnx_attr, inputs = cls.make_attrs_and_inputs(graph, node, **kw)
-        onnx_node = graph.make_node(
-            'BatchNormalization',
-            inputs=inputs,
-            outputs=node.output('Y'),
-            **onnx_attr)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        onnx_attr, inputs = cls.make_attrs_and_inputs(graph, node, **kw)
-        onnx_attr['spatial'] = 1
-        onnx_node = graph.make_node(
-            'BatchNormalization',
-            inputs=inputs,
-            outputs=node.output('Y'),
-            **onnx_attr)
-
-
-@op_mapper('group_norm')
-class GroupNorm():
-    support_opset_version_range = (6, 15)
-
-    @classmethod
-    def opset_6(cls, graph, node, **kw):
-        num_groups = node.attr('groups')
-        epsilon = node.attr('epsilon')
-        ipt = node.input('X')[0]
-
-        ipt_shape = node.input_shape('X', 0)
-        assert len(
-            ipt_shape) == 4, "Only support 4D-Tensor as input for GroupNorm"
-
-        dtype = node.block.vars[node.input('X', 0)].dtype
-        dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype]
-
-        shape = graph.make_node(
-            'Constant', dtype=dtypes.ONNX.INT64, value=[0, num_groups, -1])
-        reshape_input = graph.make_node('Reshape', inputs=[ipt, shape])
-        scale_ = graph.make_node(
-            'Constant', dtype=dtype, value=[1.0] * num_groups)
-        bias_ = graph.make_node(
-            'Constant', dtype=dtype, value=[0.0] * num_groups)
-        reshaped_output = graph.make_node(
-            'InstanceNormalization',
-            inputs=[reshape_input, scale_, bias_],
-            epsilon=epsilon)
-        origin_shape = graph.make_node('Shape', inputs=[ipt])
-
-        if len(node.input('Scale')) > 0 and len(node.input('Bias')) > 0:
-            output = graph.make_node(
-                'Reshape', inputs=[reshaped_output, origin_shape])
-            unsqueezed_scale = mapper_helper.unsqueeze_helper(
-                graph, node.input('Scale', 0), [1, 2])
-            unsqueezed_bias = mapper_helper.unsqueeze_helper(
-                graph, node.input('Bias', 0), [1, 2])
-            part0 = graph.make_node('Mul', inputs=[output, unsqueezed_scale])
-            graph.make_node(
-                'Add',
-                inputs=[part0, unsqueezed_bias],
-                outputs=node.output('Y'))
-        else:
-            output = graph.make_node(
-                'Reshape',
-                inputs=[reshaped_output, origin_shape],
-                outputs=node.output('Y'))
-
-
-@op_mapper('instance_norm')
-class InstanceNorm():
-    support_opset_version_range = (6, 15)
-
-    @classmethod
-    def opset_6(cls, graph, node, **kw):
-        onnx_attr = {'epsilon': node.attr('epsilon'), }
-        num_groups = node.block.vars[node.input('X')[0]].shape[1]
-
-        dtype = node.block.vars[node.input('X', 0)].dtype
-        dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype]
-
-        if len(node.input('Scale')) == 0:
-            scale_ = graph.make_node(
-                'Constant', dtype=dtype, value=[1.0] * num_groups)
-        else:
-            scale_ = node.input('Scale')[0]
-        if len(node.input('Bias')) == 0:
-            bias_ = graph.make_node(
-                'Constant', dtype=dtype, value=[0.0] * num_groups)
-        else:
-            bias_ = node.input('Bias')[0]
-
-        inputs = node.input('X') + [scale_] + [bias_]
-        onnx_node = graph.make_node(
-            'InstanceNormalization',
-            inputs=inputs,
-            outputs=node.output('Y'),
-            **onnx_attr)
-
-
-@op_mapper('dropout')
-class Dropout():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        dropout_mode = node.attr('dropout_implementation')
-        dropout_prob = node.attr('dropout_prob')
-        if dropout_mode == 'upscale_in_train':
-            onnx_node = graph.make_node(
-                'Identity', inputs=node.input('X'), outputs=node.output('Out'))
-        elif dropout_mode == 'downgrade_in_infer':
-            scale_node = graph.make_node(
-                'Constant',
-                attrs={'dtype': dtypes.ONNX.FLOAT,
-                       'value': 1 - dropout_prob})
-            graph.make_node(
-                "Mul",
-                inputs=[node.input('X')[0], scale_node],
-                outputs=node.output('Out'))
-        else:
-            raise Exception("Unexpected situation happend")
-
-
-@op_mapper('roi_align')
-class RoiAlign():
-    support_opset_version_range = (10, 16)
-
-    @classmethod
-    def opset_10(cls, graph, node, **kw):
-        if node.attr('aligned') and graph.opset_version < 16:
-            raise Exception(
-                'when aligned is true, onnx opset should be (onnx_opset>= 16)')
-        rois_shape = graph.make_node('Shape', inputs=[node.input('ROIs', 0)])
-        starts = graph.make_node(
-            'Constant', attrs={'dtype': dtypes.ONNX.INT64,
-                               'value': [0]})
-        ends = graph.make_node(
-            'Constant', attrs={'dtype': dtypes.ONNX.INT64,
-                               'value': [1]})
-        num_rois = graph.make_node('Slice', inputs=[rois_shape, starts, ends])
-        zero = graph.make_node(
-            'Constant', dims=[1], dtype=dtypes.ONNX.INT64, value=[0])
-        batch_indices = graph.make_node('Expand', inputs=[zero, num_rois])
-        node = graph.make_node(
-            'RoiAlign',
-            inputs=[node.input('X', 0), node.input('ROIs', 0), batch_indices],
-            outputs=node.output('Out'),
-            mode='avg',
-            output_height=node.attr('pooled_height'),
-            output_width=node.attr('pooled_width'),
-            sampling_ratio=node.attr('sampling_ratio'),
-            spatial_scale=node.attr('spatial_scale'))
-
-
-@op_mapper('rnn')
-class RNN():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def make_param_inputs(cls, graph, node, layer, hidden_size, num_layers):
-        # weight assign order:
-        # (F_whi F_whh B_whi B_whh* layer_num  + (F_bias_hi F_bias_hh B_bias_hi  B_bias_hi)* layer_num
-        def reform_weights(g, w, n, intervals):
-            slices = [
-                mapper_helper.slice_helper(
-                    g, w, axes=[1], starts=[x * n], ends=[y * n])
-                for x, y in intervals
-            ]
-            return g.make_node('Concat', slices, axis=1)
-
-        def transform_weight_with_bias(g, weights, n, intervals):
-            return [reform_weights(g, w, n, intervals) for w in weights]
-
-        if node.attr('mode') == 'LSTM':
-            reform_permutation = [(0, 1), (3, 4), (1, 3)]
-        elif node.attr('mode') == 'GRU':
-            reform_permutation = [(1, 2), (0, 1), (2, 3)]
-        bidirect_len = 4 if node.attr('is_bidirec') else 2
-        all_layer_param_len = len(node.input('WeightList'))
-        weight_list = node.input('WeightList')[:all_layer_param_len // 2]
-        bias_list = node.input('WeightList')[all_layer_param_len // 2:]
-        single_layer_param_len = all_layer_param_len // num_layers
-
-        unsqueeze_weights = []
-        layer_weight_list = weight_list[layer * bidirect_len:layer *
-                                        bidirect_len + bidirect_len]
-        layer_bias_list = bias_list[layer * bidirect_len:layer * bidirect_len +
-                                    bidirect_len]
-        param_list = layer_weight_list + layer_bias_list
-        param_list_len = len(param_list)
-        for i in range(param_list_len):
-            weight = mapper_helper.unsqueeze_helper(graph, param_list[i], [0])
-            unsqueeze_weights.append(weight)
-
-        input_weights = unsqueeze_weights[0:param_list_len // 2:2]
-        hidden_weights = unsqueeze_weights[1:param_list_len // 2:2]
-
-        input_weight = graph.make_node('Concat', inputs=input_weights, axis=0)
-        hidden_weight = graph.make_node('Concat', inputs=hidden_weights, axis=0)
-        input_bias = unsqueeze_weights[param_list_len // 2:param_list_len:2]
-        hidden_bias = unsqueeze_weights[param_list_len // 2 + 1:param_list_len:
-                                        2]
-
-        input_bias = graph.make_node('Concat', inputs=input_bias, axis=0)
-        hidden_bias = graph.make_node('Concat', inputs=hidden_bias, axis=0)
-        input_weight, hidden_weight, input_bias, hidden_bias = transform_weight_with_bias(
-            graph, [input_weight, hidden_weight, input_bias, hidden_bias],
-            hidden_size, reform_permutation)
-        bias = graph.make_node(
-            'Concat', inputs=[input_bias, hidden_bias], axis=1)
-        return [input_weight, hidden_weight, bias, '']
-
-    @classmethod
-    def make_init_param_inputs(cls, graph, node, layer):
-        if node.attr('mode') == 'LSTM':
-            all_init_h, all_init_c = node.input('PreState')
-            bidirect_len = 2 if node.attr('is_bidirec') else 1
-            init_h = mapper_helper.slice_helper(
-                graph, all_init_h, [0], [layer * bidirect_len],
-                [layer * bidirect_len + bidirect_len])
-            init_c = mapper_helper.slice_helper(
-                graph, all_init_c, [0], [layer * bidirect_len],
-                [layer * bidirect_len + bidirect_len])
-            return [init_h, init_c]
-        elif node.attr('mode') == 'GRU':
-            all_init_h = node.input('PreState', 0)
-            bidirect_len = 2 if node.attr('is_bidirec') else 1
-            init_h = mapper_helper.slice_helper(
-                graph, all_init_h, [0], [layer * bidirect_len],
-                [layer * bidirect_len + bidirect_len])
-            return [init_h]
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        mode = node.attr('mode')
-        hidden_size = node.attr('hidden_size')
-        num_layers = node.attr('num_layers')
-        prev_output = node.input('Input', 0)
-        if node.attr('mode') == 'LSTM':
-            for layer in range(num_layers):
-                param_inputs = cls.make_param_inputs(graph, node, layer,
-                                                     hidden_size, num_layers)
-                init_param_inputs = cls.make_init_param_inputs(graph, node,
-                                                               layer)
-                if layer + 1 < num_layers:
-                    rnn_outputs = 3
-                    output_y = None
-                else:
-                    rnn_outputs = [1] + node.output('State')
-                    output_y = node.output('Out')
-                prev_output, h_out, c_out = graph.make_node(
-                    node.attr('mode'),
-                    inputs=[prev_output] + param_inputs + init_param_inputs,
-                    outputs=rnn_outputs,
-                    direction='bidirectional'
-                    if node.attr('is_bidirec') else 'forward',
-                    hidden_size=node.attr('hidden_size'))
-                prev_output = graph.make_node(
-                    'Transpose', inputs=[prev_output], perm=[0, 2, 1, 3])
-
-                prev_shape = graph.make_node(
-                    'Constant', dtype=dtypes.ONNX.INT64, value=[0, 0, -1])
-                prev_output = graph.make_node(
-                    'Reshape',
-                    inputs=[prev_output, prev_shape],
-                    outputs=output_y)
-        elif node.attr('mode') == 'GRU':
-            for layer in range(num_layers):
-                param_inputs = cls.make_param_inputs(graph, node, layer,
-                                                     hidden_size, num_layers)
-                init_param_inputs = cls.make_init_param_inputs(graph, node,
-                                                               layer)
-                if layer + 1 < num_layers:
-                    rnn_outputs = 2
-                    output_y = None
-                else:
-                    rnn_outputs = [1] + node.output('State')
-                    output_y = node.output('Out')
-                attrs = {
-                    'direction': 'bidirectional'
-                    if node.attr('is_bidirec') else 'forward',
-                    'hidden_size': node.attr('hidden_size'),
-                    'linear_before_reset': 1,
-                }
-                prev_output, h_out = graph.make_node(
-                    node.attr('mode'),
-                    inputs=[prev_output] + param_inputs + init_param_inputs,
-                    outputs=rnn_outputs,
-                    attrs=attrs)
-                prev_output = graph.make_node(
-                    'Transpose', inputs=[prev_output], perm=[0, 2, 1, 3])
-                prev_shape = graph.make_node(
-                    'Constant', dtype=dtypes.ONNX.INT64, value=[0, 0, -1])
-                prev_output = graph.make_node(
-                    'Reshape',
-                    inputs=[prev_output, prev_shape],
-                    outputs=output_y)
-
-
-@op_mapper('thresholded_relu')
-class ThresholdedRelu():
-    support_opset_version_range = (10, 15)
-
-    @classmethod
-    def opset_10(cls, graph, node, **kw):
-        x_dtype = node.input_dtype('X', 0)
-        if x_dtype != paddle.float32:
-            x = graph.make_node(
-                'Cast', inputs=node.input('X'), to=dtypes.ONNX.FLOAT)
-            threshholdedrelu_node = graph.make_node(
-                'ThresholdedRelu', inputs=[x], alpha=node.attr('threshold'))
-            graph.make_node(
-                'Cast',
-                inputs=[threshholdedrelu_node],
-                outputs=node.output('Out'),
-                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])
-        else:
-            graph.make_node(
-                'ThresholdedRelu',
-                inputs=node.input('X'),
-                alpha=node.attr('threshold'),
-                outputs=node.output('Out'))
+#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import numpy as np
+import math
+import collections
+from paddle2onnx.legacy.constant import dtypes
+from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
+from paddle2onnx.legacy.op_mapper import mapper_helper
+from paddle2onnx import utils
+import paddle
+
+
+@op_mapper(['conv2d', 'depthwise_conv2d', 'conv3d'])
+class Conv():
+    support_opset_version_range = (1, 12)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        kernel_shape = node.input_shape('Filter', 0)
+        dilations = node.attr('dilations')
+        kernel_shape = kernel_shape[2:]
+        strides = node.attr('strides')
+        group = node.attr('groups')
+        pads = node.attr('paddings')
+        assert node.attrs['data_format'] == 'NCHW' or node.attrs['data_format'] == 'NCDHW' or node.attrs['data_format'] == "AnyLayout",  \
+                            "The conv data format should be 'NCHW' or 'NCDHW', but received data format " \
+                            "is %s." % node.attrs['data_format']
+        # onnx padding is [x1_begin, x2_begin...x1_end, x2_end, ...]
+        if len(pads) == 2 or len(pads) == 3:
+            pads = pads + pads
+        elif len(pads) == 4:
+            pads = [pads[i] for i in [0, 2, 1, 3]]
+        elif len(pads) == 6:
+            pads = [pads[i] for i in [0, 2, 4, 1, 3, 5]]
+        attrs = {
+            'dilations': dilations,
+            'kernel_shape': kernel_shape,
+            'strides': strides,
+            'group': group
+        }
+        auto_pad = node.attr('padding_algorithm')
+        if auto_pad == 'SAME':
+            attrs['auto_pad'] = 'SAME_UPPER'
+        elif auto_pad == 'VALID':
+            attrs['auto_pad'] = 'VALID'
+        else:
+            attrs['pads'] = pads
+        graph.make_node(
+            'Conv',
+            inputs=node.input('Input') + node.input('Filter'),
+            outputs=node.output('Output'),
+            attrs=attrs)
+
+
+@op_mapper(
+    ['conv2d_transpose', 'depthwise_conv2d_transpose', 'conv3d_transpose'])
+class ConvTranspose():
+    support_opset_version_range = (1, 12)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        output_padding = node.attr('output_padding')
+        kernel_shape = node.input_shape('Filter', 0)
+        dilations = node.attr('dilations')
+        kernel_shape = kernel_shape[2:]
+        strides = node.attr('strides')
+        group = node.attr('groups')
+        pads = node.attr('paddings')
+        assert node.attrs['data_format'] == 'NCHW' or node.attrs['data_format'] == 'NCDHW', \
+            "The conv data format should be 'NCHW' or 'NCDHW', but received data format " \
+            "is %s." % node.attrs['data_format']
+
+        if len(pads) == 2 or len(pads) == 3:
+            pads = pads + pads
+        elif len(pads) == 4:
+            pads = [pads[i] for i in [0, 2, 1, 3]]
+        elif len(pads) == 6:
+            pads = [pads[i] for i in [0, 2, 4, 1, 3, 5]]
+
+        attrs = {
+            'dilations': dilations,
+            'kernel_shape': kernel_shape,
+            'strides': strides,
+            'group': group
+        }
+        auto_pad = node.attr('padding_algorithm')
+        if auto_pad == 'SAME':
+            attrs['auto_pad'] = 'SAME_UPPER'
+        elif auto_pad == 'VALID':
+            attrs['auto_pad'] = 'VALID'
+        else:
+            attrs['pads'] = pads
+        if output_padding and len(output_padding) > 0:
+            attrs['output_padding'] = output_padding
+        graph.make_node(
+            'ConvTranspose',
+            inputs=node.input('Input') + node.input('Filter'),
+            outputs=node.output('Output'),
+            attrs=attrs)
+
+
+@op_mapper('pool2d')
+class Pool():
+    support_opset_version_range = (1, 12)
+    pool_type = {
+        'max': ('MaxPool', 'GlobalMaxPool'),
+        'avg': ('AveragePool', 'GlobalAveragePool')
+    }
+
+    @classmethod
+    def is_same_span(cls, in_size, out_size):
+        spans = []
+        for i in range(out_size):
+            start = math.floor(i * (in_size / out_size))
+            end = math.ceil((i + 1) * (in_size / out_size))
+            spans.append(end - start)
+        if len(set(spans)) == 1:
+            return True
+        return False
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        assert node.attrs['data_format'] == 'NCHW' or node.attrs['data_format'] == "AnyLayout",  \
+                            "The conv data format should be 'NCHW', but received data format " \
+                            "is %s." % node.attrs['data_format']
+        x_dtype = node.input_dtype('X', 0)
+        need_dtype_convert = False
+        input_name = node.input('X', 0)
+        if x_dtype != paddle.float32:
+            need_dtype_convert = True
+            input_name = graph.make_node(
+                'Cast', inputs=node.input('X'), to=dtypes.ONNX.FLOAT)
+
+        if node.attr('global_pooling') or (node.attr('adaptive') and
+                                           node.attr('ksize') == [1, 1]):
+            if need_dtype_convert:
+                onnx_node = graph.make_node(
+                    cls.pool_type[node.attr('pooling_type')][1],
+                    inputs=[input_name])
+                graph.make_node(
+                    'Cast',
+                    inputs=[onnx_node],
+                    outputs=node.output('Out'),
+                    to=dtypes.ONNX.DOUBLE)
+            else:
+                onnx_node = graph.make_node(
+                    cls.pool_type[node.attr('pooling_type')][1],
+                    inputs=[input_name],
+                    outputs=node.output('Out'))
+        elif node.attr('adaptive'):
+            # if pool is adaptive, check if input shape of pool is fixed.
+            if node.input_shape('X', 0)[2:].count(-1) > 0:
+                raise Exception(
+                    "Converting this model to ONNX need with static input shape," \
+                    " please fix input shape of this model, see doc Q2 in" \
+                    " https://github.com/PaddlePaddle/paddle2onnx/blob/develop/docs/en/FAQ.md."
+                )
+            input_h, input_w = node.input_shape('X', 0)[2:]
+            output_h, output_w = node.output_shape('Out', 0)[2:]
+            stride_h = int(input_h / output_h)
+            stride_w = int(input_w / output_w)
+
+            kernel_h = input_h - (output_h - 1) * stride_h
+            kernel_w = input_w - (output_w - 1) * stride_w
+
+            #check if kernel_size is fixed.
+            if not cls.is_same_span(input_h, output_h) or not cls.is_same_span(
+                    input_w, output_w):
+                raise Exception(
+                    "Cannot convert adaptive pool with input_size: {}, output_size: {}"
+                    .format(
+                        node.input_shape('X', 0), node.output_shape('Out', 0)))
+            else:
+                attrs = {
+                    'kernel_shape': (kernel_h, kernel_w),
+                    'strides': (stride_h, stride_w),
+                }
+                if node.attr('ceil_mode') and graph.opset_version < 10:
+                    raise Exception(
+                        "Cannot convert pool with ceil_model == True to ONNX Opset version < 10."
+                    )
+                elif graph.opset_version > 10:
+                    attrs['ceil_mode'] = node.attr('ceil_mode')
+                auto_pad = node.attr('padding_algorithm')
+                if auto_pad == 'SAME':
+                    attrs['auto_pad'] = 'SAME_UPPER'
+                elif auto_pad == 'VALID':
+                    attrs['auto_pad'] = 'VALID'
+                if node.attr('pooling_type') == 'avg':
+                    attrs['count_include_pad'] = not node.attr('exclusive')
+                if need_dtype_convert:
+                    onnx_node = graph.make_node(
+                        cls.pool_type[node.attr('pooling_type')][0],
+                        inputs=[input_name],
+                        attrs=attrs)
+                    graph.make_node(
+                        'Cast',
+                        inputs=[onnx_node],
+                        outputs=node.output('Out'),
+                        to=dtypes.ONNX.DOUBLE)
+                else:
+                    onnx_node = graph.make_node(
+                        cls.pool_type[node.attr('pooling_type')][0],
+                        inputs=[input_name],
+                        outputs=node.output('Out'),
+                        attrs=attrs)
+        else:
+            input_shape = node.input_shape('X', 0)
+            k_size = node.attr('ksize')
+            pads = node.attr('paddings')
+            strides = node.attr('strides')
+
+            if len(pads) == 2:
+                pads = pads + pads
+            elif len(pads) == 4:
+                pads = [pads[i] for i in [0, 2, 1, 3]]
+
+            if input_shape[2] > 0 and input_shape[2] + pads[0] < k_size[0]:
+                k_size[0] = input_shape[2] + pads[0]
+            if input_shape[3] > 0 and input_shape[3] + pads[1] < k_size[1]:
+                k_size[1] = input_shape[3] + pads[1]
+
+            input_x = [input_name]
+            if max(k_size) <= max(pads):
+                onnx_paddings = [0, 0, pads[0], pads[1], 0, 0, pads[2], pads[3]]
+                attrs_pad = {'mode': 'constant', }
+                if graph.opset_version >= 11:
+                    pads_node = graph.make_node(
+                        'Constant',
+                        attrs={
+                            'dtype': dtypes.ONNX.INT64,
+                            'value': onnx_paddings
+                        })
+                    value_node = graph.make_node(
+                        'Constant',
+                        attrs={'dtype': dtypes.ONNX.FLOAT,
+                               'value': 0.0})
+                    input_x = input_x + [pads_node, value_node]
+                else:
+                    attrs_pad['pads'] = onnx_paddings
+                    attrs_pad['value'] = 0.0
+                input_x = graph.make_node(
+                    'Pad', inputs=input_x, attrs=attrs_pad)
+                pads = [0, 0, 0, 0]
+
+            attrs = {
+                'kernel_shape': k_size,
+                'strides': strides,
+            }
+            auto_pad = node.attr('padding_algorithm')
+            if auto_pad == 'SAME':
+                attrs['auto_pad'] = 'SAME_UPPER'
+            elif auto_pad == 'VALID':
+                attrs['auto_pad'] = 'VALID'
+            else:
+                attrs['pads'] = pads
+            if node.attr('ceil_mode') and graph.opset_version < 10:
+                raise Exception(
+                    "Cannot convert pool with ceil_model == True to ONNX Opset version < 10"
+                )
+            elif graph.opset_version >= 10:
+                attrs['ceil_mode'] = node.attr('ceil_mode')
+
+            if node.attr('pooling_type') == 'avg':
+                attrs['count_include_pad'] = not node.attr('exclusive')
+            if need_dtype_convert:
+                onnx_node = graph.make_node(
+                    cls.pool_type[node.attr('pooling_type')][0],
+                    inputs=input_x,
+                    attrs=attrs)
+                graph.make_node(
+                    'Cast',
+                    inputs=[onnx_node],
+                    outputs=node.output('Out'),
+                    to=dtypes.ONNX.DOUBLE)
+            else:
+                onnx_node = graph.make_node(
+                    cls.pool_type[node.attr('pooling_type')][0],
+                    inputs=input_x,
+                    outputs=node.output('Out'),
+                    attrs=attrs)
+
+
+@op_mapper('pool3d')
+class Pool3D():
+    support_opset_version_range = (1, 12)
+    pool_type = {
+        'max': ('MaxPool', 'GlobalMaxPool'),
+        'avg': ('AveragePool', 'GlobalAveragePool')
+    }
+
+    @classmethod
+    def is_same_span(cls, in_size, out_size):
+        spans = []
+        for i in range(out_size):
+            start = math.floor(i * (in_size / out_size))
+            end = math.ceil((i + 1) * (in_size / out_size))
+            spans.append(end - start)
+        if len(set(spans)) == 1:
+            return True
+        return False
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        assert node.attrs['data_format'] == 'NCDHW' or node.attrs['data_format'] == "AnyLayout",  \
+                            "The conv data format should be 'NCDHW', but received data format " \
+                            "is %s." % node.attrs['data_format']
+
+        if node.attr('global_pooling') or (node.attr('adaptive') and
+                                           node.attr('ksize') == [1, 1, 1]):
+            onnx_node = graph.make_node(
+                cls.pool_type[node.attr('pooling_type')][1],
+                inputs=node.input('X'),
+                outputs=node.output('Out'))
+        elif node.attr('adaptive'):
+            # if pool is adaptive, check if input shape of pool is fixed.
+            if node.input_shape('X', 0)[2:].count(-1) > 0:
+                raise Exception(
+                    "Converting this model to ONNX need with static input shape," \
+                    " please fix input shape of this model, see doc Q2 in" \
+                    " https://github.com/PaddlePaddle/paddle2onnx/blob/develop/docs/en/FAQ.md."
+                )
+            input_d, input_h, input_w = node.input_shape('X', 0)[2:]
+            output_d, output_h, output_w = node.output_shape('Out', 0)[2:]
+            stride_d = int(input_d / output_d)
+            stride_h = int(input_h / output_h)
+            stride_w = int(input_w / output_w)
+
+            kernel_d = input_d - (output_d - 1) * stride_d
+            kernel_h = input_h - (output_h - 1) * stride_h
+            kernel_w = input_w - (output_w - 1) * stride_w
+
+            #check if kernel_size is fixed.
+            if not cls.is_same_span(input_h, output_h) or not cls.is_same_span(
+                    input_w, output_w) or not cls.is_same_span(input_d,
+                                                               output_d):
+                raise Exception(
+                    "Cannot convert adaptive pool with input_size: {}, output_size: {}"
+                    .format(
+                        node.input_shape('X', 0), node.output_shape('Out', 0)))
+            else:
+                attrs = {
+                    'kernel_shape': (kernel_d, kernel_h, kernel_w),
+                    'strides': (stride_d, stride_h, stride_w),
+                }
+                if node.attr('ceil_mode') and graph.opset_version < 10:
+                    raise Exception(
+                        "Cannot convert pool with ceil_model == True to ONNX Opset version < 10."
+                    )
+                elif graph.opset_version > 10:
+                    attrs['ceil_mode'] = node.attr('ceil_mode')
+                auto_pad = node.attr('padding_algorithm')
+                if auto_pad == 'SAME':
+                    attrs['auto_pad'] = 'SAME_UPPER'
+                elif auto_pad == 'VALID':
+                    attrs['auto_pad'] = 'VALID'
+                if node.attr('pooling_type') == 'avg':
+                    attrs['count_include_pad'] = not node.attr('exclusive')
+                onnx_node = graph.make_node(
+                    cls.pool_type[node.attr('pooling_type')][0],
+                    inputs=node.input('X'),
+                    outputs=node.output('Out'),
+                    attrs=attrs)
+        else:
+            input_shape = node.input_shape('X', 0)
+            k_size = node.attr('ksize')
+            paddings = node.attr('paddings')
+            if input_shape[2] > 0 and input_shape[2] + paddings[0] < k_size[0]:
+                k_size[0] = input_shape[2] + paddings[0]
+            if input_shape[3] > 0 and input_shape[3] + paddings[1] < k_size[1]:
+                k_size[1] = input_shape[3] + paddings[1]
+            if input_shape[4] > 0 and input_shape[4] + paddings[2] < k_size[2]:
+                k_size[2] = input_shape[4] + paddings[2]
+            attrs = {
+                'kernel_shape': k_size,
+                'strides': node.attr('strides'),
+                'pads': node.attr('paddings') + node.attr('paddings'),
+            }
+            if node.attr('ceil_mode') and graph.opset_version < 10:
+                raise Exception(
+                    "Cannot convert pool with ceil_model == True to ONNX Opset version < 10"
+                )
+            elif graph.opset_version >= 10:
+                attrs['ceil_mode'] = node.attr('ceil_mode')
+
+            if node.attr('pooling_type') == 'avg':
+                attrs['count_include_pad'] = not node.attr('exclusive')
+            onnx_node = graph.make_node(
+                cls.pool_type[node.attr('pooling_type')][0],
+                inputs=node.input('X'),
+                outputs=node.output('Out'),
+                attrs=attrs)
+
+
+@op_mapper('elu')
+class ELU():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        node = graph.make_node(
+            'Elu',
+            inputs=node.input('X'),
+            outputs=node.output('Out'),
+            alpha=node.attr('alpha'))
+
+
+@op_mapper('softsign')
+class SoftSign():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        graph.make_node(
+            'Softsign', inputs=node.input('X'), outputs=node.output('Out'))
+
+
+@op_mapper('hard_shrink')
+class Hardshrink():
+    support_opset_version_range = (9, 15)
+
+    @classmethod
+    def opset_9(cls, graph, node, **kw):
+        node = graph.make_node(
+            'Shrink',
+            inputs=node.input('X'),
+            outputs=node.output('Out'),
+            lambd=node.attr('threshold'))
+
+
+@op_mapper('logsigmoid')
+class LogSigmoid():
+    support_opset_version_range = (1, 12)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        sigmoid_node = graph.make_node('Sigmoid', inputs=node.input('X'))
+        graph.make_node('Log', inputs=sigmoid_node, outputs=node.output('Out'))
+
+
+@op_mapper('norm')
+class Norm():
+    support_opset_version_range = (1, 12)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        node = graph.make_node(
+            'LpNormalization',
+            inputs=node.input('X'),
+            outputs=node.output('Out'),
+            axis=node.attr('axis'))
+
+
+@op_mapper('softshrink')
+class SoftShrink():
+    support_opset_version_range = (9, 15)
+
+    @classmethod
+    def opset_9(cls, graph, node, **kw):
+        graph.make_node(
+            'Shrink',
+            inputs=node.input('X'),
+            bias=node.attr('lambda'),
+            lambd=node.attr('lambda'),
+            outputs=node.output('Out'))
+
+
+@op_mapper('tanh_shrink')
+class TanhShrink():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        tanh_node = graph.make_node(
+            'Tanh',
+            inputs=node.input('X', 0), )
+        graph.make_node(
+            'Sub',
+            inputs=[node.input('X', 0), tanh_node],
+            outputs=node.output('Out'))
+
+
+@op_mapper('log_softmax')
+class LogSoftmax():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        axis = node.attr('axis')
+        shape = node.output_shape('Out', 0)
+        if axis is None:
+            axis = -1
+        if axis < 0:
+            axis += len(shape)
+        if axis == len(shape) - 1:
+            node = graph.make_node(
+                'LogSoftmax',
+                inputs=node.input('X'),
+                outputs=node.output('Out'),
+                attrs={'axis': axis})
+        else:
+            perm = [i for i in range(len(shape))]
+            perm[-1] = axis
+            perm[axis] = len(shape) - 1
+            transpose_node = graph.make_node(
+                'Transpose', inputs=node.input('X'), attrs={'perm': perm})
+            softmax_node = graph.make_node(
+                'LogSoftmax', inputs=[transpose_node], axis=-1)
+            transpose_node1 = graph.make_node(
+                'Transpose',
+                inputs=[softmax_node],
+                outputs=node.output('Out'),
+                attrs={'perm': perm})
+
+    @classmethod
+    def opset_13(cls, graph, node, **kw):
+        graph.make_node(
+            'LogSoftmax',
+            inputs=node.input('X'),
+            axis=node.attr('axis'),
+            outputs=node.output('Out'))
+
+
+@op_mapper('layer_norm')
+class LayerNorm():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        ipt = node.input('X', 0)
+        ipt_dims = len(node.input_shape('X', 0))
+        normalized_shape = node.attr('begin_norm_axis')
+        axes = None
+        if isinstance(normalized_shape, collections.Iterable):
+            axes = [-i for i in range(len(normalized_shape), 0, -1)]
+        else:
+            axes = [i for i in range(normalized_shape, ipt_dims)]
+        dtype = node.block.vars[node.input('X', 0)].dtype
+        dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype]
+        epsilon = graph.make_node(
+            'Constant', dtype=dtype, value=node.attr('epsilon'))
+        two = graph.make_node('Constant', dtype=dtype, value=2.0)
+        mean = graph.make_node("ReduceMean", inputs=[ipt], axes=axes)
+        numerator = graph.make_node("Sub", inputs=[ipt, mean])
+        pow_num = graph.make_node("Pow", inputs=[numerator, two])
+        variance = graph.make_node("ReduceMean", inputs=[pow_num], axes=axes)
+        add_eps = graph.make_node("Add", inputs=[variance, epsilon])
+        denominator = graph.make_node("Sqrt", inputs=[add_eps])
+
+        ipt_shape = graph.make_node("Shape", inputs=[ipt])
+        weight_shape = mapper_helper.slice_helper(
+            graph, ipt_shape, [0], [ipt_dims - len(axes)], [ipt_dims])
+        if 'Bias' in node.inputs and 'Scale' in node.inputs and len(
+                node.input('Scale')) > 0 and len(node.input('Bias')) > 0:
+            if normalized_shape == ipt_dims - 1:
+                shape_const = graph.make_node(
+                    'Constant', dtype=dtypes.ONNX.INT64, value=[-1])
+                scale = graph.make_node(
+                    "Reshape", inputs=[node.input('Scale', 0), shape_const])
+                bias = graph.make_node(
+                    "Reshape", inputs=[node.input('Bias', 0), shape_const])
+            else:
+                scale = graph.make_node(
+                    "Reshape", inputs=[node.input('Scale', 0), weight_shape])
+                bias = graph.make_node(
+                    "Reshape", inputs=[node.input('Bias', 0), weight_shape])
+            layer_norm = graph.make_node("Div", inputs=[numerator, denominator])
+            layer_norm = graph.make_node("Mul", inputs=[layer_norm, scale])
+            graph.make_node(
+                "Add", inputs=[layer_norm, bias], outputs=node.output('Y'))
+        elif 'Bias' in node.inputs and len(node.input('Bias')) > 0:
+            if normalized_shape == ipt_dims - 1:
+                shape_const = graph.make_node(
+                    'Constant', dtype=dtypes.ONNX.INT64, value=[-1])
+                bias = graph.make_node(
+                    "Reshape", inputs=[node.input('Bias', 0), shape_const])
+            else:
+                bias = graph.make_node(
+                    "Reshape", inputs=[node.input('Bias', 0), weight_shape])
+            layer_norm = graph.make_node("Div", inputs=[numerator, denominator])
+            graph.make_node(
+                "Add", inputs=[layer_norm, bias], outputs=node.output('Y'))
+        elif 'Scale' in node.inputs and len(node.input('Scale')) > 0:
+            if normalized_shape == ipt_dims - 1:
+                shape_const = graph.make_node(
+                    'Constant', dtype=dtypes.ONNX.INT64, value=[-1])
+                scale = graph.make_node(
+                    "Reshape", inputs=[node.input('Scale', 0), shape_const])
+            else:
+                scale = graph.make_node(
+                    "Reshape", inputs=[node.input('Scale', 0), weight_shape])
+            layer_norm = graph.make_node("Div", inputs=[numerator, denominator])
+            graph.make_node(
+                "Mul", inputs=[layer_norm, scale], outputs=node.output('Y'))
+        else:
+            layer_norm = graph.make_node(
+                "Div",
+                inputs=[numerator, denominator],
+                outputs=node.output('Y'))
+
+
+@op_mapper('batch_norm')
+class BatchNorm():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def make_attrs_and_inputs(cls, graph, node, **kw):
+        onnx_attr = {
+            'epsilon': node.attr('epsilon'),
+            'momentum': node.attr('momentum')
+        }
+        inputs = node.input('X') + node.input('Scale') + node.input(
+            'Bias') + node.input('Mean') + node.input('Variance')
+        return onnx_attr, inputs
+
+    @classmethod
+    def opset_9(cls, graph, node, **kw):
+        onnx_attr, inputs = cls.make_attrs_and_inputs(graph, node, **kw)
+        onnx_node = graph.make_node(
+            'BatchNormalization',
+            inputs=inputs,
+            outputs=node.output('Y'),
+            **onnx_attr)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        onnx_attr, inputs = cls.make_attrs_and_inputs(graph, node, **kw)
+        onnx_attr['spatial'] = 1
+        onnx_node = graph.make_node(
+            'BatchNormalization',
+            inputs=inputs,
+            outputs=node.output('Y'),
+            **onnx_attr)
+
+
+@op_mapper('group_norm')
+class GroupNorm():
+    support_opset_version_range = (6, 15)
+
+    @classmethod
+    def opset_6(cls, graph, node, **kw):
+        num_groups = node.attr('groups')
+        epsilon = node.attr('epsilon')
+        ipt = node.input('X')[0]
+
+        ipt_shape = node.input_shape('X', 0)
+        assert len(
+            ipt_shape) == 4, "Only support 4D-Tensor as input for GroupNorm"
+
+        dtype = node.block.vars[node.input('X', 0)].dtype
+        dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype]
+
+        shape = graph.make_node(
+            'Constant', dtype=dtypes.ONNX.INT64, value=[0, num_groups, -1])
+        reshape_input = graph.make_node('Reshape', inputs=[ipt, shape])
+        scale_ = graph.make_node(
+            'Constant', dtype=dtype, value=[1.0] * num_groups)
+        bias_ = graph.make_node(
+            'Constant', dtype=dtype, value=[0.0] * num_groups)
+        reshaped_output = graph.make_node(
+            'InstanceNormalization',
+            inputs=[reshape_input, scale_, bias_],
+            epsilon=epsilon)
+        origin_shape = graph.make_node('Shape', inputs=[ipt])
+
+        if len(node.input('Scale')) > 0 and len(node.input('Bias')) > 0:
+            output = graph.make_node(
+                'Reshape', inputs=[reshaped_output, origin_shape])
+            unsqueezed_scale = mapper_helper.unsqueeze_helper(
+                graph, node.input('Scale', 0), [1, 2])
+            unsqueezed_bias = mapper_helper.unsqueeze_helper(
+                graph, node.input('Bias', 0), [1, 2])
+            part0 = graph.make_node('Mul', inputs=[output, unsqueezed_scale])
+            graph.make_node(
+                'Add',
+                inputs=[part0, unsqueezed_bias],
+                outputs=node.output('Y'))
+        else:
+            output = graph.make_node(
+                'Reshape',
+                inputs=[reshaped_output, origin_shape],
+                outputs=node.output('Y'))
+
+
+@op_mapper('instance_norm')
+class InstanceNorm():
+    support_opset_version_range = (6, 15)
+
+    @classmethod
+    def opset_6(cls, graph, node, **kw):
+        onnx_attr = {'epsilon': node.attr('epsilon'), }
+        num_groups = node.block.vars[node.input('X')[0]].shape[1]
+
+        dtype = node.block.vars[node.input('X', 0)].dtype
+        dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype]
+
+        if len(node.input('Scale')) == 0:
+            scale_ = graph.make_node(
+                'Constant', dtype=dtype, value=[1.0] * num_groups)
+        else:
+            scale_ = node.input('Scale')[0]
+        if len(node.input('Bias')) == 0:
+            bias_ = graph.make_node(
+                'Constant', dtype=dtype, value=[0.0] * num_groups)
+        else:
+            bias_ = node.input('Bias')[0]
+
+        inputs = node.input('X') + [scale_] + [bias_]
+        onnx_node = graph.make_node(
+            'InstanceNormalization',
+            inputs=inputs,
+            outputs=node.output('Y'),
+            **onnx_attr)
+
+
+@op_mapper('dropout')
+class Dropout():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        dropout_mode = node.attr('dropout_implementation')
+        dropout_prob = node.attr('dropout_prob')
+        if dropout_mode == 'upscale_in_train':
+            onnx_node = graph.make_node(
+                'Identity', inputs=node.input('X'), outputs=node.output('Out'))
+        elif dropout_mode == 'downgrade_in_infer':
+            scale_node = graph.make_node(
+                'Constant',
+                attrs={'dtype': dtypes.ONNX.FLOAT,
+                       'value': 1 - dropout_prob})
+            graph.make_node(
+                "Mul",
+                inputs=[node.input('X')[0], scale_node],
+                outputs=node.output('Out'))
+        else:
+            raise Exception("Unexpected situation happend")
+
+
+@op_mapper('roi_align')
+class RoiAlign():
+    support_opset_version_range = (10, 16)
+
+    @classmethod
+    def opset_10(cls, graph, node, **kw):
+        if node.attr('aligned') and graph.opset_version < 16:
+            raise Exception(
+                'when aligned is true, onnx opset should be (onnx_opset>= 16)')
+        rois_shape = graph.make_node('Shape', inputs=[node.input('ROIs', 0)])
+        starts = graph.make_node(
+            'Constant', attrs={'dtype': dtypes.ONNX.INT64,
+                               'value': [0]})
+        ends = graph.make_node(
+            'Constant', attrs={'dtype': dtypes.ONNX.INT64,
+                               'value': [1]})
+        num_rois = graph.make_node('Slice', inputs=[rois_shape, starts, ends])
+        zero = graph.make_node(
+            'Constant', dims=[1], dtype=dtypes.ONNX.INT64, value=[0])
+        batch_indices = graph.make_node('Expand', inputs=[zero, num_rois])
+        node = graph.make_node(
+            'RoiAlign',
+            inputs=[node.input('X', 0), node.input('ROIs', 0), batch_indices],
+            outputs=node.output('Out'),
+            mode='avg',
+            output_height=node.attr('pooled_height'),
+            output_width=node.attr('pooled_width'),
+            sampling_ratio=node.attr('sampling_ratio'),
+            spatial_scale=node.attr('spatial_scale'))
+
+
+@op_mapper('rnn')
+class RNN():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def make_param_inputs(cls, graph, node, layer, hidden_size, num_layers):
+        # weight assign order:
+        # (F_whi F_whh B_whi B_whh* layer_num  + (F_bias_hi F_bias_hh B_bias_hi  B_bias_hi)* layer_num
+        def reform_weights(g, w, n, intervals):
+            slices = [
+                mapper_helper.slice_helper(
+                    g, w, axes=[1], starts=[x * n], ends=[y * n])
+                for x, y in intervals
+            ]
+            return g.make_node('Concat', slices, axis=1)
+
+        def transform_weight_with_bias(g, weights, n, intervals):
+            return [reform_weights(g, w, n, intervals) for w in weights]
+
+        if node.attr('mode') == 'LSTM':
+            reform_permutation = [(0, 1), (3, 4), (1, 3)]
+        elif node.attr('mode') == 'GRU':
+            reform_permutation = [(1, 2), (0, 1), (2, 3)]
+        bidirect_len = 4 if node.attr('is_bidirec') else 2
+        all_layer_param_len = len(node.input('WeightList'))
+        weight_list = node.input('WeightList')[:all_layer_param_len // 2]
+        bias_list = node.input('WeightList')[all_layer_param_len // 2:]
+        single_layer_param_len = all_layer_param_len // num_layers
+
+        unsqueeze_weights = []
+        layer_weight_list = weight_list[layer * bidirect_len:layer *
+                                        bidirect_len + bidirect_len]
+        layer_bias_list = bias_list[layer * bidirect_len:layer * bidirect_len +
+                                    bidirect_len]
+        param_list = layer_weight_list + layer_bias_list
+        param_list_len = len(param_list)
+        for i in range(param_list_len):
+            weight = mapper_helper.unsqueeze_helper(graph, param_list[i], [0])
+            unsqueeze_weights.append(weight)
+
+        input_weights = unsqueeze_weights[0:param_list_len // 2:2]
+        hidden_weights = unsqueeze_weights[1:param_list_len // 2:2]
+
+        input_weight = graph.make_node('Concat', inputs=input_weights, axis=0)
+        hidden_weight = graph.make_node('Concat', inputs=hidden_weights, axis=0)
+        input_bias = unsqueeze_weights[param_list_len // 2:param_list_len:2]
+        hidden_bias = unsqueeze_weights[param_list_len // 2 + 1:param_list_len:
+                                        2]
+
+        input_bias = graph.make_node('Concat', inputs=input_bias, axis=0)
+        hidden_bias = graph.make_node('Concat', inputs=hidden_bias, axis=0)
+        input_weight, hidden_weight, input_bias, hidden_bias = transform_weight_with_bias(
+            graph, [input_weight, hidden_weight, input_bias, hidden_bias],
+            hidden_size, reform_permutation)
+        bias = graph.make_node(
+            'Concat', inputs=[input_bias, hidden_bias], axis=1)
+        return [input_weight, hidden_weight, bias, '']
+
+    @classmethod
+    def make_init_param_inputs(cls, graph, node, layer):
+        if node.attr('mode') == 'LSTM':
+            all_init_h, all_init_c = node.input('PreState')
+            bidirect_len = 2 if node.attr('is_bidirec') else 1
+            init_h = mapper_helper.slice_helper(
+                graph, all_init_h, [0], [layer * bidirect_len],
+                [layer * bidirect_len + bidirect_len])
+            init_c = mapper_helper.slice_helper(
+                graph, all_init_c, [0], [layer * bidirect_len],
+                [layer * bidirect_len + bidirect_len])
+            return [init_h, init_c]
+        elif node.attr('mode') == 'GRU':
+            all_init_h = node.input('PreState', 0)
+            bidirect_len = 2 if node.attr('is_bidirec') else 1
+            init_h = mapper_helper.slice_helper(
+                graph, all_init_h, [0], [layer * bidirect_len],
+                [layer * bidirect_len + bidirect_len])
+            return [init_h]
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        mode = node.attr('mode')
+        hidden_size = node.attr('hidden_size')
+        num_layers = node.attr('num_layers')
+        prev_output = node.input('Input', 0)
+        if node.attr('mode') == 'LSTM':
+            for layer in range(num_layers):
+                param_inputs = cls.make_param_inputs(graph, node, layer,
+                                                     hidden_size, num_layers)
+                init_param_inputs = cls.make_init_param_inputs(graph, node,
+                                                               layer)
+                if layer + 1 < num_layers:
+                    rnn_outputs = 3
+                    output_y = None
+                else:
+                    rnn_outputs = [1] + node.output('State')
+                    output_y = node.output('Out')
+                prev_output, h_out, c_out = graph.make_node(
+                    node.attr('mode'),
+                    inputs=[prev_output] + param_inputs + init_param_inputs,
+                    outputs=rnn_outputs,
+                    direction='bidirectional'
+                    if node.attr('is_bidirec') else 'forward',
+                    hidden_size=node.attr('hidden_size'))
+                prev_output = graph.make_node(
+                    'Transpose', inputs=[prev_output], perm=[0, 2, 1, 3])
+
+                prev_shape = graph.make_node(
+                    'Constant', dtype=dtypes.ONNX.INT64, value=[0, 0, -1])
+                prev_output = graph.make_node(
+                    'Reshape',
+                    inputs=[prev_output, prev_shape],
+                    outputs=output_y)
+        elif node.attr('mode') == 'GRU':
+            for layer in range(num_layers):
+                param_inputs = cls.make_param_inputs(graph, node, layer,
+                                                     hidden_size, num_layers)
+                init_param_inputs = cls.make_init_param_inputs(graph, node,
+                                                               layer)
+                if layer + 1 < num_layers:
+                    rnn_outputs = 2
+                    output_y = None
+                else:
+                    rnn_outputs = [1] + node.output('State')
+                    output_y = node.output('Out')
+                attrs = {
+                    'direction': 'bidirectional'
+                    if node.attr('is_bidirec') else 'forward',
+                    'hidden_size': node.attr('hidden_size'),
+                    'linear_before_reset': 1,
+                }
+                prev_output, h_out = graph.make_node(
+                    node.attr('mode'),
+                    inputs=[prev_output] + param_inputs + init_param_inputs,
+                    outputs=rnn_outputs,
+                    attrs=attrs)
+                prev_output = graph.make_node(
+                    'Transpose', inputs=[prev_output], perm=[0, 2, 1, 3])
+                prev_shape = graph.make_node(
+                    'Constant', dtype=dtypes.ONNX.INT64, value=[0, 0, -1])
+                prev_output = graph.make_node(
+                    'Reshape',
+                    inputs=[prev_output, prev_shape],
+                    outputs=output_y)
+
+
+@op_mapper('thresholded_relu')
+class ThresholdedRelu():
+    support_opset_version_range = (10, 15)
+
+    @classmethod
+    def opset_10(cls, graph, node, **kw):
+        x_dtype = node.input_dtype('X', 0)
+        if x_dtype != paddle.float32:
+            x = graph.make_node(
+                'Cast', inputs=node.input('X'), to=dtypes.ONNX.FLOAT)
+            threshholdedrelu_node = graph.make_node(
+                'ThresholdedRelu', inputs=[x], alpha=node.attr('threshold'))
+            graph.make_node(
+                'Cast',
+                inputs=[threshholdedrelu_node],
+                outputs=node.output('Out'),
+                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])
+        else:
+            graph.make_node(
+                'ThresholdedRelu',
+                inputs=node.input('X'),
+                alpha=node.attr('threshold'),
+                outputs=node.output('Out'))
```

## paddle2onnx/legacy/op_mapper/op_mapper.py

 * *Ordering differences only*

```diff
@@ -1,305 +1,305 @@
-# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import inspect
-import six
-import numpy as np
-import paddle
-from paddle import fluid
-from paddle.fluid import layers
-
-from paddle2onnx.legacy.graph import graph_helper, PaddleGraph
-from paddle2onnx.utils import logging
-from paddle2onnx.legacy.constant.op_mapping_status import *
-
-
-REGISTER_CUSTOM_PADDLE_OP = {}
-
-
-def get_max_support_version(versions, opset_version):
-    max_version = -1
-    for vs in sorted(versions):
-        if vs <= opset_version:
-            max_version = vs
-    return max_version
-
-
-def register_op_mapper(paddle_op, mapper_obj):
-    paddle_op_list = []
-
-    if isinstance(paddle_op, six.string_types):
-        paddle_op_list.append(paddle_op)
-    elif isinstance(paddle_op, list):
-        paddle_op_list = paddle_op
-    else:
-        raise ValueError('paddle_op must be List or string, but got type {}.'.
-                         format(type(paddle_op)))
-
-    if not isinstance(mapper_obj, six.class_types):
-        raise ValueError('mapper_obj must be Class, but got type {}.'.format(
-            type(mapper_obj)))
-
-    valid_register_func = 0
-    for k, v in inspect.getmembers(mapper_obj, inspect.ismethod):
-        if k.startswith("opset_"):
-            version = int(k.replace("opset_", ""))
-            if version > 13 or version < 1:
-                raise Exception(
-                    'the specific method of operator mapper must be named opset_[number](1<=number<=13), such as opset_9, but got {}.'.
-                    format(k))
-            valid_register_func += 1
-
-    if valid_register_func == 0:
-        raise Exception(
-            'the specific method of operator mapper must be classmethod, which named opset_[number](1<=number<=13), such as opset_9, but none achieved.'
-        )
-
-    mapper = OpMapper(paddle_op_list)
-    mapper(mapper_obj)
-
-
-class OpMapper(object):
-    OPSETS = {}
-    REGISTER_CUSTOM_PADDLE_OP = {}
-
-    def __init__(self, paddle_op, **kwargs):
-        if not isinstance(paddle_op, list):
-            paddle_op = [paddle_op]
-        self.paddle_op = paddle_op
-        self.kwargs = kwargs
-
-    def __call__(self, cls):
-        for k, v in inspect.getmembers(cls, inspect.ismethod):
-            if k.startswith("opset_"):
-                version = int(k.replace("opset_", ""))
-                for op in self.paddle_op:
-                    if op not in OpMapper.OPSETS:
-                        OpMapper.OPSETS[op] = {}
-                    opset_dict = OpMapper.OPSETS[op]
-                    opset_dict[version] = (v, self.kwargs)
-
-    @staticmethod
-    def mapping(graph, node, operator_export_type="ONNX"):
-        try:
-            if node.type in OpMapper.REGISTER_CUSTOM_PADDLE_OP:
-                if operator_export_type in ["PaddleFallback"]:
-                    opsets = OpMapper.OPSETS[node.type]
-                    versions = list(opsets.keys())
-                    convert_version = get_max_support_version(
-                        versions, graph.opset_version)
-                    mapper_func, kw = opsets[convert_version]
-                    mapper_func(graph, node, **kw)
-                else:
-                    custom_paddle_op = OpMapper.REGISTER_CUSTOM_PADDLE_OP[
-                        node.type](node)
-                    custom_paddle_graph, output_results = custom_paddle_op.get_paddle_graph(
-                    )
-                    OpMapper.check_support_status(custom_paddle_graph.node_map,
-                                                  graph.opset_version)
-                    graph.build_op_nodes(custom_paddle_graph.node_map)
-
-                    node_output_results = dict()
-                    for k in node.output_names:
-                        custom_outs = output_results[k]
-                        node_outs = node.output(k)
-                        assert len(custom_outs) == len(
-                            node_outs
-                        ), "Length of custom implementation operator's outputs is not same with the length of original operator's outputs."
-                        for i in range(len(custom_outs)):
-                            graph.make_node(
-                                "Identity",
-                                inputs=[custom_outs[i]],
-                                outputs=[node_outs[i]])
-            else:
-                opsets = OpMapper.OPSETS[node.type]
-                versions = list(opsets.keys())
-                convert_version = get_max_support_version(versions,
-                                                          graph.opset_version)
-                mapper_func, kw = opsets[convert_version]
-                mapper_func(graph, node, **kw)
-        except Exception as e:
-            raise Exception(
-                "Error happened when mapping node ['{}'] to onnx, which op_type is '{}' with inputs: {} and outputs: {}, specific error: ".
-                format(node.layer_name, node.type, node.inputs,
-                       node.outputs) + str(e))
-
-    @staticmethod
-    def get_recommend_opset_version(node_map, opset_version):
-        recommend_opset_version = OpMapper.check_support_status(
-            node_map, opset_version, True)
-        for name, node in list(node_map.items()):
-            if node.type in OpMapper.REGISTER_CUSTOM_PADDLE_OP:  #customopcustomop
-                custom_paddle_op = OpMapper.REGISTER_CUSTOM_PADDLE_OP[
-                    node.type](node)
-                custom_paddle_graph, output_results = custom_paddle_op.get_paddle_graph(
-                )
-                custom_recommend_opset_version = OpMapper.check_support_status(
-                    custom_paddle_graph.node_map, opset_version, True)
-                recommend_opset_version = max(recommend_opset_version,
-                                              custom_recommend_opset_version)
-        if opset_version != recommend_opset_version:
-            warning_info = "\n======================\n"
-            warning_info += "\nFor a successful conversion, set the recommended opset version : {}\n".format(
-                recommend_opset_version)
-            warning_info += "\n======================\n"
-            logging.warning(warning_info)
-        return recommend_opset_version
-
-    @staticmethod
-    def check_support_status(node_map, opset_version, for_check=False):
-        op_mapping_status = {
-            OP_MAPPING_NO_REGISTER: [],
-            OP_MAPPING_NO_VERSION: [],
-        }
-        for name, node in list(node_map.items()):
-            if node.type in OpMapper.REGISTER_CUSTOM_PADDLE_OP:
-                continue
-            if node.type not in OpMapper.OPSETS:
-                op_mapping_status[OP_MAPPING_NO_REGISTER].append(node)
-            else:
-                opsets = OpMapper.OPSETS[node.type]
-                versions = list(opsets.keys())
-                convert_version = get_max_support_version(versions,
-                                                          opset_version)
-                if convert_version == -1:
-                    op_mapping_status[OP_MAPPING_NO_VERSION].append(node)
-
-        if len(op_mapping_status[OP_MAPPING_NO_REGISTER]) > 0:
-            unsupported_op_types = set([
-                node.type for node in op_mapping_status[OP_MAPPING_NO_REGISTER]
-            ])
-            error_info = "\nThere's {} ops are not supported yet\n".format(
-                len(unsupported_op_types))
-            for op_type in unsupported_op_types:
-                error_info += "=========== {} ===========\n".format(op_type)
-            raise NotImplementedError(error_info)
-
-        if len(op_mapping_status[OP_MAPPING_NO_VERSION]) > 0:
-            unsupported_op_types = set([
-                node.type for node in op_mapping_status[OP_MAPPING_NO_VERSION]
-            ])
-
-            recommend_opset_version = -1
-            for op_type in unsupported_op_types:
-                opsets = OpMapper.OPSETS[op_type]
-                if min(opsets.keys()) > recommend_opset_version:
-                    recommend_opset_version = min(opsets.keys())
-            warning_info = "\nThere are {} ops that are not supported in opset version {}, please set opset version >= {}.\n".format(
-                len(unsupported_op_types), opset_version,
-                recommend_opset_version)
-
-            for op_type in unsupported_op_types:
-                warning_info += "=========== {} ===========\n".format(op_type)
-            if for_check:
-                logging.warning(warning_info)
-                return recommend_opset_version
-            raise NotImplementedError(warning_info)
-        return opset_version
-
-
-class CustomPaddleOp(object):
-    CREATE_TIMES = {}
-
-    def __init__(self, node):
-        self.main_program = paddle.static.Program()
-        self.startup_program = paddle.static.Program()
-        self.inputs = self.create_place_holder(node)
-        self.node = node
-
-    def generate_scope_name(self, node):
-        if node.type in CustomPaddleOp.CREATE_TIMES:
-            CustomPaddleOp.CREATE_TIMES[node.type] += 1
-        else:
-            CustomPaddleOp.CREATE_TIMES[node.type] = 1
-        scope_prefix = node.type + str(CustomPaddleOp.CREATE_TIMES[node.type] -
-                                       1) + '_'
-        return scope_prefix
-
-    def create_place_holder(self, node):
-        place_holders = {}
-        with paddle.static.program_guard(self.main_program,
-                                         self.startup_program):
-            for arg_name, idxs in node.inputs.items():
-                place_holders[arg_name] = []
-                for idx in range(len(idxs)):
-                    shape = node.input_shape(arg_name, idx)
-                    dtype = node.input_dtype(arg_name, idx)
-                    name = node.input(arg_name, idx)
-                    data = paddle.static.data(
-                        name=name, shape=shape, dtype=dtype)
-                    place_holders[arg_name].append(data)
-        return place_holders
-
-    def input(self, name, idx=None):
-        if name not in self.inputs:
-            return None
-        if idx is None:
-            return self.inputs[name]
-        if len(self.inputs[name]) <= idx:
-            return None
-        return self.inputs[name][idx]
-
-    def get_paddle_graph(self):
-        scope_prefix = self.generate_scope_name(self.node)
-        scope = paddle.static.Scope()
-        with paddle.static.scope_guard(scope):
-            with paddle.static.program_guard(self.main_program,
-                                             self.startup_program):
-                with paddle.utils.unique_name.guard(scope_prefix):
-                    res = self.forward()
-                    feed_var_names = [
-                        var.name for vars in self.inputs.values()
-                        for var in vars
-                    ]
-                    fetch_vars = [var for vars in res.values() for var in vars]
-                    inference_program = graph_helper.get_program(
-                        self.main_program, feed_var_names, fetch_vars)
-                    paddle_graph = PaddleGraph.build_from_program(
-                        inference_program,
-                        feed_var_names,
-                        fetch_vars,
-                        scope=scope)
-
-        output_results = dict()
-        for arg_name, outs in res.items():
-            output_results[arg_name] = [out.name for out in outs]
-        return paddle_graph, output_results
-
-
-def register_custom_paddle_op(paddle_op, custom_op):
-    paddle_op_list = []
-
-    if isinstance(paddle_op, six.string_types):
-        paddle_op_list.append(paddle_op)
-    elif isinstance(paddle_op, list):
-        paddle_op_list = paddle_op
-    else:
-        raise ValueError("paddle_op' must be List or string, but got type {}.".
-                         format(type(paddle_op)))
-
-    if not isinstance(custom_op, six.class_types):
-        raise ValueError("'custom_op' must be Class, but got type {}.".format(
-            type(custom_op)))
-
-    forward = getattr(custom_op, "forward", None)
-    if not callable(forward):
-        raise Exception(
-            "Custom paddle operators must be implemented in function named 'forward'."
-        )
-
-    for op in paddle_op_list:
-        if op not in OpMapper.REGISTER_CUSTOM_PADDLE_OP:
-            OpMapper.REGISTER_CUSTOM_PADDLE_OP[op] = custom_op
+# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import inspect
+import six
+import numpy as np
+import paddle
+from paddle import fluid
+from paddle.fluid import layers
+
+from paddle2onnx.legacy.graph import graph_helper, PaddleGraph
+from paddle2onnx.utils import logging
+from paddle2onnx.legacy.constant.op_mapping_status import *
+
+
+REGISTER_CUSTOM_PADDLE_OP = {}
+
+
+def get_max_support_version(versions, opset_version):
+    max_version = -1
+    for vs in sorted(versions):
+        if vs <= opset_version:
+            max_version = vs
+    return max_version
+
+
+def register_op_mapper(paddle_op, mapper_obj):
+    paddle_op_list = []
+
+    if isinstance(paddle_op, six.string_types):
+        paddle_op_list.append(paddle_op)
+    elif isinstance(paddle_op, list):
+        paddle_op_list = paddle_op
+    else:
+        raise ValueError('paddle_op must be List or string, but got type {}.'.
+                         format(type(paddle_op)))
+
+    if not isinstance(mapper_obj, six.class_types):
+        raise ValueError('mapper_obj must be Class, but got type {}.'.format(
+            type(mapper_obj)))
+
+    valid_register_func = 0
+    for k, v in inspect.getmembers(mapper_obj, inspect.ismethod):
+        if k.startswith("opset_"):
+            version = int(k.replace("opset_", ""))
+            if version > 13 or version < 1:
+                raise Exception(
+                    'the specific method of operator mapper must be named opset_[number](1<=number<=13), such as opset_9, but got {}.'.
+                    format(k))
+            valid_register_func += 1
+
+    if valid_register_func == 0:
+        raise Exception(
+            'the specific method of operator mapper must be classmethod, which named opset_[number](1<=number<=13), such as opset_9, but none achieved.'
+        )
+
+    mapper = OpMapper(paddle_op_list)
+    mapper(mapper_obj)
+
+
+class OpMapper(object):
+    OPSETS = {}
+    REGISTER_CUSTOM_PADDLE_OP = {}
+
+    def __init__(self, paddle_op, **kwargs):
+        if not isinstance(paddle_op, list):
+            paddle_op = [paddle_op]
+        self.paddle_op = paddle_op
+        self.kwargs = kwargs
+
+    def __call__(self, cls):
+        for k, v in inspect.getmembers(cls, inspect.ismethod):
+            if k.startswith("opset_"):
+                version = int(k.replace("opset_", ""))
+                for op in self.paddle_op:
+                    if op not in OpMapper.OPSETS:
+                        OpMapper.OPSETS[op] = {}
+                    opset_dict = OpMapper.OPSETS[op]
+                    opset_dict[version] = (v, self.kwargs)
+
+    @staticmethod
+    def mapping(graph, node, operator_export_type="ONNX"):
+        try:
+            if node.type in OpMapper.REGISTER_CUSTOM_PADDLE_OP:
+                if operator_export_type in ["PaddleFallback"]:
+                    opsets = OpMapper.OPSETS[node.type]
+                    versions = list(opsets.keys())
+                    convert_version = get_max_support_version(
+                        versions, graph.opset_version)
+                    mapper_func, kw = opsets[convert_version]
+                    mapper_func(graph, node, **kw)
+                else:
+                    custom_paddle_op = OpMapper.REGISTER_CUSTOM_PADDLE_OP[
+                        node.type](node)
+                    custom_paddle_graph, output_results = custom_paddle_op.get_paddle_graph(
+                    )
+                    OpMapper.check_support_status(custom_paddle_graph.node_map,
+                                                  graph.opset_version)
+                    graph.build_op_nodes(custom_paddle_graph.node_map)
+
+                    node_output_results = dict()
+                    for k in node.output_names:
+                        custom_outs = output_results[k]
+                        node_outs = node.output(k)
+                        assert len(custom_outs) == len(
+                            node_outs
+                        ), "Length of custom implementation operator's outputs is not same with the length of original operator's outputs."
+                        for i in range(len(custom_outs)):
+                            graph.make_node(
+                                "Identity",
+                                inputs=[custom_outs[i]],
+                                outputs=[node_outs[i]])
+            else:
+                opsets = OpMapper.OPSETS[node.type]
+                versions = list(opsets.keys())
+                convert_version = get_max_support_version(versions,
+                                                          graph.opset_version)
+                mapper_func, kw = opsets[convert_version]
+                mapper_func(graph, node, **kw)
+        except Exception as e:
+            raise Exception(
+                "Error happened when mapping node ['{}'] to onnx, which op_type is '{}' with inputs: {} and outputs: {}, specific error: ".
+                format(node.layer_name, node.type, node.inputs,
+                       node.outputs) + str(e))
+
+    @staticmethod
+    def get_recommend_opset_version(node_map, opset_version):
+        recommend_opset_version = OpMapper.check_support_status(
+            node_map, opset_version, True)
+        for name, node in list(node_map.items()):
+            if node.type in OpMapper.REGISTER_CUSTOM_PADDLE_OP:  #customopcustomop
+                custom_paddle_op = OpMapper.REGISTER_CUSTOM_PADDLE_OP[
+                    node.type](node)
+                custom_paddle_graph, output_results = custom_paddle_op.get_paddle_graph(
+                )
+                custom_recommend_opset_version = OpMapper.check_support_status(
+                    custom_paddle_graph.node_map, opset_version, True)
+                recommend_opset_version = max(recommend_opset_version,
+                                              custom_recommend_opset_version)
+        if opset_version != recommend_opset_version:
+            warning_info = "\n======================\n"
+            warning_info += "\nFor a successful conversion, set the recommended opset version : {}\n".format(
+                recommend_opset_version)
+            warning_info += "\n======================\n"
+            logging.warning(warning_info)
+        return recommend_opset_version
+
+    @staticmethod
+    def check_support_status(node_map, opset_version, for_check=False):
+        op_mapping_status = {
+            OP_MAPPING_NO_REGISTER: [],
+            OP_MAPPING_NO_VERSION: [],
+        }
+        for name, node in list(node_map.items()):
+            if node.type in OpMapper.REGISTER_CUSTOM_PADDLE_OP:
+                continue
+            if node.type not in OpMapper.OPSETS:
+                op_mapping_status[OP_MAPPING_NO_REGISTER].append(node)
+            else:
+                opsets = OpMapper.OPSETS[node.type]
+                versions = list(opsets.keys())
+                convert_version = get_max_support_version(versions,
+                                                          opset_version)
+                if convert_version == -1:
+                    op_mapping_status[OP_MAPPING_NO_VERSION].append(node)
+
+        if len(op_mapping_status[OP_MAPPING_NO_REGISTER]) > 0:
+            unsupported_op_types = set([
+                node.type for node in op_mapping_status[OP_MAPPING_NO_REGISTER]
+            ])
+            error_info = "\nThere's {} ops are not supported yet\n".format(
+                len(unsupported_op_types))
+            for op_type in unsupported_op_types:
+                error_info += "=========== {} ===========\n".format(op_type)
+            raise NotImplementedError(error_info)
+
+        if len(op_mapping_status[OP_MAPPING_NO_VERSION]) > 0:
+            unsupported_op_types = set([
+                node.type for node in op_mapping_status[OP_MAPPING_NO_VERSION]
+            ])
+
+            recommend_opset_version = -1
+            for op_type in unsupported_op_types:
+                opsets = OpMapper.OPSETS[op_type]
+                if min(opsets.keys()) > recommend_opset_version:
+                    recommend_opset_version = min(opsets.keys())
+            warning_info = "\nThere are {} ops that are not supported in opset version {}, please set opset version >= {}.\n".format(
+                len(unsupported_op_types), opset_version,
+                recommend_opset_version)
+
+            for op_type in unsupported_op_types:
+                warning_info += "=========== {} ===========\n".format(op_type)
+            if for_check:
+                logging.warning(warning_info)
+                return recommend_opset_version
+            raise NotImplementedError(warning_info)
+        return opset_version
+
+
+class CustomPaddleOp(object):
+    CREATE_TIMES = {}
+
+    def __init__(self, node):
+        self.main_program = paddle.static.Program()
+        self.startup_program = paddle.static.Program()
+        self.inputs = self.create_place_holder(node)
+        self.node = node
+
+    def generate_scope_name(self, node):
+        if node.type in CustomPaddleOp.CREATE_TIMES:
+            CustomPaddleOp.CREATE_TIMES[node.type] += 1
+        else:
+            CustomPaddleOp.CREATE_TIMES[node.type] = 1
+        scope_prefix = node.type + str(CustomPaddleOp.CREATE_TIMES[node.type] -
+                                       1) + '_'
+        return scope_prefix
+
+    def create_place_holder(self, node):
+        place_holders = {}
+        with paddle.static.program_guard(self.main_program,
+                                         self.startup_program):
+            for arg_name, idxs in node.inputs.items():
+                place_holders[arg_name] = []
+                for idx in range(len(idxs)):
+                    shape = node.input_shape(arg_name, idx)
+                    dtype = node.input_dtype(arg_name, idx)
+                    name = node.input(arg_name, idx)
+                    data = paddle.static.data(
+                        name=name, shape=shape, dtype=dtype)
+                    place_holders[arg_name].append(data)
+        return place_holders
+
+    def input(self, name, idx=None):
+        if name not in self.inputs:
+            return None
+        if idx is None:
+            return self.inputs[name]
+        if len(self.inputs[name]) <= idx:
+            return None
+        return self.inputs[name][idx]
+
+    def get_paddle_graph(self):
+        scope_prefix = self.generate_scope_name(self.node)
+        scope = paddle.static.Scope()
+        with paddle.static.scope_guard(scope):
+            with paddle.static.program_guard(self.main_program,
+                                             self.startup_program):
+                with paddle.utils.unique_name.guard(scope_prefix):
+                    res = self.forward()
+                    feed_var_names = [
+                        var.name for vars in self.inputs.values()
+                        for var in vars
+                    ]
+                    fetch_vars = [var for vars in res.values() for var in vars]
+                    inference_program = graph_helper.get_program(
+                        self.main_program, feed_var_names, fetch_vars)
+                    paddle_graph = PaddleGraph.build_from_program(
+                        inference_program,
+                        feed_var_names,
+                        fetch_vars,
+                        scope=scope)
+
+        output_results = dict()
+        for arg_name, outs in res.items():
+            output_results[arg_name] = [out.name for out in outs]
+        return paddle_graph, output_results
+
+
+def register_custom_paddle_op(paddle_op, custom_op):
+    paddle_op_list = []
+
+    if isinstance(paddle_op, six.string_types):
+        paddle_op_list.append(paddle_op)
+    elif isinstance(paddle_op, list):
+        paddle_op_list = paddle_op
+    else:
+        raise ValueError("paddle_op' must be List or string, but got type {}.".
+                         format(type(paddle_op)))
+
+    if not isinstance(custom_op, six.class_types):
+        raise ValueError("'custom_op' must be Class, but got type {}.".format(
+            type(custom_op)))
+
+    forward = getattr(custom_op, "forward", None)
+    if not callable(forward):
+        raise Exception(
+            "Custom paddle operators must be implemented in function named 'forward'."
+        )
+
+    for op in paddle_op_list:
+        if op not in OpMapper.REGISTER_CUSTOM_PADDLE_OP:
+            OpMapper.REGISTER_CUSTOM_PADDLE_OP[op] = custom_op
```

## paddle2onnx/legacy/op_mapper/search.py

 * *Ordering differences only*

```diff
@@ -1,233 +1,233 @@
-#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import numpy as np
-from paddle2onnx.legacy.constant import dtypes
-from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
-
-
-@op_mapper('where_index')
-class WhereIndex():
-    support_opset_version_range = (9, 15)
-
-    @classmethod
-    def opset_9(cls, graph, node, **kw):
-        nonzero_node = graph.make_node(
-            'NonZero', inputs=node.input('Condition'))
-        graph.make_node(
-            'Transpose',
-            inputs=[nonzero_node],
-            outputs=node.output('Out'),
-            perm=[1, 0])
-
-
-@op_mapper('top_k_v2')
-class TopKV2():
-    support_opset_version_range = (11, 15)
-
-    @classmethod
-    def opset_11(cls, graph, node, **kw):
-        sorted = node.attr('sorted')
-        # for paddle, In gpu device, it always return the sorted value
-        # if not sorted:
-        #     sorted = True
-        if 'K' in node.inputs and len(node.input('K')) > 0:
-            k_node = node.input('K', 0)
-            k_node_dtype = node.input_dtype('K', 0)
-            if dtypes.DTYPE_PADDLE_STR_MAP[k_node_dtype] != 'int64':
-                k_node = graph.make_node(
-                    'Cast', inputs=[k_node], to=dtypes.ONNX.INT64)
-            graph.make_node(
-                'TopK',
-                inputs=[node.input('X', 0), k_node],
-                outputs=[node.output('Out', 0), node.output('Indices', 0)],
-                largest=node.attr('largest'),
-                sorted=sorted,
-                axis=node.attr('axis'))
-        else:
-            k = node.attr('k')
-            k_node = graph.make_node(
-                'Constant', attrs={'dtype': dtypes.ONNX.INT64,
-                                   'value': [k]})
-            graph.make_node(
-                'TopK',
-                inputs=[node.input('X', 0), k_node],
-                outputs=[node.output('Out', 0), node.output('Indices', 0)],
-                largest=node.attr('largest'),
-                sorted=sorted,
-                axis=node.attr('axis'))
-
-
-@op_mapper('top_k')
-class TopK():
-    support_opset_version_range = (11, 15)
-
-    @classmethod
-    def opset_11(cls, graph, node, **kw):
-        if 'K' in node.inputs and len(node.input('K')) > 0:
-            k_node = node.input('K', 0)
-            k_node_dtype = node.input_dtype('K', 0)
-            if dtypes.DTYPE_PADDLE_STR_MAP[k_node_dtype] != 'int64':
-                k_node = graph.make_node(
-                    'Cast', inputs=[k_node], to=dtypes.ONNX.INT64)
-            graph.make_node(
-                'TopK',
-                inputs=[node.input('X', 0), k_node],
-                outputs=[node.output('Out', 0), node.output('Indices', 0)])
-        else:
-            k = node.attr('k')
-            k_node = graph.make_node(
-                'Constant', attrs={'dtype': dtypes.ONNX.INT64,
-                                   'value': [k]})
-            graph.make_node(
-                'TopK',
-                inputs=[node.input('X', 0), k_node],
-                outputs=[node.output('Out', 0), node.output('Indices', 0)])
-
-
-@op_mapper('argsort')
-class ArgSort():
-    support_opset_version_range = (6, 15)
-
-    @classmethod
-    def opset_10(cls, graph, node, **kw):
-        shape = graph.make_node('Shape', inputs=node.input('X', 0))
-        from paddle2onnx.legacy.op_mapper import mapper_helper
-        axis = node.attr('axis')
-        if axis < 0:
-            axis = axis + len(node.input_shape('X', 0))
-        dim_size = mapper_helper.slice_helper(
-            graph, shape, axes=[0], starts=[axis], ends=[axis + 1])
-        if graph.opset_version > 10:
-            if not node.attr('descending'):
-                graph.make_node(
-                    'TopK',
-                    inputs=[node.input('X', 0), dim_size],
-                    outputs=[node.output('Out', 0), node.output('Indices', 0)],
-                    axis=node.attr('axis'),
-                    largest=0)
-            else:
-                graph.make_node(
-                    'TopK',
-                    inputs=[node.input('X', 0), dim_size],
-                    outputs=[node.output('Out', 0), node.output('Indices', 0)],
-                    axis=node.attr('axis'),
-                    largest=1)
-        else:
-            if not node.attr('descending'):
-                raise Exception(
-                    "descending=False only support opset version>=11.")
-            else:
-                graph.make_node(
-                    'TopK',
-                    inputs=[node.input('X', 0), dim_size],
-                    outputs=[node.output('Out', 0), node.output('Indices', 0)],
-                    axis=node.attr('axis'))
-
-    @classmethod
-    def opset_6(cls, graph, node, **kw):
-        shape = node.input_shape('X', 0)
-        k = shape[node.attr('axis')]
-        assert k > 0, "while input shape is dynamic, it only support opset version>=10."
-        input_dtype = node.input_dtype('X', 0)
-        dtype = dtypes.DTYPE_PADDLE_STR_MAP[input_dtype]
-        inputs = node.input('X', 0)
-        if dtype in ["int32", "int64"]:
-            inputs = graph.make_node(
-                'Cast', inputs=inputs, to=dtypes.ONNX.FLOAT)
-        if not node.attr('descending'):
-            raise Exception("descending=False only support opset version>=11.")
-        else:
-            output_node = node.output('Out', 0)
-            graph.make_node(
-                'TopK',
-                inputs=[inputs],
-                outputs=[output_node, node.output('Indices', 0)],
-                axis=node.attr('axis'),
-                k=k)
-            if dtype in ["int32", "int64"]:
-                graph.make_node(
-                    'Cast',
-                    inputs=[output_node],
-                    to=dtypes.DTYPE_PADDLE_ONNX_MAP[input_dtype],
-                    outputs=[output_node])
-
-
-@op_mapper('index_select')
-class IndexSelect():
-    support_opset_version_range = (1, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        graph.make_node(
-            'Gather',
-            inputs=[node.input('X', 0), node.input('Index', 0)],
-            axis=node.attr('dim'),
-            outputs=node.output('Out'))
-
-
-@op_mapper('unique')
-class Unique():
-    support_opset_version_range = (11, 15)
-
-    @classmethod
-    def opset_11(cls, graph, node, **kw):
-        if node.attr('axis') == []:
-            graph.make_node(
-                'Unique',
-                inputs=node.input('X'),
-                outputs=[
-                    node.output('Out', 0), node.output('Indices', 0),
-                    node.output('Index', 0), node.output('Counts', 0)
-                ])
-        else:
-            graph.make_node(
-                'Unique',
-                inputs=node.input('X'),
-                axis=node.attr('axis')[0],
-                outputs=[
-                    node.output('Out', 0), node.output('Indices', 0),
-                    node.output('Index', 0), node.output('Counts', 0)
-                ])
-
-
-@op_mapper('where')
-class Where():
-    support_opset_version_range = (9, 15)
-
-    @classmethod
-    def opset_9(cls, graph, node, **kw):
-        graph.make_node(
-            'Where',
-            inputs=[
-                node.input('Condition', 0), node.input('X', 0),
-                node.input('Y', 0)
-            ],
-            outputs=node.output('Out'))
-
-
-@op_mapper('masked_select')
-class MaskSelect():
-    support_opset_version_range = (11, 15)
-
-    @classmethod
-    def opset_11(cls, graph, node, **kw):
-        index = graph.make_node('NonZero', inputs=node.input('Mask', 0))
-        index = graph.make_node('Transpose', inputs=[index], perm=[1, 0])
-        graph.make_node(
-            'GatherND',
-            inputs=[node.input('X', 0), index],
-            outputs=node.output('Y'))
+#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import numpy as np
+from paddle2onnx.legacy.constant import dtypes
+from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
+
+
+@op_mapper('where_index')
+class WhereIndex():
+    support_opset_version_range = (9, 15)
+
+    @classmethod
+    def opset_9(cls, graph, node, **kw):
+        nonzero_node = graph.make_node(
+            'NonZero', inputs=node.input('Condition'))
+        graph.make_node(
+            'Transpose',
+            inputs=[nonzero_node],
+            outputs=node.output('Out'),
+            perm=[1, 0])
+
+
+@op_mapper('top_k_v2')
+class TopKV2():
+    support_opset_version_range = (11, 15)
+
+    @classmethod
+    def opset_11(cls, graph, node, **kw):
+        sorted = node.attr('sorted')
+        # for paddle, In gpu device, it always return the sorted value
+        # if not sorted:
+        #     sorted = True
+        if 'K' in node.inputs and len(node.input('K')) > 0:
+            k_node = node.input('K', 0)
+            k_node_dtype = node.input_dtype('K', 0)
+            if dtypes.DTYPE_PADDLE_STR_MAP[k_node_dtype] != 'int64':
+                k_node = graph.make_node(
+                    'Cast', inputs=[k_node], to=dtypes.ONNX.INT64)
+            graph.make_node(
+                'TopK',
+                inputs=[node.input('X', 0), k_node],
+                outputs=[node.output('Out', 0), node.output('Indices', 0)],
+                largest=node.attr('largest'),
+                sorted=sorted,
+                axis=node.attr('axis'))
+        else:
+            k = node.attr('k')
+            k_node = graph.make_node(
+                'Constant', attrs={'dtype': dtypes.ONNX.INT64,
+                                   'value': [k]})
+            graph.make_node(
+                'TopK',
+                inputs=[node.input('X', 0), k_node],
+                outputs=[node.output('Out', 0), node.output('Indices', 0)],
+                largest=node.attr('largest'),
+                sorted=sorted,
+                axis=node.attr('axis'))
+
+
+@op_mapper('top_k')
+class TopK():
+    support_opset_version_range = (11, 15)
+
+    @classmethod
+    def opset_11(cls, graph, node, **kw):
+        if 'K' in node.inputs and len(node.input('K')) > 0:
+            k_node = node.input('K', 0)
+            k_node_dtype = node.input_dtype('K', 0)
+            if dtypes.DTYPE_PADDLE_STR_MAP[k_node_dtype] != 'int64':
+                k_node = graph.make_node(
+                    'Cast', inputs=[k_node], to=dtypes.ONNX.INT64)
+            graph.make_node(
+                'TopK',
+                inputs=[node.input('X', 0), k_node],
+                outputs=[node.output('Out', 0), node.output('Indices', 0)])
+        else:
+            k = node.attr('k')
+            k_node = graph.make_node(
+                'Constant', attrs={'dtype': dtypes.ONNX.INT64,
+                                   'value': [k]})
+            graph.make_node(
+                'TopK',
+                inputs=[node.input('X', 0), k_node],
+                outputs=[node.output('Out', 0), node.output('Indices', 0)])
+
+
+@op_mapper('argsort')
+class ArgSort():
+    support_opset_version_range = (6, 15)
+
+    @classmethod
+    def opset_10(cls, graph, node, **kw):
+        shape = graph.make_node('Shape', inputs=node.input('X', 0))
+        from paddle2onnx.legacy.op_mapper import mapper_helper
+        axis = node.attr('axis')
+        if axis < 0:
+            axis = axis + len(node.input_shape('X', 0))
+        dim_size = mapper_helper.slice_helper(
+            graph, shape, axes=[0], starts=[axis], ends=[axis + 1])
+        if graph.opset_version > 10:
+            if not node.attr('descending'):
+                graph.make_node(
+                    'TopK',
+                    inputs=[node.input('X', 0), dim_size],
+                    outputs=[node.output('Out', 0), node.output('Indices', 0)],
+                    axis=node.attr('axis'),
+                    largest=0)
+            else:
+                graph.make_node(
+                    'TopK',
+                    inputs=[node.input('X', 0), dim_size],
+                    outputs=[node.output('Out', 0), node.output('Indices', 0)],
+                    axis=node.attr('axis'),
+                    largest=1)
+        else:
+            if not node.attr('descending'):
+                raise Exception(
+                    "descending=False only support opset version>=11.")
+            else:
+                graph.make_node(
+                    'TopK',
+                    inputs=[node.input('X', 0), dim_size],
+                    outputs=[node.output('Out', 0), node.output('Indices', 0)],
+                    axis=node.attr('axis'))
+
+    @classmethod
+    def opset_6(cls, graph, node, **kw):
+        shape = node.input_shape('X', 0)
+        k = shape[node.attr('axis')]
+        assert k > 0, "while input shape is dynamic, it only support opset version>=10."
+        input_dtype = node.input_dtype('X', 0)
+        dtype = dtypes.DTYPE_PADDLE_STR_MAP[input_dtype]
+        inputs = node.input('X', 0)
+        if dtype in ["int32", "int64"]:
+            inputs = graph.make_node(
+                'Cast', inputs=inputs, to=dtypes.ONNX.FLOAT)
+        if not node.attr('descending'):
+            raise Exception("descending=False only support opset version>=11.")
+        else:
+            output_node = node.output('Out', 0)
+            graph.make_node(
+                'TopK',
+                inputs=[inputs],
+                outputs=[output_node, node.output('Indices', 0)],
+                axis=node.attr('axis'),
+                k=k)
+            if dtype in ["int32", "int64"]:
+                graph.make_node(
+                    'Cast',
+                    inputs=[output_node],
+                    to=dtypes.DTYPE_PADDLE_ONNX_MAP[input_dtype],
+                    outputs=[output_node])
+
+
+@op_mapper('index_select')
+class IndexSelect():
+    support_opset_version_range = (1, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        graph.make_node(
+            'Gather',
+            inputs=[node.input('X', 0), node.input('Index', 0)],
+            axis=node.attr('dim'),
+            outputs=node.output('Out'))
+
+
+@op_mapper('unique')
+class Unique():
+    support_opset_version_range = (11, 15)
+
+    @classmethod
+    def opset_11(cls, graph, node, **kw):
+        if node.attr('axis') == []:
+            graph.make_node(
+                'Unique',
+                inputs=node.input('X'),
+                outputs=[
+                    node.output('Out', 0), node.output('Indices', 0),
+                    node.output('Index', 0), node.output('Counts', 0)
+                ])
+        else:
+            graph.make_node(
+                'Unique',
+                inputs=node.input('X'),
+                axis=node.attr('axis')[0],
+                outputs=[
+                    node.output('Out', 0), node.output('Indices', 0),
+                    node.output('Index', 0), node.output('Counts', 0)
+                ])
+
+
+@op_mapper('where')
+class Where():
+    support_opset_version_range = (9, 15)
+
+    @classmethod
+    def opset_9(cls, graph, node, **kw):
+        graph.make_node(
+            'Where',
+            inputs=[
+                node.input('Condition', 0), node.input('X', 0),
+                node.input('Y', 0)
+            ],
+            outputs=node.output('Out'))
+
+
+@op_mapper('masked_select')
+class MaskSelect():
+    support_opset_version_range = (11, 15)
+
+    @classmethod
+    def opset_11(cls, graph, node, **kw):
+        index = graph.make_node('NonZero', inputs=node.input('Mask', 0))
+        index = graph.make_node('Transpose', inputs=[index], perm=[1, 0])
+        graph.make_node(
+            'GatherND',
+            inputs=[node.input('X', 0), index],
+            outputs=node.output('Y'))
```

## paddle2onnx/legacy/op_mapper/tensor.py

 * *Ordering differences only*

```diff
@@ -1,2155 +1,2155 @@
-#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import numpy as np
-from paddle2onnx.legacy.constant import dtypes
-from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
-from paddle2onnx.legacy.op_mapper import mapper_helper
-import copy
-import six
-import paddle
-
-
-@op_mapper('set_value')
-class SetValue():
-    support_opset_version_range = (11, 15)
-
-    @classmethod
-    def opset_11(cls, graph, node, **kw):
-        axes = node.attr('axes')
-        steps, is_steps_tensor = mapper_helper.get_node_attr_value(
-            graph,
-            node,
-            'steps',
-            'StepsTensor',
-            'StepsTensorList',
-            return_list=True,
-            dtype=dtypes.ONNX.INT64)
-
-        starts, is_starts_tensor = mapper_helper.get_node_attr_value(
-            graph,
-            node,
-            'starts',
-            'StartsTensor',
-            'StartsTensorList',
-            return_list=True,
-            dtype=dtypes.ONNX.INT64)
-
-        ends, is_ends_tensor = mapper_helper.get_node_attr_value(
-            graph,
-            node,
-            'ends',
-            'EndsTensor',
-            'EndsTensorList',
-            return_list=True,
-            dtype=dtypes.ONNX.INT64)
-
-        contain_step_bigger_than_1 = False
-        for i in steps:
-            contain_step_bigger_than_1 = i > 1
-            if not isinstance(i, int) or contain_step_bigger_than_1:
-                contain_step_bigger_than_1 = True
-                break
-        condition = is_steps_tensor or is_starts_tensor or is_ends_tensor or contain_step_bigger_than_1
-        assert not condition, "Currently not supported convert now"
-
-        input_x_shape = node.input_shape('Input', 0)
-        onnx_paddings = [0] * len(input_x_shape) * 2
-        value_shape = list(copy.copy(node.input_shape('Input', 0)))
-        for i in range(len(axes)):
-            axis = axes[i]
-            if starts[i] < 0:
-                starts[i] = starts[i] + input_x_shape[i]
-            if ends[i] < 0:
-                ends[i] = ends[i] + input_x_shape[i]
-            onnx_paddings[axis] = starts[i]
-            value_shape[axis] = value_shape[axis] - onnx_paddings[axis]
-            onnx_paddings[axis + len(input_x_shape)] = input_x_shape[
-                axis] - ends[i]
-            if onnx_paddings[axis + len(input_x_shape)] < 0:
-                onnx_paddings[axis + len(input_x_shape)] = 0
-            value_shape[axis] = value_shape[axis] - onnx_paddings[axis + len(
-                input_x_shape)]
-        dtype_paddle = node.input_dtype('Input', 0)
-        dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype_paddle]
-        value_tensor = None
-        shape = node.attr('shape')
-        if len(shape) > 0:
-            dtypes_list = [
-                'fp32_values', 'fp64_values', 'int32_values', 'int64_values',
-                'bool_values'
-            ]
-            for i in range(len(dtypes_list)):
-                value = node.attr(dtypes_list[i])
-                if value is not None:
-                    break
-            if len(value) == 1:
-                total_nums = 1
-                for i in value_shape:
-                    total_nums *= i
-                value = value * total_nums
-                value_tensor = mapper_helper.constant_helper(
-                    graph, dtype_paddle, value, shape=value_shape)
-            else:
-                value_tensor = mapper_helper.constant_helper(
-                    graph, dtype_paddle, value, shape=shape)
-        else:
-            value_tensor = node.input('ValueTensor', 0)
-        MAX_FLOAT32 = 3.402823466E+38
-        max_node = graph.make_node(
-            'Constant', attrs={'dtype': dtype,
-                               'value': [MAX_FLOAT32]})
-        pads_node = graph.make_node(
-            'Constant',
-            attrs={'dtype': dtypes.ONNX.INT64,
-                   'value': onnx_paddings})
-        value_pad_node = graph.make_node(
-            'Pad', inputs=[value_tensor, pads_node, max_node])
-
-        condition_dtype = graph.make_node(
-            "Equal", inputs=[value_pad_node, max_node])
-        condition_node = graph.make_node(
-            'Cast', inputs=[condition_dtype], to=dtypes.ONNX.BOOL)
-        graph.make_node(
-            "Where",
-            inputs=[condition_node, node.input('Input', 0), value_pad_node],
-            outputs=node.output('Out'))
-
-
-@op_mapper('one_hot_v2')
-class OneHotV2():
-    support_opset_version_range = (9, )
-
-    @classmethod
-    def opset_9(cls, graph, node, **kw):
-        allow_out_of_range = node.attr('allow_out_of_range')
-        assert not allow_out_of_range, "allow_out_of_range can not be true in one_hot_v2."
-        in_dtype_paddle = node.input_dtype('X', 0)
-        in_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[in_dtype_paddle]
-        out_dtype = node.output_dtype('Out', 0)
-        out_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[out_dtype]
-        inputs = node.input('X', 0)
-        if in_dtype_paddle == paddle.int32:
-            inputs = graph.make_node(
-                'Cast', inputs=[inputs], to=dtypes.ONNX.INT64)
-            in_dtype = dtypes.ONNX.INT64
-        value_node = graph.make_node('Constant', dtype=out_dtype, value=[0, 1])
-        depth = node.attr('depth')
-        if node.input('depth_tensor', 0) is not None:
-            depth_node = node.input('depth_tensor', 0)
-        else:
-            depth_node = graph.make_node(
-                'Constant', dtype=in_dtype, value=[depth])
-        reshaped_input_node = graph.make_node(
-            'OneHot',
-            inputs=[inputs, depth_node, value_node],
-            outputs=node.output('Out'))
-
-
-@op_mapper('concat')
-class Concat():
-    support_opset_version_range = (4, 15)
-
-    @classmethod
-    def opset_4(cls, graph, node, **kw):
-        inputs = node.input('X')
-
-        input_dtypes = [node.input_dtype('X', i) for i in range(len(inputs))]
-        inputs = mapper_helper.dtype_alignment(graph, inputs, input_dtypes)
-        node_axis = node.input('AxisTensor')
-        if node_axis is not None and len(node_axis) > 0:
-            axis_node = node.input('AxisTensor')[0]
-            try:
-                axis = mapper_helper.get_value_from_parameters(graph,
-                                                               axis_node)[0]
-            except Exception as e:
-                raise Exception(
-                    "Currently does not support the axis parameter as input tensor"
-                    + str(e))
-        else:
-            axis = node.attr('axis')
-        if axis < 0:
-            axis = axis + len(node.input_shape('X', 0))
-
-        node = graph.make_node(
-            'Concat', inputs=inputs, outputs=node.output('Out'), axis=axis)
-
-
-@op_mapper('assign')
-class Assign():
-    support_opset_version_range = (1, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        inputs = node.input('X')
-        graph.make_node('Identity', inputs=inputs, outputs=node.output('Out'))
-
-
-@op_mapper('lod_reset')
-class LodReset():
-    support_opset_version_range = (1, )
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        graph.make_node(
-            'Identity', inputs=node.input('X'), outputs=node.output('Out'))
-
-
-@op_mapper('eye')
-class Eye():
-    support_opset_version_range = (9, )
-
-    @classmethod
-    def opset_9(cls, graph, node, **kw):
-        num_rows = node.attr('num_rows')
-        num_columns = node.attr('num_columns')
-        dtype = node.output_dtype('Out', 0)
-        value = [0] * num_rows * num_columns
-        value_tensor = mapper_helper.constant_helper(
-            graph, dtype, value, shape=[num_rows, num_columns])
-        graph.make_node(
-            'EyeLike', inputs=[value_tensor], outputs=node.output('Out'))
-
-
-@op_mapper('stack')
-class Stack():
-    support_opset_version_range = (4, 15)
-
-    @classmethod
-    def opset_4(cls, graph, node, **kw):
-        inputs = node.input('X')
-        input_dtypes = [node.input_dtype('X', i) for i in range(len(inputs))]
-        inputs = mapper_helper.dtype_alignment(graph, inputs, input_dtypes)
-        axis = node.attr('axis')
-
-        unsqueezed_inputs = list()
-        for ipt in inputs:
-            unsqueezed_ipt = mapper_helper.unsqueeze_helper(graph, ipt, [axis])
-            unsqueezed_inputs.append(unsqueezed_ipt)
-        graph.make_node(
-            'Concat',
-            inputs=unsqueezed_inputs,
-            outputs=node.output('Y'),
-            axis=axis)
-
-
-@op_mapper('unstack')
-class Unstack():
-    support_opset_version_range = (2, 15)
-
-    @classmethod
-    def opset_2(cls, graph, node, **kw):
-        axis = node.attr('axis')
-        ndim = node.block.vars[node.input('X')[0]].ndim
-        axis = axis + ndim if axis < 0 else axis
-        output_y = mapper_helper.split_helper(
-            graph,
-            node.input('X'),
-            axis=axis,
-            split=[1] * len(node.output('Y')),
-            outputs=len(node.output('Y')))
-
-        if isinstance(output_y, six.string_types):
-            output_y = [output_y]
-
-        for i in range(len(output_y)):
-            mapper_helper.squeeze_helper(graph, output_y[i], [axis],
-                                         node.output('Y', i))
-
-
-@op_mapper('expand_as_v2')
-class ExpandAsV2():
-    support_opset_version_range = (8, 15)
-
-    @classmethod
-    def opset_8(cls, graph, node, **kw):
-        target_shape = node.attr('target_shape')
-        if node.input('target_tensor', 0) is not None:
-            target_shape = graph.make_node(
-                'Shape', inputs=[node.input('target_tensor', 0)])
-        elif target_shape is not None:
-            target_shape = graph.make_node(
-                'Constant',
-                attrs={'dtype': dtypes.ONNX.INT64,
-                       'value': target_shape})
-        else:
-            raise Exception(
-                "Not find attribute: 'target_shape' or tensor 'target_tensor'")
-        node = graph.make_node(
-            'Expand',
-            inputs=[node.input('X', 0), target_shape],
-            outputs=node.output('Out'))
-
-
-@op_mapper('expand_v2')
-class ExpandV2():
-    support_opset_version_range = (8, 15)
-
-    @classmethod
-    def opset_8(cls, graph, node, **kw):
-        expand_shape, _ = mapper_helper.get_node_attr_value(
-            graph,
-            node,
-            'shape',
-            'Shape',
-            'expand_shapes_tensor',
-            dtype=dtypes.ONNX.INT64)
-
-        input_shape = node.input_shape('X', 0)
-        input_shape_node = graph.make_node('Shape', inputs=node.input('X', 0))
-
-        node_shape = node.attr('shape')
-        node_shape_tensor = node.input('Shape')
-        node_shape_tensor_list = node.input('expand_shapes_tensor')
-        if node_shape_tensor is not None and len(node_shape_tensor) > 0:
-            diff = node.input_shape('Shape', 0)[0] - len(input_shape)
-        elif node_shape_tensor_list is not None and \
-                len(node_shape_tensor_list) > 0:
-            diff = len(node_shape_tensor_list) - len(input_shape)
-        elif node_shape is not None and len(node_shape) > 0:
-            diff = len(node_shape) - len(input_shape)
-            expand_shape = graph.make_node(
-                'Constant', dtype=dtypes.ONNX.INT64, value=expand_shape)
-
-        if diff > 0:
-            one_node = graph.make_node(
-                'Constant',
-                attrs={'dtype': dtypes.ONNX.INT64,
-                       'value': [1] * diff})
-            input_shape_node = graph.make_node(
-                'Concat', inputs=[one_node, input_shape_node], axis=0)
-
-        if graph.opset_version < 12:
-            input_shape_node = graph.make_node(
-                'Cast', inputs=[input_shape_node], to=dtypes.ONNX.FLOAT)
-            expand_shape = graph.make_node(
-                'Cast', inputs=[expand_shape], to=dtypes.ONNX.FLOAT)
-            shape = graph.make_node(
-                'Max', inputs=[input_shape_node, expand_shape])
-            shape = graph.make_node(
-                'Cast', inputs=[shape], to=dtypes.ONNX.INT64)
-        else:
-            shape = graph.make_node(
-                'Max', inputs=[input_shape_node, expand_shape])
-        node = graph.make_node(
-            'Expand',
-            inputs=[node.input('X', 0), shape],
-            outputs=node.output('Out'))
-
-
-@op_mapper('shape')
-class Shape():
-    support_opset_version_range = (6, 15)
-
-    @classmethod
-    def opset_6(cls, graph, node, **kw):
-        shape_node = graph.make_node('Shape', inputs=node.input('Input'))
-        graph.make_node(
-            'Cast',
-            inputs=[shape_node],
-            outputs=node.output('Out'),
-            to=dtypes.ONNX.INT32)
-
-
-@op_mapper('size')
-class Numel():
-    supports_opset_version_range = (1, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        size_node = graph.make_node('Size', inputs=node.input('Input'))
-        mapper_helper.unsqueeze_helper(graph, size_node, [0],
-                                       node.output('Out'))
-
-
-@op_mapper('split')
-class Split():
-    support_opset_version_range = (1, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        sections = node.attr('sections')
-        axis = cls.get_axis(graph, node)
-        if isinstance(sections, list) and len(sections) == 1:
-            graph.make_node(
-                'Identity', inputs=node.input('X'), outputs=node.output('Out'))
-        else:
-            if len(sections) > 0:
-                input_shape = node.block.vars[node.input('X')[0]].shape
-                section_index = [
-                    i for i, val in enumerate(sections) if val == -1
-                ]
-                if input_shape[axis] != -1 and len(section_index) == 1:
-                    sections[section_index[0]] = input_shape[axis] - sum(
-                        sections) - 1
-                mapper_helper.split_helper(
-                    graph,
-                    node.input('X'),
-                    axis=axis,
-                    split=sections,
-                    outputs=node.output('Out'))
-            else:
-                graph.make_node(
-                    'Split',
-                    inputs=node.input('X'),
-                    outputs=node.output('Out'),
-                    axis=axis)
-
-    @classmethod
-    def get_axis(cls, graph, node):
-        if len(node.input('AxisTensor')) > 0:
-            axis_node = node.input('AxisTensor')[0]
-            # When axis is tensor, only int32 and int64 are supported
-            if axis_node not in graph.parameters:
-                raise Exception(
-                    "Currently does not support the axis parameter as input tensor!"
-                )
-            else:
-                axis = graph.parameters[axis_node].attribute[0].t.int32_data
-                if axis is None or len(axis) < 1:
-                    axis = graph.parameters[axis_node].attribute[
-                        0].t.int64_data[0]
-        else:
-            axis = node.attr('axis')
-        return axis
-
-
-@op_mapper(['roll'])
-class Roll():
-    support_opset_version_range = (4, 15)
-
-    @classmethod
-    def roll(cls, graph, node, input_x, dims, shifts):
-        for i in range(len(dims)):
-            if graph.opset_version >= 10 and isinstance(shifts,
-                                                        six.string_types):
-                to_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype(
-                    'ShiftsTensor', 0)]
-                const_i = graph.make_node('Constant', dtype=to_dtype, value=i)
-                const_0 = graph.make_node('Constant', dtype=to_dtype, value=0)
-                shift_node = graph.make_node(
-                    'Gather', inputs=[shifts, const_i], axis=0)
-                shift_node = graph.make_node(
-                    "Sub", inputs=[const_0, shift_node])
-                shift_node = mapper_helper.unsqueeze_helper(graph, shift_node,
-                                                            [0])
-            elif graph.opset_version < 10 and isinstance(shifts,
-                                                         six.string_types):
-                raise Exception(
-                    "shifts of roll is Tensor, please try with higher onnx opset_version>=10."
-                )
-            else:
-                shift_node = [-shifts[i]]
-                to_dtype = dtypes.ONNX.INT64
-            shapes = []
-            shape = mapper_helper.slice_helper(
-                graph, input_x, [dims[i]], shift_node, [60000], dtype=to_dtype)
-            shapes.append(shape)
-            shape = mapper_helper.slice_helper(
-                graph, input_x, [dims[i]], [0], shift_node, dtype=to_dtype)
-            shapes.append(shape)
-            input_x = graph.make_node('Concat', inputs=shapes, axis=dims[i])
-        return input_x
-
-    @classmethod
-    def flatten(cls, graph, node):
-        dims = len(node.input_shape('X', 0))
-        start_axis = 0
-        end_axis = dims - 1
-        shape_node = graph.make_node('Shape', inputs=node.input('X'))
-        if end_axis < dims - 1:
-            slice1 = mapper_helper.slice_helper(
-                graph, shape_node, axes=[0], starts=[0], ends=[start_axis])
-            slice3 = mapper_helper.slice_helper(
-                graph, shape_node, axes=[0], starts=[end_axis + 1],
-                ends=[dims])
-            slices = [
-                slice1, graph.make_node(
-                    'Constant', value=[-1], dtype=dtypes.ONNX.INT64), slice3
-            ]
-        else:
-            slice1 = mapper_helper.slice_helper(
-                graph, shape_node, axes=[0], starts=[0], ends=[start_axis])
-            slices = [
-                slice1, graph.make_node(
-                    'Constant', value=[-1], dtype=dtypes.ONNX.INT64)
-            ]
-        final_shape = graph.make_node('Concat', inputs=slices, axis=0)
-        output = graph.make_node(
-            'Reshape', inputs=[node.input('X')[0], final_shape])
-        return output
-
-    @classmethod
-    def opset_4(cls, graph, node, **kw):
-        dims = node.attr('axis')
-        shifts = node.attr('shifts')
-        input_x = node.input('X')[0]
-        input_shape = node.input_shape('X', 0)
-        shifts_node = node.input('ShiftsTensor')
-        if len(dims) > 0:
-            axes = [
-                axis + len(input_shape) if axis < 0 else axis
-                for i, axis in enumerate(dims)
-            ]
-            if shifts_node is not None and len(shifts_node) > 0:
-                shifts = shifts_node[0]
-            else:
-                for i in range(0, len(axes)):
-                    if input_shape[axes[i]] > 0:
-                        assert -input_shape[axes[i]] <= shifts[i] <= input_shape[axes[i]], \
-                            "the value of shifts in axis is less than the value of input_shape in axis."
-
-            input_x = cls.roll(graph, node, input_x, axes, shifts)
-            graph.make_node(
-                'Identity', inputs=[input_x], outputs=node.output('Out'))
-        else:
-            if shifts_node is not None and len(shifts_node) > 0:
-                shifts = shifts_node[0]
-            input_x = cls.flatten(graph, node)
-            input_x = cls.roll(graph, node, input_x, [0], shifts)
-            shape_node = graph.make_node(
-                'Constant',
-                attrs={'dtype': dtypes.ONNX.INT64,
-                       'value': list(input_shape)})
-            graph.make_node(
-                'Reshape',
-                inputs=[input_x, shape_node],
-                outputs=node.output('Out'))
-
-
-@op_mapper(['slice', 'strided_slice'])
-class Slice():
-    support_opset_version_range = (1, 15)
-
-    @classmethod
-    def decrease_axis(cls, node):
-        # tensor[i,:] will decrease rank of origin input, example:
-        # paddle.slice() will not decrease rank of origin input
-        # if input shape is [2, 3], input[0, :] will generate output with shape [3], not [1, 3].
-        # paddle.slice(input, 0, 1, 0) will  generate output with shape [1, 3], not [3].
-
-        decrease_axis = node.attr('decrease_axis')
-        if len(decrease_axis) == 0:
-            return None
-        if node.output_shape('Out', 0) == [0]:
-            return decrease_axis
-        if len(node.input_shape('Input', 0)) > len(node.output_shape('Out', 0)):
-            return decrease_axis
-        return None
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        axes = node.attr('axes')
-        strides, strides_is_tensor = mapper_helper.get_node_attr_value(
-            graph, node, 'strides', 'StridesTensor', 'StridesTensorList', True)
-        strides = [1] * len(axes) if strides is None else strides
-        steps = [i for i, val in enumerate(strides) if val == 1]
-        assert len(steps) == len(axes), \
-            "Slice in onnx(opset<10) not support attribute 'step', Try converting with opset_version >=10"
-
-        starts, start_is_tensor = mapper_helper.get_node_attr_value(
-            graph, node, 'starts', 'StartsTensor', 'StartsTensorList', True)
-        ends, end_is_tensor = mapper_helper.get_node_attr_value(
-            graph, node, 'ends', 'EndsTensor', 'EndsTensorList', True)
-
-        assert not strides_is_tensor and not start_is_tensor and not end_is_tensor, \
-            "Slice in onnx(opset<10) not support attribute 'steps','starts' or 'ends' which have tensor value, " \
-            "Try converting with opset_version >=10 "
-
-        decrease_axis = cls.decrease_axis(node)
-        if decrease_axis is None:
-            graph.make_node(
-                "Slice",
-                inputs=[node.input('Input')[0]],
-                outputs=node.output('Out'),
-                axes=axes,
-                starts=starts,
-                ends=ends)
-        else:
-            sliced = graph.make_node(
-                "Slice",
-                inputs=[node.input('Input')[0]],
-                axes=axes,
-                starts=starts,
-                ends=ends)
-            mapper_helper.squeeze_helper(graph, sliced, decrease_axis,
-                                         node.output('Out'))
-
-    @classmethod
-    def opset_10(cls, graph, node, **kw):
-        axes = node.attr('axes')
-        strides, _ = mapper_helper.get_node_attr_value(
-            graph,
-            node,
-            'strides',
-            'StridesTensor',
-            'StridesTensorList',
-            dtype=dtypes.ONNX.INT64)
-        strides = [1] * len(axes) if strides is None else strides
-
-        starts, _ = mapper_helper.get_node_attr_value(
-            graph,
-            node,
-            'starts',
-            'StartsTensor',
-            'StartsTensorList',
-            dtype=dtypes.ONNX.INT64)
-        ends, _ = mapper_helper.get_node_attr_value(
-            graph,
-            node,
-            'ends',
-            'EndsTensor',
-            'EndsTensorList',
-            dtype=dtypes.ONNX.INT64)
-
-        if isinstance(starts, list):
-            starts_node = graph.make_node(
-                'Constant',
-                attrs={'dtype': dtypes.ONNX.INT64,
-                       'value': starts})
-        else:
-            starts_node = starts
-        if isinstance(ends, list):
-            ends_node = graph.make_node(
-                'Constant', attrs={'dtype': dtypes.ONNX.INT64,
-                                   'value': ends})
-        else:
-            ends_node = ends
-
-        if isinstance(strides, list):
-            strides_node = graph.make_node(
-                'Constant',
-                attrs={'dtype': dtypes.ONNX.INT64,
-                       'value': strides})
-        else:
-            strides_node = strides
-
-        steps_node = strides_node
-        axes_node = graph.make_node(
-            'Constant', attrs={'dtype': dtypes.ONNX.INT64,
-                               'value': axes})
-
-        decrease_axis = cls.decrease_axis(node)
-        if decrease_axis is None:
-            sliced = graph.make_node(
-                "Slice",
-                inputs=[
-                    node.input('Input')[0], starts_node, ends_node, axes_node,
-                    steps_node
-                ],
-                outputs=node.output('Out'))
-        else:
-            sliced = graph.make_node(
-                "Slice",
-                inputs=[
-                    node.input('Input')[0], starts_node, ends_node, axes_node,
-                    steps_node
-                ])
-            mapper_helper.squeeze_helper(graph, sliced, decrease_axis,
-                                         node.output('Out'))
-
-
-@op_mapper(['sequence_expand'])
-class SequenceExpand():
-    support_opset_version_range = ()
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        graph.make_node(
-            'Identity', inputs=node.input('X'), outputs=node.output('Out'))
-
-
-@op_mapper(['expand'])
-class Expand():
-    support_opset_version_range = (6, 15)
-
-    @classmethod
-    def opset_6(cls, graph, node, **kw):
-        expand_times, _ = mapper_helper.get_node_attr_value(
-            graph,
-            node,
-            'expand_times',
-            'ExpandTimes',
-            'expand_times_tensor',
-            dtype=dtypes.ONNX.INT64)
-
-        if isinstance(expand_times, list):
-            expand_times = graph.make_node(
-                'Constant',
-                attrs={'dtype': dtypes.ONNX.INT64,
-                       'value': expand_times})
-
-        graph.make_node(
-            "Tile",
-            inputs=[node.input('X', 0), expand_times],
-            outputs=node.output('Out'))
-
-
-@op_mapper(['tile'])
-class Tile():
-    support_opset_version_range = (6, 15)
-
-    @classmethod
-    def opset_6(cls, graph, node, **kw):
-        repeat_times, _ = mapper_helper.get_node_attr_value(
-            graph,
-            node,
-            'repeat_times',
-            'RepeatTimes',
-            'repeat_times_tensor',
-            dtype=dtypes.ONNX.INT64)
-
-        if isinstance(repeat_times, list):
-            repeat_times = graph.make_node(
-                'Constant',
-                attrs={'dtype': dtypes.ONNX.INT64,
-                       'value': repeat_times})
-
-        graph.make_node(
-            "Tile",
-            inputs=[node.input('X', 0), repeat_times],
-            outputs=node.output('Out'))
-
-
-@op_mapper('range')
-class Range():
-    support_opset_version_range = (11, 15)
-
-    @classmethod
-    def opset_11(cls, graph, node, **kw):
-        start = node.input('Start', 0)
-        end = node.input('End', 0)
-        step = node.input('Step', 0)
-        start_t = mapper_helper.squeeze_helper(graph, start, [0])
-        end_t = mapper_helper.squeeze_helper(graph, end, [0])
-        step_t = mapper_helper.squeeze_helper(graph, step, [0])
-        graph.make_node(
-            "Range",
-            inputs=[start_t, end_t, step_t],
-            outputs=node.output('Out'))
-
-
-@op_mapper('fill_constant')
-class Constant():
-    support_opset_version_range = (1, 15)
-
-    @classmethod
-    def check_int_type(cls, dtype):
-        if dtype in [dtypes.ONNX.INT16, dtypes.ONNX.INT32, dtypes.ONNX.INT64]:
-            return True
-        return False
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        value = node.attr('value')
-        dtype = node.attr('dtype')
-        value_is_scalar_tensor = False
-        if 'ValueTensor' in node.inputs and len(node.input('ValueTensor')) > 0:
-            rank = len(node.input_shape("ValueTensor", 0))
-            if rank == 1 and node.input_shape("ValueTensor", 0)[0] == 1:
-                value_is_scalar_tensor = True
-                value = node.input("ValueTensor")[0]
-            else:
-                raise Exception(
-                    "paddle.full with tensor value parameter is not supported yet."
-                )
-
-        shape, is_shape_tensor = mapper_helper.get_node_attr_value(
-            graph,
-            node,
-            'shape',
-            'ShapeTensor',
-            'ShapeTensorList',
-            dtype=dtypes.ONNX.INT64)
-
-        if graph.opset_version >= 9 and (is_shape_tensor or
-                                         value_is_scalar_tensor):
-            if not is_shape_tensor:
-                shape = graph.make_node(
-                    'Constant', dtype=dtypes.ONNX.INT64, value=shape)
-            input_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype]
-            if not value_is_scalar_tensor and cls.check_int_type(input_dtype):
-                to_dtype = dtypes.ONNX.DOUBLE
-                outputs = None
-            else:
-                to_dtype = input_dtype
-                outputs = node.output('Out')
-
-            if value_is_scalar_tensor:
-                base_value = graph.make_node(
-                    'ConstantOfShape',
-                    inputs=shape,
-                    attrs={'dims': [1],
-                           'dtype': to_dtype,
-                           'value': 0})
-                node2 = graph.make_node(
-                    "Add", inputs=[base_value, value], outputs=outputs)
-            else:
-                node2 = graph.make_node(
-                    'ConstantOfShape',
-                    inputs=shape,
-                    outputs=outputs,
-                    attrs={'dims': [1],
-                           'dtype': to_dtype,
-                           'value': value})
-
-            if not value_is_scalar_tensor and cls.check_int_type(input_dtype):
-                graph.make_node(
-                    'Cast',
-                    inputs=node2,
-                    outputs=node.output('Out'),
-                    attrs={'to': input_dtype})
-        else:
-            assert not is_shape_tensor and not value_is_scalar_tensor, \
-                "Currently op ['fill_constant'] does not support in onnx(opset<9) when 'shape' or 'fill_value' has " \
-                "tensor, Try converting with opset_version >=9 "
-
-            value = np.ones(shape) * value
-            value = value.astype(dtypes.DTYPE_PADDLE_NUMPY_MAP[dtype])
-            value = value.flatten().tolist()
-
-            graph.make_node(
-                'Constant',
-                inputs=[],
-                outputs=node.output('Out'),
-                attrs={
-                    'dims': shape,
-                    'dtype': dtypes.DTYPE_PADDLE_ONNX_MAP[dtype],
-                    'value': value
-                })
-
-
-@op_mapper(['lookup_table_v2', 'lookup_table'])
-class Embedding():
-    support_opset_version_range = (1, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        ids = node.input('Ids', 0)
-        if node.type == 'lookup_table' and node.input_shape('Ids', 0)[-1] == 1:
-            ids = mapper_helper.squeeze_helper(graph,
-                                               node.input('Ids', 0), [-1])
-        padding_idx = node.attr('padding_idx')
-        input_shape = node.input_shape('W', 0)
-        if padding_idx != -1:
-            key = node.input('W', 0)
-            if -1 in input_shape:
-                assert False, "opset version < 11 do not support padding_idx !=-1 and weight is tensor with dynamic shape, please set opset version > 11 or use input_spec to set input shape"
-            else:
-                data = np.ones(shape=input_shape, dtype=np.float32)
-                data[padding_idx] = 0.0
-                dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('W', 0)]
-                constant = graph.make_node(
-                    'Constant',
-                    dtype=dtype,
-                    dims=input_shape,
-                    value=data.flatten().tolist())
-                weight_node = graph.make_node(
-                    'Mul', inputs=[node.input('W', 0), constant])
-                graph.make_node(
-                    'Gather',
-                    inputs=[weight_node, ids],
-                    outputs=node.output('Out'))
-        else:
-            graph.make_node(
-                'Gather',
-                inputs=[node.input('W', 0), ids],
-                outputs=node.output('Out'))
-
-    @classmethod
-    def opset_11(cls, graph, node, **kw):
-        ids = node.input('Ids', 0)
-        if node.type == 'lookup_table' and node.input_shape('Ids', 0)[-1] == 1:
-            ids = mapper_helper.squeeze_helper(graph,
-                                               node.input('Ids', 0), [-1])
-
-        padding_idx = node.attr('padding_idx')
-        input_shape = node.input_shape('W', 0)
-        if padding_idx != -1:
-            if -1 in input_shape:
-                replace_shape = list(copy.copy(input_shape))
-                del (replace_shape[0])
-                replace_data = graph.make_node(
-                    'Constant',
-                    dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('W',
-                                                                        0)],
-                    dims=replace_shape,
-                    value=[0.0] * np.prod(replace_shape))
-                index = graph.make_node(
-                    'Constant', dtype=dtypes.ONNX.INT64, value=[padding_idx])
-                Scatter_node = graph.make_node(
-                    'ScatterND',
-                    inputs=[node.input('W', 0), index, replace_data])
-                graph.make_node(
-                    'Gather',
-                    inputs=[Scatter_node, ids],
-                    outputs=node.output('Out'))
-            else:
-                data = np.ones(shape=input_shape, dtype=np.float32)
-                data[padding_idx] = 0.0
-                dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('W', 0)]
-                constant = graph.make_node(
-                    'Constant',
-                    dtype=dtype,
-                    dims=input_shape,
-                    value=data.flatten().tolist())
-                weight_node = graph.make_node(
-                    'Mul', inputs=[node.input('W', 0), constant])
-                graph.make_node(
-                    'Gather',
-                    inputs=[weight_node, ids],
-                    outputs=node.output('Out'))
-        else:
-            graph.make_node(
-                'Gather',
-                inputs=[node.input('W', 0), ids],
-                outputs=node.output('Out'))
-
-
-@op_mapper('fill_constant_batch_size_like')
-class FillConstantBatchSizeLike():
-    support_opset_version_range = (9, 12)
-
-    @classmethod
-    def opset_10(cls, graph, node, **kw):
-        out_shape = node.attr('shape')
-        input_dim_idx = node.attr('input_dim_idx')
-        output_dim_idx = node.attr('output_dim_idx')
-
-        del out_shape[output_dim_idx]
-        out_shape.insert(0, 1)
-
-        dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('dtype')]
-        if node.attr("str_value") is not None and node.attr("str_value") != "":
-            value = eval(node.attr("str_value"))
-        else:
-            value = node.attr('value')
-        input_shape = node.input_shape('Input', 0)
-        constant = graph.make_node(
-            'Constant',
-            dtype=dtype,
-            dims=out_shape,
-            value=[value] * np.prod(out_shape))
-
-        shape = graph.make_node('Shape', inputs=node.input('Input'))
-        start = graph.make_node(
-            'Constant', dtype=dtypes.ONNX.INT64, value=[input_dim_idx])
-        end = graph.make_node(
-            'Constant', dtype=dtypes.ONNX.INT64, value=[input_dim_idx + 1])
-        batch = graph.make_node('Slice', inputs=[shape, start, end])
-        repeat = batch
-        if len(out_shape) > 1:
-            repeat = graph.make_node(
-                'Constant',
-                dtype=dtypes.ONNX.INT64,
-                value=[1] * (len(out_shape) - 1))
-            repeat = graph.make_node('Concat', inputs=[batch, repeat], axis=-1)
-        if output_dim_idx == 0:
-            graph.make_node(
-                'Tile', inputs=[constant, repeat], outputs=node.output('Out'))
-        else:
-            out = graph.make_node('Tile', inputs=[constant, repeat])
-            perm = list(range(len(out_shape)))
-            del perm[0]
-            perm.insert(output_dim_idx, 0)
-            graph.make_node(
-                'Transpose',
-                inputs=[out],
-                perm=perm,
-                outputs=node.output('Out'))
-
-
-@op_mapper('fill_any_like')
-class FullLike():
-    '''
-    fill_any_like is kernel for paddle op::full_like & ones_like
-    '''
-    support_opset_version_range = (9, 15)
-
-    @classmethod
-    def opset_9(cls, graph, node, **kw):
-        shape_node = graph.make_node('Shape', inputs=node.input('X'))
-        value = node.attr('value')
-        dtype = node.attr('dtype')
-        input_dtype = node.input_dtype('X', 0)
-        if dtype is None:
-            dtype = input_dtype
-        np_dtype = dtypes.DTYPE_PADDLE_STR_MAP[dtype]
-        onnx_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype]
-        graph.make_node(
-            'ConstantOfShape',
-            inputs=[shape_node],
-            outputs=node.output('Out'),
-            dims=[1],
-            dtype=onnx_dtype,
-            value=np.array(value).astype(np_dtype).tolist())
-
-
-@op_mapper('fill_zeros_like')
-class FullZeroLike():
-    '''
-    fill_zeros_like is kernel for paddle op::zeros_like
-    '''
-    support_opset_version_range = (9, 15)
-
-    @classmethod
-    def opset_9(cls, graph, node, **kw):
-        shape_node = graph.make_node('Shape', inputs=node.input('X'))
-        value = 0
-        dtype = node.attr('dtype')
-        input_dtype = node.input_dtype('X', 0)
-        if dtype is None:
-            dtype = input_dtype
-        np_dtype = dtypes.DTYPE_PADDLE_STR_MAP[dtype]
-        onnx_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype]
-        graph.make_node(
-            'ConstantOfShape',
-            inputs=[shape_node],
-            outputs=node.output('Out'),
-            dims=[1],
-            dtype=onnx_dtype,
-            value=np.array(value).astype(np_dtype).tolist())
-
-
-@op_mapper('gather_nd')
-class Gather_nd():
-    support_opset_version_range = (11, 15)
-
-    @classmethod
-    def opset_11(cls, graph, node, **kw):
-        data = node.input('X', 0)
-        index = node.input('Index', 0)
-        index_dtype = node.input_dtype('Index', 0)
-        index_node = None
-        if index_dtype != paddle.int64:
-            index_node = graph.make_node(
-                'Cast', inputs=[node.input('Index', 0)], to=dtypes.ONNX.INT64)
-        else:
-            index_node = index
-        graph.make_node(
-            'GatherND', inputs=[data, index_node], outputs=node.output('Out'))
-
-
-@op_mapper('gather')
-class Gather():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        axis = node.attr('axis')
-        if node.input('Axis', 0) != None:
-            axis_node = node.input('Axis', 0)
-            try:
-                axis = mapper_helper.get_value_from_parameters(graph,
-                                                               axis_node)[0]
-            except Exception as e:
-                raise Exception(
-                    "Currently does not support the axis parameter as input tensor"
-                    + str(e))
-        if axis is None:
-            axis = 0
-        if len(node.input_shape('Index', 0)) == 1:
-            # gather
-            graph.make_node(
-                'Gather',
-                inputs=[node.input('X', 0), node.input('Index', 0)],
-                outputs=node.output('Out'),
-                attrs={'axis': axis})
-        else:
-            raise Exception(
-                "please try to convert OP:gather(indices's rank >1) with opset_version >= 11."
-            )
-
-    @classmethod
-    def opset_11(cls, graph, node, **kw):
-        axis = node.attr('axis')
-        if node.input('Axis', 0) != None:
-            axis_node = node.input('Axis', 0)
-            try:
-                axis = mapper_helper.get_value_from_parameters(graph,
-                                                               axis_node)[0]
-            except Exception as e:
-                raise Exception(
-                    "Currently does not support the axis parameter as input tensor"
-                    + str(e))
-        if axis is None:
-            axis = 0
-        if len(node.input_shape('Index', 0)) == 1:
-            # gather
-            graph.make_node(
-                'Gather',
-                inputs=[node.input('X', 0), node.input('Index', 0)],
-                outputs=node.output('Out'),
-                attrs={'axis': axis})
-        else:
-            # gather_nd
-            index_dtype = node.input_dtype('Index', 0)
-            if index_dtype != paddle.int64:
-                index_node = graph.make_node(
-                    'Cast',
-                    inputs=[node.input('Index', 0)],
-                    to=dtypes.ONNX.INT64)
-                graph.make_node(
-                    'GatherND',
-                    inputs=[node.input('X', 0), index_node],
-                    outputs=node.output('Out'))
-            else:
-                graph.make_node(
-                    'GatherND',
-                    inputs=[node.input('X', 0), node.input('Index', 0)],
-                    outputs=node.output('Out'))
-
-
-@op_mapper('squeeze2')
-class Squeeze():
-    support_opset_version_range = (1, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        shape = node.input_shape('X', 0)
-        ret = [i for i, val in enumerate(shape) if val > 1]
-        if len(ret) == len(shape):
-            graph.make_node(
-                'Identity', inputs=node.input('X'), outputs=node.output('Out'))
-        else:
-            axes = cls.compute_axes(graph, node)
-            if len(axes) > 0:
-                axes.sort()
-                mapper_helper.squeeze_helper(graph,
-                                             node.input('X', 0), axes,
-                                             node.output('Out'))
-            else:
-                graph.make_node(
-                    'Squeeze',
-                    inputs=[node.input('X', 0)],
-                    outputs=node.output('Out'))
-
-    @classmethod
-    def compute_axes(cls, graph, node):
-        shape = node.input_shape('X', 0)
-        axes = node.attr('axes')
-        if len(axes) > 0:
-            axes = [
-                axis + len(shape) if axis < 0 else axis
-                for i, axis in enumerate(axes)
-            ]
-        return axes
-
-
-@op_mapper('assign_value')
-class Assign():
-    support_opset_version_range = (1, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        if len(node.input_names) > 0:
-            graph.make_node(
-                'Identity', inputs=node.input('X'), outputs=node.output('Out'))
-        else:
-            parameters = {}
-            value = np.array(node.attr('fp32_values'))
-            if value is None or value.size < 1:
-                value = np.array(node.attr('int32_values'))
-            if value is None or value.size < 1:
-                value = np.array(node.attr('int64_values'))
-            parameter = {
-                'data': value,
-                'dtype': node.output_dtype("Out", 0),
-                'shape': node.attr('shape')
-            }
-            parameters[node.output('Out', 0)] = parameter
-            graph.build_parameters(parameters)
-
-
-@op_mapper('transpose2')
-class Transpose():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        graph.make_node(
-            'Transpose',
-            inputs=node.input('X'),
-            outputs=node.output('Out'),
-            perm=node.attr('axis'))
-
-
-@op_mapper('flatten2')
-class Flatten():
-    support_opset_version_range = (1, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        input_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)]
-        if input_dtype in [dtypes.ONNX.INT32, dtypes.ONNX.INT64
-                           ] and graph.opset_version < 9:
-            raise Exception(
-                "int32 or int64 not supported in onnx <9, please try with higher onnx opset_version>=9."
-            )
-
-        graph.make_node(
-            'Flatten',
-            inputs=node.input('X'),
-            outputs=node.output('Out'),
-            axis=node.attr('axis'))
-
-
-@op_mapper('flatten_contiguous_range')
-class FlattenContiguousRange():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        dims = len(node.input_shape('X', 0))
-        start_axis = node.attr('start_axis')
-        end_axis = node.attr('stop_axis')
-        shape_node = graph.make_node('Shape', inputs=node.input('X'))
-        if start_axis < 0:
-            start_axis += dims
-        if end_axis < 0:
-            end_axis += dims
-        if start_axis == 0 and end_axis == dims - 1:
-            final_shape = graph.make_node(
-                'Constant', value=[-1], dtype=dtypes.ONNX.INT64)
-        elif start_axis == 0:
-            slice_end = mapper_helper.slice_helper(
-                graph, shape_node, axes=[0], starts=[end_axis + 1],
-                ends=[dims])
-            slices = [
-                graph.make_node(
-                    'Constant', value=[-1], dtype=dtypes.ONNX.INT64), slice_end
-            ]
-            final_shape = graph.make_node('Concat', inputs=slices, axis=0)
-        elif end_axis == dims - 1:
-            slice_start = mapper_helper.slice_helper(
-                graph, shape_node, axes=[0], starts=[0], ends=[start_axis])
-            slices = [
-                slice_start, graph.make_node(
-                    'Constant', value=[-1], dtype=dtypes.ONNX.INT64)
-            ]
-            final_shape = graph.make_node('Concat', inputs=slices, axis=0)
-        else:
-            slice_start = mapper_helper.slice_helper(
-                graph, shape_node, axes=[0], starts=[0], ends=[start_axis])
-            slice_end = mapper_helper.slice_helper(
-                graph, shape_node, axes=[0], starts=[end_axis + 1],
-                ends=[dims])
-            slices = [
-                slice_start, graph.make_node(
-                    'Constant', value=[-1], dtype=dtypes.ONNX.INT64), slice_end
-            ]
-            final_shape = graph.make_node('Concat', inputs=slices, axis=0)
-        graph.make_node(
-            'Reshape',
-            inputs=[node.input('X')[0], final_shape],
-            outputs=node.output('Out'))
-
-
-@op_mapper('reshape2')
-class Reshape():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        shape_name = 'ShapeTensor'
-        if shape_name not in node.inputs or len(node.input(shape_name)) == 0:
-            shape_name = 'Shape'
-        if shape_name not in node.inputs or len(node.input(shape_name)) == 0:
-            if node.attr('shape') is None or len(node.attr('shape')) == 0:
-                raise Exception("shape tensor and shape attrubite all unkown.")
-        if len(node.input(shape_name)) > 1:
-            dims = []
-            for i in range(len(node.input(shape_name))):
-                dim = node.input(shape_name)[i]
-                dim = graph.make_node(
-                    'Cast', inputs=[dim], to=dtypes.ONNX.INT64)
-                dims.append(dim)
-            shape = graph.make_node('Concat', inputs=dims, axis=-1)
-            graph.make_node(
-                'Reshape',
-                inputs=[node.input('X')[0], shape],
-                outputs=node.output('Out'))
-        elif len(node.input(shape_name)) == 1:
-            cast_shape_node = graph.make_node(
-                'Cast', inputs=node.input(shape_name), to=dtypes.ONNX.INT64)
-            graph.make_node(
-                'Reshape',
-                inputs=[node.input('X')[0], cast_shape_node],
-                outputs=node.output('Out'))
-        elif node.attr('shape') is not None and len(node.attr('shape')) > 0:
-            shape_node = graph.make_node(
-                'Constant',
-                attrs={
-                    'dtype': dtypes.ONNX.INT64,
-                    'value': node.attr('shape')
-                })
-            reshape_node = graph.make_node(
-                'Reshape',
-                inputs=[node.input('X')[0], shape_node],
-                outputs=node.output('Out'))
-
-
-@op_mapper('unsqueeze2')
-class Unsqueeze():
-    support_opset_version_range = (1, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        axes = cls.get_axes(graph, node)
-        mapper_helper.unsqueeze_helper(graph,
-                                       node.input('X'), axes,
-                                       node.output('Out'))
-
-    @classmethod
-    def opset_13(cls, graph, node, **kw):
-        axes_node = cls.get_axes(graph, node, return_node=True)
-        graph.make_node(
-            'Unsqueeze',
-            inputs=node.input('X') + [axes_node],
-            outputs=node.output('Out'))
-
-    @classmethod
-    def get_axes(cls, graph, node, return_node=False):
-        axes_node = None
-        ndim = node.block.vars[node.input('X')[0]].ndim
-        if len(node.attr('axes')) > 0:
-            axes = node.attr('axes')
-        else:
-            axes_node = node.input('AxesTensor')[0]
-            if axes_node is not None and graph.opset_version > 12 and return_node:
-                return axes_node
-            try:
-                axes = mapper_helper.get_value_from_parameters(graph, axes_node)
-            except Exception as e:
-                raise Exception(
-                    "Currently does not support the axes parameter as input tensor in onnx(opset<13), "
-                    "Try converting with opset_version >=13 " + str(e))
-        # axes is list of non-negative integers
-        axes = [
-            axis + ndim + i + 1 if axis < 0 else axis
-            for i, axis in enumerate(axes)
-        ]
-
-        axes_copy = axes.copy()
-        assert sorted(
-            axes) == axes_copy, "axes must be arranged in the following order"
-        assert len(set(axes)) == len(axes), "axes have duplicate axis"
-
-        if return_node:
-            if axes_node is None:
-                axes_node = graph.make_node(
-                    'Constant',
-                    attrs={'dtype': dtypes.ONNX.INT64,
-                           'value': axes})
-            return axes_node
-        return axes
-
-
-@op_mapper('reciprocal')
-class Reciprocal():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        graph.make_node(
-            'Reciprocal', inputs=node.input('X'), outputs=node.output('Out'))
-
-
-@op_mapper('cast')
-class Cast():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        graph.make_node(
-            'Cast',
-            inputs=node.input('X'),
-            outputs=node.output('Out'),
-            to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('out_dtype')])
-
-
-@op_mapper('linspace')
-class Linspace():
-    support_opset_version_range = (9, 15)
-
-    @classmethod
-    def opset_9(cls, graph, node, **kw):
-        start = node.input('Start', 0)
-        stop = node.input('Stop', 0)
-        num = node.input('Num', 0)
-        dtype = node.attr('dtype')
-
-        start = graph.make_node('Cast', inputs=[start], to=dtypes.ONNX.FLOAT)
-        stop = graph.make_node('Cast', inputs=[stop], to=dtypes.ONNX.FLOAT)
-
-        sub_a_node = graph.make_node('Sub', inputs=[stop, start])
-
-        one_node = graph.make_node(
-            'Constant',
-            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('Num', 0)],
-            value=[1])
-
-        sub_b_node = graph.make_node('Sub', inputs=[num, one_node])
-
-        sub_b_float_node = graph.make_node(
-            'Cast', inputs=[sub_b_node], to=dtypes.ONNX.FLOAT)
-
-        step = graph.make_node('Div', inputs=[sub_a_node, sub_b_float_node])
-
-        range_tensor = graph.make_node(
-            'Cast', inputs=[num], to=dtypes.ONNX.INT64)
-
-        one_like_node = graph.make_node(
-            'ConstantOfShape',
-            inputs=[range_tensor],
-            dtype=dtypes.ONNX.FLOAT,
-            value=[1])
-
-        none_zero_node = graph.make_node('NonZero', inputs=[one_like_node])
-
-        trans_none_zero_node = graph.make_node(
-            'Transpose', inputs=[none_zero_node], perm=[1, 0])
-
-        trans_squeeze = mapper_helper.squeeze_helper(graph,
-                                                     trans_none_zero_node, [1])
-
-        trans_squeeze = graph.make_node(
-            'Cast', inputs=[trans_squeeze], to=dtypes.ONNX.FLOAT)
-
-        mul_node = graph.make_node('Mul', inputs=[trans_squeeze, step])
-
-        add_node = graph.make_node('Add', inputs=[mul_node, start])
-        graph.make_node(
-            'Cast',
-            inputs=[add_node],
-            outputs=node.output('Out'),
-            to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('Start', 0)])
-
-
-@op_mapper('clip')
-class Clip():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        min_value = node.attr('min')
-        max_value = node.attr('max')
-        if node.input('Max', 0) is None or len(node.input('Max')) == 0:
-            max_ = max_value
-        else:
-            max_ = node.input('Max', 0)
-        if node.input('Min', 0) is None or len(node.input('Min')) == 0:
-            min_ = min_value
-        else:
-            min_ = node.input('Min', 0)
-        mapper_helper.clip_helper(graph, node,
-                                  node.input('X', 0), max_, min_,
-                                  node.output('Out', 0))
-
-
-@op_mapper(['pad2d', 'pad3d'])
-class Pad():
-    support_opset_version_range = (1, 12)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        if node.attr('mode') == 'replicate':
-            mode = 'edge'
-        elif node.attr('mode') == 'circular':
-            raise Exception("The padding mode = circular is not supported, " \
-                            "Please try the other three ways")
-        else:
-            mode = node.attr('mode')
-        pads = cls.convert_padding(node, **kw)
-        if pads is None:
-            key = node.input('Paddings', 0)
-            padding = None
-            if key in graph.parameters.keys():
-                paddings = graph.parameters[key].attribute[0].t.int32_data
-                if node.attr('data_format') == 'NCHW':
-                    pads = [
-                        0, 0, paddings[0], paddings[2], 0, 0, paddings[1],
-                        paddings[3]
-                    ]
-                elif node.attr('data_format') == 'NHWC':
-                    pads = [
-                        0, paddings[0], paddings[2], 0, 0, paddings[1],
-                        paddings[3], 0
-                    ]
-                elif node.attr('data_format') == 'NCDHW':
-                    pads = [
-                        0, 0, paddings[4], paddings[2], paddings[0], 0, 0,
-                        paddings[5], paddings[3], paddings[1]
-                    ]
-                elif node.attr('data_format') == 'NDHWC':
-                    pads = [
-                        0, paddings[4], paddings[2], paddings[0], 0, 0,
-                        paddings[5], paddings[3], paddings[1], 0
-                    ]
-            else:
-                raise Exception("In Pad op, padding can not be tensor" \
-                                "Please set opset version >= 11")
-
-        value = None
-        if node.attr('pad_value') is not None:
-            value = node.attr('pad_value')
-        elif node.attr('value') is not None:
-            value = node.attr('value')
-        graph.make_node(
-            'Pad',
-            inputs=node.input('X'),
-            outputs=node.output('Out'),
-            mode=mode,
-            value=value,
-            pads=pads)
-
-    @classmethod
-    def opset_11(cls, graph, node, **kw):
-        pads = cls.convert_padding(node, **kw)
-        if node.attr('mode') == 'replicate':
-            mode = 'edge'
-        elif node.attr('mode') == 'circular':
-            raise Exception("The padding mode = circular is not supported, " \
-                            "Please try the other three ways")
-        else:
-            mode = node.attr('mode')
-        pads_node = None
-        if isinstance(pads, list):
-            pads_node = graph.make_node(
-                'Constant', attrs={'dtype': dtypes.ONNX.INT64,
-                                   'value': pads})
-        else:
-            key = node.input('Paddings', 0)
-            padding = None
-            if key in graph.parameters.keys():
-                paddings = graph.parameters[key].attribute[0].t.int32_data
-                onnx_paddings = None
-                if node.attr('data_format') == 'NCHW':
-                    onnx_paddings = [
-                        0, 0, paddings[0], paddings[2], 0, 0, paddings[1],
-                        paddings[3]
-                    ]
-                elif node.attr('data_format') == 'NHWC':
-                    onnx_paddings = [
-                        0, paddings[0], paddings[2], 0, 0, paddings[1],
-                        paddings[3], 0
-                    ]
-                elif node.attr('data_format') == 'NCDHW':
-                    onnx_paddings = [
-                        0, 0, paddings[4], paddings[2], paddings[0], 0, 0,
-                        paddings[5], paddings[3], paddings[1]
-                    ]
-                elif node.attr('data_format') == 'NDHWC':
-                    onnx_paddings = [
-                        0, paddings[4], paddings[2], paddings[0], 0, 0,
-                        paddings[5], paddings[3], paddings[1], 0
-                    ]
-
-                pads_node = graph.make_node(
-                    'Constant',
-                    attrs={'dtype': dtypes.ONNX.INT64,
-                           'value': onnx_paddings})
-            else:
-                padding_node = node.input('Paddings', 0)
-                casted_padding_node = graph.make_node(
-                    'Cast', inputs=[padding_node], to=dtypes.ONNX.FLOAT)
-                zero_node = None
-                if node.attr('data_format') == 'NCHW' or node.attr(
-                        'data_format') == 'NHWC':
-                    zero_node = graph.make_node(
-                        'Constant', dtype=dtypes.ONNX.FLOAT, value=[0] * 8)
-                else:
-                    zero_node = graph.make_node(
-                        'Constant', dtype=dtypes.ONNX.FLOAT, value=[0] * 10)
-                index = None
-                if node.attr('data_format') == 'NCHW':
-                    index = graph.make_node(
-                        'Constant', dtype=dtypes.ONNX.INT32,
-                        value=[2, 6, 3, 7])
-                elif node.attr('data_format') == 'NHWC':
-                    index = graph.make_node(
-                        'Constant', dtype=dtypes.ONNX.INT32,
-                        value=[1, 5, 2, 6])
-                elif node.attr('data_format') == 'NCDHW':
-                    index = graph.make_node(
-                        'Constant',
-                        dtype=dtypes.ONNX.INT32,
-                        value=[4, 9, 3, 8, 2, 7])
-                elif node.attr('data_format') == 'NDHWC':
-                    index = graph.make_node(
-                        'Constant',
-                        dtype=dtypes.ONNX.INT32,
-                        value=[3, 8, 2, 7, 1, 6])
-
-                float_paddle_node = graph.make_node(
-                    'ScatterElements',
-                    inputs=[zero_node, index, casted_padding_node])
-                paddle_node = graph.make_node(
-                    'Cast', inputs=[float_paddle_node], to=dtypes.ONNX.INT64)
-                pads_node = paddle_node
-
-        value = None
-        if node.attr('pad_value') is not None:
-            value = node.attr('pad_value')
-        elif node.attr('value') is not None:
-            value = node.attr('value')
-        value_node = graph.make_node(
-            'Constant',
-            attrs={
-                'dtype': dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
-                'value': value
-            })
-
-        graph.make_node(
-            'Pad',
-            inputs=node.input('X') + [pads_node, value_node],
-            outputs=node.output('Out'),
-            mode=mode)
-
-    @classmethod
-    def convert_padding(cls, node, **kw):
-        x_shape = node.input_shape('X', 0)
-        paddings = node.attr('paddings')
-        if paddings == []:
-            return None
-        onnx_paddings = None
-        if node.attr('data_format') == 'NCHW':
-            onnx_paddings = [
-                0, 0, paddings[0], paddings[2], 0, 0, paddings[1], paddings[3]
-            ]
-        elif node.attr('data_format') == 'NHWC':
-            onnx_paddings = [
-                0, paddings[0], paddings[2], 0, 0, paddings[1], paddings[3], 0
-            ]
-        elif node.attr('data_format') == 'NCDHW':
-            onnx_paddings = [
-                0, 0, paddings[4], paddings[2], paddings[0], 0, 0, paddings[5],
-                paddings[3], paddings[1]
-            ]
-        elif node.attr('data_format') == 'NDHWC':
-            onnx_paddings = [
-                0, paddings[4], paddings[2], paddings[0], 0, 0, paddings[5],
-                paddings[3], paddings[1], 0
-            ]
-        return onnx_paddings
-
-
-@op_mapper('gaussian_random')
-class GaussianRandom():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        shape_input_list = node.input('ShapeTensorList')
-        shape_input = None
-        if len(shape_input_list) == 0:
-            shape_input = node.input('ShapeTensor')
-        else:
-            shape_input = graph.make_node(
-                "Concat", inputs=node.input('ShapeTensorList'), axis=0)
-        if shape_input is None or len(shape_input) == 0:
-            graph.make_node(
-                'RandomNormal',
-                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('dtype')],
-                outputs=node.output('Out'),
-                shape=node.attr('shape'),
-                seed=float(node.attr('seed')),
-                mean=node.attr('mean'),
-                scale=node.attr('std'))
-        else:
-            cast_input_shape = graph.make_node(
-                'Cast', inputs=shape_input, to=dtypes.ONNX.INT64)
-            zero_like_node = graph.make_node(
-                'ConstantOfShape',
-                inputs=cast_input_shape,
-                dims=[1],
-                dtype=dtypes.ONNX.FLOAT,
-                value=[0])
-            graph.make_node(
-                'RandomNormalLike',
-                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('dtype')],
-                outputs=node.output('Out'),
-                inputs=zero_like_node,
-                seed=float(node.attr('seed')),
-                mean=node.attr('mean'),
-                scale=node.attr('std'))
-
-
-@op_mapper('uniform_random_batch_size_like')
-class UniformRandom():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        graph.make_node(
-            'RandomUniformLike',
-            inputs=node.input('Input'),
-            outputs=node.output('Out'),
-            high=node.attr('max'),
-            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('dtype')],
-            low=node.attr('min'),
-            seed=float(node.attr('seed')), )
-
-
-@op_mapper('uniform_random')
-class UniformRandom():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        shape_input_list = node.input('ShapeTensorList')
-        shape_input = None
-        if len(shape_input_list) == 0:
-            shape_input = node.input('ShapeTensor')
-        else:
-            shape_input = graph.make_node(
-                "Concat", inputs=node.input('ShapeTensorList'), axis=0)
-        if shape_input is None or len(shape_input) == 0:
-            graph.make_node(
-                'RandomUniform',
-                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('dtype')],
-                outputs=node.output('Out'),
-                shape=node.attr('shape'),
-                seed=float(node.attr('seed')),
-                low=node.attr('min'),
-                high=node.attr('max'))
-        else:
-            cast_input_shape = graph.make_node(
-                'Cast', inputs=shape_input, to=dtypes.ONNX.INT64)
-            zero_like_node = graph.make_node(
-                'ConstantOfShape',
-                inputs=cast_input_shape,
-                dtype=dtypes.ONNX.FLOAT,
-                value=[0])
-            graph.make_node(
-                'RandomUniformLike',
-                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('dtype')],
-                outputs=node.output('Out'),
-                inputs=zero_like_node,
-                seed=float(node.attr('seed')),
-                low=node.attr('min'),
-                high=node.attr('max'))
-
-
-# 'bilinear_interp', 'nearest_interp', scale only support 2, 4, 6, 8, 10
-@op_mapper(
-    [
-        'bilinear_interp', 'nearest_interp', 'bilinear_interp_v2',
-        'nearest_interp_v2', 'bicubic_interp_v2', 'linear_interp_v2',
-        'trilinear_interp_v2', 'trilinear_interp', 'linear_interp'
-    ],
-    mapper_dict={
-        'bilinear_interp': 'linear',
-        'nearest_interp': 'nearest',
-        'bilinear_interp_v2': 'linear',
-        'nearest_interp_v2': 'nearest',
-        'bicubic_interp_v2': 'cubic',
-        'linear_interp_v2': 'linear',
-        'trilinear_interp_v2': 'linear',
-        'trilinear_interp': 'linear',
-        'linear_interp': 'linear',
-    },
-    opset_op_dict={
-        9: 'Upsample',
-        10: 'Resize',
-    })
-class Resize():
-    support_opset_version_range = (9, 15)
-
-    @classmethod
-    def opset_9(cls, graph, node, **kw):
-        inputs = [node.input('X')[0]]
-        resize_type = kw['mapper_dict'][node.type]
-        cls.waringInfo(graph, node, resize_type)
-        if len(node.input('OutSize')) > 0 or len(node.input('SizeTensor')) > 0:
-            output_node = cls.compute_outsize_node(
-                graph, node, return_scale=True)
-        elif 'Scale' in node.inputs and len(node.input('Scale')) > 0:
-            output_node = cls.compute_scale_node(graph, node)
-        else:
-            output_node = cls.compute_attrs_node(graph, node, return_scale=True)
-
-        inputs = inputs + output_node
-        op = kw['opset_op_dict'][graph.opset_version]
-        graph.make_node(
-            op, inputs=inputs, outputs=node.output('Out'), mode=resize_type)
-
-    @classmethod
-    def opset_11(cls, graph, node, **kw):
-        inputs = [node.input('X')[0]]
-        resize_type = kw['mapper_dict'][node.type]
-        cls.waringInfo(graph, node, resize_type)
-        if node.attr('align_corners'):
-            coordinate_transformation_mode = 'align_corners'
-        elif node.attr('align_mode') == 1 and resize_type is not 'cubic':
-            coordinate_transformation_mode = 'asymmetric'
-        elif resize_type == 'nearest':
-            coordinate_transformation_mode = 'asymmetric'
-        else:
-            coordinate_transformation_mode = 'half_pixel'
-        roi_node = graph.make_node(
-            'Constant',
-            attrs={
-                'dtype': dtypes.ONNX.FLOAT,
-                'value': [1, 1, 1, 1, 1, 1, 1, 1]
-            })
-
-        inputs.append(roi_node)
-        if len(node.input('OutSize')) > 0 or len(node.input('SizeTensor')) > 0:
-            output_node = cls.compute_outsize_node(graph, node)
-        elif 'Scale' in node.inputs and len(node.input('Scale')) > 0:
-            output_node = cls.compute_scale_node(graph, node)
-        else:
-            output_node = cls.compute_attrs_node(graph, node)
-        inputs = inputs + output_node
-        attrs = {
-            'mode': resize_type,
-            'coordinate_transformation_mode': coordinate_transformation_mode
-        }
-        if resize_type == 'nearest' and coordinate_transformation_mode == 'asymmetric':
-            attrs['nearest_mode'] = 'floor'
-        graph.make_node(
-            'Resize', inputs=inputs, outputs=node.output('Out'), attrs=attrs)
-
-    @classmethod
-    def compute_outsize_node(cls, graph, node, return_scale=False):
-        dtype = dtypes.ONNX.INT64
-        if return_scale:
-            dtype = dtypes.ONNX.FLOAT
-        input_shape_node = graph.make_node('Shape', inputs=node.input('X'))
-        if dtype != dtypes.ONNX.INT64:
-            input_shape_node = graph.make_node(
-                'Cast', inputs=[input_shape_node], to=dtype)
-        shape_pre_node = mapper_helper.slice_helper(
-            graph, input_shape_node, axes=[], starts=[0], ends=[2])
-
-        out_size = [node.attr('out_d'), node.attr('out_h'), node.attr('out_w')]
-        out_size = [val for val in out_size if val > 0]
-        use_tensor = False
-        if len(node.input('OutSize')) > 0 or len(node.input('SizeTensor')) > 0:
-            use_tensor = True
-        if len(out_size) > 0 and not use_tensor:
-            out_size_node = graph.make_node(
-                'Constant', attrs={'dtype': dtype,
-                                   'value': out_size})
-        else:
-            out_size_node, _ = mapper_helper.get_node_attr_value(
-                graph, node, None, 'OutSize', 'SizeTensor', dtype=dtype)
-        out_size_node = graph.make_node(
-            'Concat', inputs=[shape_pre_node, out_size_node], axis=0)
-
-        if return_scale:
-            scale_node = graph.make_node(
-                'Div', inputs=[out_size_node, input_shape_node])
-            return [scale_node]
-
-        scale_empty_node = graph.make_node(
-            'Constant', attrs={'dtype': dtypes.ONNX.FLOAT,
-                               'value': []})
-        return [scale_empty_node, out_size_node]
-
-    @classmethod
-    def compute_scale_node(cls, graph, node):
-        cast_scale = graph.make_node(
-            'Cast', inputs=node.input('Scale'), to=dtypes.ONNX.FLOAT)
-        inputs_cocat = []
-        const_node = graph.make_node(
-            'Constant', attrs={'dtype': dtypes.ONNX.FLOAT,
-                               'value': [1, 1]})
-        inputs_cocat.append(const_node)
-        scale = node.attr('scale')
-        if isinstance(scale, (float, int)):
-            cast_scale = [cast_scale] * (len(node.input_shape('X', 0)) - 2)
-            inputs_cocat = inputs_cocat + cast_scale
-        else:
-            inputs_cocat = inputs_cocat + [cast_scale]
-        scale_node = graph.make_node('Concat', inputs=inputs_cocat, axis=0)
-        return [scale_node]
-
-    @classmethod
-    def compute_attrs_node(cls, graph, node, return_scale=False):
-        out_size = [node.attr('out_d'), node.attr('out_h'), node.attr('out_w')]
-        scale = node.attr('scale')
-        if isinstance(scale, (float, int)):
-            scale = [scale] * (len(node.input_shape('X', 0)) - 2)
-
-        out_size = [val for val in out_size if val > 0]
-        if len(out_size) > 0:
-            output_node = cls.compute_outsize_node(
-                graph, node, return_scale=return_scale)
-            return output_node
-
-        assert len(scale) > 0, Exception("scale size should > 0!")
-        scale_node = graph.make_node(
-            'Constant',
-            attrs={'dtype': dtypes.ONNX.FLOAT,
-                   'value': [1, 1] + scale})
-        return [scale_node]
-
-    @classmethod
-    def waringInfo(cls, graph, node, resize_type):
-        assert node.attrs['data_layout'] == 'NCHW', \
-            "The conv data layout should be 'NCHW' , but received data format " \
-            "is %s." % node.attrs['data_format']
-
-        if graph.opset_version < 11:
-            if node.attr('align_corners') or resize_type in ["cubic"]:
-                raise Exception(
-                    "When align_corners is true or resize_type is 'cubic', the case isn't supported in onnx(opset<=10), "
-                    "Try converting with opset_version>= 11 ")
-            if node.attr('align_mode') == 0 and resize_type in [
-                    "bilinear", "linear", "trilinear"
-            ]:
-                raise Exception(
-                    "When align_mode == 0 and resize_type is 'bilinear' or 'linear or 'trilinear', the case isn't "
-                    "supported in onnx(opset<=10), Try converting with opset_version>= 11 "
-                )
-
-
-@op_mapper('pixel_shuffle')
-class PixelShuffle():
-    support_opset_version_range = (11, 15)
-
-    @classmethod
-    def opset_11(cls, graph, node, **kw):
-        upscale_factor = node.attr('upscale_factor')
-
-        node = graph.make_node(
-            'DepthToSpace',
-            inputs=node.input('X'),
-            outputs=node.output('Out'),
-            blocksize=upscale_factor,
-            mode='CRD')
-
-
-@op_mapper('scatter')
-class Scatter():
-    support_opset_version_range = (11, 15)
-
-    @classmethod
-    def opset_11(cls, graph, node, **kw):
-        ids = node.input('Ids', 0)
-        input_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('Ids', 0)]
-        if input_dtype != dtypes.ONNX.INT64:
-            ids = graph.make_node('Cast', inputs=[ids], to=dtypes.ONNX.INT64)
-
-        shape = graph.make_node(
-            'Constant',
-            value=[node.input_shape('Ids', 0)[0], 1],
-            dtype=dtypes.ONNX.INT64)
-        reshape_index = graph.make_node('Reshape', inputs=[ids, shape])
-        if not node.attr('overwrite'):
-            raise Exception("overwrite = False not support yet.")
-        else:
-            graph.make_node(
-                'ScatterND',
-                inputs=[
-                    node.input('X', 0), reshape_index, node.input('Updates', 0)
-                ],
-                outputs=node.output('Out'))
-
-
-@op_mapper('scatter_nd_add')
-class ScatterndAdd():
-    support_opset_version_range = (11, 12)
-
-    @classmethod
-    def opset_11(cls, graph, node, **kw):
-        shape = graph.make_node('Shape', inputs=node.input('X', 0))
-        zero_like_node = graph.make_node(
-            'ConstantOfShape',
-            inputs=[shape],
-            dims=[1],
-            dtype=dtypes.ONNX.FLOAT,
-            value=[0])
-        add_node = graph.make_node(
-            'ScatterND',
-            inputs=[
-                zero_like_node, node.input('Index', 0), node.input('Updates', 0)
-            ], )
-        graph.make_node(
-            'Add',
-            inputs=[node.input('X', 0), add_node],
-            outputs=node.output('Out'))
-
-
-@op_mapper('meshgrid')
-class Meshgrid():
-    support_opset_version_range = (8, 15)
-
-    @classmethod
-    def opset_8(cls, graph, node, **kw):
-        tensors = [t for t in list(node.input('X'))]
-        tensors_shape = [graph.make_node('Shape', inputs=t) for t in tensors]
-        out_shape = graph.make_node('Concat', inputs=tensors_shape, axis=0)
-        out = []
-        for i, t in enumerate(tensors):
-            shape_i = [
-                graph.make_node(
-                    'Constant',
-                    attrs={'dtype': dtypes.ONNX.INT64,
-                           'value': [1]})
-            ] * len(tensors)
-            shape_i[i] = tensors_shape[i]
-            t_reshaped = graph.make_node(
-                'Reshape',
-                inputs=[t, graph.make_node(
-                    'Concat', inputs=shape_i, axis=0)])
-            out.append(
-                graph.make_node(
-                    'Expand',
-                    inputs=[t_reshaped, out_shape],
-                    outputs=node.output('Out')[i]))
-
-
-@op_mapper('flip')
-class Flip():
-    support_opset_version_range = (7, 15)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        inputs = node.input('X')
-        x_dtype = node.input_dtype('X', 0)
-        if x_dtype == paddle.bool or x_dtype == paddle.float64:
-            inputs = [
-                graph.make_node(
-                    "Cast", inputs=inputs, to=dtypes.ONNX.FLOAT)
-            ]
-        axes = node.attr("axis")
-        if not isinstance(axes, list):
-            axes = [axes]
-        input_shape = node.input_shape('X', 0)
-
-        for i, axis in enumerate(axes):
-            if axis < 0:
-                axes[i] += len(input_shape)
-            assert input_shape[
-                axis] > 0, "The dimension in axis of input must be fixed for flip operator, but now the input shape({}) in axis({}) is unknow.".format(
-                    input_shape, axis)
-
-        temp_input = inputs[0]
-        for i, axis in enumerate(axes):
-            if input_shape[axis] == 1:
-                if i != len(axes) - 1:
-                    continue
-                else:
-                    if x_dtype == paddle.bool or x_dtype == paddle.float64:
-                        graph.make_node(
-                            "Cast",
-                            inputs=[temp_input],
-                            outputs=node.output("Out"),
-                            to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])
-                    else:
-                        graph.make_node(
-                            "Identity",
-                            inputs=[temp_input],
-                            outputs=node.output("Out"))
-            else:
-                splits = graph.make_node(
-                    "Split",
-                    inputs=[temp_input],
-                    outputs=input_shape[axis],
-                    axis=axis,
-                    split=[1] * input_shape[axis])
-                reversed_splits = splits[::-1]
-                if i != len(axes) - 1:
-                    temp_input = graph.make_node(
-                        "Concat", inputs=reversed_splits, axis=axis)
-                else:
-                    if x_dtype == paddle.bool or x_dtype == paddle.float64:
-                        out = graph.make_node(
-                            "Concat", inputs=reversed_splits, axis=axis)
-                        graph.make_node(
-                            "Cast",
-                            inputs=[out],
-                            outputs=node.output("Out"),
-                            to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])
-                    else:
-                        graph.make_node(
-                            "Concat",
-                            inputs=reversed_splits,
-                            outputs=node.output("Out"),
-                            axis=axis)
-
-    @classmethod
-    def opset_13(cls, graph, node, **kw):
-        inputs = node.input('X')
-        x_dtype = node.input_dtype('X', 0)
-        if x_dtype == paddle.bool or x_dtype == paddle.float64:
-            inputs = [
-                graph.make_node(
-                    "Cast", inputs=inputs, to=dtypes.ONNX.FLOAT)
-            ]
-        axes = node.attr("axis")
-        if not isinstance(axes, list):
-            axes = [axes]
-        input_shape = node.input_shape('X', 0)
-
-        for i, axis in enumerate(axes):
-            if axis < 0:
-                axes[i] += len(input_shape)
-            assert input_shape[
-                axis] > 0, "The dimension in axis of input must be fixed for flip operator, but now the input shape({}) in axis({}) is unknow.".format(
-                    input_shape, axis)
-
-        temp_input = inputs[0]
-        for i, axis in enumerate(axes):
-            if input_shape[axis] == 1:
-                if i != len(axes) - 1:
-                    continue
-                else:
-                    if x_dtype == paddle.bool or x_dtype == paddle.float64:
-                        graph.make_node(
-                            "Cast",
-                            inputs=[temp_input],
-                            outputs=node.output("Out"),
-                            to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])
-                    else:
-                        graph.make_node(
-                            "Identity",
-                            inputs=[temp_input],
-                            outputs=node.output("Out"))
-            else:
-                split = graph.make_node(
-                    'Constant',
-                    attrs={
-                        'dtype': dtypes.ONNX.INT64,
-                        'value': [1] * input_shape[axis]
-                    })
-                splits = graph.make_node(
-                    "Split",
-                    inputs=[temp_input, split],
-                    outputs=input_shape[axis],
-                    axis=axis)
-                reversed_splits = splits[::-1]
-                if i != len(axes) - 1:
-                    temp_input = graph.make_node(
-                        "Concat", inputs=reversed_splits, axis=axis)
-                else:
-                    if x_dtype == paddle.bool or x_dtype == paddle.float64:
-                        out = graph.make_node(
-                            "Concat", inputs=reversed_splits, axis=axis)
-                        graph.make_node(
-                            "Cast",
-                            inputs=[out],
-                            outputs=node.output("Out"),
-                            to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])
-                    else:
-                        graph.make_node(
-                            "Concat",
-                            inputs=reversed_splits,
-                            outputs=node.output("Out"),
-                            axis=axis)
+#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import numpy as np
+from paddle2onnx.legacy.constant import dtypes
+from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
+from paddle2onnx.legacy.op_mapper import mapper_helper
+import copy
+import six
+import paddle
+
+
+@op_mapper('set_value')
+class SetValue():
+    support_opset_version_range = (11, 15)
+
+    @classmethod
+    def opset_11(cls, graph, node, **kw):
+        axes = node.attr('axes')
+        steps, is_steps_tensor = mapper_helper.get_node_attr_value(
+            graph,
+            node,
+            'steps',
+            'StepsTensor',
+            'StepsTensorList',
+            return_list=True,
+            dtype=dtypes.ONNX.INT64)
+
+        starts, is_starts_tensor = mapper_helper.get_node_attr_value(
+            graph,
+            node,
+            'starts',
+            'StartsTensor',
+            'StartsTensorList',
+            return_list=True,
+            dtype=dtypes.ONNX.INT64)
+
+        ends, is_ends_tensor = mapper_helper.get_node_attr_value(
+            graph,
+            node,
+            'ends',
+            'EndsTensor',
+            'EndsTensorList',
+            return_list=True,
+            dtype=dtypes.ONNX.INT64)
+
+        contain_step_bigger_than_1 = False
+        for i in steps:
+            contain_step_bigger_than_1 = i > 1
+            if not isinstance(i, int) or contain_step_bigger_than_1:
+                contain_step_bigger_than_1 = True
+                break
+        condition = is_steps_tensor or is_starts_tensor or is_ends_tensor or contain_step_bigger_than_1
+        assert not condition, "Currently not supported convert now"
+
+        input_x_shape = node.input_shape('Input', 0)
+        onnx_paddings = [0] * len(input_x_shape) * 2
+        value_shape = list(copy.copy(node.input_shape('Input', 0)))
+        for i in range(len(axes)):
+            axis = axes[i]
+            if starts[i] < 0:
+                starts[i] = starts[i] + input_x_shape[i]
+            if ends[i] < 0:
+                ends[i] = ends[i] + input_x_shape[i]
+            onnx_paddings[axis] = starts[i]
+            value_shape[axis] = value_shape[axis] - onnx_paddings[axis]
+            onnx_paddings[axis + len(input_x_shape)] = input_x_shape[
+                axis] - ends[i]
+            if onnx_paddings[axis + len(input_x_shape)] < 0:
+                onnx_paddings[axis + len(input_x_shape)] = 0
+            value_shape[axis] = value_shape[axis] - onnx_paddings[axis + len(
+                input_x_shape)]
+        dtype_paddle = node.input_dtype('Input', 0)
+        dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype_paddle]
+        value_tensor = None
+        shape = node.attr('shape')
+        if len(shape) > 0:
+            dtypes_list = [
+                'fp32_values', 'fp64_values', 'int32_values', 'int64_values',
+                'bool_values'
+            ]
+            for i in range(len(dtypes_list)):
+                value = node.attr(dtypes_list[i])
+                if value is not None:
+                    break
+            if len(value) == 1:
+                total_nums = 1
+                for i in value_shape:
+                    total_nums *= i
+                value = value * total_nums
+                value_tensor = mapper_helper.constant_helper(
+                    graph, dtype_paddle, value, shape=value_shape)
+            else:
+                value_tensor = mapper_helper.constant_helper(
+                    graph, dtype_paddle, value, shape=shape)
+        else:
+            value_tensor = node.input('ValueTensor', 0)
+        MAX_FLOAT32 = 3.402823466E+38
+        max_node = graph.make_node(
+            'Constant', attrs={'dtype': dtype,
+                               'value': [MAX_FLOAT32]})
+        pads_node = graph.make_node(
+            'Constant',
+            attrs={'dtype': dtypes.ONNX.INT64,
+                   'value': onnx_paddings})
+        value_pad_node = graph.make_node(
+            'Pad', inputs=[value_tensor, pads_node, max_node])
+
+        condition_dtype = graph.make_node(
+            "Equal", inputs=[value_pad_node, max_node])
+        condition_node = graph.make_node(
+            'Cast', inputs=[condition_dtype], to=dtypes.ONNX.BOOL)
+        graph.make_node(
+            "Where",
+            inputs=[condition_node, node.input('Input', 0), value_pad_node],
+            outputs=node.output('Out'))
+
+
+@op_mapper('one_hot_v2')
+class OneHotV2():
+    support_opset_version_range = (9, )
+
+    @classmethod
+    def opset_9(cls, graph, node, **kw):
+        allow_out_of_range = node.attr('allow_out_of_range')
+        assert not allow_out_of_range, "allow_out_of_range can not be true in one_hot_v2."
+        in_dtype_paddle = node.input_dtype('X', 0)
+        in_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[in_dtype_paddle]
+        out_dtype = node.output_dtype('Out', 0)
+        out_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[out_dtype]
+        inputs = node.input('X', 0)
+        if in_dtype_paddle == paddle.int32:
+            inputs = graph.make_node(
+                'Cast', inputs=[inputs], to=dtypes.ONNX.INT64)
+            in_dtype = dtypes.ONNX.INT64
+        value_node = graph.make_node('Constant', dtype=out_dtype, value=[0, 1])
+        depth = node.attr('depth')
+        if node.input('depth_tensor', 0) is not None:
+            depth_node = node.input('depth_tensor', 0)
+        else:
+            depth_node = graph.make_node(
+                'Constant', dtype=in_dtype, value=[depth])
+        reshaped_input_node = graph.make_node(
+            'OneHot',
+            inputs=[inputs, depth_node, value_node],
+            outputs=node.output('Out'))
+
+
+@op_mapper('concat')
+class Concat():
+    support_opset_version_range = (4, 15)
+
+    @classmethod
+    def opset_4(cls, graph, node, **kw):
+        inputs = node.input('X')
+
+        input_dtypes = [node.input_dtype('X', i) for i in range(len(inputs))]
+        inputs = mapper_helper.dtype_alignment(graph, inputs, input_dtypes)
+        node_axis = node.input('AxisTensor')
+        if node_axis is not None and len(node_axis) > 0:
+            axis_node = node.input('AxisTensor')[0]
+            try:
+                axis = mapper_helper.get_value_from_parameters(graph,
+                                                               axis_node)[0]
+            except Exception as e:
+                raise Exception(
+                    "Currently does not support the axis parameter as input tensor"
+                    + str(e))
+        else:
+            axis = node.attr('axis')
+        if axis < 0:
+            axis = axis + len(node.input_shape('X', 0))
+
+        node = graph.make_node(
+            'Concat', inputs=inputs, outputs=node.output('Out'), axis=axis)
+
+
+@op_mapper('assign')
+class Assign():
+    support_opset_version_range = (1, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        inputs = node.input('X')
+        graph.make_node('Identity', inputs=inputs, outputs=node.output('Out'))
+
+
+@op_mapper('lod_reset')
+class LodReset():
+    support_opset_version_range = (1, )
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        graph.make_node(
+            'Identity', inputs=node.input('X'), outputs=node.output('Out'))
+
+
+@op_mapper('eye')
+class Eye():
+    support_opset_version_range = (9, )
+
+    @classmethod
+    def opset_9(cls, graph, node, **kw):
+        num_rows = node.attr('num_rows')
+        num_columns = node.attr('num_columns')
+        dtype = node.output_dtype('Out', 0)
+        value = [0] * num_rows * num_columns
+        value_tensor = mapper_helper.constant_helper(
+            graph, dtype, value, shape=[num_rows, num_columns])
+        graph.make_node(
+            'EyeLike', inputs=[value_tensor], outputs=node.output('Out'))
+
+
+@op_mapper('stack')
+class Stack():
+    support_opset_version_range = (4, 15)
+
+    @classmethod
+    def opset_4(cls, graph, node, **kw):
+        inputs = node.input('X')
+        input_dtypes = [node.input_dtype('X', i) for i in range(len(inputs))]
+        inputs = mapper_helper.dtype_alignment(graph, inputs, input_dtypes)
+        axis = node.attr('axis')
+
+        unsqueezed_inputs = list()
+        for ipt in inputs:
+            unsqueezed_ipt = mapper_helper.unsqueeze_helper(graph, ipt, [axis])
+            unsqueezed_inputs.append(unsqueezed_ipt)
+        graph.make_node(
+            'Concat',
+            inputs=unsqueezed_inputs,
+            outputs=node.output('Y'),
+            axis=axis)
+
+
+@op_mapper('unstack')
+class Unstack():
+    support_opset_version_range = (2, 15)
+
+    @classmethod
+    def opset_2(cls, graph, node, **kw):
+        axis = node.attr('axis')
+        ndim = node.block.vars[node.input('X')[0]].ndim
+        axis = axis + ndim if axis < 0 else axis
+        output_y = mapper_helper.split_helper(
+            graph,
+            node.input('X'),
+            axis=axis,
+            split=[1] * len(node.output('Y')),
+            outputs=len(node.output('Y')))
+
+        if isinstance(output_y, six.string_types):
+            output_y = [output_y]
+
+        for i in range(len(output_y)):
+            mapper_helper.squeeze_helper(graph, output_y[i], [axis],
+                                         node.output('Y', i))
+
+
+@op_mapper('expand_as_v2')
+class ExpandAsV2():
+    support_opset_version_range = (8, 15)
+
+    @classmethod
+    def opset_8(cls, graph, node, **kw):
+        target_shape = node.attr('target_shape')
+        if node.input('target_tensor', 0) is not None:
+            target_shape = graph.make_node(
+                'Shape', inputs=[node.input('target_tensor', 0)])
+        elif target_shape is not None:
+            target_shape = graph.make_node(
+                'Constant',
+                attrs={'dtype': dtypes.ONNX.INT64,
+                       'value': target_shape})
+        else:
+            raise Exception(
+                "Not find attribute: 'target_shape' or tensor 'target_tensor'")
+        node = graph.make_node(
+            'Expand',
+            inputs=[node.input('X', 0), target_shape],
+            outputs=node.output('Out'))
+
+
+@op_mapper('expand_v2')
+class ExpandV2():
+    support_opset_version_range = (8, 15)
+
+    @classmethod
+    def opset_8(cls, graph, node, **kw):
+        expand_shape, _ = mapper_helper.get_node_attr_value(
+            graph,
+            node,
+            'shape',
+            'Shape',
+            'expand_shapes_tensor',
+            dtype=dtypes.ONNX.INT64)
+
+        input_shape = node.input_shape('X', 0)
+        input_shape_node = graph.make_node('Shape', inputs=node.input('X', 0))
+
+        node_shape = node.attr('shape')
+        node_shape_tensor = node.input('Shape')
+        node_shape_tensor_list = node.input('expand_shapes_tensor')
+        if node_shape_tensor is not None and len(node_shape_tensor) > 0:
+            diff = node.input_shape('Shape', 0)[0] - len(input_shape)
+        elif node_shape_tensor_list is not None and \
+                len(node_shape_tensor_list) > 0:
+            diff = len(node_shape_tensor_list) - len(input_shape)
+        elif node_shape is not None and len(node_shape) > 0:
+            diff = len(node_shape) - len(input_shape)
+            expand_shape = graph.make_node(
+                'Constant', dtype=dtypes.ONNX.INT64, value=expand_shape)
+
+        if diff > 0:
+            one_node = graph.make_node(
+                'Constant',
+                attrs={'dtype': dtypes.ONNX.INT64,
+                       'value': [1] * diff})
+            input_shape_node = graph.make_node(
+                'Concat', inputs=[one_node, input_shape_node], axis=0)
+
+        if graph.opset_version < 12:
+            input_shape_node = graph.make_node(
+                'Cast', inputs=[input_shape_node], to=dtypes.ONNX.FLOAT)
+            expand_shape = graph.make_node(
+                'Cast', inputs=[expand_shape], to=dtypes.ONNX.FLOAT)
+            shape = graph.make_node(
+                'Max', inputs=[input_shape_node, expand_shape])
+            shape = graph.make_node(
+                'Cast', inputs=[shape], to=dtypes.ONNX.INT64)
+        else:
+            shape = graph.make_node(
+                'Max', inputs=[input_shape_node, expand_shape])
+        node = graph.make_node(
+            'Expand',
+            inputs=[node.input('X', 0), shape],
+            outputs=node.output('Out'))
+
+
+@op_mapper('shape')
+class Shape():
+    support_opset_version_range = (6, 15)
+
+    @classmethod
+    def opset_6(cls, graph, node, **kw):
+        shape_node = graph.make_node('Shape', inputs=node.input('Input'))
+        graph.make_node(
+            'Cast',
+            inputs=[shape_node],
+            outputs=node.output('Out'),
+            to=dtypes.ONNX.INT32)
+
+
+@op_mapper('size')
+class Numel():
+    supports_opset_version_range = (1, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        size_node = graph.make_node('Size', inputs=node.input('Input'))
+        mapper_helper.unsqueeze_helper(graph, size_node, [0],
+                                       node.output('Out'))
+
+
+@op_mapper('split')
+class Split():
+    support_opset_version_range = (1, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        sections = node.attr('sections')
+        axis = cls.get_axis(graph, node)
+        if isinstance(sections, list) and len(sections) == 1:
+            graph.make_node(
+                'Identity', inputs=node.input('X'), outputs=node.output('Out'))
+        else:
+            if len(sections) > 0:
+                input_shape = node.block.vars[node.input('X')[0]].shape
+                section_index = [
+                    i for i, val in enumerate(sections) if val == -1
+                ]
+                if input_shape[axis] != -1 and len(section_index) == 1:
+                    sections[section_index[0]] = input_shape[axis] - sum(
+                        sections) - 1
+                mapper_helper.split_helper(
+                    graph,
+                    node.input('X'),
+                    axis=axis,
+                    split=sections,
+                    outputs=node.output('Out'))
+            else:
+                graph.make_node(
+                    'Split',
+                    inputs=node.input('X'),
+                    outputs=node.output('Out'),
+                    axis=axis)
+
+    @classmethod
+    def get_axis(cls, graph, node):
+        if len(node.input('AxisTensor')) > 0:
+            axis_node = node.input('AxisTensor')[0]
+            # When axis is tensor, only int32 and int64 are supported
+            if axis_node not in graph.parameters:
+                raise Exception(
+                    "Currently does not support the axis parameter as input tensor!"
+                )
+            else:
+                axis = graph.parameters[axis_node].attribute[0].t.int32_data
+                if axis is None or len(axis) < 1:
+                    axis = graph.parameters[axis_node].attribute[
+                        0].t.int64_data[0]
+        else:
+            axis = node.attr('axis')
+        return axis
+
+
+@op_mapper(['roll'])
+class Roll():
+    support_opset_version_range = (4, 15)
+
+    @classmethod
+    def roll(cls, graph, node, input_x, dims, shifts):
+        for i in range(len(dims)):
+            if graph.opset_version >= 10 and isinstance(shifts,
+                                                        six.string_types):
+                to_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype(
+                    'ShiftsTensor', 0)]
+                const_i = graph.make_node('Constant', dtype=to_dtype, value=i)
+                const_0 = graph.make_node('Constant', dtype=to_dtype, value=0)
+                shift_node = graph.make_node(
+                    'Gather', inputs=[shifts, const_i], axis=0)
+                shift_node = graph.make_node(
+                    "Sub", inputs=[const_0, shift_node])
+                shift_node = mapper_helper.unsqueeze_helper(graph, shift_node,
+                                                            [0])
+            elif graph.opset_version < 10 and isinstance(shifts,
+                                                         six.string_types):
+                raise Exception(
+                    "shifts of roll is Tensor, please try with higher onnx opset_version>=10."
+                )
+            else:
+                shift_node = [-shifts[i]]
+                to_dtype = dtypes.ONNX.INT64
+            shapes = []
+            shape = mapper_helper.slice_helper(
+                graph, input_x, [dims[i]], shift_node, [60000], dtype=to_dtype)
+            shapes.append(shape)
+            shape = mapper_helper.slice_helper(
+                graph, input_x, [dims[i]], [0], shift_node, dtype=to_dtype)
+            shapes.append(shape)
+            input_x = graph.make_node('Concat', inputs=shapes, axis=dims[i])
+        return input_x
+
+    @classmethod
+    def flatten(cls, graph, node):
+        dims = len(node.input_shape('X', 0))
+        start_axis = 0
+        end_axis = dims - 1
+        shape_node = graph.make_node('Shape', inputs=node.input('X'))
+        if end_axis < dims - 1:
+            slice1 = mapper_helper.slice_helper(
+                graph, shape_node, axes=[0], starts=[0], ends=[start_axis])
+            slice3 = mapper_helper.slice_helper(
+                graph, shape_node, axes=[0], starts=[end_axis + 1],
+                ends=[dims])
+            slices = [
+                slice1, graph.make_node(
+                    'Constant', value=[-1], dtype=dtypes.ONNX.INT64), slice3
+            ]
+        else:
+            slice1 = mapper_helper.slice_helper(
+                graph, shape_node, axes=[0], starts=[0], ends=[start_axis])
+            slices = [
+                slice1, graph.make_node(
+                    'Constant', value=[-1], dtype=dtypes.ONNX.INT64)
+            ]
+        final_shape = graph.make_node('Concat', inputs=slices, axis=0)
+        output = graph.make_node(
+            'Reshape', inputs=[node.input('X')[0], final_shape])
+        return output
+
+    @classmethod
+    def opset_4(cls, graph, node, **kw):
+        dims = node.attr('axis')
+        shifts = node.attr('shifts')
+        input_x = node.input('X')[0]
+        input_shape = node.input_shape('X', 0)
+        shifts_node = node.input('ShiftsTensor')
+        if len(dims) > 0:
+            axes = [
+                axis + len(input_shape) if axis < 0 else axis
+                for i, axis in enumerate(dims)
+            ]
+            if shifts_node is not None and len(shifts_node) > 0:
+                shifts = shifts_node[0]
+            else:
+                for i in range(0, len(axes)):
+                    if input_shape[axes[i]] > 0:
+                        assert -input_shape[axes[i]] <= shifts[i] <= input_shape[axes[i]], \
+                            "the value of shifts in axis is less than the value of input_shape in axis."
+
+            input_x = cls.roll(graph, node, input_x, axes, shifts)
+            graph.make_node(
+                'Identity', inputs=[input_x], outputs=node.output('Out'))
+        else:
+            if shifts_node is not None and len(shifts_node) > 0:
+                shifts = shifts_node[0]
+            input_x = cls.flatten(graph, node)
+            input_x = cls.roll(graph, node, input_x, [0], shifts)
+            shape_node = graph.make_node(
+                'Constant',
+                attrs={'dtype': dtypes.ONNX.INT64,
+                       'value': list(input_shape)})
+            graph.make_node(
+                'Reshape',
+                inputs=[input_x, shape_node],
+                outputs=node.output('Out'))
+
+
+@op_mapper(['slice', 'strided_slice'])
+class Slice():
+    support_opset_version_range = (1, 15)
+
+    @classmethod
+    def decrease_axis(cls, node):
+        # tensor[i,:] will decrease rank of origin input, example:
+        # paddle.slice() will not decrease rank of origin input
+        # if input shape is [2, 3], input[0, :] will generate output with shape [3], not [1, 3].
+        # paddle.slice(input, 0, 1, 0) will  generate output with shape [1, 3], not [3].
+
+        decrease_axis = node.attr('decrease_axis')
+        if len(decrease_axis) == 0:
+            return None
+        if node.output_shape('Out', 0) == [0]:
+            return decrease_axis
+        if len(node.input_shape('Input', 0)) > len(node.output_shape('Out', 0)):
+            return decrease_axis
+        return None
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        axes = node.attr('axes')
+        strides, strides_is_tensor = mapper_helper.get_node_attr_value(
+            graph, node, 'strides', 'StridesTensor', 'StridesTensorList', True)
+        strides = [1] * len(axes) if strides is None else strides
+        steps = [i for i, val in enumerate(strides) if val == 1]
+        assert len(steps) == len(axes), \
+            "Slice in onnx(opset<10) not support attribute 'step', Try converting with opset_version >=10"
+
+        starts, start_is_tensor = mapper_helper.get_node_attr_value(
+            graph, node, 'starts', 'StartsTensor', 'StartsTensorList', True)
+        ends, end_is_tensor = mapper_helper.get_node_attr_value(
+            graph, node, 'ends', 'EndsTensor', 'EndsTensorList', True)
+
+        assert not strides_is_tensor and not start_is_tensor and not end_is_tensor, \
+            "Slice in onnx(opset<10) not support attribute 'steps','starts' or 'ends' which have tensor value, " \
+            "Try converting with opset_version >=10 "
+
+        decrease_axis = cls.decrease_axis(node)
+        if decrease_axis is None:
+            graph.make_node(
+                "Slice",
+                inputs=[node.input('Input')[0]],
+                outputs=node.output('Out'),
+                axes=axes,
+                starts=starts,
+                ends=ends)
+        else:
+            sliced = graph.make_node(
+                "Slice",
+                inputs=[node.input('Input')[0]],
+                axes=axes,
+                starts=starts,
+                ends=ends)
+            mapper_helper.squeeze_helper(graph, sliced, decrease_axis,
+                                         node.output('Out'))
+
+    @classmethod
+    def opset_10(cls, graph, node, **kw):
+        axes = node.attr('axes')
+        strides, _ = mapper_helper.get_node_attr_value(
+            graph,
+            node,
+            'strides',
+            'StridesTensor',
+            'StridesTensorList',
+            dtype=dtypes.ONNX.INT64)
+        strides = [1] * len(axes) if strides is None else strides
+
+        starts, _ = mapper_helper.get_node_attr_value(
+            graph,
+            node,
+            'starts',
+            'StartsTensor',
+            'StartsTensorList',
+            dtype=dtypes.ONNX.INT64)
+        ends, _ = mapper_helper.get_node_attr_value(
+            graph,
+            node,
+            'ends',
+            'EndsTensor',
+            'EndsTensorList',
+            dtype=dtypes.ONNX.INT64)
+
+        if isinstance(starts, list):
+            starts_node = graph.make_node(
+                'Constant',
+                attrs={'dtype': dtypes.ONNX.INT64,
+                       'value': starts})
+        else:
+            starts_node = starts
+        if isinstance(ends, list):
+            ends_node = graph.make_node(
+                'Constant', attrs={'dtype': dtypes.ONNX.INT64,
+                                   'value': ends})
+        else:
+            ends_node = ends
+
+        if isinstance(strides, list):
+            strides_node = graph.make_node(
+                'Constant',
+                attrs={'dtype': dtypes.ONNX.INT64,
+                       'value': strides})
+        else:
+            strides_node = strides
+
+        steps_node = strides_node
+        axes_node = graph.make_node(
+            'Constant', attrs={'dtype': dtypes.ONNX.INT64,
+                               'value': axes})
+
+        decrease_axis = cls.decrease_axis(node)
+        if decrease_axis is None:
+            sliced = graph.make_node(
+                "Slice",
+                inputs=[
+                    node.input('Input')[0], starts_node, ends_node, axes_node,
+                    steps_node
+                ],
+                outputs=node.output('Out'))
+        else:
+            sliced = graph.make_node(
+                "Slice",
+                inputs=[
+                    node.input('Input')[0], starts_node, ends_node, axes_node,
+                    steps_node
+                ])
+            mapper_helper.squeeze_helper(graph, sliced, decrease_axis,
+                                         node.output('Out'))
+
+
+@op_mapper(['sequence_expand'])
+class SequenceExpand():
+    support_opset_version_range = ()
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        graph.make_node(
+            'Identity', inputs=node.input('X'), outputs=node.output('Out'))
+
+
+@op_mapper(['expand'])
+class Expand():
+    support_opset_version_range = (6, 15)
+
+    @classmethod
+    def opset_6(cls, graph, node, **kw):
+        expand_times, _ = mapper_helper.get_node_attr_value(
+            graph,
+            node,
+            'expand_times',
+            'ExpandTimes',
+            'expand_times_tensor',
+            dtype=dtypes.ONNX.INT64)
+
+        if isinstance(expand_times, list):
+            expand_times = graph.make_node(
+                'Constant',
+                attrs={'dtype': dtypes.ONNX.INT64,
+                       'value': expand_times})
+
+        graph.make_node(
+            "Tile",
+            inputs=[node.input('X', 0), expand_times],
+            outputs=node.output('Out'))
+
+
+@op_mapper(['tile'])
+class Tile():
+    support_opset_version_range = (6, 15)
+
+    @classmethod
+    def opset_6(cls, graph, node, **kw):
+        repeat_times, _ = mapper_helper.get_node_attr_value(
+            graph,
+            node,
+            'repeat_times',
+            'RepeatTimes',
+            'repeat_times_tensor',
+            dtype=dtypes.ONNX.INT64)
+
+        if isinstance(repeat_times, list):
+            repeat_times = graph.make_node(
+                'Constant',
+                attrs={'dtype': dtypes.ONNX.INT64,
+                       'value': repeat_times})
+
+        graph.make_node(
+            "Tile",
+            inputs=[node.input('X', 0), repeat_times],
+            outputs=node.output('Out'))
+
+
+@op_mapper('range')
+class Range():
+    support_opset_version_range = (11, 15)
+
+    @classmethod
+    def opset_11(cls, graph, node, **kw):
+        start = node.input('Start', 0)
+        end = node.input('End', 0)
+        step = node.input('Step', 0)
+        start_t = mapper_helper.squeeze_helper(graph, start, [0])
+        end_t = mapper_helper.squeeze_helper(graph, end, [0])
+        step_t = mapper_helper.squeeze_helper(graph, step, [0])
+        graph.make_node(
+            "Range",
+            inputs=[start_t, end_t, step_t],
+            outputs=node.output('Out'))
+
+
+@op_mapper('fill_constant')
+class Constant():
+    support_opset_version_range = (1, 15)
+
+    @classmethod
+    def check_int_type(cls, dtype):
+        if dtype in [dtypes.ONNX.INT16, dtypes.ONNX.INT32, dtypes.ONNX.INT64]:
+            return True
+        return False
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        value = node.attr('value')
+        dtype = node.attr('dtype')
+        value_is_scalar_tensor = False
+        if 'ValueTensor' in node.inputs and len(node.input('ValueTensor')) > 0:
+            rank = len(node.input_shape("ValueTensor", 0))
+            if rank == 1 and node.input_shape("ValueTensor", 0)[0] == 1:
+                value_is_scalar_tensor = True
+                value = node.input("ValueTensor")[0]
+            else:
+                raise Exception(
+                    "paddle.full with tensor value parameter is not supported yet."
+                )
+
+        shape, is_shape_tensor = mapper_helper.get_node_attr_value(
+            graph,
+            node,
+            'shape',
+            'ShapeTensor',
+            'ShapeTensorList',
+            dtype=dtypes.ONNX.INT64)
+
+        if graph.opset_version >= 9 and (is_shape_tensor or
+                                         value_is_scalar_tensor):
+            if not is_shape_tensor:
+                shape = graph.make_node(
+                    'Constant', dtype=dtypes.ONNX.INT64, value=shape)
+            input_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype]
+            if not value_is_scalar_tensor and cls.check_int_type(input_dtype):
+                to_dtype = dtypes.ONNX.DOUBLE
+                outputs = None
+            else:
+                to_dtype = input_dtype
+                outputs = node.output('Out')
+
+            if value_is_scalar_tensor:
+                base_value = graph.make_node(
+                    'ConstantOfShape',
+                    inputs=shape,
+                    attrs={'dims': [1],
+                           'dtype': to_dtype,
+                           'value': 0})
+                node2 = graph.make_node(
+                    "Add", inputs=[base_value, value], outputs=outputs)
+            else:
+                node2 = graph.make_node(
+                    'ConstantOfShape',
+                    inputs=shape,
+                    outputs=outputs,
+                    attrs={'dims': [1],
+                           'dtype': to_dtype,
+                           'value': value})
+
+            if not value_is_scalar_tensor and cls.check_int_type(input_dtype):
+                graph.make_node(
+                    'Cast',
+                    inputs=node2,
+                    outputs=node.output('Out'),
+                    attrs={'to': input_dtype})
+        else:
+            assert not is_shape_tensor and not value_is_scalar_tensor, \
+                "Currently op ['fill_constant'] does not support in onnx(opset<9) when 'shape' or 'fill_value' has " \
+                "tensor, Try converting with opset_version >=9 "
+
+            value = np.ones(shape) * value
+            value = value.astype(dtypes.DTYPE_PADDLE_NUMPY_MAP[dtype])
+            value = value.flatten().tolist()
+
+            graph.make_node(
+                'Constant',
+                inputs=[],
+                outputs=node.output('Out'),
+                attrs={
+                    'dims': shape,
+                    'dtype': dtypes.DTYPE_PADDLE_ONNX_MAP[dtype],
+                    'value': value
+                })
+
+
+@op_mapper(['lookup_table_v2', 'lookup_table'])
+class Embedding():
+    support_opset_version_range = (1, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        ids = node.input('Ids', 0)
+        if node.type == 'lookup_table' and node.input_shape('Ids', 0)[-1] == 1:
+            ids = mapper_helper.squeeze_helper(graph,
+                                               node.input('Ids', 0), [-1])
+        padding_idx = node.attr('padding_idx')
+        input_shape = node.input_shape('W', 0)
+        if padding_idx != -1:
+            key = node.input('W', 0)
+            if -1 in input_shape:
+                assert False, "opset version < 11 do not support padding_idx !=-1 and weight is tensor with dynamic shape, please set opset version > 11 or use input_spec to set input shape"
+            else:
+                data = np.ones(shape=input_shape, dtype=np.float32)
+                data[padding_idx] = 0.0
+                dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('W', 0)]
+                constant = graph.make_node(
+                    'Constant',
+                    dtype=dtype,
+                    dims=input_shape,
+                    value=data.flatten().tolist())
+                weight_node = graph.make_node(
+                    'Mul', inputs=[node.input('W', 0), constant])
+                graph.make_node(
+                    'Gather',
+                    inputs=[weight_node, ids],
+                    outputs=node.output('Out'))
+        else:
+            graph.make_node(
+                'Gather',
+                inputs=[node.input('W', 0), ids],
+                outputs=node.output('Out'))
+
+    @classmethod
+    def opset_11(cls, graph, node, **kw):
+        ids = node.input('Ids', 0)
+        if node.type == 'lookup_table' and node.input_shape('Ids', 0)[-1] == 1:
+            ids = mapper_helper.squeeze_helper(graph,
+                                               node.input('Ids', 0), [-1])
+
+        padding_idx = node.attr('padding_idx')
+        input_shape = node.input_shape('W', 0)
+        if padding_idx != -1:
+            if -1 in input_shape:
+                replace_shape = list(copy.copy(input_shape))
+                del (replace_shape[0])
+                replace_data = graph.make_node(
+                    'Constant',
+                    dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('W',
+                                                                        0)],
+                    dims=replace_shape,
+                    value=[0.0] * np.prod(replace_shape))
+                index = graph.make_node(
+                    'Constant', dtype=dtypes.ONNX.INT64, value=[padding_idx])
+                Scatter_node = graph.make_node(
+                    'ScatterND',
+                    inputs=[node.input('W', 0), index, replace_data])
+                graph.make_node(
+                    'Gather',
+                    inputs=[Scatter_node, ids],
+                    outputs=node.output('Out'))
+            else:
+                data = np.ones(shape=input_shape, dtype=np.float32)
+                data[padding_idx] = 0.0
+                dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('W', 0)]
+                constant = graph.make_node(
+                    'Constant',
+                    dtype=dtype,
+                    dims=input_shape,
+                    value=data.flatten().tolist())
+                weight_node = graph.make_node(
+                    'Mul', inputs=[node.input('W', 0), constant])
+                graph.make_node(
+                    'Gather',
+                    inputs=[weight_node, ids],
+                    outputs=node.output('Out'))
+        else:
+            graph.make_node(
+                'Gather',
+                inputs=[node.input('W', 0), ids],
+                outputs=node.output('Out'))
+
+
+@op_mapper('fill_constant_batch_size_like')
+class FillConstantBatchSizeLike():
+    support_opset_version_range = (9, 12)
+
+    @classmethod
+    def opset_10(cls, graph, node, **kw):
+        out_shape = node.attr('shape')
+        input_dim_idx = node.attr('input_dim_idx')
+        output_dim_idx = node.attr('output_dim_idx')
+
+        del out_shape[output_dim_idx]
+        out_shape.insert(0, 1)
+
+        dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('dtype')]
+        if node.attr("str_value") is not None and node.attr("str_value") != "":
+            value = eval(node.attr("str_value"))
+        else:
+            value = node.attr('value')
+        input_shape = node.input_shape('Input', 0)
+        constant = graph.make_node(
+            'Constant',
+            dtype=dtype,
+            dims=out_shape,
+            value=[value] * np.prod(out_shape))
+
+        shape = graph.make_node('Shape', inputs=node.input('Input'))
+        start = graph.make_node(
+            'Constant', dtype=dtypes.ONNX.INT64, value=[input_dim_idx])
+        end = graph.make_node(
+            'Constant', dtype=dtypes.ONNX.INT64, value=[input_dim_idx + 1])
+        batch = graph.make_node('Slice', inputs=[shape, start, end])
+        repeat = batch
+        if len(out_shape) > 1:
+            repeat = graph.make_node(
+                'Constant',
+                dtype=dtypes.ONNX.INT64,
+                value=[1] * (len(out_shape) - 1))
+            repeat = graph.make_node('Concat', inputs=[batch, repeat], axis=-1)
+        if output_dim_idx == 0:
+            graph.make_node(
+                'Tile', inputs=[constant, repeat], outputs=node.output('Out'))
+        else:
+            out = graph.make_node('Tile', inputs=[constant, repeat])
+            perm = list(range(len(out_shape)))
+            del perm[0]
+            perm.insert(output_dim_idx, 0)
+            graph.make_node(
+                'Transpose',
+                inputs=[out],
+                perm=perm,
+                outputs=node.output('Out'))
+
+
+@op_mapper('fill_any_like')
+class FullLike():
+    '''
+    fill_any_like is kernel for paddle op::full_like & ones_like
+    '''
+    support_opset_version_range = (9, 15)
+
+    @classmethod
+    def opset_9(cls, graph, node, **kw):
+        shape_node = graph.make_node('Shape', inputs=node.input('X'))
+        value = node.attr('value')
+        dtype = node.attr('dtype')
+        input_dtype = node.input_dtype('X', 0)
+        if dtype is None:
+            dtype = input_dtype
+        np_dtype = dtypes.DTYPE_PADDLE_STR_MAP[dtype]
+        onnx_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype]
+        graph.make_node(
+            'ConstantOfShape',
+            inputs=[shape_node],
+            outputs=node.output('Out'),
+            dims=[1],
+            dtype=onnx_dtype,
+            value=np.array(value).astype(np_dtype).tolist())
+
+
+@op_mapper('fill_zeros_like')
+class FullZeroLike():
+    '''
+    fill_zeros_like is kernel for paddle op::zeros_like
+    '''
+    support_opset_version_range = (9, 15)
+
+    @classmethod
+    def opset_9(cls, graph, node, **kw):
+        shape_node = graph.make_node('Shape', inputs=node.input('X'))
+        value = 0
+        dtype = node.attr('dtype')
+        input_dtype = node.input_dtype('X', 0)
+        if dtype is None:
+            dtype = input_dtype
+        np_dtype = dtypes.DTYPE_PADDLE_STR_MAP[dtype]
+        onnx_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype]
+        graph.make_node(
+            'ConstantOfShape',
+            inputs=[shape_node],
+            outputs=node.output('Out'),
+            dims=[1],
+            dtype=onnx_dtype,
+            value=np.array(value).astype(np_dtype).tolist())
+
+
+@op_mapper('gather_nd')
+class Gather_nd():
+    support_opset_version_range = (11, 15)
+
+    @classmethod
+    def opset_11(cls, graph, node, **kw):
+        data = node.input('X', 0)
+        index = node.input('Index', 0)
+        index_dtype = node.input_dtype('Index', 0)
+        index_node = None
+        if index_dtype != paddle.int64:
+            index_node = graph.make_node(
+                'Cast', inputs=[node.input('Index', 0)], to=dtypes.ONNX.INT64)
+        else:
+            index_node = index
+        graph.make_node(
+            'GatherND', inputs=[data, index_node], outputs=node.output('Out'))
+
+
+@op_mapper('gather')
+class Gather():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        axis = node.attr('axis')
+        if node.input('Axis', 0) != None:
+            axis_node = node.input('Axis', 0)
+            try:
+                axis = mapper_helper.get_value_from_parameters(graph,
+                                                               axis_node)[0]
+            except Exception as e:
+                raise Exception(
+                    "Currently does not support the axis parameter as input tensor"
+                    + str(e))
+        if axis is None:
+            axis = 0
+        if len(node.input_shape('Index', 0)) == 1:
+            # gather
+            graph.make_node(
+                'Gather',
+                inputs=[node.input('X', 0), node.input('Index', 0)],
+                outputs=node.output('Out'),
+                attrs={'axis': axis})
+        else:
+            raise Exception(
+                "please try to convert OP:gather(indices's rank >1) with opset_version >= 11."
+            )
+
+    @classmethod
+    def opset_11(cls, graph, node, **kw):
+        axis = node.attr('axis')
+        if node.input('Axis', 0) != None:
+            axis_node = node.input('Axis', 0)
+            try:
+                axis = mapper_helper.get_value_from_parameters(graph,
+                                                               axis_node)[0]
+            except Exception as e:
+                raise Exception(
+                    "Currently does not support the axis parameter as input tensor"
+                    + str(e))
+        if axis is None:
+            axis = 0
+        if len(node.input_shape('Index', 0)) == 1:
+            # gather
+            graph.make_node(
+                'Gather',
+                inputs=[node.input('X', 0), node.input('Index', 0)],
+                outputs=node.output('Out'),
+                attrs={'axis': axis})
+        else:
+            # gather_nd
+            index_dtype = node.input_dtype('Index', 0)
+            if index_dtype != paddle.int64:
+                index_node = graph.make_node(
+                    'Cast',
+                    inputs=[node.input('Index', 0)],
+                    to=dtypes.ONNX.INT64)
+                graph.make_node(
+                    'GatherND',
+                    inputs=[node.input('X', 0), index_node],
+                    outputs=node.output('Out'))
+            else:
+                graph.make_node(
+                    'GatherND',
+                    inputs=[node.input('X', 0), node.input('Index', 0)],
+                    outputs=node.output('Out'))
+
+
+@op_mapper('squeeze2')
+class Squeeze():
+    support_opset_version_range = (1, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        shape = node.input_shape('X', 0)
+        ret = [i for i, val in enumerate(shape) if val > 1]
+        if len(ret) == len(shape):
+            graph.make_node(
+                'Identity', inputs=node.input('X'), outputs=node.output('Out'))
+        else:
+            axes = cls.compute_axes(graph, node)
+            if len(axes) > 0:
+                axes.sort()
+                mapper_helper.squeeze_helper(graph,
+                                             node.input('X', 0), axes,
+                                             node.output('Out'))
+            else:
+                graph.make_node(
+                    'Squeeze',
+                    inputs=[node.input('X', 0)],
+                    outputs=node.output('Out'))
+
+    @classmethod
+    def compute_axes(cls, graph, node):
+        shape = node.input_shape('X', 0)
+        axes = node.attr('axes')
+        if len(axes) > 0:
+            axes = [
+                axis + len(shape) if axis < 0 else axis
+                for i, axis in enumerate(axes)
+            ]
+        return axes
+
+
+@op_mapper('assign_value')
+class Assign():
+    support_opset_version_range = (1, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        if len(node.input_names) > 0:
+            graph.make_node(
+                'Identity', inputs=node.input('X'), outputs=node.output('Out'))
+        else:
+            parameters = {}
+            value = np.array(node.attr('fp32_values'))
+            if value is None or value.size < 1:
+                value = np.array(node.attr('int32_values'))
+            if value is None or value.size < 1:
+                value = np.array(node.attr('int64_values'))
+            parameter = {
+                'data': value,
+                'dtype': node.output_dtype("Out", 0),
+                'shape': node.attr('shape')
+            }
+            parameters[node.output('Out', 0)] = parameter
+            graph.build_parameters(parameters)
+
+
+@op_mapper('transpose2')
+class Transpose():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        graph.make_node(
+            'Transpose',
+            inputs=node.input('X'),
+            outputs=node.output('Out'),
+            perm=node.attr('axis'))
+
+
+@op_mapper('flatten2')
+class Flatten():
+    support_opset_version_range = (1, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        input_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)]
+        if input_dtype in [dtypes.ONNX.INT32, dtypes.ONNX.INT64
+                           ] and graph.opset_version < 9:
+            raise Exception(
+                "int32 or int64 not supported in onnx <9, please try with higher onnx opset_version>=9."
+            )
+
+        graph.make_node(
+            'Flatten',
+            inputs=node.input('X'),
+            outputs=node.output('Out'),
+            axis=node.attr('axis'))
+
+
+@op_mapper('flatten_contiguous_range')
+class FlattenContiguousRange():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        dims = len(node.input_shape('X', 0))
+        start_axis = node.attr('start_axis')
+        end_axis = node.attr('stop_axis')
+        shape_node = graph.make_node('Shape', inputs=node.input('X'))
+        if start_axis < 0:
+            start_axis += dims
+        if end_axis < 0:
+            end_axis += dims
+        if start_axis == 0 and end_axis == dims - 1:
+            final_shape = graph.make_node(
+                'Constant', value=[-1], dtype=dtypes.ONNX.INT64)
+        elif start_axis == 0:
+            slice_end = mapper_helper.slice_helper(
+                graph, shape_node, axes=[0], starts=[end_axis + 1],
+                ends=[dims])
+            slices = [
+                graph.make_node(
+                    'Constant', value=[-1], dtype=dtypes.ONNX.INT64), slice_end
+            ]
+            final_shape = graph.make_node('Concat', inputs=slices, axis=0)
+        elif end_axis == dims - 1:
+            slice_start = mapper_helper.slice_helper(
+                graph, shape_node, axes=[0], starts=[0], ends=[start_axis])
+            slices = [
+                slice_start, graph.make_node(
+                    'Constant', value=[-1], dtype=dtypes.ONNX.INT64)
+            ]
+            final_shape = graph.make_node('Concat', inputs=slices, axis=0)
+        else:
+            slice_start = mapper_helper.slice_helper(
+                graph, shape_node, axes=[0], starts=[0], ends=[start_axis])
+            slice_end = mapper_helper.slice_helper(
+                graph, shape_node, axes=[0], starts=[end_axis + 1],
+                ends=[dims])
+            slices = [
+                slice_start, graph.make_node(
+                    'Constant', value=[-1], dtype=dtypes.ONNX.INT64), slice_end
+            ]
+            final_shape = graph.make_node('Concat', inputs=slices, axis=0)
+        graph.make_node(
+            'Reshape',
+            inputs=[node.input('X')[0], final_shape],
+            outputs=node.output('Out'))
+
+
+@op_mapper('reshape2')
+class Reshape():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        shape_name = 'ShapeTensor'
+        if shape_name not in node.inputs or len(node.input(shape_name)) == 0:
+            shape_name = 'Shape'
+        if shape_name not in node.inputs or len(node.input(shape_name)) == 0:
+            if node.attr('shape') is None or len(node.attr('shape')) == 0:
+                raise Exception("shape tensor and shape attrubite all unkown.")
+        if len(node.input(shape_name)) > 1:
+            dims = []
+            for i in range(len(node.input(shape_name))):
+                dim = node.input(shape_name)[i]
+                dim = graph.make_node(
+                    'Cast', inputs=[dim], to=dtypes.ONNX.INT64)
+                dims.append(dim)
+            shape = graph.make_node('Concat', inputs=dims, axis=-1)
+            graph.make_node(
+                'Reshape',
+                inputs=[node.input('X')[0], shape],
+                outputs=node.output('Out'))
+        elif len(node.input(shape_name)) == 1:
+            cast_shape_node = graph.make_node(
+                'Cast', inputs=node.input(shape_name), to=dtypes.ONNX.INT64)
+            graph.make_node(
+                'Reshape',
+                inputs=[node.input('X')[0], cast_shape_node],
+                outputs=node.output('Out'))
+        elif node.attr('shape') is not None and len(node.attr('shape')) > 0:
+            shape_node = graph.make_node(
+                'Constant',
+                attrs={
+                    'dtype': dtypes.ONNX.INT64,
+                    'value': node.attr('shape')
+                })
+            reshape_node = graph.make_node(
+                'Reshape',
+                inputs=[node.input('X')[0], shape_node],
+                outputs=node.output('Out'))
+
+
+@op_mapper('unsqueeze2')
+class Unsqueeze():
+    support_opset_version_range = (1, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        axes = cls.get_axes(graph, node)
+        mapper_helper.unsqueeze_helper(graph,
+                                       node.input('X'), axes,
+                                       node.output('Out'))
+
+    @classmethod
+    def opset_13(cls, graph, node, **kw):
+        axes_node = cls.get_axes(graph, node, return_node=True)
+        graph.make_node(
+            'Unsqueeze',
+            inputs=node.input('X') + [axes_node],
+            outputs=node.output('Out'))
+
+    @classmethod
+    def get_axes(cls, graph, node, return_node=False):
+        axes_node = None
+        ndim = node.block.vars[node.input('X')[0]].ndim
+        if len(node.attr('axes')) > 0:
+            axes = node.attr('axes')
+        else:
+            axes_node = node.input('AxesTensor')[0]
+            if axes_node is not None and graph.opset_version > 12 and return_node:
+                return axes_node
+            try:
+                axes = mapper_helper.get_value_from_parameters(graph, axes_node)
+            except Exception as e:
+                raise Exception(
+                    "Currently does not support the axes parameter as input tensor in onnx(opset<13), "
+                    "Try converting with opset_version >=13 " + str(e))
+        # axes is list of non-negative integers
+        axes = [
+            axis + ndim + i + 1 if axis < 0 else axis
+            for i, axis in enumerate(axes)
+        ]
+
+        axes_copy = axes.copy()
+        assert sorted(
+            axes) == axes_copy, "axes must be arranged in the following order"
+        assert len(set(axes)) == len(axes), "axes have duplicate axis"
+
+        if return_node:
+            if axes_node is None:
+                axes_node = graph.make_node(
+                    'Constant',
+                    attrs={'dtype': dtypes.ONNX.INT64,
+                           'value': axes})
+            return axes_node
+        return axes
+
+
+@op_mapper('reciprocal')
+class Reciprocal():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        graph.make_node(
+            'Reciprocal', inputs=node.input('X'), outputs=node.output('Out'))
+
+
+@op_mapper('cast')
+class Cast():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        graph.make_node(
+            'Cast',
+            inputs=node.input('X'),
+            outputs=node.output('Out'),
+            to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('out_dtype')])
+
+
+@op_mapper('linspace')
+class Linspace():
+    support_opset_version_range = (9, 15)
+
+    @classmethod
+    def opset_9(cls, graph, node, **kw):
+        start = node.input('Start', 0)
+        stop = node.input('Stop', 0)
+        num = node.input('Num', 0)
+        dtype = node.attr('dtype')
+
+        start = graph.make_node('Cast', inputs=[start], to=dtypes.ONNX.FLOAT)
+        stop = graph.make_node('Cast', inputs=[stop], to=dtypes.ONNX.FLOAT)
+
+        sub_a_node = graph.make_node('Sub', inputs=[stop, start])
+
+        one_node = graph.make_node(
+            'Constant',
+            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('Num', 0)],
+            value=[1])
+
+        sub_b_node = graph.make_node('Sub', inputs=[num, one_node])
+
+        sub_b_float_node = graph.make_node(
+            'Cast', inputs=[sub_b_node], to=dtypes.ONNX.FLOAT)
+
+        step = graph.make_node('Div', inputs=[sub_a_node, sub_b_float_node])
+
+        range_tensor = graph.make_node(
+            'Cast', inputs=[num], to=dtypes.ONNX.INT64)
+
+        one_like_node = graph.make_node(
+            'ConstantOfShape',
+            inputs=[range_tensor],
+            dtype=dtypes.ONNX.FLOAT,
+            value=[1])
+
+        none_zero_node = graph.make_node('NonZero', inputs=[one_like_node])
+
+        trans_none_zero_node = graph.make_node(
+            'Transpose', inputs=[none_zero_node], perm=[1, 0])
+
+        trans_squeeze = mapper_helper.squeeze_helper(graph,
+                                                     trans_none_zero_node, [1])
+
+        trans_squeeze = graph.make_node(
+            'Cast', inputs=[trans_squeeze], to=dtypes.ONNX.FLOAT)
+
+        mul_node = graph.make_node('Mul', inputs=[trans_squeeze, step])
+
+        add_node = graph.make_node('Add', inputs=[mul_node, start])
+        graph.make_node(
+            'Cast',
+            inputs=[add_node],
+            outputs=node.output('Out'),
+            to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('Start', 0)])
+
+
+@op_mapper('clip')
+class Clip():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        min_value = node.attr('min')
+        max_value = node.attr('max')
+        if node.input('Max', 0) is None or len(node.input('Max')) == 0:
+            max_ = max_value
+        else:
+            max_ = node.input('Max', 0)
+        if node.input('Min', 0) is None or len(node.input('Min')) == 0:
+            min_ = min_value
+        else:
+            min_ = node.input('Min', 0)
+        mapper_helper.clip_helper(graph, node,
+                                  node.input('X', 0), max_, min_,
+                                  node.output('Out', 0))
+
+
+@op_mapper(['pad2d', 'pad3d'])
+class Pad():
+    support_opset_version_range = (1, 12)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        if node.attr('mode') == 'replicate':
+            mode = 'edge'
+        elif node.attr('mode') == 'circular':
+            raise Exception("The padding mode = circular is not supported, " \
+                            "Please try the other three ways")
+        else:
+            mode = node.attr('mode')
+        pads = cls.convert_padding(node, **kw)
+        if pads is None:
+            key = node.input('Paddings', 0)
+            padding = None
+            if key in graph.parameters.keys():
+                paddings = graph.parameters[key].attribute[0].t.int32_data
+                if node.attr('data_format') == 'NCHW':
+                    pads = [
+                        0, 0, paddings[0], paddings[2], 0, 0, paddings[1],
+                        paddings[3]
+                    ]
+                elif node.attr('data_format') == 'NHWC':
+                    pads = [
+                        0, paddings[0], paddings[2], 0, 0, paddings[1],
+                        paddings[3], 0
+                    ]
+                elif node.attr('data_format') == 'NCDHW':
+                    pads = [
+                        0, 0, paddings[4], paddings[2], paddings[0], 0, 0,
+                        paddings[5], paddings[3], paddings[1]
+                    ]
+                elif node.attr('data_format') == 'NDHWC':
+                    pads = [
+                        0, paddings[4], paddings[2], paddings[0], 0, 0,
+                        paddings[5], paddings[3], paddings[1], 0
+                    ]
+            else:
+                raise Exception("In Pad op, padding can not be tensor" \
+                                "Please set opset version >= 11")
+
+        value = None
+        if node.attr('pad_value') is not None:
+            value = node.attr('pad_value')
+        elif node.attr('value') is not None:
+            value = node.attr('value')
+        graph.make_node(
+            'Pad',
+            inputs=node.input('X'),
+            outputs=node.output('Out'),
+            mode=mode,
+            value=value,
+            pads=pads)
+
+    @classmethod
+    def opset_11(cls, graph, node, **kw):
+        pads = cls.convert_padding(node, **kw)
+        if node.attr('mode') == 'replicate':
+            mode = 'edge'
+        elif node.attr('mode') == 'circular':
+            raise Exception("The padding mode = circular is not supported, " \
+                            "Please try the other three ways")
+        else:
+            mode = node.attr('mode')
+        pads_node = None
+        if isinstance(pads, list):
+            pads_node = graph.make_node(
+                'Constant', attrs={'dtype': dtypes.ONNX.INT64,
+                                   'value': pads})
+        else:
+            key = node.input('Paddings', 0)
+            padding = None
+            if key in graph.parameters.keys():
+                paddings = graph.parameters[key].attribute[0].t.int32_data
+                onnx_paddings = None
+                if node.attr('data_format') == 'NCHW':
+                    onnx_paddings = [
+                        0, 0, paddings[0], paddings[2], 0, 0, paddings[1],
+                        paddings[3]
+                    ]
+                elif node.attr('data_format') == 'NHWC':
+                    onnx_paddings = [
+                        0, paddings[0], paddings[2], 0, 0, paddings[1],
+                        paddings[3], 0
+                    ]
+                elif node.attr('data_format') == 'NCDHW':
+                    onnx_paddings = [
+                        0, 0, paddings[4], paddings[2], paddings[0], 0, 0,
+                        paddings[5], paddings[3], paddings[1]
+                    ]
+                elif node.attr('data_format') == 'NDHWC':
+                    onnx_paddings = [
+                        0, paddings[4], paddings[2], paddings[0], 0, 0,
+                        paddings[5], paddings[3], paddings[1], 0
+                    ]
+
+                pads_node = graph.make_node(
+                    'Constant',
+                    attrs={'dtype': dtypes.ONNX.INT64,
+                           'value': onnx_paddings})
+            else:
+                padding_node = node.input('Paddings', 0)
+                casted_padding_node = graph.make_node(
+                    'Cast', inputs=[padding_node], to=dtypes.ONNX.FLOAT)
+                zero_node = None
+                if node.attr('data_format') == 'NCHW' or node.attr(
+                        'data_format') == 'NHWC':
+                    zero_node = graph.make_node(
+                        'Constant', dtype=dtypes.ONNX.FLOAT, value=[0] * 8)
+                else:
+                    zero_node = graph.make_node(
+                        'Constant', dtype=dtypes.ONNX.FLOAT, value=[0] * 10)
+                index = None
+                if node.attr('data_format') == 'NCHW':
+                    index = graph.make_node(
+                        'Constant', dtype=dtypes.ONNX.INT32,
+                        value=[2, 6, 3, 7])
+                elif node.attr('data_format') == 'NHWC':
+                    index = graph.make_node(
+                        'Constant', dtype=dtypes.ONNX.INT32,
+                        value=[1, 5, 2, 6])
+                elif node.attr('data_format') == 'NCDHW':
+                    index = graph.make_node(
+                        'Constant',
+                        dtype=dtypes.ONNX.INT32,
+                        value=[4, 9, 3, 8, 2, 7])
+                elif node.attr('data_format') == 'NDHWC':
+                    index = graph.make_node(
+                        'Constant',
+                        dtype=dtypes.ONNX.INT32,
+                        value=[3, 8, 2, 7, 1, 6])
+
+                float_paddle_node = graph.make_node(
+                    'ScatterElements',
+                    inputs=[zero_node, index, casted_padding_node])
+                paddle_node = graph.make_node(
+                    'Cast', inputs=[float_paddle_node], to=dtypes.ONNX.INT64)
+                pads_node = paddle_node
+
+        value = None
+        if node.attr('pad_value') is not None:
+            value = node.attr('pad_value')
+        elif node.attr('value') is not None:
+            value = node.attr('value')
+        value_node = graph.make_node(
+            'Constant',
+            attrs={
+                'dtype': dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
+                'value': value
+            })
+
+        graph.make_node(
+            'Pad',
+            inputs=node.input('X') + [pads_node, value_node],
+            outputs=node.output('Out'),
+            mode=mode)
+
+    @classmethod
+    def convert_padding(cls, node, **kw):
+        x_shape = node.input_shape('X', 0)
+        paddings = node.attr('paddings')
+        if paddings == []:
+            return None
+        onnx_paddings = None
+        if node.attr('data_format') == 'NCHW':
+            onnx_paddings = [
+                0, 0, paddings[0], paddings[2], 0, 0, paddings[1], paddings[3]
+            ]
+        elif node.attr('data_format') == 'NHWC':
+            onnx_paddings = [
+                0, paddings[0], paddings[2], 0, 0, paddings[1], paddings[3], 0
+            ]
+        elif node.attr('data_format') == 'NCDHW':
+            onnx_paddings = [
+                0, 0, paddings[4], paddings[2], paddings[0], 0, 0, paddings[5],
+                paddings[3], paddings[1]
+            ]
+        elif node.attr('data_format') == 'NDHWC':
+            onnx_paddings = [
+                0, paddings[4], paddings[2], paddings[0], 0, 0, paddings[5],
+                paddings[3], paddings[1], 0
+            ]
+        return onnx_paddings
+
+
+@op_mapper('gaussian_random')
+class GaussianRandom():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        shape_input_list = node.input('ShapeTensorList')
+        shape_input = None
+        if len(shape_input_list) == 0:
+            shape_input = node.input('ShapeTensor')
+        else:
+            shape_input = graph.make_node(
+                "Concat", inputs=node.input('ShapeTensorList'), axis=0)
+        if shape_input is None or len(shape_input) == 0:
+            graph.make_node(
+                'RandomNormal',
+                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('dtype')],
+                outputs=node.output('Out'),
+                shape=node.attr('shape'),
+                seed=float(node.attr('seed')),
+                mean=node.attr('mean'),
+                scale=node.attr('std'))
+        else:
+            cast_input_shape = graph.make_node(
+                'Cast', inputs=shape_input, to=dtypes.ONNX.INT64)
+            zero_like_node = graph.make_node(
+                'ConstantOfShape',
+                inputs=cast_input_shape,
+                dims=[1],
+                dtype=dtypes.ONNX.FLOAT,
+                value=[0])
+            graph.make_node(
+                'RandomNormalLike',
+                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('dtype')],
+                outputs=node.output('Out'),
+                inputs=zero_like_node,
+                seed=float(node.attr('seed')),
+                mean=node.attr('mean'),
+                scale=node.attr('std'))
+
+
+@op_mapper('uniform_random_batch_size_like')
+class UniformRandom():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        graph.make_node(
+            'RandomUniformLike',
+            inputs=node.input('Input'),
+            outputs=node.output('Out'),
+            high=node.attr('max'),
+            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('dtype')],
+            low=node.attr('min'),
+            seed=float(node.attr('seed')), )
+
+
+@op_mapper('uniform_random')
+class UniformRandom():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        shape_input_list = node.input('ShapeTensorList')
+        shape_input = None
+        if len(shape_input_list) == 0:
+            shape_input = node.input('ShapeTensor')
+        else:
+            shape_input = graph.make_node(
+                "Concat", inputs=node.input('ShapeTensorList'), axis=0)
+        if shape_input is None or len(shape_input) == 0:
+            graph.make_node(
+                'RandomUniform',
+                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('dtype')],
+                outputs=node.output('Out'),
+                shape=node.attr('shape'),
+                seed=float(node.attr('seed')),
+                low=node.attr('min'),
+                high=node.attr('max'))
+        else:
+            cast_input_shape = graph.make_node(
+                'Cast', inputs=shape_input, to=dtypes.ONNX.INT64)
+            zero_like_node = graph.make_node(
+                'ConstantOfShape',
+                inputs=cast_input_shape,
+                dtype=dtypes.ONNX.FLOAT,
+                value=[0])
+            graph.make_node(
+                'RandomUniformLike',
+                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('dtype')],
+                outputs=node.output('Out'),
+                inputs=zero_like_node,
+                seed=float(node.attr('seed')),
+                low=node.attr('min'),
+                high=node.attr('max'))
+
+
+# 'bilinear_interp', 'nearest_interp', scale only support 2, 4, 6, 8, 10
+@op_mapper(
+    [
+        'bilinear_interp', 'nearest_interp', 'bilinear_interp_v2',
+        'nearest_interp_v2', 'bicubic_interp_v2', 'linear_interp_v2',
+        'trilinear_interp_v2', 'trilinear_interp', 'linear_interp'
+    ],
+    mapper_dict={
+        'bilinear_interp': 'linear',
+        'nearest_interp': 'nearest',
+        'bilinear_interp_v2': 'linear',
+        'nearest_interp_v2': 'nearest',
+        'bicubic_interp_v2': 'cubic',
+        'linear_interp_v2': 'linear',
+        'trilinear_interp_v2': 'linear',
+        'trilinear_interp': 'linear',
+        'linear_interp': 'linear',
+    },
+    opset_op_dict={
+        9: 'Upsample',
+        10: 'Resize',
+    })
+class Resize():
+    support_opset_version_range = (9, 15)
+
+    @classmethod
+    def opset_9(cls, graph, node, **kw):
+        inputs = [node.input('X')[0]]
+        resize_type = kw['mapper_dict'][node.type]
+        cls.waringInfo(graph, node, resize_type)
+        if len(node.input('OutSize')) > 0 or len(node.input('SizeTensor')) > 0:
+            output_node = cls.compute_outsize_node(
+                graph, node, return_scale=True)
+        elif 'Scale' in node.inputs and len(node.input('Scale')) > 0:
+            output_node = cls.compute_scale_node(graph, node)
+        else:
+            output_node = cls.compute_attrs_node(graph, node, return_scale=True)
+
+        inputs = inputs + output_node
+        op = kw['opset_op_dict'][graph.opset_version]
+        graph.make_node(
+            op, inputs=inputs, outputs=node.output('Out'), mode=resize_type)
+
+    @classmethod
+    def opset_11(cls, graph, node, **kw):
+        inputs = [node.input('X')[0]]
+        resize_type = kw['mapper_dict'][node.type]
+        cls.waringInfo(graph, node, resize_type)
+        if node.attr('align_corners'):
+            coordinate_transformation_mode = 'align_corners'
+        elif node.attr('align_mode') == 1 and resize_type is not 'cubic':
+            coordinate_transformation_mode = 'asymmetric'
+        elif resize_type == 'nearest':
+            coordinate_transformation_mode = 'asymmetric'
+        else:
+            coordinate_transformation_mode = 'half_pixel'
+        roi_node = graph.make_node(
+            'Constant',
+            attrs={
+                'dtype': dtypes.ONNX.FLOAT,
+                'value': [1, 1, 1, 1, 1, 1, 1, 1]
+            })
+
+        inputs.append(roi_node)
+        if len(node.input('OutSize')) > 0 or len(node.input('SizeTensor')) > 0:
+            output_node = cls.compute_outsize_node(graph, node)
+        elif 'Scale' in node.inputs and len(node.input('Scale')) > 0:
+            output_node = cls.compute_scale_node(graph, node)
+        else:
+            output_node = cls.compute_attrs_node(graph, node)
+        inputs = inputs + output_node
+        attrs = {
+            'mode': resize_type,
+            'coordinate_transformation_mode': coordinate_transformation_mode
+        }
+        if resize_type == 'nearest' and coordinate_transformation_mode == 'asymmetric':
+            attrs['nearest_mode'] = 'floor'
+        graph.make_node(
+            'Resize', inputs=inputs, outputs=node.output('Out'), attrs=attrs)
+
+    @classmethod
+    def compute_outsize_node(cls, graph, node, return_scale=False):
+        dtype = dtypes.ONNX.INT64
+        if return_scale:
+            dtype = dtypes.ONNX.FLOAT
+        input_shape_node = graph.make_node('Shape', inputs=node.input('X'))
+        if dtype != dtypes.ONNX.INT64:
+            input_shape_node = graph.make_node(
+                'Cast', inputs=[input_shape_node], to=dtype)
+        shape_pre_node = mapper_helper.slice_helper(
+            graph, input_shape_node, axes=[], starts=[0], ends=[2])
+
+        out_size = [node.attr('out_d'), node.attr('out_h'), node.attr('out_w')]
+        out_size = [val for val in out_size if val > 0]
+        use_tensor = False
+        if len(node.input('OutSize')) > 0 or len(node.input('SizeTensor')) > 0:
+            use_tensor = True
+        if len(out_size) > 0 and not use_tensor:
+            out_size_node = graph.make_node(
+                'Constant', attrs={'dtype': dtype,
+                                   'value': out_size})
+        else:
+            out_size_node, _ = mapper_helper.get_node_attr_value(
+                graph, node, None, 'OutSize', 'SizeTensor', dtype=dtype)
+        out_size_node = graph.make_node(
+            'Concat', inputs=[shape_pre_node, out_size_node], axis=0)
+
+        if return_scale:
+            scale_node = graph.make_node(
+                'Div', inputs=[out_size_node, input_shape_node])
+            return [scale_node]
+
+        scale_empty_node = graph.make_node(
+            'Constant', attrs={'dtype': dtypes.ONNX.FLOAT,
+                               'value': []})
+        return [scale_empty_node, out_size_node]
+
+    @classmethod
+    def compute_scale_node(cls, graph, node):
+        cast_scale = graph.make_node(
+            'Cast', inputs=node.input('Scale'), to=dtypes.ONNX.FLOAT)
+        inputs_cocat = []
+        const_node = graph.make_node(
+            'Constant', attrs={'dtype': dtypes.ONNX.FLOAT,
+                               'value': [1, 1]})
+        inputs_cocat.append(const_node)
+        scale = node.attr('scale')
+        if isinstance(scale, (float, int)):
+            cast_scale = [cast_scale] * (len(node.input_shape('X', 0)) - 2)
+            inputs_cocat = inputs_cocat + cast_scale
+        else:
+            inputs_cocat = inputs_cocat + [cast_scale]
+        scale_node = graph.make_node('Concat', inputs=inputs_cocat, axis=0)
+        return [scale_node]
+
+    @classmethod
+    def compute_attrs_node(cls, graph, node, return_scale=False):
+        out_size = [node.attr('out_d'), node.attr('out_h'), node.attr('out_w')]
+        scale = node.attr('scale')
+        if isinstance(scale, (float, int)):
+            scale = [scale] * (len(node.input_shape('X', 0)) - 2)
+
+        out_size = [val for val in out_size if val > 0]
+        if len(out_size) > 0:
+            output_node = cls.compute_outsize_node(
+                graph, node, return_scale=return_scale)
+            return output_node
+
+        assert len(scale) > 0, Exception("scale size should > 0!")
+        scale_node = graph.make_node(
+            'Constant',
+            attrs={'dtype': dtypes.ONNX.FLOAT,
+                   'value': [1, 1] + scale})
+        return [scale_node]
+
+    @classmethod
+    def waringInfo(cls, graph, node, resize_type):
+        assert node.attrs['data_layout'] == 'NCHW', \
+            "The conv data layout should be 'NCHW' , but received data format " \
+            "is %s." % node.attrs['data_format']
+
+        if graph.opset_version < 11:
+            if node.attr('align_corners') or resize_type in ["cubic"]:
+                raise Exception(
+                    "When align_corners is true or resize_type is 'cubic', the case isn't supported in onnx(opset<=10), "
+                    "Try converting with opset_version>= 11 ")
+            if node.attr('align_mode') == 0 and resize_type in [
+                    "bilinear", "linear", "trilinear"
+            ]:
+                raise Exception(
+                    "When align_mode == 0 and resize_type is 'bilinear' or 'linear or 'trilinear', the case isn't "
+                    "supported in onnx(opset<=10), Try converting with opset_version>= 11 "
+                )
+
+
+@op_mapper('pixel_shuffle')
+class PixelShuffle():
+    support_opset_version_range = (11, 15)
+
+    @classmethod
+    def opset_11(cls, graph, node, **kw):
+        upscale_factor = node.attr('upscale_factor')
+
+        node = graph.make_node(
+            'DepthToSpace',
+            inputs=node.input('X'),
+            outputs=node.output('Out'),
+            blocksize=upscale_factor,
+            mode='CRD')
+
+
+@op_mapper('scatter')
+class Scatter():
+    support_opset_version_range = (11, 15)
+
+    @classmethod
+    def opset_11(cls, graph, node, **kw):
+        ids = node.input('Ids', 0)
+        input_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('Ids', 0)]
+        if input_dtype != dtypes.ONNX.INT64:
+            ids = graph.make_node('Cast', inputs=[ids], to=dtypes.ONNX.INT64)
+
+        shape = graph.make_node(
+            'Constant',
+            value=[node.input_shape('Ids', 0)[0], 1],
+            dtype=dtypes.ONNX.INT64)
+        reshape_index = graph.make_node('Reshape', inputs=[ids, shape])
+        if not node.attr('overwrite'):
+            raise Exception("overwrite = False not support yet.")
+        else:
+            graph.make_node(
+                'ScatterND',
+                inputs=[
+                    node.input('X', 0), reshape_index, node.input('Updates', 0)
+                ],
+                outputs=node.output('Out'))
+
+
+@op_mapper('scatter_nd_add')
+class ScatterndAdd():
+    support_opset_version_range = (11, 12)
+
+    @classmethod
+    def opset_11(cls, graph, node, **kw):
+        shape = graph.make_node('Shape', inputs=node.input('X', 0))
+        zero_like_node = graph.make_node(
+            'ConstantOfShape',
+            inputs=[shape],
+            dims=[1],
+            dtype=dtypes.ONNX.FLOAT,
+            value=[0])
+        add_node = graph.make_node(
+            'ScatterND',
+            inputs=[
+                zero_like_node, node.input('Index', 0), node.input('Updates', 0)
+            ], )
+        graph.make_node(
+            'Add',
+            inputs=[node.input('X', 0), add_node],
+            outputs=node.output('Out'))
+
+
+@op_mapper('meshgrid')
+class Meshgrid():
+    support_opset_version_range = (8, 15)
+
+    @classmethod
+    def opset_8(cls, graph, node, **kw):
+        tensors = [t for t in list(node.input('X'))]
+        tensors_shape = [graph.make_node('Shape', inputs=t) for t in tensors]
+        out_shape = graph.make_node('Concat', inputs=tensors_shape, axis=0)
+        out = []
+        for i, t in enumerate(tensors):
+            shape_i = [
+                graph.make_node(
+                    'Constant',
+                    attrs={'dtype': dtypes.ONNX.INT64,
+                           'value': [1]})
+            ] * len(tensors)
+            shape_i[i] = tensors_shape[i]
+            t_reshaped = graph.make_node(
+                'Reshape',
+                inputs=[t, graph.make_node(
+                    'Concat', inputs=shape_i, axis=0)])
+            out.append(
+                graph.make_node(
+                    'Expand',
+                    inputs=[t_reshaped, out_shape],
+                    outputs=node.output('Out')[i]))
+
+
+@op_mapper('flip')
+class Flip():
+    support_opset_version_range = (7, 15)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        inputs = node.input('X')
+        x_dtype = node.input_dtype('X', 0)
+        if x_dtype == paddle.bool or x_dtype == paddle.float64:
+            inputs = [
+                graph.make_node(
+                    "Cast", inputs=inputs, to=dtypes.ONNX.FLOAT)
+            ]
+        axes = node.attr("axis")
+        if not isinstance(axes, list):
+            axes = [axes]
+        input_shape = node.input_shape('X', 0)
+
+        for i, axis in enumerate(axes):
+            if axis < 0:
+                axes[i] += len(input_shape)
+            assert input_shape[
+                axis] > 0, "The dimension in axis of input must be fixed for flip operator, but now the input shape({}) in axis({}) is unknow.".format(
+                    input_shape, axis)
+
+        temp_input = inputs[0]
+        for i, axis in enumerate(axes):
+            if input_shape[axis] == 1:
+                if i != len(axes) - 1:
+                    continue
+                else:
+                    if x_dtype == paddle.bool or x_dtype == paddle.float64:
+                        graph.make_node(
+                            "Cast",
+                            inputs=[temp_input],
+                            outputs=node.output("Out"),
+                            to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])
+                    else:
+                        graph.make_node(
+                            "Identity",
+                            inputs=[temp_input],
+                            outputs=node.output("Out"))
+            else:
+                splits = graph.make_node(
+                    "Split",
+                    inputs=[temp_input],
+                    outputs=input_shape[axis],
+                    axis=axis,
+                    split=[1] * input_shape[axis])
+                reversed_splits = splits[::-1]
+                if i != len(axes) - 1:
+                    temp_input = graph.make_node(
+                        "Concat", inputs=reversed_splits, axis=axis)
+                else:
+                    if x_dtype == paddle.bool or x_dtype == paddle.float64:
+                        out = graph.make_node(
+                            "Concat", inputs=reversed_splits, axis=axis)
+                        graph.make_node(
+                            "Cast",
+                            inputs=[out],
+                            outputs=node.output("Out"),
+                            to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])
+                    else:
+                        graph.make_node(
+                            "Concat",
+                            inputs=reversed_splits,
+                            outputs=node.output("Out"),
+                            axis=axis)
+
+    @classmethod
+    def opset_13(cls, graph, node, **kw):
+        inputs = node.input('X')
+        x_dtype = node.input_dtype('X', 0)
+        if x_dtype == paddle.bool or x_dtype == paddle.float64:
+            inputs = [
+                graph.make_node(
+                    "Cast", inputs=inputs, to=dtypes.ONNX.FLOAT)
+            ]
+        axes = node.attr("axis")
+        if not isinstance(axes, list):
+            axes = [axes]
+        input_shape = node.input_shape('X', 0)
+
+        for i, axis in enumerate(axes):
+            if axis < 0:
+                axes[i] += len(input_shape)
+            assert input_shape[
+                axis] > 0, "The dimension in axis of input must be fixed for flip operator, but now the input shape({}) in axis({}) is unknow.".format(
+                    input_shape, axis)
+
+        temp_input = inputs[0]
+        for i, axis in enumerate(axes):
+            if input_shape[axis] == 1:
+                if i != len(axes) - 1:
+                    continue
+                else:
+                    if x_dtype == paddle.bool or x_dtype == paddle.float64:
+                        graph.make_node(
+                            "Cast",
+                            inputs=[temp_input],
+                            outputs=node.output("Out"),
+                            to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])
+                    else:
+                        graph.make_node(
+                            "Identity",
+                            inputs=[temp_input],
+                            outputs=node.output("Out"))
+            else:
+                split = graph.make_node(
+                    'Constant',
+                    attrs={
+                        'dtype': dtypes.ONNX.INT64,
+                        'value': [1] * input_shape[axis]
+                    })
+                splits = graph.make_node(
+                    "Split",
+                    inputs=[temp_input, split],
+                    outputs=input_shape[axis],
+                    axis=axis)
+                reversed_splits = splits[::-1]
+                if i != len(axes) - 1:
+                    temp_input = graph.make_node(
+                        "Concat", inputs=reversed_splits, axis=axis)
+                else:
+                    if x_dtype == paddle.bool or x_dtype == paddle.float64:
+                        out = graph.make_node(
+                            "Concat", inputs=reversed_splits, axis=axis)
+                        graph.make_node(
+                            "Cast",
+                            inputs=[out],
+                            outputs=node.output("Out"),
+                            to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])
+                    else:
+                        graph.make_node(
+                            "Concat",
+                            inputs=reversed_splits,
+                            outputs=node.output("Out"),
+                            axis=axis)
```

## paddle2onnx/legacy/op_mapper/custom_paddle_op/__init__.py

 * *Ordering differences only*

```diff
@@ -1,13 +1,13 @@
-#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
```

## paddle2onnx/legacy/op_mapper/custom_paddle_op/anchor_generator.py

 * *Ordering differences only*

```diff
@@ -1,97 +1,97 @@
-# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import numpy as np
-import paddle
-from paddle.fluid import layers
-from paddle2onnx.legacy.op_mapper import CustomPaddleOp, register_custom_paddle_op
-from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
-from paddle2onnx.legacy.op_mapper import mapper_helper
-
-class AnchorGenerator(CustomPaddleOp):
-    def __init__(self, node, **kw):
-        super(AnchorGenerator, self).__init__(node)
-        #self.x_shape = node.input_shape('Input', 0)
-        self.anchor_sizes = node.attr('anchor_sizes')
-        self.aspect_ratios = node.attr('aspect_ratios')
-        self.offset = node.attr('offset')
-        self.strides = node.attr('stride')
-        self.variances = node.attr('variances')
-        self.shapes = self.compute_shapes()
-
-    def compute_shapes(self):
-        shapes = list()
-        for r in range(len(self.aspect_ratios)):
-            ar = self.aspect_ratios[r]
-            for s in range(len(self.anchor_sizes)):
-                anchor_size = self.anchor_sizes[s]
-                area = self.strides[0] * self.strides[1]
-                area_ratios = area / ar
-                base_w = np.floor(np.sqrt(area_ratios) + 0.5)
-                base_h = np.floor(base_w * ar + 0.5)
-                scale_w = anchor_size / self.strides[0]
-                scale_h = anchor_size / self.strides[1]
-                w = scale_w * base_w
-                h = scale_h * base_h
-                shapes.append([
-                    -0.5 * (w - 1), -0.5 * (h - 1), 0.5 * (w - 1), 0.5 * (h - 1)
-                ])
-        return shapes
-
-    def forward(self):
-        input_feature = self.input('Input', 0)
-        input_shape = paddle.shape(input_feature)
-        n, c, h, w = paddle.tensor.split(input_shape, num_or_sections=4)
-        x_ctr = paddle.arange(start=0, end=w, step=1, dtype=input_feature.dtype)
-        y_ctr = paddle.arange(start=0, end=h, step=1, dtype=input_feature.dtype)
-        x_ctr = x_ctr * self.strides[0] + self.offset * (self.strides[0] - 1)
-        y_ctr = y_ctr * self.strides[1] + self.offset * (self.strides[1] - 1)
-        tensor_one = paddle.ones(shape=[1], dtype='int64')
-        tensor_len_shape = paddle.full(
-            shape=[1], fill_value=len(self.shapes), dtype='int64')
-        x_ctr = paddle.reshape(x_ctr, shape=(1, -1))
-        y_ctr = paddle.reshape(y_ctr, shape=(1, -1))
-        x_ctr = paddle.tile(x_ctr, repeat_times=(h, tensor_one))
-        y_ctr = paddle.tile(y_ctr, repeat_times=(w, tensor_one))
-        y_ctr = paddle.transpose(y_ctr, perm=[1, 0])
-        centers = paddle.stack([x_ctr, y_ctr], axis=-1)
-        centers = paddle.tensor.unsqueeze(centers, axis=[2])
-        centers = paddle.tile(centers, repeat_times=(1, 1, len(self.shapes), 2))
-        shape_tensor = paddle.assign(np.array(self.shapes).astype('float32'))
-        anchors = centers + shape_tensor
-        variance_tensor = paddle.assign(
-            np.asarray(self.variances).astype('float32'))
-        vars = paddle.reshape(variance_tensor, shape=[1, 1, 1, -1])
-        vars = paddle.tile(
-            vars, repeat_times=(h, w, tensor_len_shape, tensor_one))
-        return {'Anchors': [anchors], 'Variances': [vars]}
-
-@op_mapper('anchor_generator')
-class Anchors_generator:
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        node = graph.make_node(
-            'anchor_generator',
-            inputs=node.input('Input'),
-            outputs=node.output('Anchors') + node.output('Variances'),
-            anchor_sizes = node.attr('anchor_sizes'),
-            aspect_ratios = node.attr('aspect_ratios'),
-            offset = node.attr('offset'),
-            strides = node.attr('stride'),
-            variances = node.attr('variances'),
-            domain = 'custom')
-
-register_custom_paddle_op('anchor_generator', AnchorGenerator)
+# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import numpy as np
+import paddle
+from paddle.fluid import layers
+from paddle2onnx.legacy.op_mapper import CustomPaddleOp, register_custom_paddle_op
+from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
+from paddle2onnx.legacy.op_mapper import mapper_helper
+
+class AnchorGenerator(CustomPaddleOp):
+    def __init__(self, node, **kw):
+        super(AnchorGenerator, self).__init__(node)
+        #self.x_shape = node.input_shape('Input', 0)
+        self.anchor_sizes = node.attr('anchor_sizes')
+        self.aspect_ratios = node.attr('aspect_ratios')
+        self.offset = node.attr('offset')
+        self.strides = node.attr('stride')
+        self.variances = node.attr('variances')
+        self.shapes = self.compute_shapes()
+
+    def compute_shapes(self):
+        shapes = list()
+        for r in range(len(self.aspect_ratios)):
+            ar = self.aspect_ratios[r]
+            for s in range(len(self.anchor_sizes)):
+                anchor_size = self.anchor_sizes[s]
+                area = self.strides[0] * self.strides[1]
+                area_ratios = area / ar
+                base_w = np.floor(np.sqrt(area_ratios) + 0.5)
+                base_h = np.floor(base_w * ar + 0.5)
+                scale_w = anchor_size / self.strides[0]
+                scale_h = anchor_size / self.strides[1]
+                w = scale_w * base_w
+                h = scale_h * base_h
+                shapes.append([
+                    -0.5 * (w - 1), -0.5 * (h - 1), 0.5 * (w - 1), 0.5 * (h - 1)
+                ])
+        return shapes
+
+    def forward(self):
+        input_feature = self.input('Input', 0)
+        input_shape = paddle.shape(input_feature)
+        n, c, h, w = paddle.tensor.split(input_shape, num_or_sections=4)
+        x_ctr = paddle.arange(start=0, end=w, step=1, dtype=input_feature.dtype)
+        y_ctr = paddle.arange(start=0, end=h, step=1, dtype=input_feature.dtype)
+        x_ctr = x_ctr * self.strides[0] + self.offset * (self.strides[0] - 1)
+        y_ctr = y_ctr * self.strides[1] + self.offset * (self.strides[1] - 1)
+        tensor_one = paddle.ones(shape=[1], dtype='int64')
+        tensor_len_shape = paddle.full(
+            shape=[1], fill_value=len(self.shapes), dtype='int64')
+        x_ctr = paddle.reshape(x_ctr, shape=(1, -1))
+        y_ctr = paddle.reshape(y_ctr, shape=(1, -1))
+        x_ctr = paddle.tile(x_ctr, repeat_times=(h, tensor_one))
+        y_ctr = paddle.tile(y_ctr, repeat_times=(w, tensor_one))
+        y_ctr = paddle.transpose(y_ctr, perm=[1, 0])
+        centers = paddle.stack([x_ctr, y_ctr], axis=-1)
+        centers = paddle.tensor.unsqueeze(centers, axis=[2])
+        centers = paddle.tile(centers, repeat_times=(1, 1, len(self.shapes), 2))
+        shape_tensor = paddle.assign(np.array(self.shapes).astype('float32'))
+        anchors = centers + shape_tensor
+        variance_tensor = paddle.assign(
+            np.asarray(self.variances).astype('float32'))
+        vars = paddle.reshape(variance_tensor, shape=[1, 1, 1, -1])
+        vars = paddle.tile(
+            vars, repeat_times=(h, w, tensor_len_shape, tensor_one))
+        return {'Anchors': [anchors], 'Variances': [vars]}
+
+@op_mapper('anchor_generator')
+class Anchors_generator:
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        node = graph.make_node(
+            'anchor_generator',
+            inputs=node.input('Input'),
+            outputs=node.output('Anchors') + node.output('Variances'),
+            anchor_sizes = node.attr('anchor_sizes'),
+            aspect_ratios = node.attr('aspect_ratios'),
+            offset = node.attr('offset'),
+            strides = node.attr('stride'),
+            variances = node.attr('variances'),
+            domain = 'custom')
+
+register_custom_paddle_op('anchor_generator', AnchorGenerator)
```

## paddle2onnx/legacy/op_mapper/custom_paddle_op/box_clip.py

 * *Ordering differences only*

```diff
@@ -1,56 +1,56 @@
-# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import numpy as np
-import paddle
-from paddle.fluid import layers
-from paddle2onnx.legacy.op_mapper import CustomPaddleOp, register_custom_paddle_op
-from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
-from paddle2onnx.legacy.op_mapper import mapper_helper
-
-class BoxClip(CustomPaddleOp):
-    def __init__(self, node, **kw):
-        super(BoxClip, self).__init__(node)
-
-    def forward(self):
-        input = self.input('Input', 0)
-        im_info = self.input('ImInfo', 0)
-        im_info = paddle.reshape(im_info, shape=[3])
-        h, w, s = paddle.tensor.split(im_info, axis=0, num_or_sections=3)
-        tensor_one = paddle.full(shape=[1], dtype='float32', fill_value=1.0)
-        tensor_zero = paddle.full(shape=[1], dtype='float32', fill_value=0.0)
-        h = paddle.subtract(h, tensor_one)
-        w = paddle.subtract(w, tensor_one)
-        xmin, ymin, xmax, ymax = paddle.tensor.split(
-            input, axis=-1, num_or_sections=4)
-        xmin = paddle.maximum(paddle.minimum(xmin, w), tensor_zero)
-        ymin = paddle.maximum(paddle.minimum(ymin, h), tensor_zero)
-        xmax = paddle.maximum(paddle.minimum(xmax, w), tensor_zero)
-        ymax = paddle.maximum(paddle.minimum(ymax, h), tensor_zero)
-        cliped_box = paddle.concat([xmin, ymin, xmax, ymax], axis=-1)
-
-        return {'Output': [cliped_box]}
-
-@op_mapper('box_clip')
-class Boxclip:
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        node = graph.make_node(
-            'box_clip',
-            inputs=node.input('Input')+node.input('ImInfo'),
-            outputs=node.output('Output'),
-            domain = 'custom')
-register_custom_paddle_op('box_clip', BoxClip)
+# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import numpy as np
+import paddle
+from paddle.fluid import layers
+from paddle2onnx.legacy.op_mapper import CustomPaddleOp, register_custom_paddle_op
+from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
+from paddle2onnx.legacy.op_mapper import mapper_helper
+
+class BoxClip(CustomPaddleOp):
+    def __init__(self, node, **kw):
+        super(BoxClip, self).__init__(node)
+
+    def forward(self):
+        input = self.input('Input', 0)
+        im_info = self.input('ImInfo', 0)
+        im_info = paddle.reshape(im_info, shape=[3])
+        h, w, s = paddle.tensor.split(im_info, axis=0, num_or_sections=3)
+        tensor_one = paddle.full(shape=[1], dtype='float32', fill_value=1.0)
+        tensor_zero = paddle.full(shape=[1], dtype='float32', fill_value=0.0)
+        h = paddle.subtract(h, tensor_one)
+        w = paddle.subtract(w, tensor_one)
+        xmin, ymin, xmax, ymax = paddle.tensor.split(
+            input, axis=-1, num_or_sections=4)
+        xmin = paddle.maximum(paddle.minimum(xmin, w), tensor_zero)
+        ymin = paddle.maximum(paddle.minimum(ymin, h), tensor_zero)
+        xmax = paddle.maximum(paddle.minimum(xmax, w), tensor_zero)
+        ymax = paddle.maximum(paddle.minimum(ymax, h), tensor_zero)
+        cliped_box = paddle.concat([xmin, ymin, xmax, ymax], axis=-1)
+
+        return {'Output': [cliped_box]}
+
+@op_mapper('box_clip')
+class Boxclip:
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        node = graph.make_node(
+            'box_clip',
+            inputs=node.input('Input')+node.input('ImInfo'),
+            outputs=node.output('Output'),
+            domain = 'custom')
+register_custom_paddle_op('box_clip', BoxClip)
```

## paddle2onnx/legacy/op_mapper/custom_paddle_op/collect_fpn_proposals.py

 * *Ordering differences only*

```diff
@@ -1,55 +1,55 @@
-# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import numpy as np
-import paddle
-from paddle.fluid import layers
-from paddle2onnx.legacy.op_mapper import CustomPaddleOp, register_custom_paddle_op
-from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
-from paddle2onnx.legacy.op_mapper import mapper_helper
-
-
-class CollectFpnProposals(CustomPaddleOp):
-    def __init__(self, node, **kw):
-        super(CollectFpnProposals, self).__init__(node)
-        self.post_nms_top_n = node.attr('post_nms_topN')
-
-    def forward(self):
-        multi_level_rois = self.input('MultiLevelRois')
-        multi_level_scores = self.input('MultiLevelScores')
-        multi_level_rois = paddle.concat(multi_level_rois, axis=0)
-        multi_level_scores = paddle.concat(multi_level_scores, axis=0)
-        proposal_num = paddle.shape(multi_level_scores)[0]
-        post_nms_top_n_tensor = paddle.assign(
-            np.array([self.post_nms_top_n]).astype('int32'))
-        k_candidate = paddle.concat([proposal_num, post_nms_top_n_tensor])
-        k = paddle.min(k_candidate)
-        scores, index = paddle.topk(multi_level_scores, k=k, axis=0)
-        rois = paddle.gather(multi_level_rois, index, axis=0)
-        return {"FpnRois": [rois]}
-
-@op_mapper('collect_fpn_proposals')
-class Collectfpnproposals:
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        node = graph.make_node(
-            'collect_fpn_proposals',
-            inputs=node.input('MultiLevelRois')+ node.input('MultiLevelScores'),
-            outputs=node.output('FpnRois'),
-            post_nms_top_n = node.attr('post_nms_topN'),
-            domain = 'custom')
-            
-register_custom_paddle_op('collect_fpn_proposals', CollectFpnProposals)
+# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import numpy as np
+import paddle
+from paddle.fluid import layers
+from paddle2onnx.legacy.op_mapper import CustomPaddleOp, register_custom_paddle_op
+from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
+from paddle2onnx.legacy.op_mapper import mapper_helper
+
+
+class CollectFpnProposals(CustomPaddleOp):
+    def __init__(self, node, **kw):
+        super(CollectFpnProposals, self).__init__(node)
+        self.post_nms_top_n = node.attr('post_nms_topN')
+
+    def forward(self):
+        multi_level_rois = self.input('MultiLevelRois')
+        multi_level_scores = self.input('MultiLevelScores')
+        multi_level_rois = paddle.concat(multi_level_rois, axis=0)
+        multi_level_scores = paddle.concat(multi_level_scores, axis=0)
+        proposal_num = paddle.shape(multi_level_scores)[0]
+        post_nms_top_n_tensor = paddle.assign(
+            np.array([self.post_nms_top_n]).astype('int32'))
+        k_candidate = paddle.concat([proposal_num, post_nms_top_n_tensor])
+        k = paddle.min(k_candidate)
+        scores, index = paddle.topk(multi_level_scores, k=k, axis=0)
+        rois = paddle.gather(multi_level_rois, index, axis=0)
+        return {"FpnRois": [rois]}
+
+@op_mapper('collect_fpn_proposals')
+class Collectfpnproposals:
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        node = graph.make_node(
+            'collect_fpn_proposals',
+            inputs=node.input('MultiLevelRois')+ node.input('MultiLevelScores'),
+            outputs=node.output('FpnRois'),
+            post_nms_top_n = node.attr('post_nms_topN'),
+            domain = 'custom')
+            
+register_custom_paddle_op('collect_fpn_proposals', CollectFpnProposals)
```

## paddle2onnx/legacy/op_mapper/custom_paddle_op/deformable_conv.py

 * *Ordering differences only*

```diff
@@ -1,296 +1,296 @@
-# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import numpy as np
-import paddle
-from paddle.fluid import layers
-from paddle2onnx.legacy.op_mapper import CustomPaddleOp, register_custom_paddle_op
-from paddle2onnx import utils
-from paddle2onnx.legacy.constant import dtypes
-from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
-from paddle2onnx.legacy.op_mapper import mapper_helper
-
-
-class DeformConv2d(CustomPaddleOp):
-    def check_attribute(self, node):
-        utils.compare_attr_between_dims(
-            node.attr('strides'), (0, 1), 'strides', 'equal')
-        utils.compare_attr_between_dims(
-            node.attr('paddings'), (0, 1), 'paddings', 'equal')
-        utils.compare_attr_between_dims(
-            node.input_shape('Offset', 0), (2, 3), 'Offset', 'equal')
-        utils.compare_attr(
-            node.attr('deformable_groups'), 1, 'deformable_groups', 'equal')
-
-    def __init__(self, node, **kw):
-        super(DeformConv2d, self).__init__(node)
-        self.check_attribute(node)
-        self.in_channel = node.input_shape('Input', 0)[1]
-        self.offset_channel = node.input_shape('Offset', 0)[1]
-        self.stride = node.attr('strides')[0]
-        self.padding = node.attr('paddings')
-        if len(self.padding) == 2:
-            self.padding += self.padding
-        self.groups = node.attr('groups')
-        self.dilation = node.attr('dilations')[0]
-        self.padded_x_h = node.input_shape('Input', 0)[2]
-        self.padded_x_w = node.input_shape('Input', 0)[3]
-        if self.padded_x_h > 0:
-            self.padded_x_h = self.padded_x_h + self.padding[0] + self.padding[1]
-        if self.padded_x_w > 0:
-            self.padded_x_w = self.padded_x_w + self.padding[2] + self.padding[3]
-
-        self.kernel_size = node.input_shape('Filter', 0)[2]
-        self.N = self.kernel_size**2
-        self.num_filters = node.input_shape('Filter', 0)[0]
-
-    def forward(self):
-        input = self.input('Input', 0)
-        weight = self.input('Filter', 0)
-        mask = self.input('Mask', 0)
-        offset = self.input('Offset', 0)
-
-        input = layers.pad2d(input, self.padding)
-        input_shape = paddle.shape(input)
-        if self.padded_x_h < 0 or self.padded_x_w < 0:
-            self.padded_x_h = input_shape[2]
-            self.padded_x_w = input_shape[3]
-
-        offset_x = paddle.strided_slice(
-            offset,
-            axes=[1],
-            starts=[0],
-            ends=[self.offset_channel],
-            strides=[2])
-        offset_y = paddle.strided_slice(
-            offset,
-            axes=[1],
-            starts=[1],
-            ends=[self.offset_channel],
-            strides=[2])
-        offset = paddle.concat([offset_x, offset_y], axis=1)
-        offset_shape = paddle.shape(offset)
-        offset_h = offset_shape[2]
-        offset_w = offset_shape[3]
-
-        coordinate = self.get_offset_coordinate(offset, 'float32', offset_shape)
-
-        coordinate = coordinate.transpose((0, 2, 3, 1))
-        coord_lt, coord_rb, coord_lb, coord_rt = self.get_bilinear_corner_coordinate(
-            coordinate, self.padded_x_h, self.padded_x_w)
-
-        # clip coordinate
-        coordinate = paddle.concat(
-            [
-                paddle.clip(coordinate[:, :, :, :self.N], 0,
-                            self.padded_x_h - 1),
-                paddle.clip(coordinate[:, :, :, self.N:], 0,
-                            self.padded_x_w - 1)
-            ],
-            axis=-1)
-
-        cof_lt, cof_rb, cof_lb, cof_rt = self.get_bilinear_coefficient(
-            coord_lt, coord_rb, coord_lb, coord_rt, coordinate)
-
-        feature_lt = self.get_feature_by_coordinate(input, coord_lt, offset_h,
-                                                    offset_w, self.padded_x_w)
-        feature_rb = self.get_feature_by_coordinate(input, coord_rb, offset_h,
-                                                    offset_w, self.padded_x_w)
-        feature_lb = self.get_feature_by_coordinate(input, coord_lb, offset_h,
-                                                    offset_w, self.padded_x_w)
-        feature_rt = self.get_feature_by_coordinate(input, coord_rt, offset_h,
-                                                    offset_w, self.padded_x_w)
-
-        feature_after_deformation = paddle.unsqueeze(cof_lt, 1) * feature_lt + \
-                   paddle.unsqueeze(cof_rb, 1) * feature_rb + \
-                   paddle.unsqueeze(cof_lb, 1) * feature_lb + \
-                   paddle.unsqueeze(cof_rt, 1) * feature_rt
-
-        # modulation
-        if mask is not None:
-            mask = paddle.transpose(mask, (0, 2, 3, 1))
-            mask = paddle.unsqueeze(mask, 1)
-            mask = paddle.tile(mask, [1, self.in_channel, 1, 1, 1])
-            feature_after_deformation *= mask
-
-        feature_after_deformation = self.reshape_feature(
-            feature_after_deformation, offset_h, offset_w)
-
-        out = paddle.nn.functional.conv2d(
-            feature_after_deformation,
-            weight,
-            stride=self.kernel_size,
-            groups=self.groups)
-
-        return {'Output': [out]}
-
-    def get_offset_coordinate(self, offset, dtype, offset_shape):
-        kernel_grid_origin_x = paddle.arange(
-            0,
-            self.kernel_size + (self.kernel_size - 1) * (self.dilation - 1),
-            step=self.dilation,
-            dtype=dtype)
-        kernel_grid_origin_x = kernel_grid_origin_x.unsqueeze(1)
-        kernel_grid_origin_x = paddle.tile(kernel_grid_origin_x,
-                                           [1, self.kernel_size])
-        kernel_grid_origin_y = paddle.arange(
-            0,
-            self.kernel_size + (self.kernel_size - 1) * (self.dilation - 1),
-            step=self.dilation,
-            dtype=dtype)
-        kernel_grid_origin_y = kernel_grid_origin_y.unsqueeze(0)
-        kernel_grid_origin_y = paddle.tile(kernel_grid_origin_y,
-                                           [self.kernel_size, 1])
-        kernel_grid_origin_x = paddle.reshape(kernel_grid_origin_x, [-1])
-        kernel_grid_origin_y = paddle.reshape(kernel_grid_origin_y, [-1])
-        kernel_grid_origin = paddle.concat(
-            [kernel_grid_origin_x, kernel_grid_origin_y], -1)
-        kernel_grid_origin = paddle.reshape(kernel_grid_origin,
-                                            (1, 2 * self.N, 1, 1))
-
-        kernel_offset_x = paddle.arange(
-            0, offset_shape[2] * self.stride, step=self.stride, dtype=dtype)
-        kernel_offset_x = kernel_offset_x.unsqueeze(1)
-        kernel_offset_x = paddle.expand(kernel_offset_x, offset_shape[2:])
-        kernel_offset_y = paddle.arange(
-            0, offset_shape[3] * self.stride, step=self.stride, dtype=dtype)
-        kernel_offset_y = kernel_offset_y.unsqueeze(0)
-        kernel_offset_y = paddle.expand(kernel_offset_y, offset_shape[2:])
-        kernel_offset_x = kernel_offset_x.unsqueeze([0, 1])
-        kernel_offset_x = paddle.tile(kernel_offset_x, (1, self.N, 1, 1))
-        kernel_offset_y = kernel_offset_y.unsqueeze([0, 1])
-        kernel_offset_y = paddle.tile(kernel_offset_y, (1, self.N, 1, 1))
-
-        kernel_offset = paddle.concat([kernel_offset_x, kernel_offset_y], 1)
-        offset = offset + paddle.cast(kernel_offset, 'float32') + paddle.cast(
-            kernel_grid_origin, 'float32')
-
-        return offset
-
-    def get_bilinear_corner_coordinate(self, coord, padded_h, padded_w):
-        coord_lt = coord.floor()
-        coord_rb = coord_lt + 1
-        coord_lt = paddle.cast(
-            paddle.concat(
-                [
-                    paddle.clip(coord_lt[:, :, :, :self.N], 0, padded_h - 1),
-                    paddle.clip(coord_lt[:, :, :, self.N:], 0, padded_w - 1)
-                ],
-                axis=-1),
-            dtype='int64')
-        coord_rb = paddle.cast(
-            paddle.concat(
-                [
-                    paddle.clip(coord_rb[:, :, :, :self.N], 0, padded_h - 1),
-                    paddle.clip(coord_rb[:, :, :, self.N:], 0, padded_w - 1)
-                ],
-                axis=-1),
-            dtype='int64')
-        coord_lb = paddle.concat(
-            [coord_lt[:, :, :, :self.N], coord_rb[:, :, :, self.N:]], axis=-1)
-        coord_rt = paddle.concat(
-            [coord_rb[:, :, :, :self.N], coord_lt[:, :, :, self.N:]], axis=-1)
-
-        return coord_lt, coord_rb, coord_lb, coord_rt
-
-    def get_bilinear_coefficient(self, coord_lt, coord_rb, coord_lb, coord_rt,
-                                 p):
-        cof_lt = (1 + (paddle.cast(
-            coord_lt[:, :, :, :self.N], dtype='float32') - p[:, :, :, :self.N])
-                  ) * (1 + paddle.cast(
-                      coord_lt[:, :, :, self.N:], dtype='float32') -
-                       p[:, :, :, self.N:])
-        cof_rb = (1 - (paddle.cast(
-            coord_rb[:, :, :, :self.N], dtype='float32') - p[:, :, :, :self.N])
-                  ) * (1 - (paddle.cast(
-                      coord_rb[:, :, :, self.N:], dtype='float32') -
-                            p[:, :, :, self.N:]))
-        cof_lb = (1 + (paddle.cast(
-            coord_lb[:, :, :, :self.N], dtype='float32') - p[:, :, :, :self.N])
-                  ) * (1 - (paddle.cast(
-                      coord_lb[:, :, :, self.N:], dtype='float32') -
-                            p[:, :, :, self.N:]))
-        cof_rt = (1 - (paddle.cast(
-            coord_rt[:, :, :, :self.N], dtype='float32') - p[:, :, :, :self.N])
-                  ) * (1 + paddle.cast(
-                      coord_rt[:, :, :, self.N:], dtype='float32') -
-                       p[:, :, :, self.N:])
-
-        return cof_lt, cof_rb, cof_lb, cof_rt
-
-    def get_feature_by_coordinate(self, x, coord, offset_h, offset_w,
-                                  padded_x_w):
-        x = paddle.reshape(x, [0, 0, -1])
-        index = paddle.cast(
-            coord[:, :, :, :self.N] * padded_x_w,
-            dtype='int64') + coord[:, :, :, self.N:]  # offset_x*w + offset_y
-        index = paddle.unsqueeze(index, 1)
-        index = paddle.tile(index, [1, self.in_channel, 1, 1, 1])
-        index = paddle.reshape(index, (0, 0, -1))
-        x_range = list(range(3))
-        dim = 2
-        x_range[0] = dim
-        x_range[dim] = 0
-        x_swaped = paddle.transpose(x, perm=x_range)
-        index_range = list(range(3))
-        index_range[0] = dim
-        index_range[dim] = 0
-        index_swaped = paddle.transpose(index, perm=index_range)
-        x_shape = layers.shape(x_swaped)
-        index_shape = layers.shape(index_swaped)
-        prod = paddle.prod(x_shape[1:], keepdim=True)
-        x_swaped_flattend = paddle.reshape(x_swaped, [-1])
-        index_swaped_flattend = paddle.reshape(index_swaped, [-1])
-        index_swaped_flattend *= prod
-        bias = paddle.arange(start=0, end=prod, step=1, dtype='float32')
-        bias = paddle.tile(bias, index_shape[0])
-        index_swaped_flattend += bias
-        gathered = paddle.gather(x_swaped_flattend, index_swaped_flattend)
-        gathered = paddle.reshape(gathered, layers.shape(index_swaped))
-        x_offset = paddle.transpose(gathered, perm=x_range)
-        x_offset = paddle.reshape(
-            x_offset, (-1, self.in_channel, offset_h, offset_w, self.N))
-        return x_offset
-
-    def reshape_feature(self, x_offset, offset_h, offset_w):
-        x_offset = paddle.concat(
-            [
-                paddle.reshape(x_offset[:, :, :, :, s:s + self.kernel_size], (
-                    -1, self.in_channel, offset_h, offset_w * self.kernel_size))
-                for s in range(0, self.N, self.kernel_size)
-            ],
-            axis=-1)
-        x_offset = paddle.reshape(x_offset, (-1, self.in_channel,
-                                             offset_h * self.kernel_size,
-                                             offset_w * self.kernel_size))
-        return x_offset
-
-@op_mapper('deformable_conv')
-class Deformconv2d:
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        node = graph.make_node(
-            'deformable_conv',
-            inputs=node.input('Input')+node.input('Filter')+node.input('Mask')+node.input('Offset'),
-            outputs=node.output('Output'),
-            stride = node.attr('strides'),
-            padding = node.attr('paddings'),
-            groups = node.attr('groups'),
-            dilation = node.attr('dilations'),
-            deformable_groups = node.attr('deformable_groups'),
-            domain = 'custom')
-            
-register_custom_paddle_op('deformable_conv', DeformConv2d)
+# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import numpy as np
+import paddle
+from paddle.fluid import layers
+from paddle2onnx.legacy.op_mapper import CustomPaddleOp, register_custom_paddle_op
+from paddle2onnx import utils
+from paddle2onnx.legacy.constant import dtypes
+from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
+from paddle2onnx.legacy.op_mapper import mapper_helper
+
+
+class DeformConv2d(CustomPaddleOp):
+    def check_attribute(self, node):
+        utils.compare_attr_between_dims(
+            node.attr('strides'), (0, 1), 'strides', 'equal')
+        utils.compare_attr_between_dims(
+            node.attr('paddings'), (0, 1), 'paddings', 'equal')
+        utils.compare_attr_between_dims(
+            node.input_shape('Offset', 0), (2, 3), 'Offset', 'equal')
+        utils.compare_attr(
+            node.attr('deformable_groups'), 1, 'deformable_groups', 'equal')
+
+    def __init__(self, node, **kw):
+        super(DeformConv2d, self).__init__(node)
+        self.check_attribute(node)
+        self.in_channel = node.input_shape('Input', 0)[1]
+        self.offset_channel = node.input_shape('Offset', 0)[1]
+        self.stride = node.attr('strides')[0]
+        self.padding = node.attr('paddings')
+        if len(self.padding) == 2:
+            self.padding += self.padding
+        self.groups = node.attr('groups')
+        self.dilation = node.attr('dilations')[0]
+        self.padded_x_h = node.input_shape('Input', 0)[2]
+        self.padded_x_w = node.input_shape('Input', 0)[3]
+        if self.padded_x_h > 0:
+            self.padded_x_h = self.padded_x_h + self.padding[0] + self.padding[1]
+        if self.padded_x_w > 0:
+            self.padded_x_w = self.padded_x_w + self.padding[2] + self.padding[3]
+
+        self.kernel_size = node.input_shape('Filter', 0)[2]
+        self.N = self.kernel_size**2
+        self.num_filters = node.input_shape('Filter', 0)[0]
+
+    def forward(self):
+        input = self.input('Input', 0)
+        weight = self.input('Filter', 0)
+        mask = self.input('Mask', 0)
+        offset = self.input('Offset', 0)
+
+        input = layers.pad2d(input, self.padding)
+        input_shape = paddle.shape(input)
+        if self.padded_x_h < 0 or self.padded_x_w < 0:
+            self.padded_x_h = input_shape[2]
+            self.padded_x_w = input_shape[3]
+
+        offset_x = paddle.strided_slice(
+            offset,
+            axes=[1],
+            starts=[0],
+            ends=[self.offset_channel],
+            strides=[2])
+        offset_y = paddle.strided_slice(
+            offset,
+            axes=[1],
+            starts=[1],
+            ends=[self.offset_channel],
+            strides=[2])
+        offset = paddle.concat([offset_x, offset_y], axis=1)
+        offset_shape = paddle.shape(offset)
+        offset_h = offset_shape[2]
+        offset_w = offset_shape[3]
+
+        coordinate = self.get_offset_coordinate(offset, 'float32', offset_shape)
+
+        coordinate = coordinate.transpose((0, 2, 3, 1))
+        coord_lt, coord_rb, coord_lb, coord_rt = self.get_bilinear_corner_coordinate(
+            coordinate, self.padded_x_h, self.padded_x_w)
+
+        # clip coordinate
+        coordinate = paddle.concat(
+            [
+                paddle.clip(coordinate[:, :, :, :self.N], 0,
+                            self.padded_x_h - 1),
+                paddle.clip(coordinate[:, :, :, self.N:], 0,
+                            self.padded_x_w - 1)
+            ],
+            axis=-1)
+
+        cof_lt, cof_rb, cof_lb, cof_rt = self.get_bilinear_coefficient(
+            coord_lt, coord_rb, coord_lb, coord_rt, coordinate)
+
+        feature_lt = self.get_feature_by_coordinate(input, coord_lt, offset_h,
+                                                    offset_w, self.padded_x_w)
+        feature_rb = self.get_feature_by_coordinate(input, coord_rb, offset_h,
+                                                    offset_w, self.padded_x_w)
+        feature_lb = self.get_feature_by_coordinate(input, coord_lb, offset_h,
+                                                    offset_w, self.padded_x_w)
+        feature_rt = self.get_feature_by_coordinate(input, coord_rt, offset_h,
+                                                    offset_w, self.padded_x_w)
+
+        feature_after_deformation = paddle.unsqueeze(cof_lt, 1) * feature_lt + \
+                   paddle.unsqueeze(cof_rb, 1) * feature_rb + \
+                   paddle.unsqueeze(cof_lb, 1) * feature_lb + \
+                   paddle.unsqueeze(cof_rt, 1) * feature_rt
+
+        # modulation
+        if mask is not None:
+            mask = paddle.transpose(mask, (0, 2, 3, 1))
+            mask = paddle.unsqueeze(mask, 1)
+            mask = paddle.tile(mask, [1, self.in_channel, 1, 1, 1])
+            feature_after_deformation *= mask
+
+        feature_after_deformation = self.reshape_feature(
+            feature_after_deformation, offset_h, offset_w)
+
+        out = paddle.nn.functional.conv2d(
+            feature_after_deformation,
+            weight,
+            stride=self.kernel_size,
+            groups=self.groups)
+
+        return {'Output': [out]}
+
+    def get_offset_coordinate(self, offset, dtype, offset_shape):
+        kernel_grid_origin_x = paddle.arange(
+            0,
+            self.kernel_size + (self.kernel_size - 1) * (self.dilation - 1),
+            step=self.dilation,
+            dtype=dtype)
+        kernel_grid_origin_x = kernel_grid_origin_x.unsqueeze(1)
+        kernel_grid_origin_x = paddle.tile(kernel_grid_origin_x,
+                                           [1, self.kernel_size])
+        kernel_grid_origin_y = paddle.arange(
+            0,
+            self.kernel_size + (self.kernel_size - 1) * (self.dilation - 1),
+            step=self.dilation,
+            dtype=dtype)
+        kernel_grid_origin_y = kernel_grid_origin_y.unsqueeze(0)
+        kernel_grid_origin_y = paddle.tile(kernel_grid_origin_y,
+                                           [self.kernel_size, 1])
+        kernel_grid_origin_x = paddle.reshape(kernel_grid_origin_x, [-1])
+        kernel_grid_origin_y = paddle.reshape(kernel_grid_origin_y, [-1])
+        kernel_grid_origin = paddle.concat(
+            [kernel_grid_origin_x, kernel_grid_origin_y], -1)
+        kernel_grid_origin = paddle.reshape(kernel_grid_origin,
+                                            (1, 2 * self.N, 1, 1))
+
+        kernel_offset_x = paddle.arange(
+            0, offset_shape[2] * self.stride, step=self.stride, dtype=dtype)
+        kernel_offset_x = kernel_offset_x.unsqueeze(1)
+        kernel_offset_x = paddle.expand(kernel_offset_x, offset_shape[2:])
+        kernel_offset_y = paddle.arange(
+            0, offset_shape[3] * self.stride, step=self.stride, dtype=dtype)
+        kernel_offset_y = kernel_offset_y.unsqueeze(0)
+        kernel_offset_y = paddle.expand(kernel_offset_y, offset_shape[2:])
+        kernel_offset_x = kernel_offset_x.unsqueeze([0, 1])
+        kernel_offset_x = paddle.tile(kernel_offset_x, (1, self.N, 1, 1))
+        kernel_offset_y = kernel_offset_y.unsqueeze([0, 1])
+        kernel_offset_y = paddle.tile(kernel_offset_y, (1, self.N, 1, 1))
+
+        kernel_offset = paddle.concat([kernel_offset_x, kernel_offset_y], 1)
+        offset = offset + paddle.cast(kernel_offset, 'float32') + paddle.cast(
+            kernel_grid_origin, 'float32')
+
+        return offset
+
+    def get_bilinear_corner_coordinate(self, coord, padded_h, padded_w):
+        coord_lt = coord.floor()
+        coord_rb = coord_lt + 1
+        coord_lt = paddle.cast(
+            paddle.concat(
+                [
+                    paddle.clip(coord_lt[:, :, :, :self.N], 0, padded_h - 1),
+                    paddle.clip(coord_lt[:, :, :, self.N:], 0, padded_w - 1)
+                ],
+                axis=-1),
+            dtype='int64')
+        coord_rb = paddle.cast(
+            paddle.concat(
+                [
+                    paddle.clip(coord_rb[:, :, :, :self.N], 0, padded_h - 1),
+                    paddle.clip(coord_rb[:, :, :, self.N:], 0, padded_w - 1)
+                ],
+                axis=-1),
+            dtype='int64')
+        coord_lb = paddle.concat(
+            [coord_lt[:, :, :, :self.N], coord_rb[:, :, :, self.N:]], axis=-1)
+        coord_rt = paddle.concat(
+            [coord_rb[:, :, :, :self.N], coord_lt[:, :, :, self.N:]], axis=-1)
+
+        return coord_lt, coord_rb, coord_lb, coord_rt
+
+    def get_bilinear_coefficient(self, coord_lt, coord_rb, coord_lb, coord_rt,
+                                 p):
+        cof_lt = (1 + (paddle.cast(
+            coord_lt[:, :, :, :self.N], dtype='float32') - p[:, :, :, :self.N])
+                  ) * (1 + paddle.cast(
+                      coord_lt[:, :, :, self.N:], dtype='float32') -
+                       p[:, :, :, self.N:])
+        cof_rb = (1 - (paddle.cast(
+            coord_rb[:, :, :, :self.N], dtype='float32') - p[:, :, :, :self.N])
+                  ) * (1 - (paddle.cast(
+                      coord_rb[:, :, :, self.N:], dtype='float32') -
+                            p[:, :, :, self.N:]))
+        cof_lb = (1 + (paddle.cast(
+            coord_lb[:, :, :, :self.N], dtype='float32') - p[:, :, :, :self.N])
+                  ) * (1 - (paddle.cast(
+                      coord_lb[:, :, :, self.N:], dtype='float32') -
+                            p[:, :, :, self.N:]))
+        cof_rt = (1 - (paddle.cast(
+            coord_rt[:, :, :, :self.N], dtype='float32') - p[:, :, :, :self.N])
+                  ) * (1 + paddle.cast(
+                      coord_rt[:, :, :, self.N:], dtype='float32') -
+                       p[:, :, :, self.N:])
+
+        return cof_lt, cof_rb, cof_lb, cof_rt
+
+    def get_feature_by_coordinate(self, x, coord, offset_h, offset_w,
+                                  padded_x_w):
+        x = paddle.reshape(x, [0, 0, -1])
+        index = paddle.cast(
+            coord[:, :, :, :self.N] * padded_x_w,
+            dtype='int64') + coord[:, :, :, self.N:]  # offset_x*w + offset_y
+        index = paddle.unsqueeze(index, 1)
+        index = paddle.tile(index, [1, self.in_channel, 1, 1, 1])
+        index = paddle.reshape(index, (0, 0, -1))
+        x_range = list(range(3))
+        dim = 2
+        x_range[0] = dim
+        x_range[dim] = 0
+        x_swaped = paddle.transpose(x, perm=x_range)
+        index_range = list(range(3))
+        index_range[0] = dim
+        index_range[dim] = 0
+        index_swaped = paddle.transpose(index, perm=index_range)
+        x_shape = layers.shape(x_swaped)
+        index_shape = layers.shape(index_swaped)
+        prod = paddle.prod(x_shape[1:], keepdim=True)
+        x_swaped_flattend = paddle.reshape(x_swaped, [-1])
+        index_swaped_flattend = paddle.reshape(index_swaped, [-1])
+        index_swaped_flattend *= prod
+        bias = paddle.arange(start=0, end=prod, step=1, dtype='float32')
+        bias = paddle.tile(bias, index_shape[0])
+        index_swaped_flattend += bias
+        gathered = paddle.gather(x_swaped_flattend, index_swaped_flattend)
+        gathered = paddle.reshape(gathered, layers.shape(index_swaped))
+        x_offset = paddle.transpose(gathered, perm=x_range)
+        x_offset = paddle.reshape(
+            x_offset, (-1, self.in_channel, offset_h, offset_w, self.N))
+        return x_offset
+
+    def reshape_feature(self, x_offset, offset_h, offset_w):
+        x_offset = paddle.concat(
+            [
+                paddle.reshape(x_offset[:, :, :, :, s:s + self.kernel_size], (
+                    -1, self.in_channel, offset_h, offset_w * self.kernel_size))
+                for s in range(0, self.N, self.kernel_size)
+            ],
+            axis=-1)
+        x_offset = paddle.reshape(x_offset, (-1, self.in_channel,
+                                             offset_h * self.kernel_size,
+                                             offset_w * self.kernel_size))
+        return x_offset
+
+@op_mapper('deformable_conv')
+class Deformconv2d:
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        node = graph.make_node(
+            'deformable_conv',
+            inputs=node.input('Input')+node.input('Filter')+node.input('Mask')+node.input('Offset'),
+            outputs=node.output('Output'),
+            stride = node.attr('strides'),
+            padding = node.attr('paddings'),
+            groups = node.attr('groups'),
+            dilation = node.attr('dilations'),
+            deformable_groups = node.attr('deformable_groups'),
+            domain = 'custom')
+            
+register_custom_paddle_op('deformable_conv', DeformConv2d)
```

## paddle2onnx/legacy/op_mapper/custom_paddle_op/distribute_fpn_proposals.py

 * *Ordering differences only*

```diff
@@ -1,100 +1,100 @@
-# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import numpy as np
-import paddle
-from paddle.fluid import layers
-from paddle2onnx.legacy.op_mapper import CustomPaddleOp, register_custom_paddle_op
-from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
-from paddle2onnx.legacy.op_mapper import mapper_helper
-
-
-class DistributeFpnProposals(CustomPaddleOp):
-    def __init__(self, node, **kw):
-        super(DistributeFpnProposals, self).__init__(node)
-        self.max_level = node.attr('max_level')
-        self.min_level = node.attr('min_level')
-        self.refer_level = node.attr('refer_level')
-        self.refer_scale = node.attr('refer_scale')
-        self.pixel_offset = node.attr('pixel_offset')
-
-    def bbox_area(self, boxes):
-        offset = 1 if self.pixel_offset else 0
-        xmin, ymin, xmax, ymax = paddle.tensor.split(
-            boxes, axis=1, num_or_sections=4)
-        width = xmax - xmin + offset
-        height = ymax - ymin + offset
-        areas = width * height
-        return areas
-
-    def forward(self):
-        fpn_rois = self.input('FpnRois', 0)
-        areas = self.bbox_area(fpn_rois)
-        scale = paddle.sqrt(areas)
-        num_level = self.max_level - self.min_level + 1
-        target_level = paddle.log(scale / self.refer_scale + 1e-06) / np.log(2)
-        target_level = paddle.floor(self.refer_level + target_level)
-        target_level = paddle.clip(
-            target_level, min=self.min_level, max=self.max_level)
-
-        rois = list()
-        rois_idx_order = list()
-        rois_num_per_level = list()
-
-        for level in range(self.min_level, self.max_level + 1):
-            level_tensor = paddle.full_like(target_level, fill_value=level)
-            res = paddle.equal(target_level, level_tensor)
-            res = paddle.squeeze(res, axis=1)
-            res = paddle.cast(res, dtype='int32')
-            index = paddle.nonzero(res)
-            roi = paddle.gather(fpn_rois, index, axis=0)
-            rois.append(roi)
-            rois_idx_order.append(index)
-            rois_num_per_level.append(paddle.shape(roi)[0])
-        rois_idx_order = paddle.concat(rois_idx_order, axis=0)
-        size = paddle.shape(rois_idx_order)[0]
-        _, rois_idx_restore = paddle.topk(
-            rois_idx_order, axis=0, sorted=True, largest=False, k=size)
-
-        rois_idx_restore = paddle.cast(rois_idx_restore, dtype='int32')
-        if len(self.input('RoisNum')) > 0:
-            # trick: to keep rois num
-            rois_num_per_level[0] += self.input('RoisNum', 0) * 0
-            return {
-                'MultiFpnRois': rois,
-                'RestoreIndex': [rois_idx_restore],
-                'MultiLevelRoIsNum': rois_num_per_level
-            }
-        else:
-            return {'MultiFpnRois': rois, 'RestoreIndex': [rois_idx_restore]}
-
-
-@op_mapper('distribute_fpn_proposals')
-class Distributefpnproposals:
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        node = graph.make_node(
-            'distribute_fpn_proposals',
-            inputs=node.input('FpnRois'),
-            outputs=node.output('MultiFpnRois') + node.output('RestoreIndex'),
-            max_level=node.attr('max_level'),
-            min_level=node.attr('min_level'),
-            refer_level=node.attr('refer_level'),
-            refer_scale=node.attr('refer_scale'),
-            domain='custom')
-
-
-register_custom_paddle_op('distribute_fpn_proposals', DistributeFpnProposals)
+# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import numpy as np
+import paddle
+from paddle.fluid import layers
+from paddle2onnx.legacy.op_mapper import CustomPaddleOp, register_custom_paddle_op
+from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
+from paddle2onnx.legacy.op_mapper import mapper_helper
+
+
+class DistributeFpnProposals(CustomPaddleOp):
+    def __init__(self, node, **kw):
+        super(DistributeFpnProposals, self).__init__(node)
+        self.max_level = node.attr('max_level')
+        self.min_level = node.attr('min_level')
+        self.refer_level = node.attr('refer_level')
+        self.refer_scale = node.attr('refer_scale')
+        self.pixel_offset = node.attr('pixel_offset')
+
+    def bbox_area(self, boxes):
+        offset = 1 if self.pixel_offset else 0
+        xmin, ymin, xmax, ymax = paddle.tensor.split(
+            boxes, axis=1, num_or_sections=4)
+        width = xmax - xmin + offset
+        height = ymax - ymin + offset
+        areas = width * height
+        return areas
+
+    def forward(self):
+        fpn_rois = self.input('FpnRois', 0)
+        areas = self.bbox_area(fpn_rois)
+        scale = paddle.sqrt(areas)
+        num_level = self.max_level - self.min_level + 1
+        target_level = paddle.log(scale / self.refer_scale + 1e-06) / np.log(2)
+        target_level = paddle.floor(self.refer_level + target_level)
+        target_level = paddle.clip(
+            target_level, min=self.min_level, max=self.max_level)
+
+        rois = list()
+        rois_idx_order = list()
+        rois_num_per_level = list()
+
+        for level in range(self.min_level, self.max_level + 1):
+            level_tensor = paddle.full_like(target_level, fill_value=level)
+            res = paddle.equal(target_level, level_tensor)
+            res = paddle.squeeze(res, axis=1)
+            res = paddle.cast(res, dtype='int32')
+            index = paddle.nonzero(res)
+            roi = paddle.gather(fpn_rois, index, axis=0)
+            rois.append(roi)
+            rois_idx_order.append(index)
+            rois_num_per_level.append(paddle.shape(roi)[0])
+        rois_idx_order = paddle.concat(rois_idx_order, axis=0)
+        size = paddle.shape(rois_idx_order)[0]
+        _, rois_idx_restore = paddle.topk(
+            rois_idx_order, axis=0, sorted=True, largest=False, k=size)
+
+        rois_idx_restore = paddle.cast(rois_idx_restore, dtype='int32')
+        if len(self.input('RoisNum')) > 0:
+            # trick: to keep rois num
+            rois_num_per_level[0] += self.input('RoisNum', 0) * 0
+            return {
+                'MultiFpnRois': rois,
+                'RestoreIndex': [rois_idx_restore],
+                'MultiLevelRoIsNum': rois_num_per_level
+            }
+        else:
+            return {'MultiFpnRois': rois, 'RestoreIndex': [rois_idx_restore]}
+
+
+@op_mapper('distribute_fpn_proposals')
+class Distributefpnproposals:
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        node = graph.make_node(
+            'distribute_fpn_proposals',
+            inputs=node.input('FpnRois'),
+            outputs=node.output('MultiFpnRois') + node.output('RestoreIndex'),
+            max_level=node.attr('max_level'),
+            min_level=node.attr('min_level'),
+            refer_level=node.attr('refer_level'),
+            refer_scale=node.attr('refer_scale'),
+            domain='custom')
+
+
+register_custom_paddle_op('distribute_fpn_proposals', DistributeFpnProposals)
```

## paddle2onnx/legacy/op_mapper/custom_paddle_op/generate_proposals.py

 * *Ordering differences only*

```diff
@@ -1,223 +1,223 @@
-# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import numpy as np
-import paddle
-import math
-from paddle.fluid import layers
-from paddle2onnx.legacy.op_mapper import CustomPaddleOp, register_custom_paddle_op
-from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
-from paddle2onnx.legacy.op_mapper import mapper_helper
-
-BBOX_CLIP_DEFAULT = math.log(1000.0 / 16.0)
-
-
-class GenerateProposals(CustomPaddleOp):
-    def __init__(self, node, **kw):
-        paddle.enable_static()
-        super(GenerateProposals, self).__init__(node)
-        self.eta = node.attr('eta')
-        self.min_size = node.attr('min_size')
-        self.nms_thresh = node.attr('nms_thresh')
-        self.post_nms_topN = node.attr('post_nms_topN')
-        self.pre_nms_topN = node.attr('pre_nms_topN')
-        self.type = node.type
-        if self.type == 'generate_proposals_v2':
-            self.pixel_offset = node.attr('pixel_offset')
-        else:
-            self.pixel_offset = True
-
-    def filter_boxes(self, boxes, im_w, im_h, im_s, min_size):
-        min_size = max(min_size, 1.0)
-        xmin, ymin, xmax, ymax = paddle.tensor.split(
-            boxes, axis=1, num_or_sections=4)
-        x_ctr = (xmax + xmin) / 2 + 0.5
-        y_ctr = (ymax + ymin) / 2 + 0.5
-        ws = (xmax - xmin) / im_s + 1
-        hs = (ymax - ymin) / im_s + 1
-
-        min_size = np.asarray([min_size], dtype='float32')
-        min_size = paddle.assign(min_size)
-        valid_flag_ws = paddle.greater_equal(ws, min_size)
-        valid_flag_hs = paddle.greater_equal(hs, min_size)
-        valid_flag_x = paddle.less_equal(x_ctr, im_w)
-        valid_flag_y = paddle.less_equal(y_ctr, im_h)
-        valid_flag = paddle.logical_and(valid_flag_ws, valid_flag_hs)
-        valid_flag = paddle.logical_and(valid_flag, valid_flag_x)
-        valid_flag = paddle.logical_and(valid_flag, valid_flag_y)
-        valid_flag = paddle.squeeze(valid_flag, axis=1)
-        valid_inds = paddle.nonzero(valid_flag)
-
-        return valid_inds
-
-    def filter_boxes_v2(self, boxes, im_w, im_h, min_size, pixel_offset=True):
-        min_size = max(min_size, 1.0)
-        xmin, ymin, xmax, ymax = paddle.tensor.split(
-            boxes, axis=1, num_or_sections=4)
-
-        offset = 1 if pixel_offset else 0
-        ws = (xmax - xmin) + offset
-        hs = (ymax - ymin) + offset
-
-        min_size = np.asarray([min_size], dtype='float32')
-        min_size = paddle.assign(min_size)
-        valid_flag_ws = paddle.greater_equal(ws, min_size)
-        valid_flag_hs = paddle.greater_equal(hs, min_size)
-        valid_flag = paddle.logical_and(valid_flag_ws, valid_flag_hs)
-        if pixel_offset:
-            x_ctr = xmin + ws / 2
-            y_ctr = ymin + hs / 2
-            valid_flag_x = paddle.less_equal(x_ctr, im_w)
-            valid_flag_y = paddle.less_equal(y_ctr, im_h)
-            valid_flag = paddle.logical_and(valid_flag, valid_flag_x)
-            valid_flag = paddle.logical_and(valid_flag, valid_flag_y)
-
-        valid_flag = paddle.squeeze(valid_flag, axis=1)
-        valid_inds = paddle.nonzero(valid_flag)
-        return valid_inds
-
-    def clip_tiled_boxes(self, im_w, im_h, input_boxes, pixel_offset=True):
-        offset = 1 if pixel_offset else 0
-        xmin, ymin, xmax, ymax = paddle.tensor.split(
-            input_boxes, axis=1, num_or_sections=4)
-        xmin = paddle.clip(xmin, max=im_w - offset, min=0)
-        ymin = paddle.clip(ymin, max=im_h - offset, min=0)
-        xmax = paddle.clip(xmax, max=im_w - offset, min=0)
-        ymax = paddle.clip(ymax, max=im_h - offset, min=0)
-        input_boxes = paddle.concat([xmin, ymin, xmax, ymax], axis=1)
-        return input_boxes
-
-    def box_encode(self, anchors, bbox_deltas, variances, pixel_offset=True):
-        offset = 1 if pixel_offset else 0
-        anchor_xmin, anchor_ymin, anchor_xmax, anchor_ymax = paddle.tensor.split(
-            anchors, axis=1, num_or_sections=4)
-        anchor_width = anchor_xmax - anchor_xmin + offset
-        anchor_height = anchor_ymax - anchor_ymin + offset
-        anchor_center_x = anchor_xmin + 0.5 * anchor_width
-        anchor_center_y = anchor_ymin + 0.5 * anchor_height
-        var_center_x, var_center_y, var_width, var_height = paddle.tensor.split(
-            variances, axis=1, num_or_sections=4)
-        delta_center_x, delta_center_y, delta_width, delta_height = paddle.tensor.split(
-            bbox_deltas, axis=1, num_or_sections=4)
-
-        bbox_center_x = var_center_x * delta_center_x * anchor_width + anchor_center_x
-        bbox_center_y = var_center_y * delta_center_y * anchor_height + anchor_center_y
-        bbox_width = paddle.exp(
-            paddle.clip(
-                var_width * delta_width, max=BBOX_CLIP_DEFAULT)) * anchor_width
-        bbox_height = paddle.exp(
-            paddle.clip(
-                var_height * delta_height,
-                max=BBOX_CLIP_DEFAULT)) * anchor_height
-
-        proposal_xmin = bbox_center_x - bbox_width / 2
-        proposal_ymin = bbox_center_y - bbox_height / 2
-        proposal_xmax = bbox_center_x + bbox_width / 2 - offset
-        proposal_ymax = bbox_center_y + bbox_height / 2 - offset
-        proposal = paddle.concat(
-            [proposal_xmin, proposal_ymin, proposal_xmax, proposal_ymax],
-            axis=1)
-        return proposal
-
-    def proposal_for_single_sample(self, anchors, bbox_deltas, im_info, scores,
-                                   variances):
-        proposal_num = paddle.shape(scores)[0]
-        pre_nms_top_n_tensor = paddle.assign(
-            np.asarray(
-                [self.pre_nms_topN], dtype='int32'))
-        k_candidate = paddle.concat([proposal_num, pre_nms_top_n_tensor])
-        k = paddle.min(k_candidate)
-        scores, index = paddle.topk(scores, k=k, axis=0)
-        bbox_deltas = paddle.gather(bbox_deltas, index, axis=0)
-        anchors = paddle.gather(anchors, index, axis=0)
-        variances = paddle.gather(variances, index, axis=0)
-
-        proposal = self.box_encode(anchors, bbox_deltas, variances,
-                                   self.pixel_offset)
-        if self.type == "generate_proposals_v2":
-            im_h, im_w = paddle.tensor.split(im_info, axis=1, num_or_sections=2)
-        else:
-            im_h, im_w, im_s = paddle.tensor.split(
-                im_info, axis=1, num_or_sections=3)
-        proposal = self.clip_tiled_boxes(im_w, im_h, proposal,
-                                         self.pixel_offset)
-
-        if self.type == "generate_proposals_v2":
-            keep = self.filter_boxes_v2(proposal, im_w, im_h, self.min_size,
-                                        self.pixel_offset)
-        else:
-            keep = self.filter_boxes(proposal, im_w, im_h, im_s, self.min_size)
-
-        tail_proposal = paddle.zeros(shape=[1, 4], dtype=proposal.dtype)
-        proposal_num = paddle.shape(proposal)[0]
-        tail_keep = paddle.reshape(proposal_num, shape=[1, 1])
-        tail_keep = paddle.cast(tail_keep, dtype=keep.dtype)
-        tail_scores = paddle.zeros(shape=[1, 1], dtype=scores.dtype)
-        # proposal = paddle.concat([proposal, tail_proposal])
-        # keep = paddle.concat([keep, tail_keep])
-        # scores = paddle.concat([scores, tail_scores])
-
-        bbox_sel = paddle.gather(proposal, keep, axis=0)
-        scores_sel = paddle.gather(scores, keep, axis=0)
-        proposal = paddle.unsqueeze(bbox_sel, axis=0)
-        scores = paddle.transpose(scores_sel, perm=[1, 0])
-        scores = paddle.unsqueeze(scores, axis=0)
-        out = layers.multiclass_nms(
-            proposal,
-            scores,
-            background_label=-1,
-            nms_top_k=self.pre_nms_topN,
-            score_threshold=-10000.,
-            keep_top_k=self.post_nms_topN,
-            nms_threshold=self.nms_thresh,
-            normalized=False if self.pixel_offset else True,
-            nms_eta=self.eta)
-        label, scores, proposal = paddle.tensor.split(
-            out, axis=1, num_or_sections=[1, 1, 4])
-        return scores, proposal
-
-    def forward(self):
-        anchors = self.input('Anchors', 0)
-        bboxdeltas = self.input('BboxDeltas', 0)
-        if self.type == 'generate_proposals_v2':
-            iminfo = self.input('ImShape', 0)
-        else:
-            iminfo = self.input('ImInfo', 0)
-        scores = self.input('Scores', 0)
-        variances = self.input('Variances', 0)
-
-        bboxdeltas = paddle.transpose(bboxdeltas, perm=[0, 2, 3, 1])
-        bboxdeltas = paddle.reshape(bboxdeltas, [-1, 4])
-        scores = paddle.transpose(scores, perm=[0, 2, 3, 1])
-        scores = paddle.reshape(scores, [-1, 1])
-        anchors = paddle.reshape(anchors, [-1, 4])
-        variances = paddle.reshape(variances, [-1, 4])
-
-        new_scores, proposals = self.proposal_for_single_sample(
-            anchors, bboxdeltas, iminfo, scores, variances)
-        if len(self.node.outputs) == 3:
-            rois_num = paddle.shape(new_scores)[0]
-            return {
-                'RpnRoiProbs': [new_scores],
-                'RpnRois': [proposals],
-                'RpnRoisNum': [rois_num]
-            }
-        else:
-            return {'RpnRoiProbs': [new_scores], 'RpnRois': [proposals]}
-
-
-register_custom_paddle_op('generate_proposals', GenerateProposals)
-register_custom_paddle_op('generate_proposals_v2', GenerateProposals)
+# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import numpy as np
+import paddle
+import math
+from paddle.fluid import layers
+from paddle2onnx.legacy.op_mapper import CustomPaddleOp, register_custom_paddle_op
+from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
+from paddle2onnx.legacy.op_mapper import mapper_helper
+
+BBOX_CLIP_DEFAULT = math.log(1000.0 / 16.0)
+
+
+class GenerateProposals(CustomPaddleOp):
+    def __init__(self, node, **kw):
+        paddle.enable_static()
+        super(GenerateProposals, self).__init__(node)
+        self.eta = node.attr('eta')
+        self.min_size = node.attr('min_size')
+        self.nms_thresh = node.attr('nms_thresh')
+        self.post_nms_topN = node.attr('post_nms_topN')
+        self.pre_nms_topN = node.attr('pre_nms_topN')
+        self.type = node.type
+        if self.type == 'generate_proposals_v2':
+            self.pixel_offset = node.attr('pixel_offset')
+        else:
+            self.pixel_offset = True
+
+    def filter_boxes(self, boxes, im_w, im_h, im_s, min_size):
+        min_size = max(min_size, 1.0)
+        xmin, ymin, xmax, ymax = paddle.tensor.split(
+            boxes, axis=1, num_or_sections=4)
+        x_ctr = (xmax + xmin) / 2 + 0.5
+        y_ctr = (ymax + ymin) / 2 + 0.5
+        ws = (xmax - xmin) / im_s + 1
+        hs = (ymax - ymin) / im_s + 1
+
+        min_size = np.asarray([min_size], dtype='float32')
+        min_size = paddle.assign(min_size)
+        valid_flag_ws = paddle.greater_equal(ws, min_size)
+        valid_flag_hs = paddle.greater_equal(hs, min_size)
+        valid_flag_x = paddle.less_equal(x_ctr, im_w)
+        valid_flag_y = paddle.less_equal(y_ctr, im_h)
+        valid_flag = paddle.logical_and(valid_flag_ws, valid_flag_hs)
+        valid_flag = paddle.logical_and(valid_flag, valid_flag_x)
+        valid_flag = paddle.logical_and(valid_flag, valid_flag_y)
+        valid_flag = paddle.squeeze(valid_flag, axis=1)
+        valid_inds = paddle.nonzero(valid_flag)
+
+        return valid_inds
+
+    def filter_boxes_v2(self, boxes, im_w, im_h, min_size, pixel_offset=True):
+        min_size = max(min_size, 1.0)
+        xmin, ymin, xmax, ymax = paddle.tensor.split(
+            boxes, axis=1, num_or_sections=4)
+
+        offset = 1 if pixel_offset else 0
+        ws = (xmax - xmin) + offset
+        hs = (ymax - ymin) + offset
+
+        min_size = np.asarray([min_size], dtype='float32')
+        min_size = paddle.assign(min_size)
+        valid_flag_ws = paddle.greater_equal(ws, min_size)
+        valid_flag_hs = paddle.greater_equal(hs, min_size)
+        valid_flag = paddle.logical_and(valid_flag_ws, valid_flag_hs)
+        if pixel_offset:
+            x_ctr = xmin + ws / 2
+            y_ctr = ymin + hs / 2
+            valid_flag_x = paddle.less_equal(x_ctr, im_w)
+            valid_flag_y = paddle.less_equal(y_ctr, im_h)
+            valid_flag = paddle.logical_and(valid_flag, valid_flag_x)
+            valid_flag = paddle.logical_and(valid_flag, valid_flag_y)
+
+        valid_flag = paddle.squeeze(valid_flag, axis=1)
+        valid_inds = paddle.nonzero(valid_flag)
+        return valid_inds
+
+    def clip_tiled_boxes(self, im_w, im_h, input_boxes, pixel_offset=True):
+        offset = 1 if pixel_offset else 0
+        xmin, ymin, xmax, ymax = paddle.tensor.split(
+            input_boxes, axis=1, num_or_sections=4)
+        xmin = paddle.clip(xmin, max=im_w - offset, min=0)
+        ymin = paddle.clip(ymin, max=im_h - offset, min=0)
+        xmax = paddle.clip(xmax, max=im_w - offset, min=0)
+        ymax = paddle.clip(ymax, max=im_h - offset, min=0)
+        input_boxes = paddle.concat([xmin, ymin, xmax, ymax], axis=1)
+        return input_boxes
+
+    def box_encode(self, anchors, bbox_deltas, variances, pixel_offset=True):
+        offset = 1 if pixel_offset else 0
+        anchor_xmin, anchor_ymin, anchor_xmax, anchor_ymax = paddle.tensor.split(
+            anchors, axis=1, num_or_sections=4)
+        anchor_width = anchor_xmax - anchor_xmin + offset
+        anchor_height = anchor_ymax - anchor_ymin + offset
+        anchor_center_x = anchor_xmin + 0.5 * anchor_width
+        anchor_center_y = anchor_ymin + 0.5 * anchor_height
+        var_center_x, var_center_y, var_width, var_height = paddle.tensor.split(
+            variances, axis=1, num_or_sections=4)
+        delta_center_x, delta_center_y, delta_width, delta_height = paddle.tensor.split(
+            bbox_deltas, axis=1, num_or_sections=4)
+
+        bbox_center_x = var_center_x * delta_center_x * anchor_width + anchor_center_x
+        bbox_center_y = var_center_y * delta_center_y * anchor_height + anchor_center_y
+        bbox_width = paddle.exp(
+            paddle.clip(
+                var_width * delta_width, max=BBOX_CLIP_DEFAULT)) * anchor_width
+        bbox_height = paddle.exp(
+            paddle.clip(
+                var_height * delta_height,
+                max=BBOX_CLIP_DEFAULT)) * anchor_height
+
+        proposal_xmin = bbox_center_x - bbox_width / 2
+        proposal_ymin = bbox_center_y - bbox_height / 2
+        proposal_xmax = bbox_center_x + bbox_width / 2 - offset
+        proposal_ymax = bbox_center_y + bbox_height / 2 - offset
+        proposal = paddle.concat(
+            [proposal_xmin, proposal_ymin, proposal_xmax, proposal_ymax],
+            axis=1)
+        return proposal
+
+    def proposal_for_single_sample(self, anchors, bbox_deltas, im_info, scores,
+                                   variances):
+        proposal_num = paddle.shape(scores)[0]
+        pre_nms_top_n_tensor = paddle.assign(
+            np.asarray(
+                [self.pre_nms_topN], dtype='int32'))
+        k_candidate = paddle.concat([proposal_num, pre_nms_top_n_tensor])
+        k = paddle.min(k_candidate)
+        scores, index = paddle.topk(scores, k=k, axis=0)
+        bbox_deltas = paddle.gather(bbox_deltas, index, axis=0)
+        anchors = paddle.gather(anchors, index, axis=0)
+        variances = paddle.gather(variances, index, axis=0)
+
+        proposal = self.box_encode(anchors, bbox_deltas, variances,
+                                   self.pixel_offset)
+        if self.type == "generate_proposals_v2":
+            im_h, im_w = paddle.tensor.split(im_info, axis=1, num_or_sections=2)
+        else:
+            im_h, im_w, im_s = paddle.tensor.split(
+                im_info, axis=1, num_or_sections=3)
+        proposal = self.clip_tiled_boxes(im_w, im_h, proposal,
+                                         self.pixel_offset)
+
+        if self.type == "generate_proposals_v2":
+            keep = self.filter_boxes_v2(proposal, im_w, im_h, self.min_size,
+                                        self.pixel_offset)
+        else:
+            keep = self.filter_boxes(proposal, im_w, im_h, im_s, self.min_size)
+
+        tail_proposal = paddle.zeros(shape=[1, 4], dtype=proposal.dtype)
+        proposal_num = paddle.shape(proposal)[0]
+        tail_keep = paddle.reshape(proposal_num, shape=[1, 1])
+        tail_keep = paddle.cast(tail_keep, dtype=keep.dtype)
+        tail_scores = paddle.zeros(shape=[1, 1], dtype=scores.dtype)
+        # proposal = paddle.concat([proposal, tail_proposal])
+        # keep = paddle.concat([keep, tail_keep])
+        # scores = paddle.concat([scores, tail_scores])
+
+        bbox_sel = paddle.gather(proposal, keep, axis=0)
+        scores_sel = paddle.gather(scores, keep, axis=0)
+        proposal = paddle.unsqueeze(bbox_sel, axis=0)
+        scores = paddle.transpose(scores_sel, perm=[1, 0])
+        scores = paddle.unsqueeze(scores, axis=0)
+        out = layers.multiclass_nms(
+            proposal,
+            scores,
+            background_label=-1,
+            nms_top_k=self.pre_nms_topN,
+            score_threshold=-10000.,
+            keep_top_k=self.post_nms_topN,
+            nms_threshold=self.nms_thresh,
+            normalized=False if self.pixel_offset else True,
+            nms_eta=self.eta)
+        label, scores, proposal = paddle.tensor.split(
+            out, axis=1, num_or_sections=[1, 1, 4])
+        return scores, proposal
+
+    def forward(self):
+        anchors = self.input('Anchors', 0)
+        bboxdeltas = self.input('BboxDeltas', 0)
+        if self.type == 'generate_proposals_v2':
+            iminfo = self.input('ImShape', 0)
+        else:
+            iminfo = self.input('ImInfo', 0)
+        scores = self.input('Scores', 0)
+        variances = self.input('Variances', 0)
+
+        bboxdeltas = paddle.transpose(bboxdeltas, perm=[0, 2, 3, 1])
+        bboxdeltas = paddle.reshape(bboxdeltas, [-1, 4])
+        scores = paddle.transpose(scores, perm=[0, 2, 3, 1])
+        scores = paddle.reshape(scores, [-1, 1])
+        anchors = paddle.reshape(anchors, [-1, 4])
+        variances = paddle.reshape(variances, [-1, 4])
+
+        new_scores, proposals = self.proposal_for_single_sample(
+            anchors, bboxdeltas, iminfo, scores, variances)
+        if len(self.node.outputs) == 3:
+            rois_num = paddle.shape(new_scores)[0]
+            return {
+                'RpnRoiProbs': [new_scores],
+                'RpnRois': [proposals],
+                'RpnRoisNum': [rois_num]
+            }
+        else:
+            return {'RpnRoiProbs': [new_scores], 'RpnRois': [proposals]}
+
+
+register_custom_paddle_op('generate_proposals', GenerateProposals)
+register_custom_paddle_op('generate_proposals_v2', GenerateProposals)
```

## paddle2onnx/legacy/op_mapper/custom_paddle_op/grid_sampler.py

 * *Ordering differences only*

```diff
@@ -1,151 +1,151 @@
-# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import numpy as np
-import paddle
-from paddle.fluid import layers
-from paddle2onnx.legacy.op_mapper import CustomPaddleOp, register_custom_paddle_op
-from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
-from paddle2onnx.legacy.op_mapper import mapper_helper
-
-
-class GridSampler(CustomPaddleOp):
-    def __init__(self, node, **kw):
-        super(GridSampler, self).__init__(node)
-        self.padding_mode = node.attr('padding_mode')
-        self.mode = node.attr('mode')
-        self.align_corners = node.attr('align_corners')
-
-    def paddle_bilinear_grid_sample(self, im, grid, align_corners=False):
-        # this code reference: https://mmcv.readthedocs.io/en/latest/_modules/mmcv/ops/point_sample.html
-        im_shape = paddle.shape(im)
-        n, c, h, w = paddle.split(im_shape, num_or_sections=4)
-        grid_shape = paddle.shape(grid)
-        gn, gh, gw, _ = paddle.split(grid_shape, num_or_sections=4)
-
-        # n, c, h, w = im.shape
-        # gn, gh, gw, _ = grid.shape
-        # assert n == gn
-
-        x = grid[:, :, :, 0]
-        y = grid[:, :, :, 1]
-
-        if align_corners:
-            x = ((x + 1) / 2) * (w - 1)
-            y = ((y + 1) / 2) * (h - 1)
-        else:
-            x = ((x + 1) * w - 1) / 2
-            y = ((y + 1) * h - 1) / 2
-
-        x = paddle.reshape(x, [n, -1])
-        y = paddle.reshape(y, [n, -1])
-
-        x0 = paddle.floor(x).astype('int64')
-        y0 = paddle.floor(y).astype('int64')
-        x1 = x0 + 1
-        y1 = y0 + 1
-
-        x1_cast = x1.astype(grid.dtype)
-        x0_cast = x0.astype(grid.dtype)
-        y1_cast = y1.astype(grid.dtype)
-        y0_cast = y0.astype(grid.dtype)
-        wa = paddle.unsqueeze(((x1_cast - x) * (y1_cast - y)), 1)
-        wb = paddle.unsqueeze(((x1_cast - x) * (y - y0_cast)), 1)
-        wc = paddle.unsqueeze(((x - x0_cast) * (y1_cast - y)), 1)
-        wd = paddle.unsqueeze(((x - x0_cast) * (y - y0_cast)), 1)
-
-        # Apply default for grid_sample function zero padding
-        im_padded = paddle.nn.functional.pad(im,
-                                             pad=[1, 1, 1, 1],
-                                             mode='constant',
-                                             value=0)
-        if im_padded.dtype != im.dtype:
-            im_padded = paddle.cast(im_padded, im.dtype)
-        padded_h = h + 2
-        padded_w = w + 2
-        # save points positions after padding
-        x0, x1, y0, y1 = x0 + 1, x1 + 1, y0 + 1, y1 + 1
-
-        # Clip coordinates to padded image size
-        tensor_zero = paddle.full(shape=[1], dtype='int64', fill_value=0.0)
-        tensor_padded_w = paddle.full(
-            shape=[1], dtype='int64', fill_value=padded_w - 1)
-        tensor_padded_h = paddle.full(
-            shape=[1], dtype='int64', fill_value=padded_h - 1)
-        x0 = paddle.where(x0 < 0, tensor_zero, x0)
-        x0 = paddle.where(x0 > padded_w - 1, tensor_padded_w, x0)
-        x1 = paddle.where(x1 < 0, tensor_zero, x1)
-        x1 = paddle.where(x1 > padded_w - 1, tensor_padded_w, x1)
-        y0 = paddle.where(y0 < 0, tensor_zero, y0)
-        y0 = paddle.where(y0 > padded_h - 1, tensor_padded_h, y0)
-        y1 = paddle.where(y1 < 0, tensor_zero, y1)
-        y1 = paddle.where(y1 > padded_h - 1, tensor_padded_h, y1)
-        im_padded = paddle.reshape(im_padded, [n, c, -1])
-
-        x0_y0 = paddle.expand(
-            paddle.unsqueeze((x0 + y0 * padded_w), 1), [-1, c, -1])
-        x0_y1 = paddle.expand(
-            paddle.unsqueeze((x0 + y1 * padded_w), 1), [-1, c, -1])
-        x1_y0 = paddle.expand(
-            paddle.unsqueeze((x1 + y0 * padded_w), 1), [-1, c, -1])
-        x1_y1 = paddle.expand(
-            paddle.unsqueeze((x1 + y1 * padded_w), 1), [-1, c, -1])
-
-        Ia = self.paddle_gather(im_padded, 2, x0_y0)
-        Ib = self.paddle_gather(im_padded, 2, x0_y1)
-        Ic = self.paddle_gather(im_padded, 2, x1_y0)
-        Id = self.paddle_gather(im_padded, 2, x1_y1)
-
-        return paddle.reshape((Ia * wa + Ib * wb + Ic * wc + Id * wd),
-                              [n, c, gh, gw])
-
-    def paddle_gather(self, x, dim, index):
-        # index_shape = index.shape
-        index_shape = paddle.shape(index)
-        x_shape = paddle.shape(x)
-        index_flatten = index.flatten()
-        if dim < 0:
-            dim = len(x.shape) + dim
-        nd_index = []
-        for k in range(len(x.shape)):
-            if k == dim:
-                nd_index.append(index_flatten)
-            else:
-                reshape_shape = [1] * len(x.shape)
-                x_shape_k = x_shape[k]
-                # x_shape_k = x.shape[k]
-                reshape_shape[k] = x_shape_k
-                x_arange = paddle.arange(x_shape_k, dtype=index.dtype)
-                x_arange = x_arange.reshape(reshape_shape)
-                dim_index = paddle.expand(x_arange, index_shape).flatten()
-                nd_index.append(dim_index)
-        ind2 = paddle.transpose(paddle.stack(nd_index), [1, 0]).astype("int64")
-        paddle_out = paddle.gather_nd(x, ind2).reshape(index_shape)
-        return paddle_out
-
-    def forward(self):
-        input = self.input('X', 0)
-        grid = self.input('Grid', 0)
-        if self.mode != 'bilinear' or self.padding_mode != 'zeros':
-            raise Exception(
-                "grid_sample only is supported with mode should be 'bilinear' and padding_mode should be 'zeros'"
-            )
-        res = self.paddle_bilinear_grid_sample(
-            input, grid, align_corners=self.align_corners)
-        return {'Output': [res]}
-
-
-register_custom_paddle_op('grid_sampler', GridSampler)
+# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import numpy as np
+import paddle
+from paddle.fluid import layers
+from paddle2onnx.legacy.op_mapper import CustomPaddleOp, register_custom_paddle_op
+from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
+from paddle2onnx.legacy.op_mapper import mapper_helper
+
+
+class GridSampler(CustomPaddleOp):
+    def __init__(self, node, **kw):
+        super(GridSampler, self).__init__(node)
+        self.padding_mode = node.attr('padding_mode')
+        self.mode = node.attr('mode')
+        self.align_corners = node.attr('align_corners')
+
+    def paddle_bilinear_grid_sample(self, im, grid, align_corners=False):
+        # this code reference: https://mmcv.readthedocs.io/en/latest/_modules/mmcv/ops/point_sample.html
+        im_shape = paddle.shape(im)
+        n, c, h, w = paddle.split(im_shape, num_or_sections=4)
+        grid_shape = paddle.shape(grid)
+        gn, gh, gw, _ = paddle.split(grid_shape, num_or_sections=4)
+
+        # n, c, h, w = im.shape
+        # gn, gh, gw, _ = grid.shape
+        # assert n == gn
+
+        x = grid[:, :, :, 0]
+        y = grid[:, :, :, 1]
+
+        if align_corners:
+            x = ((x + 1) / 2) * (w - 1)
+            y = ((y + 1) / 2) * (h - 1)
+        else:
+            x = ((x + 1) * w - 1) / 2
+            y = ((y + 1) * h - 1) / 2
+
+        x = paddle.reshape(x, [n, -1])
+        y = paddle.reshape(y, [n, -1])
+
+        x0 = paddle.floor(x).astype('int64')
+        y0 = paddle.floor(y).astype('int64')
+        x1 = x0 + 1
+        y1 = y0 + 1
+
+        x1_cast = x1.astype(grid.dtype)
+        x0_cast = x0.astype(grid.dtype)
+        y1_cast = y1.astype(grid.dtype)
+        y0_cast = y0.astype(grid.dtype)
+        wa = paddle.unsqueeze(((x1_cast - x) * (y1_cast - y)), 1)
+        wb = paddle.unsqueeze(((x1_cast - x) * (y - y0_cast)), 1)
+        wc = paddle.unsqueeze(((x - x0_cast) * (y1_cast - y)), 1)
+        wd = paddle.unsqueeze(((x - x0_cast) * (y - y0_cast)), 1)
+
+        # Apply default for grid_sample function zero padding
+        im_padded = paddle.nn.functional.pad(im,
+                                             pad=[1, 1, 1, 1],
+                                             mode='constant',
+                                             value=0)
+        if im_padded.dtype != im.dtype:
+            im_padded = paddle.cast(im_padded, im.dtype)
+        padded_h = h + 2
+        padded_w = w + 2
+        # save points positions after padding
+        x0, x1, y0, y1 = x0 + 1, x1 + 1, y0 + 1, y1 + 1
+
+        # Clip coordinates to padded image size
+        tensor_zero = paddle.full(shape=[1], dtype='int64', fill_value=0.0)
+        tensor_padded_w = paddle.full(
+            shape=[1], dtype='int64', fill_value=padded_w - 1)
+        tensor_padded_h = paddle.full(
+            shape=[1], dtype='int64', fill_value=padded_h - 1)
+        x0 = paddle.where(x0 < 0, tensor_zero, x0)
+        x0 = paddle.where(x0 > padded_w - 1, tensor_padded_w, x0)
+        x1 = paddle.where(x1 < 0, tensor_zero, x1)
+        x1 = paddle.where(x1 > padded_w - 1, tensor_padded_w, x1)
+        y0 = paddle.where(y0 < 0, tensor_zero, y0)
+        y0 = paddle.where(y0 > padded_h - 1, tensor_padded_h, y0)
+        y1 = paddle.where(y1 < 0, tensor_zero, y1)
+        y1 = paddle.where(y1 > padded_h - 1, tensor_padded_h, y1)
+        im_padded = paddle.reshape(im_padded, [n, c, -1])
+
+        x0_y0 = paddle.expand(
+            paddle.unsqueeze((x0 + y0 * padded_w), 1), [-1, c, -1])
+        x0_y1 = paddle.expand(
+            paddle.unsqueeze((x0 + y1 * padded_w), 1), [-1, c, -1])
+        x1_y0 = paddle.expand(
+            paddle.unsqueeze((x1 + y0 * padded_w), 1), [-1, c, -1])
+        x1_y1 = paddle.expand(
+            paddle.unsqueeze((x1 + y1 * padded_w), 1), [-1, c, -1])
+
+        Ia = self.paddle_gather(im_padded, 2, x0_y0)
+        Ib = self.paddle_gather(im_padded, 2, x0_y1)
+        Ic = self.paddle_gather(im_padded, 2, x1_y0)
+        Id = self.paddle_gather(im_padded, 2, x1_y1)
+
+        return paddle.reshape((Ia * wa + Ib * wb + Ic * wc + Id * wd),
+                              [n, c, gh, gw])
+
+    def paddle_gather(self, x, dim, index):
+        # index_shape = index.shape
+        index_shape = paddle.shape(index)
+        x_shape = paddle.shape(x)
+        index_flatten = index.flatten()
+        if dim < 0:
+            dim = len(x.shape) + dim
+        nd_index = []
+        for k in range(len(x.shape)):
+            if k == dim:
+                nd_index.append(index_flatten)
+            else:
+                reshape_shape = [1] * len(x.shape)
+                x_shape_k = x_shape[k]
+                # x_shape_k = x.shape[k]
+                reshape_shape[k] = x_shape_k
+                x_arange = paddle.arange(x_shape_k, dtype=index.dtype)
+                x_arange = x_arange.reshape(reshape_shape)
+                dim_index = paddle.expand(x_arange, index_shape).flatten()
+                nd_index.append(dim_index)
+        ind2 = paddle.transpose(paddle.stack(nd_index), [1, 0]).astype("int64")
+        paddle_out = paddle.gather_nd(x, ind2).reshape(index_shape)
+        return paddle_out
+
+    def forward(self):
+        input = self.input('X', 0)
+        grid = self.input('Grid', 0)
+        if self.mode != 'bilinear' or self.padding_mode != 'zeros':
+            raise Exception(
+                "grid_sample only is supported with mode should be 'bilinear' and padding_mode should be 'zeros'"
+            )
+        res = self.paddle_bilinear_grid_sample(
+            input, grid, align_corners=self.align_corners)
+        return {'Output': [res]}
+
+
+register_custom_paddle_op('grid_sampler', GridSampler)
```

## paddle2onnx/legacy/op_mapper/detection/__init__.py

 * *Ordering differences only*

```diff
@@ -1,13 +1,13 @@
-#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
```

## paddle2onnx/legacy/op_mapper/detection/box_coder.py

 * *Ordering differences only*

```diff
@@ -1,363 +1,363 @@
-# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import numpy as np
-from paddle2onnx.legacy.constant import dtypes
-from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
-from paddle2onnx.legacy.op_mapper import mapper_helper
-
-
-@op_mapper('box_coder')
-class BoxCoder():
-    """
-    we use the decode the prior box to target box,
-    we just use the decode mode to transform this op.
-    """
-    support_opset_verison_range = (7, 12)
-
-    @classmethod
-    def opset_7(cls, graph, node, **kw):
-        input_names = node.input_names
-
-        t_size = node.input_shape('TargetBox', 0)
-        p_size = node.input_shape('PriorBox', 0)
-
-        # get the outout_name
-        result_name = node.output('OutputBox', 0)
-        # n is size of batch, m is boxes num of targe_boxes
-        n = t_size[0]
-        m = t_size[0]
-
-        axis = int(node.attr('axis'))
-
-        #norm
-        norm = bool(node.attr('box_normalized'))
-
-        name_slice_x1 = node.output('OutputBox')[0] + "@x1"
-        name_slice_y1 = node.output('OutputBox')[0] + "@y1"
-        name_slice_x2 = node.output('OutputBox')[0] + "@x2"
-        name_slice_y2 = node.output('OutputBox')[0] + "@y2"
-
-        #make onnx tensor to save the intermeidate reslut
-        name_slice_indices = [
-            [node.output('OutputBox')[0] + "@slice_" + str(i)]
-            for i in range(1, 3)
-        ]
-        node_slice_indices = [None for i in range(1, 3)]
-
-        # create the range(0, 4) const data to slice
-        for i in range(1, 3):
-            tmp_node = graph.make_node(
-                'Constant',
-                inputs=[],
-                outputs=name_slice_indices[i - 1],
-                dtype=dtypes.ONNX.FLOAT,
-                dims=(),
-                value=[i])
-        # make node split data
-        name_box_split = [
-            name_slice_x1, name_slice_y1, name_slice_x2, name_slice_y2
-        ]
-        split_shape = list(p_size)
-        split_shape[-1] = 1
-
-        node_split_prior_node = graph.make_node(
-            'Split',
-            inputs=node.input('PriorBox'),
-            outputs=name_box_split,
-            axis=1)
-
-        # make node get centor node for decode
-        final_outputs_vars = []
-        if not norm:
-            name_centor_w_tmp = [node.output('OutputBox')[0] + "@centor_w_tmp"]
-            name_centor_h_tmp = [node.output('OutputBox')[0] + "@centor_h_tmp"]
-            node_centor_w_tmp = None
-            node_centor_h_tmp = None
-            name_centor_tmp_list = [name_centor_w_tmp, name_centor_h_tmp]
-            node_centor_tmp_list = [node_centor_w_tmp, node_centor_h_tmp]
-
-            count = 2
-            for (name, op_node) in zip(name_centor_tmp_list,
-                                       node_centor_tmp_list):
-                tmp_node = graph.make_node('Add',
-                       inputs=[node.output('OutputBox')[0] + "@slice_" + str(1)]\
-                           + [name_box_split[count]],
-                       outputs=name)
-                count = count + 1
-        if not norm:
-            inputs_sub = [[name_centor_w_tmp[0], name_box_split[0]],
-                          [name_centor_h_tmp[0], name_box_split[1]]]
-        else:
-            inputs_sub = [[name_box_split[2], name_box_split[0]],
-                          [name_box_split[3], name_box_split[1]]]
-        outputs_sub = [result_name + "@pb_w", result_name + "@pb_h"]
-        for i in range(0, 2):
-            tmp_node = graph.make_node(
-                'Sub', inputs=inputs_sub[i], outputs=[outputs_sub[i]])
-        # according to prior_box height and weight to get centor x, y
-        name_half_value = [result_name + "@half_value"]
-        node_half_value = graph.make_node(
-            'Constant',
-            inputs=[],
-            outputs=name_half_value,
-            dtype=dtypes.ONNX.FLOAT,
-            dims=(),
-            value=[0.5])
-        outputs_half_wh = [[result_name + "@pb_w_half"],
-                           [result_name + "@pb_h_half"]]
-        inputs_half_wh = [[result_name + "@pb_w", name_half_value[0]],
-                          [result_name + "@pb_h", name_half_value[0]]]
-
-        for i in range(0, 2):
-            tmp_node = graph.make_node(
-                'Mul', inputs=inputs_half_wh[i], outputs=outputs_half_wh[i])
-
-        inputs_centor_xy = [[outputs_half_wh[0][0], name_slice_x1],
-                            [outputs_half_wh[1][0], name_slice_y1]]
-
-        outputs_centor_xy = [[result_name + "@pb_x"], [result_name + "@pb_y"]]
-
-        # final calc the centor x ,y
-        for i in range(0, 2):
-            tmp_node = graph.make_node(
-                'Add', inputs=inputs_centor_xy[i], outputs=outputs_centor_xy[i])
-        # reshape the data
-        shape = (1, split_shape[0]) if axis == 0 else (split_shape[0], 1)
-
-        # need to reshape the data
-        inputs_transpose_pb = [
-            [result_name + "@pb_w"],
-            [result_name + "@pb_h"],
-            [result_name + "@pb_x"],
-            [result_name + "@pb_y"],
-        ]
-        outputs_transpose_pb = [
-            [result_name + "@pb_w_transpose"],
-            [result_name + "@pb_h_transpose"],
-            [result_name + "@pb_x_transpose"],
-            [result_name + "@pb_y_transpose"],
-        ]
-        if axis == 0:
-            name_reshape_pb = [result_name + "@pb_transpose"]
-            # reshape the data
-            for i in range(0, 4):
-                tmp_node = graph.make_node(
-                    'Transpose',
-                    inputs=inputs_transpose_pb[i],
-                    outputs=outputs_transpose_pb[i])
-        # decoder the box according to the target_box and variacne
-        name_variance_raw = [result_name + "@variance_raw"]
-        name_variance_unsqueeze = [result_name + "@variance_unsqueeze"]
-        shape = []
-        # make node to extend the data
-        var_split_axis = 0
-        var_split_inputs_name = []
-        if 'PriorBoxVar' in input_names and len(node.input('PriorBoxVar')) > 0:
-            if axis == 1:
-                raise Exception(
-                    "The op box_coder has variable do not support aixs broadcast"
-                )
-            axes = []
-            var_split_inputs_name = [result_name + "@variance_split"]
-            tmp_node = graph.make_node(
-                'Transpose',
-                inputs=node.input('PriorBoxVar'),
-                outputs=var_split_inputs_name)
-            var_split_axis = 0
-        else:
-            variances = [1.0, 1.0, 1.0, 1.0]
-            if 'variance' in node.attrs and len(node.attr('variance')) > 0:
-                variances = [float(var) for var in node.attr('variance')]
-            node_variance_create = graph.make_node(
-                'Constant',
-                inputs=[],
-                outputs=name_variance_raw,
-                dtype=dtypes.ONNX.FLOAT,
-                dims=[len(variances)],
-                value=variances)
-            var_split_axis = 0
-            var_split_inputs_name = name_variance_raw
-
-        # decode the result
-        outputs_split_variance = [
-            result_name + "@variance_split" + str(i) for i in range(0, 4)
-        ]
-        outputs_split_targebox = [
-            result_name + "@targebox_split" + str(i) for i in range(0, 4)
-        ]
-        node_split_var = graph.make_node(
-            'Split',
-            inputs=var_split_inputs_name,
-            outputs=outputs_split_variance,
-            axis=var_split_axis)
-        node_split_target = graph.make_node(
-            'Split',
-            inputs=node.input('TargetBox'),
-            outputs=outputs_split_targebox,
-            axis=2)
-
-        outputs_squeeze_targebox = [
-            result_name + "@targebox_squeeze" + str(i) for i in range(0, 4)
-        ]
-        for (input_name, output_name) in zip(outputs_split_targebox,
-                                             outputs_squeeze_targebox):
-            tmp_node = mapper_helper.squeeze_helper(graph, input_name, [2],
-                                                    [output_name])
-
-        output_shape_step1 = list(t_size)[:-1]
-
-        inputs_tb_step1 = [
-            [outputs_squeeze_targebox[0], outputs_split_variance[0]],
-            [outputs_squeeze_targebox[1], outputs_split_variance[1]],
-            [outputs_squeeze_targebox[2], outputs_split_variance[2]],
-            [outputs_squeeze_targebox[3], outputs_split_variance[3]]
-        ]
-        outputs_tb_step1 = [[result_name + "@decode_x_step1"],
-                            [result_name + "@decode_y_step1"],
-                            [result_name + "@decode_w_step1"],
-                            [result_name + "@decode_h_step1"]]
-
-        for input_step1, output_step_1 in zip(inputs_tb_step1,
-                                              outputs_tb_step1):
-            tmp_node = graph.make_node(
-                'Mul', inputs=input_step1, outputs=output_step_1)
-        if axis == 0:
-            inputs_tbxy_step2 = [[
-                outputs_tb_step1[0][0], outputs_transpose_pb[0][0]
-            ], [outputs_tb_step1[1][0], outputs_transpose_pb[1][0]]]
-        else:
-            inputs_tbxy_step2 = [[
-                outputs_tb_step1[0][0], inputs_transpose_pb[0][0]
-            ], [outputs_tb_step1[1][0], inputs_transpose_pb[1][0]]]
-
-        outputs_tbxy_step2 = [[result_name + "@decode_x_step2"],
-                              [result_name + "@decode_y_step2"]]
-
-        for input_step2, output_step_2 in zip(inputs_tbxy_step2,
-                                              outputs_tbxy_step2):
-            tmp_node = graph.make_node(
-                'Mul', inputs=input_step2, outputs=output_step_2)
-        if axis == 0:
-            inputs_tbxy_step3 = [[
-                outputs_tbxy_step2[0][0], outputs_transpose_pb[2][0]
-            ], [outputs_tbxy_step2[1][0], outputs_transpose_pb[3][0]]]
-        else:
-            inputs_tbxy_step3 = [[
-                outputs_tbxy_step2[0][0], inputs_transpose_pb[2][0]
-            ], [outputs_tbxy_step2[1][0], inputs_transpose_pb[3][0]]]
-
-        outputs_tbxy_step3 = [[result_name + "@decode_x_step3"],
-                              [result_name + "@decode_y_step3"]]
-
-        for input_step3, output_step_3 in zip(inputs_tbxy_step3,
-                                              outputs_tbxy_step3):
-            tmp_node = graph.make_node(
-                'Add', inputs=input_step3, outputs=output_step_3)
-
-        # deal with width & height
-        inputs_tbwh_step2 = [outputs_tb_step1[2], outputs_tb_step1[3]]
-        outputs_tbwh_step2 = [[result_name + "@decode_w_step2"],
-                              [result_name + "@decode_h_step2"]]
-
-        for input_name, output_name in zip(inputs_tbwh_step2,
-                                           outputs_tbwh_step2):
-            tmp_node = graph.make_node(
-                'Exp', inputs=input_name, outputs=output_name)
-
-        if axis == 0:
-            inputs_tbwh_step3 = [[
-                outputs_tbwh_step2[0][0], outputs_transpose_pb[0][0]
-            ], [outputs_tbwh_step2[1][0], outputs_transpose_pb[1][0]]]
-        else:
-            inputs_tbwh_step3 = [[
-                outputs_tbwh_step2[0][0], inputs_transpose_pb[0][0]
-            ], [outputs_tbwh_step2[1][0], inputs_transpose_pb[1][0]]]
-
-        outputs_tbwh_step3 = [[result_name + "@decode_w_step3"],
-                              [result_name + "@decode_h_step3"]]
-
-        for input_name, output_name in zip(inputs_tbwh_step3,
-                                           outputs_tbwh_step3):
-            tmp_node = graph.make_node(
-                'Mul', inputs=input_name, outputs=output_name)
-
-        # final step to calc the result, and concat the result to output
-        # return the output box, [(x1, y1), (x2, y2)]
-
-        inputs_half_tbwh_step4 = [[
-            outputs_tbwh_step3[0][0], result_name + "@slice_2"
-        ], [outputs_tbwh_step3[1][0], result_name + "@slice_2"]]
-
-        outputs_half_tbwh_step4 = [[result_name + "@decode_half_w_step4"],
-                                   [result_name + "@decode_half_h_step4"]]
-        for inputs_name, outputs_name in zip(inputs_half_tbwh_step4,
-                                             outputs_half_tbwh_step4):
-            tmp_node = graph.make_node(
-                'Div', inputs=inputs_name, outputs=outputs_name)
-        inputs_output_point1 = [[
-            outputs_tbxy_step3[0][0], outputs_half_tbwh_step4[0][0]
-        ], [outputs_tbxy_step3[1][0], outputs_half_tbwh_step4[1][0]]]
-
-        outputs_output_point1 = [[result_name + "@ouput_x1"],
-                                 [result_name + "@output_y1"]]
-        for input_name, output_name in zip(inputs_output_point1,
-                                           outputs_output_point1):
-            tmp_node = graph.make_node(
-                'Sub', inputs=input_name, outputs=output_name)
-
-        inputs_output_point2 = [[
-            outputs_tbxy_step3[0][0], outputs_half_tbwh_step4[0][0]
-        ], [outputs_tbxy_step3[1][0], outputs_half_tbwh_step4[1][0]]]
-
-        outputs_output_point2 = [[result_name + "@ouput_x2"],
-                                 [result_name + "@output_y2"]]
-
-        for input_name, output_name in zip(inputs_output_point2,
-                                           outputs_output_point2):
-            tmp_node = graph.make_node(
-                'Add', inputs=input_name, outputs=output_name)
-        if not norm:
-            inputs_unnorm_point2 = [[
-                outputs_output_point2[0][0], result_name + "@slice_1"
-            ], [outputs_output_point2[1][0], result_name + "@slice_1"]]
-            outputs_unnorm_point2 = [[result_name + "@ouput_unnorm_x2"],
-                                     [result_name + "@ouput_unnorm_y2"]]
-
-            for input_name, output_name in zip(inputs_unnorm_point2,
-                                               outputs_unnorm_point2):
-                tmp_node = graph.make_node(
-                    'Sub', inputs=input_name, outputs=output_name)
-            outputs_output_point2 = outputs_unnorm_point2
-
-        outputs_output_point1.extend(outputs_output_point2)
-        ouputs_points_unsqueeze = [[result_name + "@points_unsqueeze_x1"],
-                                   [result_name + "points_unsqueeze_y1"],
-                                   [result_name + "points_unsqueeze_x2"],
-                                   [result_name + "points_unsqueeze_y2"]]
-
-        for input_name, output_name in zip(outputs_output_point1,
-                                           ouputs_points_unsqueeze):
-            tmp_node = mapper_helper.unsqueeze_helper(
-                graph, input_name, [len(output_shape_step1)], output_name)
-        outputs_points_unsqueeze_list = [
-            output[0] for output in ouputs_points_unsqueeze
-        ]
-        node_point_final = graph.make_node(
-            'Concat',
-            inputs=outputs_points_unsqueeze_list,
-            outputs=node.output('OutputBox'),
-            axis=len(output_shape_step1))
+# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import numpy as np
+from paddle2onnx.legacy.constant import dtypes
+from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
+from paddle2onnx.legacy.op_mapper import mapper_helper
+
+
+@op_mapper('box_coder')
+class BoxCoder():
+    """
+    we use the decode the prior box to target box,
+    we just use the decode mode to transform this op.
+    """
+    support_opset_verison_range = (7, 12)
+
+    @classmethod
+    def opset_7(cls, graph, node, **kw):
+        input_names = node.input_names
+
+        t_size = node.input_shape('TargetBox', 0)
+        p_size = node.input_shape('PriorBox', 0)
+
+        # get the outout_name
+        result_name = node.output('OutputBox', 0)
+        # n is size of batch, m is boxes num of targe_boxes
+        n = t_size[0]
+        m = t_size[0]
+
+        axis = int(node.attr('axis'))
+
+        #norm
+        norm = bool(node.attr('box_normalized'))
+
+        name_slice_x1 = node.output('OutputBox')[0] + "@x1"
+        name_slice_y1 = node.output('OutputBox')[0] + "@y1"
+        name_slice_x2 = node.output('OutputBox')[0] + "@x2"
+        name_slice_y2 = node.output('OutputBox')[0] + "@y2"
+
+        #make onnx tensor to save the intermeidate reslut
+        name_slice_indices = [
+            [node.output('OutputBox')[0] + "@slice_" + str(i)]
+            for i in range(1, 3)
+        ]
+        node_slice_indices = [None for i in range(1, 3)]
+
+        # create the range(0, 4) const data to slice
+        for i in range(1, 3):
+            tmp_node = graph.make_node(
+                'Constant',
+                inputs=[],
+                outputs=name_slice_indices[i - 1],
+                dtype=dtypes.ONNX.FLOAT,
+                dims=(),
+                value=[i])
+        # make node split data
+        name_box_split = [
+            name_slice_x1, name_slice_y1, name_slice_x2, name_slice_y2
+        ]
+        split_shape = list(p_size)
+        split_shape[-1] = 1
+
+        node_split_prior_node = graph.make_node(
+            'Split',
+            inputs=node.input('PriorBox'),
+            outputs=name_box_split,
+            axis=1)
+
+        # make node get centor node for decode
+        final_outputs_vars = []
+        if not norm:
+            name_centor_w_tmp = [node.output('OutputBox')[0] + "@centor_w_tmp"]
+            name_centor_h_tmp = [node.output('OutputBox')[0] + "@centor_h_tmp"]
+            node_centor_w_tmp = None
+            node_centor_h_tmp = None
+            name_centor_tmp_list = [name_centor_w_tmp, name_centor_h_tmp]
+            node_centor_tmp_list = [node_centor_w_tmp, node_centor_h_tmp]
+
+            count = 2
+            for (name, op_node) in zip(name_centor_tmp_list,
+                                       node_centor_tmp_list):
+                tmp_node = graph.make_node('Add',
+                       inputs=[node.output('OutputBox')[0] + "@slice_" + str(1)]\
+                           + [name_box_split[count]],
+                       outputs=name)
+                count = count + 1
+        if not norm:
+            inputs_sub = [[name_centor_w_tmp[0], name_box_split[0]],
+                          [name_centor_h_tmp[0], name_box_split[1]]]
+        else:
+            inputs_sub = [[name_box_split[2], name_box_split[0]],
+                          [name_box_split[3], name_box_split[1]]]
+        outputs_sub = [result_name + "@pb_w", result_name + "@pb_h"]
+        for i in range(0, 2):
+            tmp_node = graph.make_node(
+                'Sub', inputs=inputs_sub[i], outputs=[outputs_sub[i]])
+        # according to prior_box height and weight to get centor x, y
+        name_half_value = [result_name + "@half_value"]
+        node_half_value = graph.make_node(
+            'Constant',
+            inputs=[],
+            outputs=name_half_value,
+            dtype=dtypes.ONNX.FLOAT,
+            dims=(),
+            value=[0.5])
+        outputs_half_wh = [[result_name + "@pb_w_half"],
+                           [result_name + "@pb_h_half"]]
+        inputs_half_wh = [[result_name + "@pb_w", name_half_value[0]],
+                          [result_name + "@pb_h", name_half_value[0]]]
+
+        for i in range(0, 2):
+            tmp_node = graph.make_node(
+                'Mul', inputs=inputs_half_wh[i], outputs=outputs_half_wh[i])
+
+        inputs_centor_xy = [[outputs_half_wh[0][0], name_slice_x1],
+                            [outputs_half_wh[1][0], name_slice_y1]]
+
+        outputs_centor_xy = [[result_name + "@pb_x"], [result_name + "@pb_y"]]
+
+        # final calc the centor x ,y
+        for i in range(0, 2):
+            tmp_node = graph.make_node(
+                'Add', inputs=inputs_centor_xy[i], outputs=outputs_centor_xy[i])
+        # reshape the data
+        shape = (1, split_shape[0]) if axis == 0 else (split_shape[0], 1)
+
+        # need to reshape the data
+        inputs_transpose_pb = [
+            [result_name + "@pb_w"],
+            [result_name + "@pb_h"],
+            [result_name + "@pb_x"],
+            [result_name + "@pb_y"],
+        ]
+        outputs_transpose_pb = [
+            [result_name + "@pb_w_transpose"],
+            [result_name + "@pb_h_transpose"],
+            [result_name + "@pb_x_transpose"],
+            [result_name + "@pb_y_transpose"],
+        ]
+        if axis == 0:
+            name_reshape_pb = [result_name + "@pb_transpose"]
+            # reshape the data
+            for i in range(0, 4):
+                tmp_node = graph.make_node(
+                    'Transpose',
+                    inputs=inputs_transpose_pb[i],
+                    outputs=outputs_transpose_pb[i])
+        # decoder the box according to the target_box and variacne
+        name_variance_raw = [result_name + "@variance_raw"]
+        name_variance_unsqueeze = [result_name + "@variance_unsqueeze"]
+        shape = []
+        # make node to extend the data
+        var_split_axis = 0
+        var_split_inputs_name = []
+        if 'PriorBoxVar' in input_names and len(node.input('PriorBoxVar')) > 0:
+            if axis == 1:
+                raise Exception(
+                    "The op box_coder has variable do not support aixs broadcast"
+                )
+            axes = []
+            var_split_inputs_name = [result_name + "@variance_split"]
+            tmp_node = graph.make_node(
+                'Transpose',
+                inputs=node.input('PriorBoxVar'),
+                outputs=var_split_inputs_name)
+            var_split_axis = 0
+        else:
+            variances = [1.0, 1.0, 1.0, 1.0]
+            if 'variance' in node.attrs and len(node.attr('variance')) > 0:
+                variances = [float(var) for var in node.attr('variance')]
+            node_variance_create = graph.make_node(
+                'Constant',
+                inputs=[],
+                outputs=name_variance_raw,
+                dtype=dtypes.ONNX.FLOAT,
+                dims=[len(variances)],
+                value=variances)
+            var_split_axis = 0
+            var_split_inputs_name = name_variance_raw
+
+        # decode the result
+        outputs_split_variance = [
+            result_name + "@variance_split" + str(i) for i in range(0, 4)
+        ]
+        outputs_split_targebox = [
+            result_name + "@targebox_split" + str(i) for i in range(0, 4)
+        ]
+        node_split_var = graph.make_node(
+            'Split',
+            inputs=var_split_inputs_name,
+            outputs=outputs_split_variance,
+            axis=var_split_axis)
+        node_split_target = graph.make_node(
+            'Split',
+            inputs=node.input('TargetBox'),
+            outputs=outputs_split_targebox,
+            axis=2)
+
+        outputs_squeeze_targebox = [
+            result_name + "@targebox_squeeze" + str(i) for i in range(0, 4)
+        ]
+        for (input_name, output_name) in zip(outputs_split_targebox,
+                                             outputs_squeeze_targebox):
+            tmp_node = mapper_helper.squeeze_helper(graph, input_name, [2],
+                                                    [output_name])
+
+        output_shape_step1 = list(t_size)[:-1]
+
+        inputs_tb_step1 = [
+            [outputs_squeeze_targebox[0], outputs_split_variance[0]],
+            [outputs_squeeze_targebox[1], outputs_split_variance[1]],
+            [outputs_squeeze_targebox[2], outputs_split_variance[2]],
+            [outputs_squeeze_targebox[3], outputs_split_variance[3]]
+        ]
+        outputs_tb_step1 = [[result_name + "@decode_x_step1"],
+                            [result_name + "@decode_y_step1"],
+                            [result_name + "@decode_w_step1"],
+                            [result_name + "@decode_h_step1"]]
+
+        for input_step1, output_step_1 in zip(inputs_tb_step1,
+                                              outputs_tb_step1):
+            tmp_node = graph.make_node(
+                'Mul', inputs=input_step1, outputs=output_step_1)
+        if axis == 0:
+            inputs_tbxy_step2 = [[
+                outputs_tb_step1[0][0], outputs_transpose_pb[0][0]
+            ], [outputs_tb_step1[1][0], outputs_transpose_pb[1][0]]]
+        else:
+            inputs_tbxy_step2 = [[
+                outputs_tb_step1[0][0], inputs_transpose_pb[0][0]
+            ], [outputs_tb_step1[1][0], inputs_transpose_pb[1][0]]]
+
+        outputs_tbxy_step2 = [[result_name + "@decode_x_step2"],
+                              [result_name + "@decode_y_step2"]]
+
+        for input_step2, output_step_2 in zip(inputs_tbxy_step2,
+                                              outputs_tbxy_step2):
+            tmp_node = graph.make_node(
+                'Mul', inputs=input_step2, outputs=output_step_2)
+        if axis == 0:
+            inputs_tbxy_step3 = [[
+                outputs_tbxy_step2[0][0], outputs_transpose_pb[2][0]
+            ], [outputs_tbxy_step2[1][0], outputs_transpose_pb[3][0]]]
+        else:
+            inputs_tbxy_step3 = [[
+                outputs_tbxy_step2[0][0], inputs_transpose_pb[2][0]
+            ], [outputs_tbxy_step2[1][0], inputs_transpose_pb[3][0]]]
+
+        outputs_tbxy_step3 = [[result_name + "@decode_x_step3"],
+                              [result_name + "@decode_y_step3"]]
+
+        for input_step3, output_step_3 in zip(inputs_tbxy_step3,
+                                              outputs_tbxy_step3):
+            tmp_node = graph.make_node(
+                'Add', inputs=input_step3, outputs=output_step_3)
+
+        # deal with width & height
+        inputs_tbwh_step2 = [outputs_tb_step1[2], outputs_tb_step1[3]]
+        outputs_tbwh_step2 = [[result_name + "@decode_w_step2"],
+                              [result_name + "@decode_h_step2"]]
+
+        for input_name, output_name in zip(inputs_tbwh_step2,
+                                           outputs_tbwh_step2):
+            tmp_node = graph.make_node(
+                'Exp', inputs=input_name, outputs=output_name)
+
+        if axis == 0:
+            inputs_tbwh_step3 = [[
+                outputs_tbwh_step2[0][0], outputs_transpose_pb[0][0]
+            ], [outputs_tbwh_step2[1][0], outputs_transpose_pb[1][0]]]
+        else:
+            inputs_tbwh_step3 = [[
+                outputs_tbwh_step2[0][0], inputs_transpose_pb[0][0]
+            ], [outputs_tbwh_step2[1][0], inputs_transpose_pb[1][0]]]
+
+        outputs_tbwh_step3 = [[result_name + "@decode_w_step3"],
+                              [result_name + "@decode_h_step3"]]
+
+        for input_name, output_name in zip(inputs_tbwh_step3,
+                                           outputs_tbwh_step3):
+            tmp_node = graph.make_node(
+                'Mul', inputs=input_name, outputs=output_name)
+
+        # final step to calc the result, and concat the result to output
+        # return the output box, [(x1, y1), (x2, y2)]
+
+        inputs_half_tbwh_step4 = [[
+            outputs_tbwh_step3[0][0], result_name + "@slice_2"
+        ], [outputs_tbwh_step3[1][0], result_name + "@slice_2"]]
+
+        outputs_half_tbwh_step4 = [[result_name + "@decode_half_w_step4"],
+                                   [result_name + "@decode_half_h_step4"]]
+        for inputs_name, outputs_name in zip(inputs_half_tbwh_step4,
+                                             outputs_half_tbwh_step4):
+            tmp_node = graph.make_node(
+                'Div', inputs=inputs_name, outputs=outputs_name)
+        inputs_output_point1 = [[
+            outputs_tbxy_step3[0][0], outputs_half_tbwh_step4[0][0]
+        ], [outputs_tbxy_step3[1][0], outputs_half_tbwh_step4[1][0]]]
+
+        outputs_output_point1 = [[result_name + "@ouput_x1"],
+                                 [result_name + "@output_y1"]]
+        for input_name, output_name in zip(inputs_output_point1,
+                                           outputs_output_point1):
+            tmp_node = graph.make_node(
+                'Sub', inputs=input_name, outputs=output_name)
+
+        inputs_output_point2 = [[
+            outputs_tbxy_step3[0][0], outputs_half_tbwh_step4[0][0]
+        ], [outputs_tbxy_step3[1][0], outputs_half_tbwh_step4[1][0]]]
+
+        outputs_output_point2 = [[result_name + "@ouput_x2"],
+                                 [result_name + "@output_y2"]]
+
+        for input_name, output_name in zip(inputs_output_point2,
+                                           outputs_output_point2):
+            tmp_node = graph.make_node(
+                'Add', inputs=input_name, outputs=output_name)
+        if not norm:
+            inputs_unnorm_point2 = [[
+                outputs_output_point2[0][0], result_name + "@slice_1"
+            ], [outputs_output_point2[1][0], result_name + "@slice_1"]]
+            outputs_unnorm_point2 = [[result_name + "@ouput_unnorm_x2"],
+                                     [result_name + "@ouput_unnorm_y2"]]
+
+            for input_name, output_name in zip(inputs_unnorm_point2,
+                                               outputs_unnorm_point2):
+                tmp_node = graph.make_node(
+                    'Sub', inputs=input_name, outputs=output_name)
+            outputs_output_point2 = outputs_unnorm_point2
+
+        outputs_output_point1.extend(outputs_output_point2)
+        ouputs_points_unsqueeze = [[result_name + "@points_unsqueeze_x1"],
+                                   [result_name + "points_unsqueeze_y1"],
+                                   [result_name + "points_unsqueeze_x2"],
+                                   [result_name + "points_unsqueeze_y2"]]
+
+        for input_name, output_name in zip(outputs_output_point1,
+                                           ouputs_points_unsqueeze):
+            tmp_node = mapper_helper.unsqueeze_helper(
+                graph, input_name, [len(output_shape_step1)], output_name)
+        outputs_points_unsqueeze_list = [
+            output[0] for output in ouputs_points_unsqueeze
+        ]
+        node_point_final = graph.make_node(
+            'Concat',
+            inputs=outputs_points_unsqueeze_list,
+            outputs=node.output('OutputBox'),
+            axis=len(output_shape_step1))
```

## paddle2onnx/legacy/op_mapper/detection/density_prior_box.py

 * *Ordering differences only*

```diff
@@ -1,125 +1,125 @@
-# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import math
-import numpy as np
-from paddle2onnx.legacy.constant import dtypes
-from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
-from paddle2onnx.utils import require_fixed_shape
-
-
-@op_mapper('density_prior_box')
-class DensityPriorBox():
-    """
-    In this function, use the attribute to get the prior box, because we do not use
-    the image data and feature map, wo could the python code to create the varaible,
-    and to create the onnx tensor as output.
-    """
-    support_opset_verison_range = (1, 12)
-
-    @classmethod
-    def opset_9(cls, graph, node, **kw):
-        clip = bool(node.attr('clip'))
-        densities = node.attr('densities')
-        fixed_ratios = node.attr('fixed_ratios')
-        fixed_sizes = node.attr('fixed_sizes')
-        flatten_to_2d = bool(node.attr('flatten_to_2d'))
-        offset = node.attr('offset')
-        step_h = node.attr('step_h')
-        step_w = node.attr('step_w')
-        variances = node.attr('variances')
-
-        input_shape = node.input_shape('Input', 0)
-        image_shape = node.input_shape('Image', 0)
-
-        img_width = image_shape[3]
-        img_height = image_shape[2]
-        feature_width = input_shape[3]
-        feature_height = input_shape[2]
-
-        assert img_width > 0 and img_height > 0, require_fixed_shape(
-            cls.__name__)
-
-        if step_w == 0.0 or step_h == 0.0:
-            step_w = float(img_width / feature_width)
-            step_h = float(img_height / feature_height)
-
-        num_priors = 0
-        if len(fixed_sizes) > 0 and len(densities) > 0:
-            for density in densities:
-                if len(fixed_ratios) > 0:
-                    num_priors += len(fixed_ratios) * (pow(density, 2))
-
-        out_dim = (feature_height, feature_width, num_priors, 4)
-        out_boxes = np.zeros(out_dim).astype('float32')
-        out_var = np.zeros(out_dim).astype('float32')
-        step_average = int((step_w + step_h) * 0.5)
-
-        for h in range(feature_height):
-            for w in range(feature_width):
-                c_x = (w + offset) * step_w
-                c_y = (h + offset) * step_h
-                idx = 0
-
-                for density, fixed_size in zip(densities, fixed_sizes):
-                    if (len(fixed_ratios) > 0):
-                        for ar in fixed_ratios:
-                            shift = int(step_average / density)
-                            box_width_ratio = fixed_size * math.sqrt(ar)
-                            box_height_ratio = fixed_size / math.sqrt(ar)
-                            for di in range(density):
-                                for dj in range(density):
-                                    c_x_temp = c_x - step_average / 2.0 + shift / 2.0 + dj * shift
-                                    c_y_temp = c_y - step_average / 2.0 + shift / 2.0 + di * shift
-                                    out_boxes[h, w, idx, :] = [
-                                        max((c_x_temp - box_width_ratio / 2.0) /
-                                            img_width, 0),
-                                        max((c_y_temp - box_height_ratio / 2.0)
-                                            / img_height, 0),
-                                        min((c_x_temp + box_width_ratio / 2.0) /
-                                            img_width, 1),
-                                        min((c_y_temp + box_height_ratio / 2.0)
-                                            / img_height, 1)
-                                    ]
-                                    idx += 1
-
-        if clip:
-            out_boxes = np.clip(out_boxes, 0.0, 1.0)
-        # set the variance.
-        out_var = np.tile(variances,
-                          (feature_height, feature_width, num_priors, 1))
-
-        if flatten_to_2d:
-            out_boxes = out_boxes.reshape((-1, 4))
-            out_var = out_var.reshape((-1, 4))
-
-        #make node that
-
-        node_boxes = graph.make_node(
-            'Constant',
-            inputs=[],
-            outputs=node.output('Boxes'),
-            dtype=dtypes.ONNX.FLOAT,
-            dims=out_boxes.shape,
-            value=out_boxes.flatten().tolist())
-
-        node_vars = graph.make_node(
-            'Constant',
-            inputs=[],
-            outputs=node.output('Variances'),
-            dtype=dtypes.ONNX.FLOAT,
-            dims=out_var.shape,
-            value=out_var.flatten().tolist())
+# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import math
+import numpy as np
+from paddle2onnx.legacy.constant import dtypes
+from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
+from paddle2onnx.utils import require_fixed_shape
+
+
+@op_mapper('density_prior_box')
+class DensityPriorBox():
+    """
+    In this function, use the attribute to get the prior box, because we do not use
+    the image data and feature map, wo could the python code to create the varaible,
+    and to create the onnx tensor as output.
+    """
+    support_opset_verison_range = (1, 12)
+
+    @classmethod
+    def opset_9(cls, graph, node, **kw):
+        clip = bool(node.attr('clip'))
+        densities = node.attr('densities')
+        fixed_ratios = node.attr('fixed_ratios')
+        fixed_sizes = node.attr('fixed_sizes')
+        flatten_to_2d = bool(node.attr('flatten_to_2d'))
+        offset = node.attr('offset')
+        step_h = node.attr('step_h')
+        step_w = node.attr('step_w')
+        variances = node.attr('variances')
+
+        input_shape = node.input_shape('Input', 0)
+        image_shape = node.input_shape('Image', 0)
+
+        img_width = image_shape[3]
+        img_height = image_shape[2]
+        feature_width = input_shape[3]
+        feature_height = input_shape[2]
+
+        assert img_width > 0 and img_height > 0, require_fixed_shape(
+            cls.__name__)
+
+        if step_w == 0.0 or step_h == 0.0:
+            step_w = float(img_width / feature_width)
+            step_h = float(img_height / feature_height)
+
+        num_priors = 0
+        if len(fixed_sizes) > 0 and len(densities) > 0:
+            for density in densities:
+                if len(fixed_ratios) > 0:
+                    num_priors += len(fixed_ratios) * (pow(density, 2))
+
+        out_dim = (feature_height, feature_width, num_priors, 4)
+        out_boxes = np.zeros(out_dim).astype('float32')
+        out_var = np.zeros(out_dim).astype('float32')
+        step_average = int((step_w + step_h) * 0.5)
+
+        for h in range(feature_height):
+            for w in range(feature_width):
+                c_x = (w + offset) * step_w
+                c_y = (h + offset) * step_h
+                idx = 0
+
+                for density, fixed_size in zip(densities, fixed_sizes):
+                    if (len(fixed_ratios) > 0):
+                        for ar in fixed_ratios:
+                            shift = int(step_average / density)
+                            box_width_ratio = fixed_size * math.sqrt(ar)
+                            box_height_ratio = fixed_size / math.sqrt(ar)
+                            for di in range(density):
+                                for dj in range(density):
+                                    c_x_temp = c_x - step_average / 2.0 + shift / 2.0 + dj * shift
+                                    c_y_temp = c_y - step_average / 2.0 + shift / 2.0 + di * shift
+                                    out_boxes[h, w, idx, :] = [
+                                        max((c_x_temp - box_width_ratio / 2.0) /
+                                            img_width, 0),
+                                        max((c_y_temp - box_height_ratio / 2.0)
+                                            / img_height, 0),
+                                        min((c_x_temp + box_width_ratio / 2.0) /
+                                            img_width, 1),
+                                        min((c_y_temp + box_height_ratio / 2.0)
+                                            / img_height, 1)
+                                    ]
+                                    idx += 1
+
+        if clip:
+            out_boxes = np.clip(out_boxes, 0.0, 1.0)
+        # set the variance.
+        out_var = np.tile(variances,
+                          (feature_height, feature_width, num_priors, 1))
+
+        if flatten_to_2d:
+            out_boxes = out_boxes.reshape((-1, 4))
+            out_var = out_var.reshape((-1, 4))
+
+        #make node that
+
+        node_boxes = graph.make_node(
+            'Constant',
+            inputs=[],
+            outputs=node.output('Boxes'),
+            dtype=dtypes.ONNX.FLOAT,
+            dims=out_boxes.shape,
+            value=out_boxes.flatten().tolist())
+
+        node_vars = graph.make_node(
+            'Constant',
+            inputs=[],
+            outputs=node.output('Variances'),
+            dtype=dtypes.ONNX.FLOAT,
+            dims=out_var.shape,
+            value=out_var.flatten().tolist())
```

## paddle2onnx/legacy/op_mapper/detection/multiclass_nms.py

 * *Ordering differences only*

```diff
@@ -1,343 +1,343 @@
-# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-import numpy as np
-from paddle2onnx.utils import logging
-from paddle2onnx.legacy.constant import dtypes
-from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
-from paddle2onnx.legacy.op_mapper import mapper_helper
-
-
-@op_mapper(
-    ['multiclass_nms', 'multiclass_nms2', 'matrix_nms', 'multiclass_nms3'])
-class MultiClassNMS():
-    support_opset_verision_range = (10, 16)
-    """
-    Convert the paddle multiclass_nms to onnx op.
-    This op is get the select boxes from origin boxes.
-    """
-
-    @classmethod
-    def opset_10(cls, graph, node, **kw):
-        if node.input_shape("BBoxes", 0)[0] != 1:
-            logging.warning(
-                "Due to the operator:{}, the converted ONNX model will only supports input[batch_size] == 1.".
-                format(node.type))
-        scores = node.input('Scores', 0)
-        bboxes = node.input('BBoxes', 0)
-        num_class = node.input_shape('Scores', 0)[1]
-        if len(node.input_shape('Scores', 0)) == 2:
-            # inputs: scores & bboxes is lod tensor
-            scores = graph.make_node('Transpose', inputs=[scores], perm=[1, 0])
-            scores = mapper_helper.unsqueeze_helper(graph, scores, [0])
-            if graph.opset_version < 13:
-                scores_list = graph.make_node(
-                    'Split',
-                    inputs=scores,
-                    outputs=num_class,
-                    axis=1,
-                    split=[1] * num_class)
-            else:
-                split_const = graph.make_node(
-                    'Constant', dtype=dtypes.ONNX.INT64, value=[1] * num_class)
-                scores_list = graph.make_node(
-                    "Split",
-                    inputs=[scores] + [split_const],
-                    outputs=num_class,
-                    axis=1)
-
-            bboxes = graph.make_node('Transpose', inputs=bboxes, perm=[1, 0, 2])
-            if graph.opset_version < 13:
-                bboxes_list = graph.make_node(
-                    'Split',
-                    inputs=bboxes,
-                    outputs=num_class,
-                    axis=0,
-                    split=[1] * num_class)
-            else:
-                split_const = graph.make_node(
-                    'Constant', dtype=dtypes.ONNX.INT64, value=[1] * num_class)
-                bboxes_list = graph.make_node(
-                    "Split",
-                    inputs=[bboxes] + [split_const],
-                    outputs=num_class,
-                    axis=0)
-            bbox_ids = []
-            if not isinstance(scores_list, list):
-                scores_list = [scores_list]
-            if not isinstance(bboxes_list, list):
-                bboxes_list = [bboxes_list]
-            for i in range(num_class):
-                bbox_id = cls.nms(graph,
-                                  node,
-                                  scores_list[i],
-                                  bboxes_list[i],
-                                  class_id=i)
-                bbox_ids.append(bbox_id)
-            bbox_ids = graph.make_node('Concat', inputs=bbox_ids, axis=0)
-            const_shape = graph.make_node(
-                'Constant', dtype=dtypes.ONNX.INT64, value=[1, -1, 4])
-            bboxes = graph.make_node('Reshape', inputs=[bboxes, const_shape])
-            cls.keep_top_k(
-                graph, node, bbox_ids, scores, bboxes, is_lod_input=True)
-        else:
-            bbox_ids = cls.nms(graph, node, scores, bboxes)
-            cls.keep_top_k(graph, node, bbox_ids, scores, bboxes)
-
-    @classmethod
-    def nms(cls, graph, node, scores, bboxes, class_id=None):
-        normalized = node.attr('normalized')
-        nms_top_k = node.attr('nms_top_k')
-        if node.type == 'matrix_nms':
-            iou_threshold = 0.5
-            logging.warning(
-                "Operator:{} is not supported completely, so we use traditional"
-                " NMS (nms_theshold={}) to instead it, which introduce some difference.".
-                format(node.type, str(iou_threshold)))
-        else:
-            iou_threshold = node.attr('nms_threshold')
-        if nms_top_k == -1:
-            nms_top_k = 100000
-
-        #convert the paddle attribute to onnx tensor
-        score_threshold = graph.make_node(
-            'Constant',
-            dtype=dtypes.ONNX.FLOAT,
-            value=[float(node.attr('score_threshold'))])
-        iou_threshold = graph.make_node(
-            'Constant', dtype=dtypes.ONNX.FLOAT, value=[float(iou_threshold)])
-        nms_top_k = graph.make_node(
-            'Constant', dtype=dtypes.ONNX.INT64, value=[np.int64(nms_top_k)])
-
-        # the paddle data format is x1,y1,x2,y2
-        kwargs = {'center_point_box': 0}
-
-        if normalized:
-            select_bbox_indices = graph.make_node(
-                'NonMaxSuppression',
-                inputs=[
-                    bboxes, scores, nms_top_k, iou_threshold, score_threshold
-                ])
-        elif not normalized:
-            value_one = graph.make_node(
-                'Constant', dims=[1], dtype=dtypes.ONNX.FLOAT, value=1.0)
-            if graph.opset_version < 13:
-                new_bboxes = graph.make_node(
-                    'Split',
-                    inputs=[bboxes],
-                    outputs=4,
-                    axis=2,
-                    split=[1, 1, 1, 1])
-            else:
-                split_const = graph.make_node(
-                    'Constant', dtype=dtypes.ONNX.INT64, value=[1, 1, 1, 1])
-                new_bboxes = graph.make_node(
-                    "Split", inputs=[bboxes] + [split_const], outputs=4, axis=2)
-            new_xmax = graph.make_node('Add', inputs=[new_bboxes[2], value_one])
-            new_ymax = graph.make_node('Add', inputs=[new_bboxes[3], value_one])
-            new_bboxes = graph.make_node(
-                'Concat',
-                inputs=[new_bboxes[0], new_bboxes[1], new_xmax, new_ymax],
-                axis=2)
-            select_bbox_indices = graph.make_node(
-                'NonMaxSuppression',
-                inputs=[
-                    new_bboxes, scores, nms_top_k, iou_threshold,
-                    score_threshold
-                ])
-
-        if class_id is not None and class_id != 0:
-            class_id = graph.make_node(
-                'Constant', dtype=dtypes.ONNX.INT64, value=[0, class_id, 0])
-            class_id = mapper_helper.unsqueeze_helper(graph, class_id, [0])
-            select_bbox_indices = graph.make_node(
-                'Add', inputs=[select_bbox_indices, class_id])
-
-        return select_bbox_indices
-
-    @classmethod
-    def keep_top_k(cls,
-                   graph,
-                   node,
-                   select_bbox_indices,
-                   scores,
-                   bboxes,
-                   is_lod_input=False):
-        # step 1 nodes select the nms class
-        # create some const value to use
-        background = node.attr('background_label')
-        const_values = []
-        for value in [0, 1, 2, -1]:
-            const_value = graph.make_node(
-                'Constant', dtype=dtypes.ONNX.INT64, value=[value])
-            const_values.append(const_value)
-
-        # In this code block, we will deocde the raw score data, reshape N * C * M to 1 * N*C*M
-        # and the same time, decode the select indices to 1 * D, gather the select_indices
-        class_id = graph.make_node(
-            'Gather', inputs=[select_bbox_indices, const_values[1]], axis=1)
-
-        squeezed_class_id = mapper_helper.squeeze_helper(graph, class_id, [1])
-
-        bbox_id = graph.make_node(
-            'Gather', inputs=[select_bbox_indices, const_values[2]], axis=1)
-
-        if background == 0:
-            nonzero = graph.make_node('NonZero', inputs=[squeezed_class_id])
-        else:
-            filter_cls_id = graph.make_node(
-                'Constant', dtype=dtypes.ONNX.INT32, value=[background])
-            cast = graph.make_node(
-                'Cast', inputs=[squeezed_class_id], to=dtypes.ONNX.INT32)
-            filter_index = graph.make_node('Sub', inputs=[cast, filter_cls_id])
-            nonzero = graph.make_node('NonZero', inputs=[filter_index])
-
-        class_id = graph.make_node('Gather', inputs=[class_id, nonzero], axis=0)
-        class_id = graph.make_node(
-            'Cast', inputs=[class_id], to=dtypes.ONNX.INT64)
-
-        bbox_id = graph.make_node('Gather', inputs=[bbox_id, nonzero], axis=0)
-        bbox_id = graph.make_node(
-            'Cast', inputs=[bbox_id], to=dtypes.ONNX.INT64)
-
-        # get the shape of scores
-        shape_scores = graph.make_node('Shape', inputs=scores)
-
-        # gather the index: 2 shape of scores
-        class_num = graph.make_node(
-            'Gather', inputs=[shape_scores, const_values[2]], axis=0)
-
-        # reshape scores N * C * M to (N*C*M) * 1
-        scores = graph.make_node('Reshape', inputs=[scores, const_values[-1]])
-
-        # mul class * M
-        mul_classnum_boxnum = graph.make_node(
-            'Mul', inputs=[class_id, class_num])
-
-        # add class * M * index
-        add_class_indices = graph.make_node(
-            'Add', inputs=[mul_classnum_boxnum, bbox_id])
-
-        # Squeeze the indices to 1 dim
-        score_indices = mapper_helper.squeeze_helper(graph, add_class_indices,
-                                                     [0, 2])
-
-        # gather the data from flatten scores
-        scores = graph.make_node(
-            'Gather', inputs=[scores, score_indices], axis=0)
-
-        keep_top_k = node.attr('keep_top_k')
-        keep_top_k = graph.make_node(
-            'Constant',
-            dtype=dtypes.ONNX.INT64,
-            dims=[1, 1],
-            value=[node.attr('keep_top_k')])
-
-        # get min(topK, num_select)
-        shape_select_num = graph.make_node('Shape', inputs=[scores])
-        const_zero = graph.make_node(
-            'Constant', dtype=dtypes.ONNX.INT64, value=[0])
-        gather_select_num = graph.make_node(
-            'Gather', inputs=[shape_select_num, const_zero], axis=0)
-        unsqueeze_select_num = mapper_helper.unsqueeze_helper(
-            graph, gather_select_num, [0])
-
-        concat_topK_select_num = graph.make_node(
-            'Concat', inputs=[unsqueeze_select_num, keep_top_k], axis=0)
-        cast_concat_topK_select_num = graph.make_node(
-            'Cast', inputs=[concat_topK_select_num], to=6)
-        keep_top_k = graph.make_node(
-            'ReduceMin', inputs=[cast_concat_topK_select_num], keepdims=0)
-        # unsqueeze the indices to 1D tensor
-        keep_top_k = mapper_helper.unsqueeze_helper(graph, keep_top_k, [0])
-
-        # cast the indices to INT64
-        keep_top_k = graph.make_node('Cast', inputs=[keep_top_k], to=7)
-
-        # select topk scores  indices
-        keep_topk_scores, keep_topk_indices = graph.make_node(
-            'TopK', inputs=[scores, keep_top_k], outputs=2)
-
-        # gather topk label, scores, boxes
-        gather_topk_scores = graph.make_node(
-            'Gather', inputs=[scores, keep_topk_indices], axis=0)
-
-        gather_topk_class = graph.make_node(
-            'Gather', inputs=[class_id, keep_topk_indices], axis=1)
-
-        # gather the boxes need to gather the boxes id, then get boxes
-        if is_lod_input:
-            gather_topk_boxes_id = graph.make_node(
-                'Gather', [add_class_indices, keep_topk_indices], axis=1)
-        else:
-            gather_topk_boxes_id = graph.make_node(
-                'Gather', [bbox_id, keep_topk_indices], axis=1)
-
-        # squeeze the gather_topk_boxes_id to 1 dim
-        squeeze_topk_boxes_id = mapper_helper.squeeze_helper(
-            graph, gather_topk_boxes_id, [0, 2])
-
-        gather_select_boxes = graph.make_node(
-            'Gather', inputs=[bboxes, squeeze_topk_boxes_id], axis=1)
-
-        # concat the final result
-        # before concat need to cast the class to float
-        cast_topk_class = graph.make_node(
-            'Cast', inputs=[gather_topk_class], to=1)
-
-        unsqueeze_topk_scores = mapper_helper.unsqueeze_helper(
-            graph, gather_topk_scores, [0, 2])
-
-        inputs_concat_final_results = [
-            cast_topk_class, unsqueeze_topk_scores, gather_select_boxes
-        ]
-
-        sort_by_socre_results = graph.make_node(
-            'Concat', inputs=inputs_concat_final_results, axis=2)
-
-        # sort by class_id
-        squeeze_cast_topk_class = mapper_helper.squeeze_helper(
-            graph, cast_topk_class, [0, 2])
-
-        neg_squeeze_cast_topk_class = graph.make_node(
-            'Neg', inputs=[squeeze_cast_topk_class])
-
-        data, indices = graph.make_node(
-            'TopK', inputs=[neg_squeeze_cast_topk_class, keep_top_k], outputs=2)
-
-        concat_final_results = graph.make_node(
-            'Gather', inputs=[sort_by_socre_results, indices], axis=1)
-
-        concat_final_results = mapper_helper.squeeze_helper(
-            graph, concat_final_results, [0], node.output('Out'))
-
-        if node.type in ['multiclass_nms2', 'matrix_nms', 'multiclass_nms3']:
-            final_indices = mapper_helper.squeeze_helper(graph, bbox_id, [0],
-                                                         node.output('Index'))
-            if node.type in ['matrix_nms', 'multiclass_nms3']:
-                select_bboxes_shape = graph.make_node('Shape', inputs=[indices])
-                select_bboxes_shape1 = graph.make_node(
-                    'Cast', inputs=[select_bboxes_shape], to=dtypes.ONNX.INT32)
-                indices = graph.make_node(
-                    'Constant', dtype=dtypes.ONNX.INT64, value=[0])
-                rois_num = None
-                if 'NmsRoisNum' in node.outputs:
-                    rois_num = node.output('NmsRoisNum')
-                elif 'RoisNum' in node.outputs:
-                    rois_num = node.output('RoisNum')
-                if rois_num is not None:
-                    graph.make_node(
-                        "Gather",
-                        inputs=[select_bboxes_shape1, indices],
-                        outputs=rois_num)
+# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import numpy as np
+from paddle2onnx.utils import logging
+from paddle2onnx.legacy.constant import dtypes
+from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
+from paddle2onnx.legacy.op_mapper import mapper_helper
+
+
+@op_mapper(
+    ['multiclass_nms', 'multiclass_nms2', 'matrix_nms', 'multiclass_nms3'])
+class MultiClassNMS():
+    support_opset_verision_range = (10, 16)
+    """
+    Convert the paddle multiclass_nms to onnx op.
+    This op is get the select boxes from origin boxes.
+    """
+
+    @classmethod
+    def opset_10(cls, graph, node, **kw):
+        if node.input_shape("BBoxes", 0)[0] != 1:
+            logging.warning(
+                "Due to the operator:{}, the converted ONNX model will only supports input[batch_size] == 1.".
+                format(node.type))
+        scores = node.input('Scores', 0)
+        bboxes = node.input('BBoxes', 0)
+        num_class = node.input_shape('Scores', 0)[1]
+        if len(node.input_shape('Scores', 0)) == 2:
+            # inputs: scores & bboxes is lod tensor
+            scores = graph.make_node('Transpose', inputs=[scores], perm=[1, 0])
+            scores = mapper_helper.unsqueeze_helper(graph, scores, [0])
+            if graph.opset_version < 13:
+                scores_list = graph.make_node(
+                    'Split',
+                    inputs=scores,
+                    outputs=num_class,
+                    axis=1,
+                    split=[1] * num_class)
+            else:
+                split_const = graph.make_node(
+                    'Constant', dtype=dtypes.ONNX.INT64, value=[1] * num_class)
+                scores_list = graph.make_node(
+                    "Split",
+                    inputs=[scores] + [split_const],
+                    outputs=num_class,
+                    axis=1)
+
+            bboxes = graph.make_node('Transpose', inputs=bboxes, perm=[1, 0, 2])
+            if graph.opset_version < 13:
+                bboxes_list = graph.make_node(
+                    'Split',
+                    inputs=bboxes,
+                    outputs=num_class,
+                    axis=0,
+                    split=[1] * num_class)
+            else:
+                split_const = graph.make_node(
+                    'Constant', dtype=dtypes.ONNX.INT64, value=[1] * num_class)
+                bboxes_list = graph.make_node(
+                    "Split",
+                    inputs=[bboxes] + [split_const],
+                    outputs=num_class,
+                    axis=0)
+            bbox_ids = []
+            if not isinstance(scores_list, list):
+                scores_list = [scores_list]
+            if not isinstance(bboxes_list, list):
+                bboxes_list = [bboxes_list]
+            for i in range(num_class):
+                bbox_id = cls.nms(graph,
+                                  node,
+                                  scores_list[i],
+                                  bboxes_list[i],
+                                  class_id=i)
+                bbox_ids.append(bbox_id)
+            bbox_ids = graph.make_node('Concat', inputs=bbox_ids, axis=0)
+            const_shape = graph.make_node(
+                'Constant', dtype=dtypes.ONNX.INT64, value=[1, -1, 4])
+            bboxes = graph.make_node('Reshape', inputs=[bboxes, const_shape])
+            cls.keep_top_k(
+                graph, node, bbox_ids, scores, bboxes, is_lod_input=True)
+        else:
+            bbox_ids = cls.nms(graph, node, scores, bboxes)
+            cls.keep_top_k(graph, node, bbox_ids, scores, bboxes)
+
+    @classmethod
+    def nms(cls, graph, node, scores, bboxes, class_id=None):
+        normalized = node.attr('normalized')
+        nms_top_k = node.attr('nms_top_k')
+        if node.type == 'matrix_nms':
+            iou_threshold = 0.5
+            logging.warning(
+                "Operator:{} is not supported completely, so we use traditional"
+                " NMS (nms_theshold={}) to instead it, which introduce some difference.".
+                format(node.type, str(iou_threshold)))
+        else:
+            iou_threshold = node.attr('nms_threshold')
+        if nms_top_k == -1:
+            nms_top_k = 100000
+
+        #convert the paddle attribute to onnx tensor
+        score_threshold = graph.make_node(
+            'Constant',
+            dtype=dtypes.ONNX.FLOAT,
+            value=[float(node.attr('score_threshold'))])
+        iou_threshold = graph.make_node(
+            'Constant', dtype=dtypes.ONNX.FLOAT, value=[float(iou_threshold)])
+        nms_top_k = graph.make_node(
+            'Constant', dtype=dtypes.ONNX.INT64, value=[np.int64(nms_top_k)])
+
+        # the paddle data format is x1,y1,x2,y2
+        kwargs = {'center_point_box': 0}
+
+        if normalized:
+            select_bbox_indices = graph.make_node(
+                'NonMaxSuppression',
+                inputs=[
+                    bboxes, scores, nms_top_k, iou_threshold, score_threshold
+                ])
+        elif not normalized:
+            value_one = graph.make_node(
+                'Constant', dims=[1], dtype=dtypes.ONNX.FLOAT, value=1.0)
+            if graph.opset_version < 13:
+                new_bboxes = graph.make_node(
+                    'Split',
+                    inputs=[bboxes],
+                    outputs=4,
+                    axis=2,
+                    split=[1, 1, 1, 1])
+            else:
+                split_const = graph.make_node(
+                    'Constant', dtype=dtypes.ONNX.INT64, value=[1, 1, 1, 1])
+                new_bboxes = graph.make_node(
+                    "Split", inputs=[bboxes] + [split_const], outputs=4, axis=2)
+            new_xmax = graph.make_node('Add', inputs=[new_bboxes[2], value_one])
+            new_ymax = graph.make_node('Add', inputs=[new_bboxes[3], value_one])
+            new_bboxes = graph.make_node(
+                'Concat',
+                inputs=[new_bboxes[0], new_bboxes[1], new_xmax, new_ymax],
+                axis=2)
+            select_bbox_indices = graph.make_node(
+                'NonMaxSuppression',
+                inputs=[
+                    new_bboxes, scores, nms_top_k, iou_threshold,
+                    score_threshold
+                ])
+
+        if class_id is not None and class_id != 0:
+            class_id = graph.make_node(
+                'Constant', dtype=dtypes.ONNX.INT64, value=[0, class_id, 0])
+            class_id = mapper_helper.unsqueeze_helper(graph, class_id, [0])
+            select_bbox_indices = graph.make_node(
+                'Add', inputs=[select_bbox_indices, class_id])
+
+        return select_bbox_indices
+
+    @classmethod
+    def keep_top_k(cls,
+                   graph,
+                   node,
+                   select_bbox_indices,
+                   scores,
+                   bboxes,
+                   is_lod_input=False):
+        # step 1 nodes select the nms class
+        # create some const value to use
+        background = node.attr('background_label')
+        const_values = []
+        for value in [0, 1, 2, -1]:
+            const_value = graph.make_node(
+                'Constant', dtype=dtypes.ONNX.INT64, value=[value])
+            const_values.append(const_value)
+
+        # In this code block, we will deocde the raw score data, reshape N * C * M to 1 * N*C*M
+        # and the same time, decode the select indices to 1 * D, gather the select_indices
+        class_id = graph.make_node(
+            'Gather', inputs=[select_bbox_indices, const_values[1]], axis=1)
+
+        squeezed_class_id = mapper_helper.squeeze_helper(graph, class_id, [1])
+
+        bbox_id = graph.make_node(
+            'Gather', inputs=[select_bbox_indices, const_values[2]], axis=1)
+
+        if background == 0:
+            nonzero = graph.make_node('NonZero', inputs=[squeezed_class_id])
+        else:
+            filter_cls_id = graph.make_node(
+                'Constant', dtype=dtypes.ONNX.INT32, value=[background])
+            cast = graph.make_node(
+                'Cast', inputs=[squeezed_class_id], to=dtypes.ONNX.INT32)
+            filter_index = graph.make_node('Sub', inputs=[cast, filter_cls_id])
+            nonzero = graph.make_node('NonZero', inputs=[filter_index])
+
+        class_id = graph.make_node('Gather', inputs=[class_id, nonzero], axis=0)
+        class_id = graph.make_node(
+            'Cast', inputs=[class_id], to=dtypes.ONNX.INT64)
+
+        bbox_id = graph.make_node('Gather', inputs=[bbox_id, nonzero], axis=0)
+        bbox_id = graph.make_node(
+            'Cast', inputs=[bbox_id], to=dtypes.ONNX.INT64)
+
+        # get the shape of scores
+        shape_scores = graph.make_node('Shape', inputs=scores)
+
+        # gather the index: 2 shape of scores
+        class_num = graph.make_node(
+            'Gather', inputs=[shape_scores, const_values[2]], axis=0)
+
+        # reshape scores N * C * M to (N*C*M) * 1
+        scores = graph.make_node('Reshape', inputs=[scores, const_values[-1]])
+
+        # mul class * M
+        mul_classnum_boxnum = graph.make_node(
+            'Mul', inputs=[class_id, class_num])
+
+        # add class * M * index
+        add_class_indices = graph.make_node(
+            'Add', inputs=[mul_classnum_boxnum, bbox_id])
+
+        # Squeeze the indices to 1 dim
+        score_indices = mapper_helper.squeeze_helper(graph, add_class_indices,
+                                                     [0, 2])
+
+        # gather the data from flatten scores
+        scores = graph.make_node(
+            'Gather', inputs=[scores, score_indices], axis=0)
+
+        keep_top_k = node.attr('keep_top_k')
+        keep_top_k = graph.make_node(
+            'Constant',
+            dtype=dtypes.ONNX.INT64,
+            dims=[1, 1],
+            value=[node.attr('keep_top_k')])
+
+        # get min(topK, num_select)
+        shape_select_num = graph.make_node('Shape', inputs=[scores])
+        const_zero = graph.make_node(
+            'Constant', dtype=dtypes.ONNX.INT64, value=[0])
+        gather_select_num = graph.make_node(
+            'Gather', inputs=[shape_select_num, const_zero], axis=0)
+        unsqueeze_select_num = mapper_helper.unsqueeze_helper(
+            graph, gather_select_num, [0])
+
+        concat_topK_select_num = graph.make_node(
+            'Concat', inputs=[unsqueeze_select_num, keep_top_k], axis=0)
+        cast_concat_topK_select_num = graph.make_node(
+            'Cast', inputs=[concat_topK_select_num], to=6)
+        keep_top_k = graph.make_node(
+            'ReduceMin', inputs=[cast_concat_topK_select_num], keepdims=0)
+        # unsqueeze the indices to 1D tensor
+        keep_top_k = mapper_helper.unsqueeze_helper(graph, keep_top_k, [0])
+
+        # cast the indices to INT64
+        keep_top_k = graph.make_node('Cast', inputs=[keep_top_k], to=7)
+
+        # select topk scores  indices
+        keep_topk_scores, keep_topk_indices = graph.make_node(
+            'TopK', inputs=[scores, keep_top_k], outputs=2)
+
+        # gather topk label, scores, boxes
+        gather_topk_scores = graph.make_node(
+            'Gather', inputs=[scores, keep_topk_indices], axis=0)
+
+        gather_topk_class = graph.make_node(
+            'Gather', inputs=[class_id, keep_topk_indices], axis=1)
+
+        # gather the boxes need to gather the boxes id, then get boxes
+        if is_lod_input:
+            gather_topk_boxes_id = graph.make_node(
+                'Gather', [add_class_indices, keep_topk_indices], axis=1)
+        else:
+            gather_topk_boxes_id = graph.make_node(
+                'Gather', [bbox_id, keep_topk_indices], axis=1)
+
+        # squeeze the gather_topk_boxes_id to 1 dim
+        squeeze_topk_boxes_id = mapper_helper.squeeze_helper(
+            graph, gather_topk_boxes_id, [0, 2])
+
+        gather_select_boxes = graph.make_node(
+            'Gather', inputs=[bboxes, squeeze_topk_boxes_id], axis=1)
+
+        # concat the final result
+        # before concat need to cast the class to float
+        cast_topk_class = graph.make_node(
+            'Cast', inputs=[gather_topk_class], to=1)
+
+        unsqueeze_topk_scores = mapper_helper.unsqueeze_helper(
+            graph, gather_topk_scores, [0, 2])
+
+        inputs_concat_final_results = [
+            cast_topk_class, unsqueeze_topk_scores, gather_select_boxes
+        ]
+
+        sort_by_socre_results = graph.make_node(
+            'Concat', inputs=inputs_concat_final_results, axis=2)
+
+        # sort by class_id
+        squeeze_cast_topk_class = mapper_helper.squeeze_helper(
+            graph, cast_topk_class, [0, 2])
+
+        neg_squeeze_cast_topk_class = graph.make_node(
+            'Neg', inputs=[squeeze_cast_topk_class])
+
+        data, indices = graph.make_node(
+            'TopK', inputs=[neg_squeeze_cast_topk_class, keep_top_k], outputs=2)
+
+        concat_final_results = graph.make_node(
+            'Gather', inputs=[sort_by_socre_results, indices], axis=1)
+
+        concat_final_results = mapper_helper.squeeze_helper(
+            graph, concat_final_results, [0], node.output('Out'))
+
+        if node.type in ['multiclass_nms2', 'matrix_nms', 'multiclass_nms3']:
+            final_indices = mapper_helper.squeeze_helper(graph, bbox_id, [0],
+                                                         node.output('Index'))
+            if node.type in ['matrix_nms', 'multiclass_nms3']:
+                select_bboxes_shape = graph.make_node('Shape', inputs=[indices])
+                select_bboxes_shape1 = graph.make_node(
+                    'Cast', inputs=[select_bboxes_shape], to=dtypes.ONNX.INT32)
+                indices = graph.make_node(
+                    'Constant', dtype=dtypes.ONNX.INT64, value=[0])
+                rois_num = None
+                if 'NmsRoisNum' in node.outputs:
+                    rois_num = node.output('NmsRoisNum')
+                elif 'RoisNum' in node.outputs:
+                    rois_num = node.output('RoisNum')
+                if rois_num is not None:
+                    graph.make_node(
+                        "Gather",
+                        inputs=[select_bboxes_shape1, indices],
+                        outputs=rois_num)
```

## paddle2onnx/legacy/op_mapper/detection/prior_box.py

 * *Ordering differences only*

```diff
@@ -1,176 +1,176 @@
-# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import math
-import numpy as np
-from paddle2onnx.legacy.constant import dtypes
-from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
-from paddle2onnx.utils import require_fixed_shape
-
-
-def expand_aspect_rations(input_aspect_ratior, flip):
-    expsilon = 1e-6
-    output_ratios = [1.0]
-    for input_ratio in input_aspect_ratior:
-        already_exis = False
-        for output_ratio in output_ratios:
-            if abs(input_ratio - output_ratio) < expsilon:
-                already_exis = True
-                break
-        if already_exis == False:
-            output_ratios.append(input_ratio)
-            if flip:
-                output_ratios.append(1.0 / input_ratio)
-    return output_ratios
-
-
-@op_mapper('prior_box')
-class PriorBox():
-    """
-    In this function, use the attribute to get the prior box, because we do not use
-    the image data and feature map, wo could the python code to create the varaible,
-    and to create the onnx tensor as output.
-    """
-    support_opset_verison_range = (1, 12)
-
-    @classmethod
-    def opset_9(cls, graph, node, **kw):
-        flip = bool(node.attr('flip'))
-        clip = bool(node.attr('clip'))
-        min_max_aspect_ratios_order = bool(
-            node.attr('min_max_aspect_ratios_order'))
-        min_sizes = [float(size) for size in node.attr('min_sizes')]
-        max_sizes = [float(size) for size in node.attr('max_sizes')]
-        if isinstance(node.attr('aspect_ratios'), list):
-            aspect_ratios = [
-                float(ratio) for ratio in node.attr('aspect_ratios')
-            ]
-        else:
-            aspect_ratios = [float(node.attr('aspect_ratios'))]
-        variances = [float(var) for var in node.attr('variances')]
-        # set min_max_aspect_ratios_order = false
-        output_ratios = expand_aspect_rations(aspect_ratios, flip)
-
-        step_w = float(node.attr('step_w'))
-        step_h = float(node.attr('step_h'))
-        offset = float(node.attr('offset'))
-
-        input_shape = node.input_shape('Input', 0)
-        image_shape = node.input_shape('Image', 0)
-
-        img_width = image_shape[3]
-        img_height = image_shape[2]
-        feature_width = input_shape[3]
-        feature_height = input_shape[2]
-        assert img_width > 0 and img_height > 0, require_fixed_shape(
-            cls.__name__)
-
-        step_width = 1.0
-        step_height = 1.0
-
-        if step_w == 0.0 or step_h == 0.0:
-            step_w = float(img_width / feature_width)
-            step_h = float(img_height / feature_height)
-
-        num_priors = len(output_ratios) * len(min_sizes)
-        if len(max_sizes) > 0:
-            num_priors += len(max_sizes)
-        out_dim = (feature_height, feature_width, num_priors, 4)
-        out_boxes = np.zeros(out_dim).astype('float32')
-        out_var = np.zeros(out_dim).astype('float32')
-
-        idx = 0
-        for h in range(feature_height):
-            for w in range(feature_width):
-                c_x = (w + offset) * step_w
-                c_y = (h + offset) * step_h
-                idx = 0
-                for s in range(len(min_sizes)):
-                    min_size = min_sizes[s]
-                    if not min_max_aspect_ratios_order:
-                        # rest of priors
-                        for r in range(len(output_ratios)):
-                            ar = output_ratios[r]
-                            c_w = min_size * math.sqrt(ar) / 2
-                            c_h = (min_size / math.sqrt(ar)) / 2
-                            out_boxes[h, w, idx, :] = [(c_x - c_w) / img_width,
-                                                       (c_y - c_h) / img_height,
-                                                       (c_x + c_w) / img_width,
-                                                       (c_y + c_h) / img_height]
-                            idx += 1
-
-                        if len(max_sizes) > 0:
-                            max_size = max_sizes[s]
-                            # second prior: aspect_ratio = 1,
-                            c_w = c_h = math.sqrt(min_size * max_size) / 2
-                            out_boxes[h, w, idx, :] = [(c_x - c_w) / img_width,
-                                                       (c_y - c_h) / img_height,
-                                                       (c_x + c_w) / img_width,
-                                                       (c_y + c_h) / img_height]
-                            idx += 1
-                    else:
-                        c_w = c_h = min_size / 2.
-                        out_boxes[h, w, idx, :] = [
-                            (c_x - c_w) / img_width, (c_y - c_h) / img_height,
-                            (c_x + c_w) / img_width, (c_y + c_h) / img_height
-                        ]
-                        idx += 1
-                        if len(max_sizes) > 0:
-                            max_size = max_sizes[s]
-                            # second prior: aspect_ratio = 1,
-                            c_w = c_h = math.sqrt(min_size * max_size) / 2
-                            out_boxes[h, w, idx, :] = [(c_x - c_w) / img_width,
-                                                       (c_y - c_h) / img_height,
-                                                       (c_x + c_w) / img_width,
-                                                       (c_y + c_h) / img_height]
-                            idx += 1
-
-                        # rest of priors
-                        for r in range(len(output_ratios)):
-                            ar = output_ratios[r]
-                            if abs(ar - 1.) < 1e-6:
-                                continue
-                            c_w = min_size * math.sqrt(ar) / 2
-                            c_h = (min_size / math.sqrt(ar)) / 2
-                            out_boxes[h, w, idx, :] = [(c_x - c_w) / img_width,
-                                                       (c_y - c_h) / img_height,
-                                                       (c_x + c_w) / img_width,
-                                                       (c_y + c_h) / img_height]
-                            idx += 1
-
-        if clip:
-            out_boxes = np.clip(out_boxes, 0.0, 1.0)
-        # set the variance.
-        out_var = np.tile(variances,
-                          (feature_height, feature_width, num_priors, 1))
-
-        #make node that
-
-        node_boxes = graph.make_node(
-            'Constant',
-            inputs=[],
-            outputs=node.output('Boxes'),
-            dtype=dtypes.ONNX.FLOAT,
-            dims=out_boxes.shape,
-            value=out_boxes.flatten().tolist())
-
-        node_vars = graph.make_node(
-            'Constant',
-            inputs=[],
-            outputs=node.output('Variances'),
-            dtype=dtypes.ONNX.FLOAT,
-            dims=out_var.shape,
-            value=out_var.flatten().tolist())
+# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import math
+import numpy as np
+from paddle2onnx.legacy.constant import dtypes
+from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
+from paddle2onnx.utils import require_fixed_shape
+
+
+def expand_aspect_rations(input_aspect_ratior, flip):
+    expsilon = 1e-6
+    output_ratios = [1.0]
+    for input_ratio in input_aspect_ratior:
+        already_exis = False
+        for output_ratio in output_ratios:
+            if abs(input_ratio - output_ratio) < expsilon:
+                already_exis = True
+                break
+        if already_exis == False:
+            output_ratios.append(input_ratio)
+            if flip:
+                output_ratios.append(1.0 / input_ratio)
+    return output_ratios
+
+
+@op_mapper('prior_box')
+class PriorBox():
+    """
+    In this function, use the attribute to get the prior box, because we do not use
+    the image data and feature map, wo could the python code to create the varaible,
+    and to create the onnx tensor as output.
+    """
+    support_opset_verison_range = (1, 12)
+
+    @classmethod
+    def opset_9(cls, graph, node, **kw):
+        flip = bool(node.attr('flip'))
+        clip = bool(node.attr('clip'))
+        min_max_aspect_ratios_order = bool(
+            node.attr('min_max_aspect_ratios_order'))
+        min_sizes = [float(size) for size in node.attr('min_sizes')]
+        max_sizes = [float(size) for size in node.attr('max_sizes')]
+        if isinstance(node.attr('aspect_ratios'), list):
+            aspect_ratios = [
+                float(ratio) for ratio in node.attr('aspect_ratios')
+            ]
+        else:
+            aspect_ratios = [float(node.attr('aspect_ratios'))]
+        variances = [float(var) for var in node.attr('variances')]
+        # set min_max_aspect_ratios_order = false
+        output_ratios = expand_aspect_rations(aspect_ratios, flip)
+
+        step_w = float(node.attr('step_w'))
+        step_h = float(node.attr('step_h'))
+        offset = float(node.attr('offset'))
+
+        input_shape = node.input_shape('Input', 0)
+        image_shape = node.input_shape('Image', 0)
+
+        img_width = image_shape[3]
+        img_height = image_shape[2]
+        feature_width = input_shape[3]
+        feature_height = input_shape[2]
+        assert img_width > 0 and img_height > 0, require_fixed_shape(
+            cls.__name__)
+
+        step_width = 1.0
+        step_height = 1.0
+
+        if step_w == 0.0 or step_h == 0.0:
+            step_w = float(img_width / feature_width)
+            step_h = float(img_height / feature_height)
+
+        num_priors = len(output_ratios) * len(min_sizes)
+        if len(max_sizes) > 0:
+            num_priors += len(max_sizes)
+        out_dim = (feature_height, feature_width, num_priors, 4)
+        out_boxes = np.zeros(out_dim).astype('float32')
+        out_var = np.zeros(out_dim).astype('float32')
+
+        idx = 0
+        for h in range(feature_height):
+            for w in range(feature_width):
+                c_x = (w + offset) * step_w
+                c_y = (h + offset) * step_h
+                idx = 0
+                for s in range(len(min_sizes)):
+                    min_size = min_sizes[s]
+                    if not min_max_aspect_ratios_order:
+                        # rest of priors
+                        for r in range(len(output_ratios)):
+                            ar = output_ratios[r]
+                            c_w = min_size * math.sqrt(ar) / 2
+                            c_h = (min_size / math.sqrt(ar)) / 2
+                            out_boxes[h, w, idx, :] = [(c_x - c_w) / img_width,
+                                                       (c_y - c_h) / img_height,
+                                                       (c_x + c_w) / img_width,
+                                                       (c_y + c_h) / img_height]
+                            idx += 1
+
+                        if len(max_sizes) > 0:
+                            max_size = max_sizes[s]
+                            # second prior: aspect_ratio = 1,
+                            c_w = c_h = math.sqrt(min_size * max_size) / 2
+                            out_boxes[h, w, idx, :] = [(c_x - c_w) / img_width,
+                                                       (c_y - c_h) / img_height,
+                                                       (c_x + c_w) / img_width,
+                                                       (c_y + c_h) / img_height]
+                            idx += 1
+                    else:
+                        c_w = c_h = min_size / 2.
+                        out_boxes[h, w, idx, :] = [
+                            (c_x - c_w) / img_width, (c_y - c_h) / img_height,
+                            (c_x + c_w) / img_width, (c_y + c_h) / img_height
+                        ]
+                        idx += 1
+                        if len(max_sizes) > 0:
+                            max_size = max_sizes[s]
+                            # second prior: aspect_ratio = 1,
+                            c_w = c_h = math.sqrt(min_size * max_size) / 2
+                            out_boxes[h, w, idx, :] = [(c_x - c_w) / img_width,
+                                                       (c_y - c_h) / img_height,
+                                                       (c_x + c_w) / img_width,
+                                                       (c_y + c_h) / img_height]
+                            idx += 1
+
+                        # rest of priors
+                        for r in range(len(output_ratios)):
+                            ar = output_ratios[r]
+                            if abs(ar - 1.) < 1e-6:
+                                continue
+                            c_w = min_size * math.sqrt(ar) / 2
+                            c_h = (min_size / math.sqrt(ar)) / 2
+                            out_boxes[h, w, idx, :] = [(c_x - c_w) / img_width,
+                                                       (c_y - c_h) / img_height,
+                                                       (c_x + c_w) / img_width,
+                                                       (c_y + c_h) / img_height]
+                            idx += 1
+
+        if clip:
+            out_boxes = np.clip(out_boxes, 0.0, 1.0)
+        # set the variance.
+        out_var = np.tile(variances,
+                          (feature_height, feature_width, num_priors, 1))
+
+        #make node that
+
+        node_boxes = graph.make_node(
+            'Constant',
+            inputs=[],
+            outputs=node.output('Boxes'),
+            dtype=dtypes.ONNX.FLOAT,
+            dims=out_boxes.shape,
+            value=out_boxes.flatten().tolist())
+
+        node_vars = graph.make_node(
+            'Constant',
+            inputs=[],
+            outputs=node.output('Variances'),
+            dtype=dtypes.ONNX.FLOAT,
+            dims=out_var.shape,
+            value=out_var.flatten().tolist())
```

## paddle2onnx/legacy/op_mapper/detection/yolo_box.py

 * *Ordering differences only*

```diff
@@ -1,506 +1,506 @@
-# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import sys
-import numpy as np
-from paddle2onnx.legacy.constant import dtypes
-from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
-from paddle2onnx.legacy.op_mapper import mapper_helper
-from onnx import TensorProto
-import paddle
-
-MAX_FLOAT32 = 3.402823466E+38
-
-
-@op_mapper('yolo_box')
-class YOLOBox():
-    support_opset_verison_range = (9, 12)
-
-    node_pred_box_x1_decode = None
-    node_pred_box_y1_decode = None
-    node_pred_box_x2_decode = None
-    node_pred_box_y2_decode = None
-    node_pred_box_x2_sub_w = None
-    node_pred_box_y2_sub_h = None
-
-    @classmethod
-    def opset_9(cls, graph, node, **kw):
-        model_name = node.output('Boxes', 0)
-        input_shape = node.input_shape('X', 0)
-        mapper_helper.is_static_shape(input_shape)
-        image_size = node.input('ImgSize')
-        input_height = input_shape[2]
-        input_width = input_shape[3]
-        class_num = node.attr('class_num')
-        anchors = node.attr('anchors')
-        num_anchors = int(len(anchors)) // 2
-        scale_x_y = node.attr('scale_x_y')
-        downsample_ratio = node.attr('downsample_ratio')
-        input_size = input_height * downsample_ratio
-        conf_thresh = node.attr('conf_thresh')
-        conf_thresh_mat = [conf_thresh
-                           ] * num_anchors * input_height * input_width
-
-        cls.score_shape = [
-            1, input_height * input_width * int(num_anchors), class_num
-        ]
-
-        im_outputs = []
-
-        x_shape = [1, num_anchors, 5 + class_num, input_height, input_width]
-        node_x_shape = graph.make_node(
-            'Constant', attrs={'dtype': dtypes.ONNX.INT64,
-                               'value': x_shape})
-
-        node_x_reshape = graph.make_node(
-            'Reshape', inputs=[node.input('X')[0], node_x_shape])
-        node_x_transpose = graph.make_node(
-            'Transpose', inputs=[node_x_reshape], perm=[0, 1, 3, 4, 2])
-
-        range_x = []
-        range_y = []
-        for i in range(0, input_width):
-            range_x.append(i)
-        for j in range(0, input_height):
-            range_y.append(j)
-
-        node_range_x = graph.make_node(
-            'Constant',
-            attrs={
-                'dtype': dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
-                'value': range_x,
-            })
-
-        node_range_y = graph.make_node(
-            'Constant',
-            inputs=[],
-            attrs={
-                'dtype': dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
-                'value': range_y,
-            })
-
-        range_x_new_shape = [1, input_width]
-        range_y_new_shape = [input_height, 1]
-
-        node_range_x_new_shape = graph.make_node(
-            'Constant',
-            inputs=[],
-            dtype=dtypes.ONNX.INT64,
-            value=range_x_new_shape)
-        node_range_y_new_shape = graph.make_node(
-            'Constant',
-            inputs=[],
-            dtype=dtypes.ONNX.INT64,
-            value=range_y_new_shape)
-
-        node_range_x_reshape = graph.make_node(
-            'Reshape', inputs=[node_range_x, node_range_x_new_shape])
-        node_range_y_reshape = graph.make_node(
-            'Reshape', inputs=[node_range_y, node_range_y_new_shape])
-
-        node_grid_x = graph.make_node(
-            "Tile", inputs=[node_range_x_reshape, node_range_y_new_shape])
-
-        node_grid_y = graph.make_node(
-            "Tile", inputs=[node_range_y_reshape, node_range_x_new_shape])
-
-        node_box_x = model_name + "@box_x"
-        node_box_y = model_name + "@box_y"
-        node_box_w = model_name + "@box_w"
-        node_box_h = model_name + "@box_h"
-        node_conf = model_name + "@conf"
-        node_prob = model_name + "@prob"
-        output = [
-            node_box_x, node_box_y, node_box_w, node_box_h, node_conf, node_prob
-        ]
-        node_split_input = mapper_helper.split_helper(
-            graph, [node_x_transpose],
-            output,
-            -1, [1, 1, 1, 1, 1, class_num],
-            dtype=node.input_dtype('X', 0))
-
-        node_box_x_sigmoid = graph.make_node("Sigmoid", inputs=[node_box_x])
-
-        node_box_y_sigmoid = graph.make_node("Sigmoid", inputs=[node_box_y])
-
-        if scale_x_y is not None:
-            bias_x_y = -0.5 * (scale_x_y - 1.0)
-            scale_x_y_node = graph.make_node(
-                'Constant',
-                attrs={
-                    'dtype':
-                    dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
-                    'value': scale_x_y
-                })
-
-            bias_x_y_node = graph.make_node(
-                'Constant',
-                attrs={
-                    'dtype':
-                    dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
-                    'value': bias_x_y
-                })
-            node_box_x_sigmoid = graph.make_node(
-                "Mul", inputs=[node_box_x_sigmoid, scale_x_y_node])
-            node_box_x_sigmoid = graph.make_node(
-                "Add", inputs=[node_box_x_sigmoid, bias_x_y_node])
-            node_box_y_sigmoid = graph.make_node(
-                "Mul", inputs=[node_box_y_sigmoid, scale_x_y_node])
-            node_box_y_sigmoid = graph.make_node(
-                "Add", inputs=[node_box_y_sigmoid, bias_x_y_node])
-        node_box_x_squeeze = mapper_helper.squeeze_helper(
-            graph, node_box_x_sigmoid, [4])
-
-        node_box_y_squeeze = mapper_helper.squeeze_helper(
-            graph, node_box_y_sigmoid, [4])
-
-        node_box_x_add_grid = graph.make_node(
-            "Add", inputs=[node_grid_x, node_box_x_squeeze])
-
-        node_box_y_add_grid = graph.make_node(
-            "Add", inputs=[node_grid_y, node_box_y_squeeze])
-
-        node_input_h = graph.make_node(
-            'Constant',
-            inputs=[],
-            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
-            value=[input_height])
-
-        node_input_w = graph.make_node(
-            'Constant',
-            inputs=[],
-            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
-            value=[input_width])
-
-        node_box_x_encode = graph.make_node(
-            'Div', inputs=[node_box_x_add_grid, node_input_w])
-
-        node_box_y_encode = graph.make_node(
-            'Div', inputs=[node_box_y_add_grid, node_input_h])
-
-        node_anchor_tensor = graph.make_node(
-            "Constant",
-            inputs=[],
-            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
-            value=anchors)
-
-        anchor_shape = [int(num_anchors), 2]
-        node_anchor_shape = graph.make_node(
-            "Constant", inputs=[], dtype=dtypes.ONNX.INT64, value=anchor_shape)
-
-        node_anchor_tensor_reshape = graph.make_node(
-            "Reshape", inputs=[node_anchor_tensor, node_anchor_shape])
-
-        node_input_size = graph.make_node(
-            "Constant",
-            inputs=[],
-            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
-            value=[input_size])
-
-        node_anchors_div_input_size = graph.make_node(
-            "Div", inputs=[node_anchor_tensor_reshape, node_input_size])
-
-        node_anchor_w = model_name + "@anchor_w"
-        node_anchor_h = model_name + "@anchor_h"
-
-        node_anchor_split = mapper_helper.split_helper(
-            graph,
-            inputs=node_anchors_div_input_size,
-            axis=1,
-            split=[1, 1],
-            outputs=[node_anchor_w, node_anchor_h],
-            dtype=node.input_dtype('X', 0))
-
-        new_anchor_shape = [1, int(num_anchors), 1, 1]
-        node_new_anchor_shape = graph.make_node(
-            'Constant',
-            inputs=[],
-            dtype=dtypes.ONNX.INT64,
-            value=new_anchor_shape)
-
-        node_anchor_w_reshape = graph.make_node(
-            'Reshape', inputs=[node_anchor_w, node_new_anchor_shape])
-
-        node_anchor_h_reshape = graph.make_node(
-            'Reshape', inputs=[node_anchor_h, node_new_anchor_shape])
-
-        node_box_w_squeeze = mapper_helper.squeeze_helper(graph, node_box_w,
-                                                          [4])
-        node_box_h_squeeze = mapper_helper.squeeze_helper(graph, node_box_h,
-                                                          [4])
-
-        node_box_w_exp = graph.make_node("Exp", inputs=[node_box_w_squeeze])
-        node_box_h_exp = graph.make_node("Exp", inputs=[node_box_h_squeeze])
-
-        node_box_w_encode = graph.make_node(
-            'Mul', inputs=[node_box_w_exp, node_anchor_w_reshape])
-
-        node_box_h_encode = graph.make_node(
-            'Mul', inputs=[node_box_h_exp, node_anchor_h_reshape])
-
-        node_conf_sigmoid = graph.make_node('Sigmoid', inputs=[node_conf])
-
-        node_conf_thresh = graph.make_node(
-            'Constant',
-            inputs=[],
-            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
-            value=conf_thresh_mat)
-
-        conf_shape = [1, int(num_anchors), input_height, input_width, 1]
-        node_conf_shape = graph.make_node(
-            'Constant', inputs=[], dtype=dtypes.ONNX.INT64, value=conf_shape)
-
-        node_conf_thresh_reshape = graph.make_node(
-            'Reshape', inputs=[node_conf_thresh, node_conf_shape])
-
-        node_conf_sub = graph.make_node(
-            'Sub', inputs=[node_conf_sigmoid, node_conf_thresh_reshape])
-
-        node_conf_clip = mapper_helper.clip_helper(graph, node, node_conf_sub,
-                                                   float(MAX_FLOAT32), 0.0)
-
-        node_zeros = graph.make_node(
-            'Constant',
-            inputs=[],
-            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
-            value=[0])
-
-        node_conf_clip_bool = graph.make_node(
-            'Greater', inputs=[node_conf_clip, node_zeros])
-
-        node_conf_clip_cast = graph.make_node(
-            'Cast',
-            inputs=[node_conf_clip_bool],
-            to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])
-
-        node_conf_set_zero = graph.make_node(
-            'Mul', inputs=[node_conf_sigmoid, node_conf_clip_cast])
-
-        node_prob_sigmoid = graph.make_node('Sigmoid', inputs=[node_prob])
-
-        new_shape = [1, int(num_anchors), input_height, input_width, 1]
-        node_new_shape = graph.make_node(
-            'Constant',
-            inputs=[],
-            dtype=dtypes.ONNX.INT64,
-            dims=[len(new_shape)],
-            value=new_shape)
-
-        node_conf_new_shape = graph.make_node(
-            'Reshape', inputs=[node_conf_set_zero, node_new_shape])
-
-        cls.node_score = graph.make_node(
-            'Mul', inputs=[node_prob_sigmoid, node_conf_new_shape])
-
-        node_conf_bool = graph.make_node(
-            'Greater', inputs=[node_conf_new_shape, node_zeros])
-
-        node_box_x_new_shape = graph.make_node(
-            'Reshape', inputs=[node_box_x_encode, node_new_shape])
-
-        node_box_y_new_shape = graph.make_node(
-            'Reshape', inputs=[node_box_y_encode, node_new_shape])
-
-        node_box_w_new_shape = graph.make_node(
-            'Reshape', inputs=[node_box_w_encode, node_new_shape])
-
-        node_box_h_new_shape = graph.make_node(
-            'Reshape', inputs=[node_box_h_encode, node_new_shape])
-
-        node_pred_box = graph.make_node(
-            'Concat',
-            inputs=[node_box_x_new_shape, node_box_y_new_shape, \
-                   node_box_w_new_shape, node_box_h_new_shape],
-            axis=4)
-
-        node_conf_cast = graph.make_node(
-            'Cast',
-            inputs=[node_conf_bool],
-            to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])
-
-        node_pred_box_mul_conf = graph.make_node(
-            'Mul', inputs=[node_pred_box, node_conf_cast])
-
-        box_shape = [1, int(num_anchors) * input_height * input_width, 4]
-        node_box_shape = graph.make_node(
-            'Constant', inputs=[], dtype=dtypes.ONNX.INT64, value=box_shape)
-
-        node_pred_box_new_shape = graph.make_node(
-            'Reshape', inputs=[node_pred_box_mul_conf, node_box_shape])
-
-        node_pred_box_x = model_name + "@_pred_box_x"
-        node_pred_box_y = model_name + "@_pred_box_y"
-        node_pred_box_w = model_name + "@_pred_box_w"
-        node_pred_box_h = model_name + "@_pred_box_h"
-        if node.input_dtype('X', 0) == paddle.float64:
-            node_pred_box_new_shape = graph.make_node(
-                'Cast', inputs=[node_pred_box_new_shape], to=TensorProto.FLOAT)
-        node_pred_box_split = mapper_helper.split_helper(
-            graph,
-            inputs=node_pred_box_new_shape,
-            axis=2,
-            split=[1, 1, 1, 1],
-            outputs=[
-                node_pred_box_x, node_pred_box_y, node_pred_box_w,
-                node_pred_box_h
-            ],
-            dtype=node.input_dtype('X', 0))
-
-        if node.input_dtype('X', 0) == paddle.float64:
-            node_pred_box_x = graph.make_node(
-                'Cast',
-                inputs=[node_pred_box_x],
-                to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])
-            node_pred_box_y = graph.make_node(
-                'Cast',
-                inputs=[node_pred_box_y],
-                to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])
-            node_pred_box_w = graph.make_node(
-                'Cast',
-                inputs=[node_pred_box_w],
-                to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])
-            node_pred_box_h = graph.make_node(
-                'Cast',
-                inputs=[node_pred_box_h],
-                to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])
-        node_number_two = graph.make_node(
-            "Constant",
-            inputs=[],
-            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
-            value=[2])
-
-        node_half_w = graph.make_node(
-            "Div", inputs=[node_pred_box_w, node_number_two])
-
-        node_half_h = graph.make_node(
-            "Div", inputs=[node_pred_box_h, node_number_two])
-
-        node_pred_box_x1 = graph.make_node(
-            'Sub', inputs=[node_pred_box_x, node_half_w])
-
-        node_pred_box_y1 = graph.make_node(
-            'Sub', inputs=[node_pred_box_y, node_half_h])
-
-        node_pred_box_x2 = graph.make_node(
-            'Add', inputs=[node_pred_box_x, node_half_w])
-
-        node_pred_box_y2 = graph.make_node(
-            'Add', inputs=[node_pred_box_y, node_half_h])
-
-        node_sqeeze_image_size = mapper_helper.squeeze_helper(
-            graph, image_size[0], [0])
-
-        node_img_height = model_name + "@img_height"
-        node_img_width = model_name + "@img_width"
-        node_image_size_split = mapper_helper.split_helper(
-            graph, [node_sqeeze_image_size], [node_img_height, node_img_width],
-            -1, [1, 1],
-            dtype=node.input_dtype('X', 0))
-
-        node_img_width_cast = graph.make_node(
-            'Cast',
-            inputs=[node_img_width],
-            to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])
-
-        node_img_height_cast = graph.make_node(
-            'Cast',
-            inputs=[node_img_height],
-            to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])
-
-        cls.node_pred_box_x1_decode = graph.make_node(
-            'Mul',
-            inputs=[node_pred_box_x1, node_img_width_cast])  #boxes[box_idx]
-
-        cls.node_pred_box_y1_decode = graph.make_node(
-            'Mul', inputs=[node_pred_box_y1,
-                           node_img_height_cast])  #boxes[box_idx + 1]
-
-        cls.node_pred_box_x2_decode = graph.make_node(
-            'Mul',
-            inputs=[node_pred_box_x2, node_img_width_cast])  #boxes[box_idx + 2]
-
-        cls.node_pred_box_y2_decode = graph.make_node(
-            'Mul', inputs=[node_pred_box_y2,
-                           node_img_height_cast])  #boxes[box_idx + 3]
-
-        if node.attr('clip_bbox'):
-            node_number_one = graph.make_node(
-                'Constant',
-                inputs=[],
-                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
-                value=[1])
-
-            node_new_img_height = graph.make_node(
-                'Sub', inputs=[node_img_height_cast, node_number_one])
-
-            node_new_img_width = graph.make_node(
-                'Sub', inputs=[node_img_width_cast, node_number_one])
-
-            cls.node_pred_box_x2_sub_w = graph.make_node(
-                'Sub',
-                inputs=[cls.node_pred_box_x2_decode, node_new_img_width])
-
-            cls.node_pred_box_y2_sub_h = graph.make_node(
-                'Sub',
-                inputs=[cls.node_pred_box_y2_decode, node_new_img_height])
-
-            node_pred_box_x1_clip = mapper_helper.clip_helper(
-                graph, node, cls.node_pred_box_x1_decode,
-                float(MAX_FLOAT32), 0.0)
-            node_pred_box_y1_clip = mapper_helper.clip_helper(
-                graph, node, cls.node_pred_box_y1_decode,
-                float(MAX_FLOAT32), 0.0)
-            node_pred_box_x2_clip = mapper_helper.clip_helper(
-                graph, node, cls.node_pred_box_x2_sub_w,
-                float(MAX_FLOAT32), 0.0)
-            node_pred_box_y2_clip = mapper_helper.clip_helper(
-                graph, node, cls.node_pred_box_y2_sub_h,
-                float(MAX_FLOAT32), 0.0)
-            node_pred_box_x2_res = graph.make_node(
-                'Sub',
-                inputs=[cls.node_pred_box_x2_decode, node_pred_box_x2_clip])
-
-            node_pred_box_y2_res = graph.make_node(
-                'Sub',
-                inputs=[cls.node_pred_box_y2_decode, node_pred_box_y2_clip])
-
-            node_pred_box_result = graph.make_node(
-                'Concat',
-                inputs=[
-                    node_pred_box_x1_clip, node_pred_box_y1_clip,
-                    node_pred_box_x2_res, node_pred_box_y2_res
-                ],
-                outputs=node.output('Boxes'),
-                axis=-1)
-        else:
-            node_pred_box_result = graph.make_node(
-                'Concat',
-                inputs=[
-                    cls.node_pred_box_x1_decode, cls.node_pred_box_y1_decode,
-                    cls.node_pred_box_x2_decode, cls.node_pred_box_y2_decode
-                ],
-                outputs=node.output('Boxes'),
-                axis=-1)
-        node_score_shape = graph.make_node(
-            "Constant",
-            inputs=[],
-            dtype=dtypes.ONNX.INT64,
-            value=cls.score_shape)
-
-        node_score_new_shape = graph.make_node(
-            'Reshape',
-            inputs=[cls.node_score, node_score_shape],
-            outputs=node.output('Scores'))
+# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import sys
+import numpy as np
+from paddle2onnx.legacy.constant import dtypes
+from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
+from paddle2onnx.legacy.op_mapper import mapper_helper
+from onnx import TensorProto
+import paddle
+
+MAX_FLOAT32 = 3.402823466E+38
+
+
+@op_mapper('yolo_box')
+class YOLOBox():
+    support_opset_verison_range = (9, 12)
+
+    node_pred_box_x1_decode = None
+    node_pred_box_y1_decode = None
+    node_pred_box_x2_decode = None
+    node_pred_box_y2_decode = None
+    node_pred_box_x2_sub_w = None
+    node_pred_box_y2_sub_h = None
+
+    @classmethod
+    def opset_9(cls, graph, node, **kw):
+        model_name = node.output('Boxes', 0)
+        input_shape = node.input_shape('X', 0)
+        mapper_helper.is_static_shape(input_shape)
+        image_size = node.input('ImgSize')
+        input_height = input_shape[2]
+        input_width = input_shape[3]
+        class_num = node.attr('class_num')
+        anchors = node.attr('anchors')
+        num_anchors = int(len(anchors)) // 2
+        scale_x_y = node.attr('scale_x_y')
+        downsample_ratio = node.attr('downsample_ratio')
+        input_size = input_height * downsample_ratio
+        conf_thresh = node.attr('conf_thresh')
+        conf_thresh_mat = [conf_thresh
+                           ] * num_anchors * input_height * input_width
+
+        cls.score_shape = [
+            1, input_height * input_width * int(num_anchors), class_num
+        ]
+
+        im_outputs = []
+
+        x_shape = [1, num_anchors, 5 + class_num, input_height, input_width]
+        node_x_shape = graph.make_node(
+            'Constant', attrs={'dtype': dtypes.ONNX.INT64,
+                               'value': x_shape})
+
+        node_x_reshape = graph.make_node(
+            'Reshape', inputs=[node.input('X')[0], node_x_shape])
+        node_x_transpose = graph.make_node(
+            'Transpose', inputs=[node_x_reshape], perm=[0, 1, 3, 4, 2])
+
+        range_x = []
+        range_y = []
+        for i in range(0, input_width):
+            range_x.append(i)
+        for j in range(0, input_height):
+            range_y.append(j)
+
+        node_range_x = graph.make_node(
+            'Constant',
+            attrs={
+                'dtype': dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
+                'value': range_x,
+            })
+
+        node_range_y = graph.make_node(
+            'Constant',
+            inputs=[],
+            attrs={
+                'dtype': dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
+                'value': range_y,
+            })
+
+        range_x_new_shape = [1, input_width]
+        range_y_new_shape = [input_height, 1]
+
+        node_range_x_new_shape = graph.make_node(
+            'Constant',
+            inputs=[],
+            dtype=dtypes.ONNX.INT64,
+            value=range_x_new_shape)
+        node_range_y_new_shape = graph.make_node(
+            'Constant',
+            inputs=[],
+            dtype=dtypes.ONNX.INT64,
+            value=range_y_new_shape)
+
+        node_range_x_reshape = graph.make_node(
+            'Reshape', inputs=[node_range_x, node_range_x_new_shape])
+        node_range_y_reshape = graph.make_node(
+            'Reshape', inputs=[node_range_y, node_range_y_new_shape])
+
+        node_grid_x = graph.make_node(
+            "Tile", inputs=[node_range_x_reshape, node_range_y_new_shape])
+
+        node_grid_y = graph.make_node(
+            "Tile", inputs=[node_range_y_reshape, node_range_x_new_shape])
+
+        node_box_x = model_name + "@box_x"
+        node_box_y = model_name + "@box_y"
+        node_box_w = model_name + "@box_w"
+        node_box_h = model_name + "@box_h"
+        node_conf = model_name + "@conf"
+        node_prob = model_name + "@prob"
+        output = [
+            node_box_x, node_box_y, node_box_w, node_box_h, node_conf, node_prob
+        ]
+        node_split_input = mapper_helper.split_helper(
+            graph, [node_x_transpose],
+            output,
+            -1, [1, 1, 1, 1, 1, class_num],
+            dtype=node.input_dtype('X', 0))
+
+        node_box_x_sigmoid = graph.make_node("Sigmoid", inputs=[node_box_x])
+
+        node_box_y_sigmoid = graph.make_node("Sigmoid", inputs=[node_box_y])
+
+        if scale_x_y is not None:
+            bias_x_y = -0.5 * (scale_x_y - 1.0)
+            scale_x_y_node = graph.make_node(
+                'Constant',
+                attrs={
+                    'dtype':
+                    dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
+                    'value': scale_x_y
+                })
+
+            bias_x_y_node = graph.make_node(
+                'Constant',
+                attrs={
+                    'dtype':
+                    dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
+                    'value': bias_x_y
+                })
+            node_box_x_sigmoid = graph.make_node(
+                "Mul", inputs=[node_box_x_sigmoid, scale_x_y_node])
+            node_box_x_sigmoid = graph.make_node(
+                "Add", inputs=[node_box_x_sigmoid, bias_x_y_node])
+            node_box_y_sigmoid = graph.make_node(
+                "Mul", inputs=[node_box_y_sigmoid, scale_x_y_node])
+            node_box_y_sigmoid = graph.make_node(
+                "Add", inputs=[node_box_y_sigmoid, bias_x_y_node])
+        node_box_x_squeeze = mapper_helper.squeeze_helper(
+            graph, node_box_x_sigmoid, [4])
+
+        node_box_y_squeeze = mapper_helper.squeeze_helper(
+            graph, node_box_y_sigmoid, [4])
+
+        node_box_x_add_grid = graph.make_node(
+            "Add", inputs=[node_grid_x, node_box_x_squeeze])
+
+        node_box_y_add_grid = graph.make_node(
+            "Add", inputs=[node_grid_y, node_box_y_squeeze])
+
+        node_input_h = graph.make_node(
+            'Constant',
+            inputs=[],
+            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
+            value=[input_height])
+
+        node_input_w = graph.make_node(
+            'Constant',
+            inputs=[],
+            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
+            value=[input_width])
+
+        node_box_x_encode = graph.make_node(
+            'Div', inputs=[node_box_x_add_grid, node_input_w])
+
+        node_box_y_encode = graph.make_node(
+            'Div', inputs=[node_box_y_add_grid, node_input_h])
+
+        node_anchor_tensor = graph.make_node(
+            "Constant",
+            inputs=[],
+            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
+            value=anchors)
+
+        anchor_shape = [int(num_anchors), 2]
+        node_anchor_shape = graph.make_node(
+            "Constant", inputs=[], dtype=dtypes.ONNX.INT64, value=anchor_shape)
+
+        node_anchor_tensor_reshape = graph.make_node(
+            "Reshape", inputs=[node_anchor_tensor, node_anchor_shape])
+
+        node_input_size = graph.make_node(
+            "Constant",
+            inputs=[],
+            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
+            value=[input_size])
+
+        node_anchors_div_input_size = graph.make_node(
+            "Div", inputs=[node_anchor_tensor_reshape, node_input_size])
+
+        node_anchor_w = model_name + "@anchor_w"
+        node_anchor_h = model_name + "@anchor_h"
+
+        node_anchor_split = mapper_helper.split_helper(
+            graph,
+            inputs=node_anchors_div_input_size,
+            axis=1,
+            split=[1, 1],
+            outputs=[node_anchor_w, node_anchor_h],
+            dtype=node.input_dtype('X', 0))
+
+        new_anchor_shape = [1, int(num_anchors), 1, 1]
+        node_new_anchor_shape = graph.make_node(
+            'Constant',
+            inputs=[],
+            dtype=dtypes.ONNX.INT64,
+            value=new_anchor_shape)
+
+        node_anchor_w_reshape = graph.make_node(
+            'Reshape', inputs=[node_anchor_w, node_new_anchor_shape])
+
+        node_anchor_h_reshape = graph.make_node(
+            'Reshape', inputs=[node_anchor_h, node_new_anchor_shape])
+
+        node_box_w_squeeze = mapper_helper.squeeze_helper(graph, node_box_w,
+                                                          [4])
+        node_box_h_squeeze = mapper_helper.squeeze_helper(graph, node_box_h,
+                                                          [4])
+
+        node_box_w_exp = graph.make_node("Exp", inputs=[node_box_w_squeeze])
+        node_box_h_exp = graph.make_node("Exp", inputs=[node_box_h_squeeze])
+
+        node_box_w_encode = graph.make_node(
+            'Mul', inputs=[node_box_w_exp, node_anchor_w_reshape])
+
+        node_box_h_encode = graph.make_node(
+            'Mul', inputs=[node_box_h_exp, node_anchor_h_reshape])
+
+        node_conf_sigmoid = graph.make_node('Sigmoid', inputs=[node_conf])
+
+        node_conf_thresh = graph.make_node(
+            'Constant',
+            inputs=[],
+            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
+            value=conf_thresh_mat)
+
+        conf_shape = [1, int(num_anchors), input_height, input_width, 1]
+        node_conf_shape = graph.make_node(
+            'Constant', inputs=[], dtype=dtypes.ONNX.INT64, value=conf_shape)
+
+        node_conf_thresh_reshape = graph.make_node(
+            'Reshape', inputs=[node_conf_thresh, node_conf_shape])
+
+        node_conf_sub = graph.make_node(
+            'Sub', inputs=[node_conf_sigmoid, node_conf_thresh_reshape])
+
+        node_conf_clip = mapper_helper.clip_helper(graph, node, node_conf_sub,
+                                                   float(MAX_FLOAT32), 0.0)
+
+        node_zeros = graph.make_node(
+            'Constant',
+            inputs=[],
+            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
+            value=[0])
+
+        node_conf_clip_bool = graph.make_node(
+            'Greater', inputs=[node_conf_clip, node_zeros])
+
+        node_conf_clip_cast = graph.make_node(
+            'Cast',
+            inputs=[node_conf_clip_bool],
+            to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])
+
+        node_conf_set_zero = graph.make_node(
+            'Mul', inputs=[node_conf_sigmoid, node_conf_clip_cast])
+
+        node_prob_sigmoid = graph.make_node('Sigmoid', inputs=[node_prob])
+
+        new_shape = [1, int(num_anchors), input_height, input_width, 1]
+        node_new_shape = graph.make_node(
+            'Constant',
+            inputs=[],
+            dtype=dtypes.ONNX.INT64,
+            dims=[len(new_shape)],
+            value=new_shape)
+
+        node_conf_new_shape = graph.make_node(
+            'Reshape', inputs=[node_conf_set_zero, node_new_shape])
+
+        cls.node_score = graph.make_node(
+            'Mul', inputs=[node_prob_sigmoid, node_conf_new_shape])
+
+        node_conf_bool = graph.make_node(
+            'Greater', inputs=[node_conf_new_shape, node_zeros])
+
+        node_box_x_new_shape = graph.make_node(
+            'Reshape', inputs=[node_box_x_encode, node_new_shape])
+
+        node_box_y_new_shape = graph.make_node(
+            'Reshape', inputs=[node_box_y_encode, node_new_shape])
+
+        node_box_w_new_shape = graph.make_node(
+            'Reshape', inputs=[node_box_w_encode, node_new_shape])
+
+        node_box_h_new_shape = graph.make_node(
+            'Reshape', inputs=[node_box_h_encode, node_new_shape])
+
+        node_pred_box = graph.make_node(
+            'Concat',
+            inputs=[node_box_x_new_shape, node_box_y_new_shape, \
+                   node_box_w_new_shape, node_box_h_new_shape],
+            axis=4)
+
+        node_conf_cast = graph.make_node(
+            'Cast',
+            inputs=[node_conf_bool],
+            to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])
+
+        node_pred_box_mul_conf = graph.make_node(
+            'Mul', inputs=[node_pred_box, node_conf_cast])
+
+        box_shape = [1, int(num_anchors) * input_height * input_width, 4]
+        node_box_shape = graph.make_node(
+            'Constant', inputs=[], dtype=dtypes.ONNX.INT64, value=box_shape)
+
+        node_pred_box_new_shape = graph.make_node(
+            'Reshape', inputs=[node_pred_box_mul_conf, node_box_shape])
+
+        node_pred_box_x = model_name + "@_pred_box_x"
+        node_pred_box_y = model_name + "@_pred_box_y"
+        node_pred_box_w = model_name + "@_pred_box_w"
+        node_pred_box_h = model_name + "@_pred_box_h"
+        if node.input_dtype('X', 0) == paddle.float64:
+            node_pred_box_new_shape = graph.make_node(
+                'Cast', inputs=[node_pred_box_new_shape], to=TensorProto.FLOAT)
+        node_pred_box_split = mapper_helper.split_helper(
+            graph,
+            inputs=node_pred_box_new_shape,
+            axis=2,
+            split=[1, 1, 1, 1],
+            outputs=[
+                node_pred_box_x, node_pred_box_y, node_pred_box_w,
+                node_pred_box_h
+            ],
+            dtype=node.input_dtype('X', 0))
+
+        if node.input_dtype('X', 0) == paddle.float64:
+            node_pred_box_x = graph.make_node(
+                'Cast',
+                inputs=[node_pred_box_x],
+                to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])
+            node_pred_box_y = graph.make_node(
+                'Cast',
+                inputs=[node_pred_box_y],
+                to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])
+            node_pred_box_w = graph.make_node(
+                'Cast',
+                inputs=[node_pred_box_w],
+                to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])
+            node_pred_box_h = graph.make_node(
+                'Cast',
+                inputs=[node_pred_box_h],
+                to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])
+        node_number_two = graph.make_node(
+            "Constant",
+            inputs=[],
+            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
+            value=[2])
+
+        node_half_w = graph.make_node(
+            "Div", inputs=[node_pred_box_w, node_number_two])
+
+        node_half_h = graph.make_node(
+            "Div", inputs=[node_pred_box_h, node_number_two])
+
+        node_pred_box_x1 = graph.make_node(
+            'Sub', inputs=[node_pred_box_x, node_half_w])
+
+        node_pred_box_y1 = graph.make_node(
+            'Sub', inputs=[node_pred_box_y, node_half_h])
+
+        node_pred_box_x2 = graph.make_node(
+            'Add', inputs=[node_pred_box_x, node_half_w])
+
+        node_pred_box_y2 = graph.make_node(
+            'Add', inputs=[node_pred_box_y, node_half_h])
+
+        node_sqeeze_image_size = mapper_helper.squeeze_helper(
+            graph, image_size[0], [0])
+
+        node_img_height = model_name + "@img_height"
+        node_img_width = model_name + "@img_width"
+        node_image_size_split = mapper_helper.split_helper(
+            graph, [node_sqeeze_image_size], [node_img_height, node_img_width],
+            -1, [1, 1],
+            dtype=node.input_dtype('X', 0))
+
+        node_img_width_cast = graph.make_node(
+            'Cast',
+            inputs=[node_img_width],
+            to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])
+
+        node_img_height_cast = graph.make_node(
+            'Cast',
+            inputs=[node_img_height],
+            to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])
+
+        cls.node_pred_box_x1_decode = graph.make_node(
+            'Mul',
+            inputs=[node_pred_box_x1, node_img_width_cast])  #boxes[box_idx]
+
+        cls.node_pred_box_y1_decode = graph.make_node(
+            'Mul', inputs=[node_pred_box_y1,
+                           node_img_height_cast])  #boxes[box_idx + 1]
+
+        cls.node_pred_box_x2_decode = graph.make_node(
+            'Mul',
+            inputs=[node_pred_box_x2, node_img_width_cast])  #boxes[box_idx + 2]
+
+        cls.node_pred_box_y2_decode = graph.make_node(
+            'Mul', inputs=[node_pred_box_y2,
+                           node_img_height_cast])  #boxes[box_idx + 3]
+
+        if node.attr('clip_bbox'):
+            node_number_one = graph.make_node(
+                'Constant',
+                inputs=[],
+                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],
+                value=[1])
+
+            node_new_img_height = graph.make_node(
+                'Sub', inputs=[node_img_height_cast, node_number_one])
+
+            node_new_img_width = graph.make_node(
+                'Sub', inputs=[node_img_width_cast, node_number_one])
+
+            cls.node_pred_box_x2_sub_w = graph.make_node(
+                'Sub',
+                inputs=[cls.node_pred_box_x2_decode, node_new_img_width])
+
+            cls.node_pred_box_y2_sub_h = graph.make_node(
+                'Sub',
+                inputs=[cls.node_pred_box_y2_decode, node_new_img_height])
+
+            node_pred_box_x1_clip = mapper_helper.clip_helper(
+                graph, node, cls.node_pred_box_x1_decode,
+                float(MAX_FLOAT32), 0.0)
+            node_pred_box_y1_clip = mapper_helper.clip_helper(
+                graph, node, cls.node_pred_box_y1_decode,
+                float(MAX_FLOAT32), 0.0)
+            node_pred_box_x2_clip = mapper_helper.clip_helper(
+                graph, node, cls.node_pred_box_x2_sub_w,
+                float(MAX_FLOAT32), 0.0)
+            node_pred_box_y2_clip = mapper_helper.clip_helper(
+                graph, node, cls.node_pred_box_y2_sub_h,
+                float(MAX_FLOAT32), 0.0)
+            node_pred_box_x2_res = graph.make_node(
+                'Sub',
+                inputs=[cls.node_pred_box_x2_decode, node_pred_box_x2_clip])
+
+            node_pred_box_y2_res = graph.make_node(
+                'Sub',
+                inputs=[cls.node_pred_box_y2_decode, node_pred_box_y2_clip])
+
+            node_pred_box_result = graph.make_node(
+                'Concat',
+                inputs=[
+                    node_pred_box_x1_clip, node_pred_box_y1_clip,
+                    node_pred_box_x2_res, node_pred_box_y2_res
+                ],
+                outputs=node.output('Boxes'),
+                axis=-1)
+        else:
+            node_pred_box_result = graph.make_node(
+                'Concat',
+                inputs=[
+                    cls.node_pred_box_x1_decode, cls.node_pred_box_y1_decode,
+                    cls.node_pred_box_x2_decode, cls.node_pred_box_y2_decode
+                ],
+                outputs=node.output('Boxes'),
+                axis=-1)
+        node_score_shape = graph.make_node(
+            "Constant",
+            inputs=[],
+            dtype=dtypes.ONNX.INT64,
+            value=cls.score_shape)
+
+        node_score_new_shape = graph.make_node(
+            'Reshape',
+            inputs=[cls.node_score, node_score_shape],
+            outputs=node.output('Scores'))
```

## paddle2onnx/legacy/op_mapper/sequence/__init__.py

 * *Ordering differences only*

```diff
@@ -1,13 +1,13 @@
-#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
```

## paddle2onnx/legacy/op_mapper/sequence/im2sequence.py

 * *Ordering differences only*

```diff
@@ -1,78 +1,78 @@
-# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from __future__ import absolute_import
-
-import numpy as np
-from paddle2onnx.legacy.constant import dtypes
-from paddle2onnx.utils import logging
-from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
-
-
-@op_mapper('im2sequence')
-class Im2Sequence():
-    support_opset_verison_range = (1, 12)
-
-    @classmethod
-    def opset_1(cls, graph, node, **kw):
-        n, c, h, w = node.input_shape('X', 0)
-        assert h > 0 and w > 0, "Only supported fixed input shape for im2sequence operator."
-        stride_h, stride_w = node.attr('strides')
-        paddings = node.attr('paddings')
-        assert node.attr(
-            'out_stride'
-        ) != 1, "Only out_stride==1 is supported for im2sequence operator."
-        h = h + paddings[0] + paddings[1]
-        w = w + paddings[1] + paddings[2]
-        kernel_h, kernel_w = node.attr('kernels')
-        out_h = 1 + (h - kernel_h + stride_h - 1) // stride_h
-        out_w = 1 + (w - kernel_w + stride_w - 1) // stride_w
-        h_steps = list()
-        for i in range(out_h):
-            h_steps.append([i * stride_h, i * stride_h + kernel_h])
-        w_steps = list()
-        for i in range(out_w):
-            w_steps.append([i * stride_w, i * stride_w + kernel_w])
-
-        slice_node_blocks = list()
-        for i in range(out_h):
-            for j in range(out_w):
-                starts_node = graph.make_node(
-                    'Constant',
-                    dtype=dtypes.ONNX.INT64,
-                    dims=[4],
-                    value=[0, 0, h_steps[i][0], w_steps[j][0]])
-                ends_node = graph.make_node(
-                    'Constant',
-                    dtype=dtypes.ONNX.INT64,
-                    dims=[4],
-                    value=[999999, 999999, h_steps[i][1], w_steps[j][1]])
-                nodes.extend([starts_node, ends_node])
-
-                slice_block_node = graph.make_node(
-                    'Slice',
-                    inputs=[node.input('X', 0), starts_node, ends_node])
-                flatten_block_node = graph.make_node(
-                    "Flatten", inputs=[slice_block_node], axis=0)
-                nodes.extend([slice_block_node, flatten_block_node])
-        concat_block_node = graph.make_node(
-            "Concat",
-            inputs=slice_node_blocks,
-            outputs=node.output('Out'),
-            axis=0)
-        logging.info("==========Importance Notice===========")
-        logging.info(
-            "Since im2sequence operator is used in your paddlepaddle model, the translated onnx model only support input data with batch_size=1."
-        )
-        logging.info("======================================")
+# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import numpy as np
+from paddle2onnx.legacy.constant import dtypes
+from paddle2onnx.utils import logging
+from paddle2onnx.legacy.op_mapper import OpMapper as op_mapper
+
+
+@op_mapper('im2sequence')
+class Im2Sequence():
+    support_opset_verison_range = (1, 12)
+
+    @classmethod
+    def opset_1(cls, graph, node, **kw):
+        n, c, h, w = node.input_shape('X', 0)
+        assert h > 0 and w > 0, "Only supported fixed input shape for im2sequence operator."
+        stride_h, stride_w = node.attr('strides')
+        paddings = node.attr('paddings')
+        assert node.attr(
+            'out_stride'
+        ) != 1, "Only out_stride==1 is supported for im2sequence operator."
+        h = h + paddings[0] + paddings[1]
+        w = w + paddings[1] + paddings[2]
+        kernel_h, kernel_w = node.attr('kernels')
+        out_h = 1 + (h - kernel_h + stride_h - 1) // stride_h
+        out_w = 1 + (w - kernel_w + stride_w - 1) // stride_w
+        h_steps = list()
+        for i in range(out_h):
+            h_steps.append([i * stride_h, i * stride_h + kernel_h])
+        w_steps = list()
+        for i in range(out_w):
+            w_steps.append([i * stride_w, i * stride_w + kernel_w])
+
+        slice_node_blocks = list()
+        for i in range(out_h):
+            for j in range(out_w):
+                starts_node = graph.make_node(
+                    'Constant',
+                    dtype=dtypes.ONNX.INT64,
+                    dims=[4],
+                    value=[0, 0, h_steps[i][0], w_steps[j][0]])
+                ends_node = graph.make_node(
+                    'Constant',
+                    dtype=dtypes.ONNX.INT64,
+                    dims=[4],
+                    value=[999999, 999999, h_steps[i][1], w_steps[j][1]])
+                nodes.extend([starts_node, ends_node])
+
+                slice_block_node = graph.make_node(
+                    'Slice',
+                    inputs=[node.input('X', 0), starts_node, ends_node])
+                flatten_block_node = graph.make_node(
+                    "Flatten", inputs=[slice_block_node], axis=0)
+                nodes.extend([slice_block_node, flatten_block_node])
+        concat_block_node = graph.make_node(
+            "Concat",
+            inputs=slice_node_blocks,
+            outputs=node.output('Out'),
+            axis=0)
+        logging.info("==========Importance Notice===========")
+        logging.info(
+            "Since im2sequence operator is used in your paddlepaddle model, the translated onnx model only support input data with batch_size=1."
+        )
+        logging.info("======================================")
```

## paddle2onnx/legacy/passes/__init__.py

 * *Ordering differences only*

```diff
@@ -1,17 +1,17 @@
-#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from .pass_manager import PassManager
-from .inplace_node_pass import InplaceNodePass
+#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from .pass_manager import PassManager
+from .inplace_node_pass import InplaceNodePass
 from .dumplicate_names_pass import DumplicateNamesPass
```

## paddle2onnx/legacy/passes/dumplicate_names_pass.py

 * *Ordering differences only*

```diff
@@ -1,91 +1,91 @@
-#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from paddle2onnx.legacy.passes import PassManager
-from paddle2onnx.utils import logging
-
-
-@PassManager('dumplicate_names_pass')
-class DumplicateNamesPass(object):
-
-    name_count = dict()
-
-    @classmethod
-    def generate_new_name(cls, name):
-        for saved_name in cls.name_count:
-            if name.startswith(saved_name):
-                cls.name_count[saved_name] += 1
-                new_name = saved_name + '.' + str(cls.name_count[saved_name])
-                return new_name
-        cls.name_count[name] = 1
-        new_name = name + '.' + str(cls.name_count[name])
-        return new_name
-
-    @classmethod
-    def run_pass(cls, onnx_graph):
-        renamer = {}
-        tensor_names = set()
-        for name, node in onnx_graph.parameters.items():
-            output = node.output
-            for opt in output:
-                assert opt not in tensor_names, "There's dumplicate names in parameters."
-                tensor_names.add(opt)
-
-        for ipt in onnx_graph.input_nodes:
-            assert ipt.name not in tensor_names, "There's dumplicate names in exported parameters and inputs."
-            tensor_names.add(ipt.name)
-
-        for name, node in onnx_graph.node_map.items():
-            inputs = node.inputs
-            outputs = node.outputs
-            update_node = False
-            for idx in range(len(inputs)):
-                ipt = inputs[idx]
-                if ipt not in renamer:
-                    continue
-                updated_name = renamer[ipt]
-                while updated_name in renamer:
-                    updated_name = renamer[updated_name]
-                inputs[idx] = updated_name
-                update_node = True
-
-            for idx in range(len(outputs)):
-                opt = outputs[idx]
-                if opt not in tensor_names:
-                    tensor_names.add(opt)
-                    continue
-                renamed_tensor_name = opt
-                while renamed_tensor_name in renamer:
-                    renamed_tensor_name = renamer[renamed_tensor_name]
-                new_name = cls.generate_new_name(renamed_tensor_name)
-                logging.warning("[Renamer Pass] Will rename {}, to {}".format(
-                    renamed_tensor_name, new_name))
-                outputs[idx] = new_name
-                update_node = True
-                renamer[renamed_tensor_name] = new_name
-
-            if update_node:
-                node.set_inputs(inputs)
-                node.set_outputs(outputs)
-                onnx_graph.update_node(node)
-
-        for opt in onnx_graph.output_nodes:
-            if opt.name not in renamer:
-                continue
-            updated_name = renamer[opt.name]
-            while updated_name in renamer:
-                updated_name = renamer[updated_name]
-            opt.name = updated_name
-
-        return onnx_graph
+#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from paddle2onnx.legacy.passes import PassManager
+from paddle2onnx.utils import logging
+
+
+@PassManager('dumplicate_names_pass')
+class DumplicateNamesPass(object):
+
+    name_count = dict()
+
+    @classmethod
+    def generate_new_name(cls, name):
+        for saved_name in cls.name_count:
+            if name.startswith(saved_name):
+                cls.name_count[saved_name] += 1
+                new_name = saved_name + '.' + str(cls.name_count[saved_name])
+                return new_name
+        cls.name_count[name] = 1
+        new_name = name + '.' + str(cls.name_count[name])
+        return new_name
+
+    @classmethod
+    def run_pass(cls, onnx_graph):
+        renamer = {}
+        tensor_names = set()
+        for name, node in onnx_graph.parameters.items():
+            output = node.output
+            for opt in output:
+                assert opt not in tensor_names, "There's dumplicate names in parameters."
+                tensor_names.add(opt)
+
+        for ipt in onnx_graph.input_nodes:
+            assert ipt.name not in tensor_names, "There's dumplicate names in exported parameters and inputs."
+            tensor_names.add(ipt.name)
+
+        for name, node in onnx_graph.node_map.items():
+            inputs = node.inputs
+            outputs = node.outputs
+            update_node = False
+            for idx in range(len(inputs)):
+                ipt = inputs[idx]
+                if ipt not in renamer:
+                    continue
+                updated_name = renamer[ipt]
+                while updated_name in renamer:
+                    updated_name = renamer[updated_name]
+                inputs[idx] = updated_name
+                update_node = True
+
+            for idx in range(len(outputs)):
+                opt = outputs[idx]
+                if opt not in tensor_names:
+                    tensor_names.add(opt)
+                    continue
+                renamed_tensor_name = opt
+                while renamed_tensor_name in renamer:
+                    renamed_tensor_name = renamer[renamed_tensor_name]
+                new_name = cls.generate_new_name(renamed_tensor_name)
+                logging.warning("[Renamer Pass] Will rename {}, to {}".format(
+                    renamed_tensor_name, new_name))
+                outputs[idx] = new_name
+                update_node = True
+                renamer[renamed_tensor_name] = new_name
+
+            if update_node:
+                node.set_inputs(inputs)
+                node.set_outputs(outputs)
+                onnx_graph.update_node(node)
+
+        for opt in onnx_graph.output_nodes:
+            if opt.name not in renamer:
+                continue
+            updated_name = renamer[opt.name]
+            while updated_name in renamer:
+                updated_name = renamer[updated_name]
+            opt.name = updated_name
+
+        return onnx_graph
```

## paddle2onnx/legacy/passes/inplace_node_pass.py

 * *Ordering differences only*

```diff
@@ -1,62 +1,62 @@
-#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from paddle2onnx.legacy.passes import PassManager
-
-
-def get_repeated_output(inputs, outputs):
-    repeated_output = {}
-    for idx in range(len(outputs)):
-        opt = outputs[idx]
-        if opt in inputs:
-            repeated_output[opt] = idx
-    return repeated_output
-
-
-@PassManager('inplace_node_pass')
-class InplaceNodePass(object):
-
-    name_count = dict()
-
-    @classmethod
-    def generate_new_name(cls, name):
-        if name in cls.name_count:
-            cls.name_count[name] += 1
-        else:
-            cls.name_count[name] = 1
-        new_name = name + '.' + str(cls.name_count[name])
-        return new_name
-
-    @classmethod
-    def run_pass(cls, onnx_graph):
-        node_map = list(onnx_graph.node_map.items())
-        name_mapping = {}
-        for idx in range(len(node_map)):
-            name, node = node_map[idx]
-            inputs = node.inputs
-            outputs = node.outputs
-            for idx in range(len(inputs)):
-                ipt = inputs[idx]
-                if ipt in name_mapping:
-                    inputs[idx] = name_mapping[ipt]
-            repeated_output = get_repeated_output(inputs, outputs)
-            if len(repeated_output) != 0:
-                for opt, idx in repeated_output.items():
-                    name_mapping[opt] = cls.generate_new_name(opt)
-                    outputs[idx] = name_mapping[opt]
-            node.set_inputs(inputs)
-            node.set_outputs(outputs)
-            onnx_graph.update_node(node)
-
-        return onnx_graph
+#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from paddle2onnx.legacy.passes import PassManager
+
+
+def get_repeated_output(inputs, outputs):
+    repeated_output = {}
+    for idx in range(len(outputs)):
+        opt = outputs[idx]
+        if opt in inputs:
+            repeated_output[opt] = idx
+    return repeated_output
+
+
+@PassManager('inplace_node_pass')
+class InplaceNodePass(object):
+
+    name_count = dict()
+
+    @classmethod
+    def generate_new_name(cls, name):
+        if name in cls.name_count:
+            cls.name_count[name] += 1
+        else:
+            cls.name_count[name] = 1
+        new_name = name + '.' + str(cls.name_count[name])
+        return new_name
+
+    @classmethod
+    def run_pass(cls, onnx_graph):
+        node_map = list(onnx_graph.node_map.items())
+        name_mapping = {}
+        for idx in range(len(node_map)):
+            name, node = node_map[idx]
+            inputs = node.inputs
+            outputs = node.outputs
+            for idx in range(len(inputs)):
+                ipt = inputs[idx]
+                if ipt in name_mapping:
+                    inputs[idx] = name_mapping[ipt]
+            repeated_output = get_repeated_output(inputs, outputs)
+            if len(repeated_output) != 0:
+                for opt, idx in repeated_output.items():
+                    name_mapping[opt] = cls.generate_new_name(opt)
+                    outputs[idx] = name_mapping[opt]
+            node.set_inputs(inputs)
+            node.set_outputs(outputs)
+            onnx_graph.update_node(node)
+
+        return onnx_graph
```

## paddle2onnx/legacy/passes/pass_manager.py

 * *Ordering differences only*

```diff
@@ -1,39 +1,39 @@
-#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-import inspect
-
-
-class PassManager(object):
-    PASSES = {}
-
-    def __init__(self, name, **kwargs):
-        self.name = name
-        self.kwargs = kwargs
-
-    def __call__(self, cls):
-        for k, v in inspect.getmembers(cls, inspect.ismethod):
-            if k == 'run_pass':
-                self.PASSES[self.name] = (v, self.kwargs)
-
-    @staticmethod
-    def run_pass(graph, custom_pass_list):
-        for pass_name in custom_pass_list:
-            try:
-                pass_func, kw = PassManager.PASSES[pass_name]
-                pass_func(graph, **kw)
-            except:
-                raise Exception("Error happened when excute pass: {}".format(
-                    pass_name))
-        return graph
+#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import inspect
+
+
+class PassManager(object):
+    PASSES = {}
+
+    def __init__(self, name, **kwargs):
+        self.name = name
+        self.kwargs = kwargs
+
+    def __call__(self, cls):
+        for k, v in inspect.getmembers(cls, inspect.ismethod):
+            if k == 'run_pass':
+                self.PASSES[self.name] = (v, self.kwargs)
+
+    @staticmethod
+    def run_pass(graph, custom_pass_list):
+        for pass_name in custom_pass_list:
+            try:
+                pass_func, kw = PassManager.PASSES[pass_name]
+                pass_func(graph, **kw)
+            except:
+                raise Exception("Error happened when excute pass: {}".format(
+                    pass_name))
+        return graph
```

## Comparing `paddle2onnx-1.0.6.dist-info/LICENSE` & `paddle2onnx-1.0.8rc0.dist-info/LICENSE`

 * *Ordering differences only*

 * *Files 9% similar despite different names*

```diff
@@ -1,203 +1,203 @@
-Copyright (c) 2016 PaddlePaddle Authors. All Rights Reserved
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright (c) 2016 PaddlePaddle Authors. All Rights Reserve.
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
+Copyright (c) 2016 PaddlePaddle Authors. All Rights Reserved
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright (c) 2016 PaddlePaddle Authors. All Rights Reserve.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
```

## Comparing `paddle2onnx-1.0.6.dist-info/RECORD` & `paddle2onnx-1.0.8rc0.dist-info/RECORD`

 * *Files 20% similar despite different names*

```diff
@@ -1,56 +1,56 @@
-paddle2onnx/__init__.py,sha256=OQ6Fqr5jWaJmqAjXp0M18aVqcIyXQx-MaKTkIWw8A6Q,2595
-paddle2onnx/command.py,sha256=ezZU4RAgFs2rre5P8hUggQXpdNrSbAHZxN5x6d33oyE,10500
-paddle2onnx/convert.py,sha256=CdAH27jdVbgXKSi_6nxe-SLOZV9MH2vkqnj1TpQ9Rao,3494
-paddle2onnx/convert_to_fp16.py,sha256=uKBfztC5fff80Em6Tzdb8dADloyphH-mUqroOveElwM,1386
-paddle2onnx/optimize.py,sha256=iH9LEFIIi7HMGu5KxE4upmG3LyJz1Ro0Yopmg7JpKw0,1686
-paddle2onnx/paddle2onnx_cpp2py_export.cp39-win_amd64.pyd,sha256=atCeZL-0hqvfYTfRGBptSbXIv_NtefGnUvsJhn6O6sk,4212224
-paddle2onnx/utils.py,sha256=HWPW0W7Jql5TUTBpN6-8jrTLFdE2Q0mdeLLl4C6Zwvo,4473
-paddle2onnx/version.py,sha256=xysyaznkNupfVXfRFj7IhAfFaCIvGF8otUux826HXuw,282
-paddle2onnx/legacy/__init__.py,sha256=B6oO5jM7iOD7rat5-JbaUkrBTaz_Gv9DfjcUQkYeMZY,6233
-paddle2onnx/legacy/command.py,sha256=LOY-w5V00u_v__VhTugOYqODuCuCqVQ3mQ5rI1w-X_U,10919
-paddle2onnx/legacy/convert.py,sha256=mpFFiY521EwznPK-SN8RrfMxbH2y04VKn0G9kIGwpzw,8532
-paddle2onnx/legacy/constant/__init__.py,sha256=2NdVpyn1b2NWyS576vLqvjH4PB5Hr5ZY-K-AgkqNIzQ,691
-paddle2onnx/legacy/constant/constant.py,sha256=wpIYkxPqBUvYLOdGjtYlzqpWfCg_f52SUBOmhup4LMY,794
-paddle2onnx/legacy/constant/dtypes.py,sha256=NzcD776g7QTwMkdAgPUvd1jR3D8xym-_a_MivXCSPP0,3132
-paddle2onnx/legacy/constant/op_mapping_status.py,sha256=cg18tvAqFN6dtLbLiwTg2PEa_NY9Q6nrXbsFzMgT-gE,756
-paddle2onnx/legacy/graph/__init__.py,sha256=yBWheEcpKEPqgvBBcA3V6Zivyf8nTmRAEGDYAziYYJE,755
-paddle2onnx/legacy/graph/dygraph_helper.py,sha256=5BZ4lGyJZGr8c8gfjN7tBZf2VuaSXiiAPyR8Z13OqK0,11288
-paddle2onnx/legacy/graph/graph.py,sha256=71xTQ3mI3KsOl2MhiX1RmgCERfb_7YdlPH0DIFxhgIw,9813
-paddle2onnx/legacy/graph/graph_helper.py,sha256=2XA96k8gJku-hCU50vras9_kSJNx-9RPMFFyzk3fKEQ,3243
-paddle2onnx/legacy/graph/onnx_graph.py,sha256=j4zH2qxQryccn8nYv6p7pw7CCcYS_yH-imcCzkl6s54,12768
-paddle2onnx/legacy/graph/paddle_graph.py,sha256=ZUQ7yP0XXt2KRhEeQwKzvzqelc-URGbpc0Pi7GhYrI4,11975
-paddle2onnx/legacy/op_mapper/__init__.py,sha256=Nkm4rwFlYPrd44w_fLva1EFEFqq3RTM4FoqhR6xI6Y0,1455
-paddle2onnx/legacy/op_mapper/activation.py,sha256=4JbIwadLdE8KK7s-RHe31L_uzJgGZv0bXjlEGAmDWlU,9517
-paddle2onnx/legacy/op_mapper/logic.py,sha256=jbCqsCefNY2VZrjgbmuTosGLD2huvZv8moqRsE6EE-8,10273
-paddle2onnx/legacy/op_mapper/mapper_helper.py,sha256=_2hGTrcib5nnuTm1W9MFSAQJXSJpPXpJ_NaUPIAmYhY,15799
-paddle2onnx/legacy/op_mapper/math.py,sha256=G2JJCMR2DRtc4Twvq_PshOt4B1krie2AM9tZLag3p1o,47055
-paddle2onnx/legacy/op_mapper/nn.py,sha256=lhsjnwTii6YVRNbuEUDk0SIVRm0fRrSXG618nUaMd4M,39833
-paddle2onnx/legacy/op_mapper/op_mapper.py,sha256=mj3tL9_SSqhzqvfEn7LoxmwJFAjjupwVt-JRoPkjrHc,13214
-paddle2onnx/legacy/op_mapper/search.py,sha256=kd-W218WdpauVZo4feSpRVaCkJnHjcbEaACr4LyEWtU,8523
-paddle2onnx/legacy/op_mapper/tensor.py,sha256=zcr_yDyocmOxI-NaOKJ_WzDJhsslIrIEmcvj54wFcNY,83058
-paddle2onnx/legacy/op_mapper/custom_paddle_op/__init__.py,sha256=wAypXLzcNXeK6mozPN3yFfgeHuZCyRcZdKutIgPIHYc,625
-paddle2onnx/legacy/op_mapper/custom_paddle_op/anchor_generator.py,sha256=S7V_l5ZTO-YIhXkBvH2Rsv2rzWfHMIznn81pYLgIjy4,4514
-paddle2onnx/legacy/op_mapper/custom_paddle_op/box_clip.py,sha256=3PvQzuG15ERHJYAOpFDkI2wWjAtoBTp6uPWEHCQhYng,2376
-paddle2onnx/legacy/op_mapper/custom_paddle_op/collect_fpn_proposals.py,sha256=gTPTYKR3gOhZ1-9hALTwWscjj42lm11x7fR4oR3tknU,2376
-paddle2onnx/legacy/op_mapper/custom_paddle_op/deformable_conv.py,sha256=BIR8JCFckOyZA2dQoZpz_dbfkhVlpBHc2Mz8hajaw_g,13342
-paddle2onnx/legacy/op_mapper/custom_paddle_op/distribute_fpn_proposals.py,sha256=Hg5Mc2-famgnNZ6DxXlD5QD3bmAZpBpY8JDf50v0bdk,4126
-paddle2onnx/legacy/op_mapper/custom_paddle_op/generate_proposals.py,sha256=zw1I5wjzHJjdCPp75KqJAsHpGY8y6wilw3L4XshfWqU,10088
-paddle2onnx/legacy/op_mapper/custom_paddle_op/grid_sampler.py,sha256=3GutrZJR6KIeyMlt7ZAx8fbo9cTdl0ze_5ZrnE6N178,6352
-paddle2onnx/legacy/op_mapper/detection/__init__.py,sha256=wAypXLzcNXeK6mozPN3yFfgeHuZCyRcZdKutIgPIHYc,625
-paddle2onnx/legacy/op_mapper/detection/box_coder.py,sha256=tdCtl1qXg7FtDiaYf8Ny5VACiIdB8oBA-h9oS8HIblg,15869
-paddle2onnx/legacy/op_mapper/detection/density_prior_box.py,sha256=eOLAsNHQWymOc_7OpAogDuJ1eaW9nMva3SsfaefAXtU,5159
-paddle2onnx/legacy/op_mapper/detection/multiclass_nms.py,sha256=k6Bsj3QTtMFAXZCck3avvotkUJ4vT44Szot9jZtKNds,14882
-paddle2onnx/legacy/op_mapper/detection/prior_box.py,sha256=O_v2xn3pdpj_Jvm8g-iklRnhEpagIxJZ0GRTnmhSs0I,7642
-paddle2onnx/legacy/op_mapper/detection/yolo_box.py,sha256=NhBAJFm1L_74qJ0XI6zzL2uX4pgRvH1HXFbvWMcw42U,19708
-paddle2onnx/legacy/op_mapper/sequence/__init__.py,sha256=wAypXLzcNXeK6mozPN3yFfgeHuZCyRcZdKutIgPIHYc,625
-paddle2onnx/legacy/op_mapper/sequence/im2sequence.py,sha256=K_QMT9u5oAS9EGDct3whCtwZuPZHRQx79sy9UnnSwvY,3322
-paddle2onnx/legacy/passes/__init__.py,sha256=mKfQHD-3GhlEj4-OMKPbjuLoLAzgIKMwQOTW6vU14FU,768
-paddle2onnx/legacy/passes/dumplicate_names_pass.py,sha256=ttdDGN-6Z_C57ZeijYX7_osjuRECZrVaFfcyNvjsaaU,3544
-paddle2onnx/legacy/passes/inplace_node_pass.py,sha256=xQr9jDxahdQ7QF9Xm2adjtBrmtGLUl_6aYGujtiVd7k,2180
-paddle2onnx/legacy/passes/pass_manager.py,sha256=TZAF9o9YCtw6v2qZHh26EuTL2hOBA7UWuLQy0i1ik5o,1364
-paddle2onnx-1.0.6.dist-info/LICENSE,sha256=CyGjDUvFD1il226O21qFzI1-7ynOHIUHx2dpaaaCMWA,11640
-paddle2onnx-1.0.6.dist-info/METADATA,sha256=oq_cD0hG21AVPZQnzv3K8smwYgREDIakzad6dzMALYc,544
-paddle2onnx-1.0.6.dist-info/WHEEL,sha256=J_4V_gB-O6Y7Pn6lk91K27JaIhI-q07YM5J8Ufzqla4,100
-paddle2onnx-1.0.6.dist-info/entry_points.txt,sha256=gn3BLPw4JZAdduQjldWwQ2YtoBBS_KSc5Te4COB6P7E,58
-paddle2onnx-1.0.6.dist-info/top_level.txt,sha256=PhCD8iFooa9uVSW4Yp2u9LfJF0e_XsunzshpXj9rtvc,12
-paddle2onnx-1.0.6.dist-info/RECORD,,
+paddle2onnx/__init__.py,sha256=RrB-Y8_PYOFRDxqnLv3miI5kvqfHjo1MF7IQ1AhHtVI,2530
+paddle2onnx/command.py,sha256=mEWoXtfPNdkaHaZOBKhv4P2tWBFvhL4hFWXL_wKArvQ,10589
+paddle2onnx/convert.py,sha256=a6VeYIbtSlGh5-sfVwc_KBAzaiiks7-DX2jdl1BfYSw,3411
+paddle2onnx/convert_to_fp16.py,sha256=KaeKgjqtBcGPSkPv5V3EHmBA-TUD-Ms0bPVh8w1WSy0,1348
+paddle2onnx/optimize.py,sha256=xKSnTptU753_fOxjE0RCPAKNbcBynvMv-mlRgkGz11I,1640
+paddle2onnx/paddle2onnx_cpp2py_export.cpython-310-darwin.so,sha256=7Qhc_Z5dp_-nodEQuFraMLo28zVRI4DRQsGMi95e2kA,7643244
+paddle2onnx/utils.py,sha256=vpDAEs_4wTo6WHxVGwDRYLvLRJDlhj2f3zUrTXdapWQ,4342
+paddle2onnx/version.py,sha256=TGYy9PFKSXtDhMXREZqG-ZRoLTWkiml8bGQQmEDpCIc,277
+paddle2onnx/legacy/__init__.py,sha256=6VXir1BgthVjmZGy4meWVJfG7yc_iB-j7LHC0jB_kMM,6105
+paddle2onnx/legacy/command.py,sha256=TzVgOoeVhsoewTpSSnegvssUX49r0mIlhsIhaRc4dPg,10632
+paddle2onnx/legacy/convert.py,sha256=XLx8aHoTXmZzneK3qx5278w5StIrLd-uL3RiDFn0ir0,8326
+paddle2onnx/legacy/constant/__init__.py,sha256=syGtBoOozwiqhlaxGimQmOoCe46ee_JIkcSfC47omj0,676
+paddle2onnx/legacy/constant/constant.py,sha256=KNA64jukR-_-1QZDnoFnjWcUsAHOhbRQWixJx0XY62E,770
+paddle2onnx/legacy/constant/dtypes.py,sha256=vyqSUBiyAVoxp_JDoFLZVFds9TXc491mqt93kDYKzIo,3048
+paddle2onnx/legacy/constant/op_mapping_status.py,sha256=WdcRaEwZcgz3IATcfFiqNIZlN7VWn7rSSmJqDFBi36s,737
+paddle2onnx/legacy/graph/__init__.py,sha256=CoK2wC3uaS-42WgmsElyEM_fES7RT5rnedCl59CPo3o,738
+paddle2onnx/legacy/graph/dygraph_helper.py,sha256=-BgNDiKsqRBjRjQlzqqj5yHtQB2R51_YpnqHejXbLOA,11017
+paddle2onnx/legacy/graph/graph.py,sha256=cosZ-n9P0zDIlvvjENktT7ektRyYDYgvwCzKhj9RGDM,9526
+paddle2onnx/legacy/graph/graph_helper.py,sha256=vGHESgT7mTmnHGugvSVourXz1QRn4cMdT4OD32elyEc,3160
+paddle2onnx/legacy/graph/onnx_graph.py,sha256=oc2_TVOqOy3YggJM2diPNzLM4c4fEXpqrhkB5w2MwRE,12435
+paddle2onnx/legacy/graph/paddle_graph.py,sha256=-TX87VQbbcpC01SCiTRMDMh6Fb41kzoMwv8PI3xav2k,11672
+paddle2onnx/legacy/op_mapper/__init__.py,sha256=5yYkjSFx1QeHamn-0vLNPmHCFNsSn_qgT91jpxyYCvA,1416
+paddle2onnx/legacy/op_mapper/activation.py,sha256=mFTxOI-xKbYOWrFC_CIG7p10xLCw7YTjVoegi_1zn18,9248
+paddle2onnx/legacy/op_mapper/logic.py,sha256=9KhWnii5WmT6bTb0htKqOhPQxAFAKThdDUFwGNfDTGQ,9993
+paddle2onnx/legacy/op_mapper/mapper_helper.py,sha256=7skwTvU2fthgtudCdPUvc4N5_l-BSxt7pz9tFipNHek,15367
+paddle2onnx/legacy/op_mapper/math.py,sha256=rJUNHPB65uoLOisg8MqTFwyrEiH5xHUFzTCHVuuMwzw,45782
+paddle2onnx/legacy/op_mapper/nn.py,sha256=ft9ylKObgZcu0exwyBXEKSprwvBLMoEmGwtLDU1ysqY,38881
+paddle2onnx/legacy/op_mapper/op_mapper.py,sha256=6hhehTGtXUQNO3S3QqMt-tw_E2XmAf7kwyV5sVv8BEM,12909
+paddle2onnx/legacy/op_mapper/search.py,sha256=icE2q7ea8hUFDagdc38r7hIqubWKgx8XpwgQRgKSxNs,8290
+paddle2onnx/legacy/op_mapper/tensor.py,sha256=yl2U4alYnEBoxG-798x30mMEKHT5L-i20WEw30XEwXk,80903
+paddle2onnx/legacy/op_mapper/custom_paddle_op/__init__.py,sha256=nuJpzcbJHZUDUU55dnUqBHYSKYx06gWDUNlYjLEvW0M,612
+paddle2onnx/legacy/op_mapper/custom_paddle_op/anchor_generator.py,sha256=O3n54cH7YFGYrLqkDAY4wsunS64tQxpooVniFRGcHcI,4417
+paddle2onnx/legacy/op_mapper/custom_paddle_op/box_clip.py,sha256=iWTPIFTcwrRjO5_bIutxtp3Y7KO__0qc-dcl17-evvM,2320
+paddle2onnx/legacy/op_mapper/custom_paddle_op/collect_fpn_proposals.py,sha256=F7gEZKCmR2UN-NWTamqLeH0SMp-oPXQ1KmMsYrjPKUw,2321
+paddle2onnx/legacy/op_mapper/custom_paddle_op/deformable_conv.py,sha256=XbJSzHD2ehNTHVZlH0tILiozgocbK2W521jXRsVKpxk,13046
+paddle2onnx/legacy/op_mapper/custom_paddle_op/distribute_fpn_proposals.py,sha256=q5wwSehC8BH7CmWYv2418gY8lz2O2BbxOvEkxDU1KNg,4026
+paddle2onnx/legacy/op_mapper/custom_paddle_op/generate_proposals.py,sha256=59tw-_JpNaAcRtl8_sePJAiViGawFhwNBvLc5TcCyeI,9865
+paddle2onnx/legacy/op_mapper/custom_paddle_op/grid_sampler.py,sha256=gZ3setzkVZWpIo0Qg73EqjZXeUBrXN5SLRrqpHU05lQ,6201
+paddle2onnx/legacy/op_mapper/detection/__init__.py,sha256=nuJpzcbJHZUDUU55dnUqBHYSKYx06gWDUNlYjLEvW0M,612
+paddle2onnx/legacy/op_mapper/detection/box_coder.py,sha256=UbVgAaOOEePB52zMLUNTg7N-ZkYcMyhYpj-XYO2cm6c,15506
+paddle2onnx/legacy/op_mapper/detection/density_prior_box.py,sha256=HMa1Ih6Fy0JNiXrl5wE6L9Ig6e4D68Gq8Z1pIPd5ljs,5034
+paddle2onnx/legacy/op_mapper/detection/multiclass_nms.py,sha256=fZzDjACkqMWOV3q_Ct87P2YYVkcPlfvYasyEc4_RV1s,14539
+paddle2onnx/legacy/op_mapper/detection/prior_box.py,sha256=DoWL_TDNG6yx8aBbAHaGXZ3qJ6A8nkEGKX3YT-ZgGXI,7466
+paddle2onnx/legacy/op_mapper/detection/yolo_box.py,sha256=znmM_FwIWzuXGn4mxMdlk45l8q_7p6LN8RHf-i06RkQ,19202
+paddle2onnx/legacy/op_mapper/sequence/__init__.py,sha256=nuJpzcbJHZUDUU55dnUqBHYSKYx06gWDUNlYjLEvW0M,612
+paddle2onnx/legacy/op_mapper/sequence/im2sequence.py,sha256=40XKVuKYS2BoU6BB5-sBADEqy4i9R7XwChfa-uWVHUE,3244
+paddle2onnx/legacy/passes/__init__.py,sha256=t2Qh2c5gSn5McAW7lDreSzQEWmNcfPgR1EGj78toInk,752
+paddle2onnx/legacy/passes/dumplicate_names_pass.py,sha256=a4YeBAFCxq5YFZwFIJ4cjHsUIU6ET0eZxMjpjHrsYog,3453
+paddle2onnx/legacy/passes/inplace_node_pass.py,sha256=OyACZq-SvObr_p01DKwZVzoQYPszZbVlvpgm1GAjSg8,2118
+paddle2onnx/legacy/passes/pass_manager.py,sha256=l_SeFVe713EpQvMYPaX24LivtnqvHL1apJDYHIU3mNI,1325
+paddle2onnx-1.0.8rc0.dist-info/LICENSE,sha256=unfdYjtkQWOBINBvBCbbgPkDLEV9rBj0rY8fBv2X4ts,11437
+paddle2onnx-1.0.8rc0.dist-info/METADATA,sha256=5PxHAdFazwpwb6sxEWj04YeKkC_FhAmx4CLMfkpG10A,482
+paddle2onnx-1.0.8rc0.dist-info/WHEEL,sha256=oMFc9-KjvMLKiqM40kAaafsHJktZGp0eIX_k197YDRk,110
+paddle2onnx-1.0.8rc0.dist-info/entry_points.txt,sha256=SRI9Ej_kX_0WOPSKjcrKxycMY7s6Po67XU9OqeoQDcI,57
+paddle2onnx-1.0.8rc0.dist-info/top_level.txt,sha256=PhCD8iFooa9uVSW4Yp2u9LfJF0e_XsunzshpXj9rtvc,12
+paddle2onnx-1.0.8rc0.dist-info/RECORD,,
```

