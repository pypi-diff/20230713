# Comparing `tmp/tensorflow_hub-0.8.0-py2.py3-none-any.whl.zip` & `tmp/tensorflow_hub-0.9.0-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,42 +1,42 @@
-Zip file size: 101616 bytes, number of entries: 40
--rw-rw-r--  2.0 unx     3797 b- defN 20-Apr-01 15:00 tensorflow_hub/__init__.py
--rw-rw-r--  2.0 unx     4061 b- defN 20-Apr-01 15:00 tensorflow_hub/compressed_module_resolver.py
--rw-rw-r--  2.0 unx     1519 b- defN 20-Apr-01 15:00 tensorflow_hub/config.py
--rw-rw-r--  2.0 unx     8508 b- defN 20-Apr-01 15:00 tensorflow_hub/estimator.py
--rw-rw-r--  2.0 unx    22026 b- defN 20-Apr-01 15:00 tensorflow_hub/feature_column.py
--rw-rw-r--  2.0 unx     6051 b- defN 20-Apr-01 15:00 tensorflow_hub/feature_column_v2.py
--rw-rw-r--  2.0 unx     4017 b- defN 20-Apr-01 15:06 tensorflow_hub/image_module_info_pb2.py
--rw-rw-r--  2.0 unx     5240 b- defN 20-Apr-01 15:00 tensorflow_hub/image_util.py
--rw-rw-r--  2.0 unx    18788 b- defN 20-Apr-01 15:00 tensorflow_hub/keras_layer.py
--rw-rw-r--  2.0 unx     5134 b- defN 20-Apr-01 15:00 tensorflow_hub/meta_graph_lib.py
--rw-rw-r--  2.0 unx    22857 b- defN 20-Apr-01 15:00 tensorflow_hub/module.py
--rw-rw-r--  2.0 unx     2520 b- defN 20-Apr-01 15:06 tensorflow_hub/module_attachment_pb2.py
--rw-rw-r--  2.0 unx     3276 b- defN 20-Apr-01 15:06 tensorflow_hub/module_def_pb2.py
--rw-rw-r--  2.0 unx     2132 b- defN 20-Apr-01 15:00 tensorflow_hub/module_impl.py
--rw-rw-r--  2.0 unx     7783 b- defN 20-Apr-01 15:00 tensorflow_hub/module_spec.py
--rw-rw-r--  2.0 unx     4196 b- defN 20-Apr-01 15:00 tensorflow_hub/module_v2.py
--rw-rw-r--  2.0 unx    47147 b- defN 20-Apr-01 15:00 tensorflow_hub/native_module.py
--rw-rw-r--  2.0 unx     1817 b- defN 20-Apr-01 15:00 tensorflow_hub/registry.py
--rw-rw-r--  2.0 unx    18873 b- defN 20-Apr-01 15:00 tensorflow_hub/resolver.py
--rw-rw-r--  2.0 unx    17628 b- defN 20-Apr-01 15:00 tensorflow_hub/saved_model_lib.py
--rw-rw-r--  2.0 unx     4106 b- defN 20-Apr-01 15:00 tensorflow_hub/saved_model_module.py
--rw-rw-r--  2.0 unx     8537 b- defN 20-Apr-01 15:00 tensorflow_hub/tensor_info.py
--rw-rw-r--  2.0 unx     7887 b- defN 20-Apr-01 15:00 tensorflow_hub/tf_utils.py
--rw-rw-r--  2.0 unx     3299 b- defN 20-Apr-01 15:00 tensorflow_hub/tf_v1.py
--rw-rw-r--  2.0 unx      751 b- defN 20-Apr-01 15:00 tensorflow_hub/version.py
--rw-rw-r--  2.0 unx      693 b- defN 20-Apr-01 15:00 tensorflow_hub/tools/__init__.py
--rw-rw-r--  2.0 unx        0 b- defN 20-Apr-01 15:06 tensorflow_hub/tools/make_image_classifier/__init__.py
--rw-rw-r--  2.0 unx     9449 b- defN 20-Apr-01 15:00 tensorflow_hub/tools/make_image_classifier/make_image_classifier.py
--rw-rw-r--  2.0 unx    10990 b- defN 20-Apr-01 15:00 tensorflow_hub/tools/make_image_classifier/make_image_classifier_lib.py
--rw-rw-r--  2.0 unx        0 b- defN 20-Apr-01 15:06 tensorflow_hub/tools/make_nearest_neighbour_index/__init__.py
--rw-rw-r--  2.0 unx     4525 b- defN 20-Apr-01 15:00 tensorflow_hub/tools/make_nearest_neighbour_index/embedding_generator.py
--rw-rw-r--  2.0 unx     4198 b- defN 20-Apr-01 15:00 tensorflow_hub/tools/make_nearest_neighbour_index/index_builder.py
--rw-rw-r--  2.0 unx     6048 b- defN 20-Apr-01 15:00 tensorflow_hub/tools/make_nearest_neighbour_index/make_nearest_neighbour_index.py
--rw-rw-r--  2.0 unx     3113 b- defN 20-Apr-01 15:00 tensorflow_hub/tools/make_nearest_neighbour_index/similarity_finder.py
--rw-rw-r--  2.0 unx    11358 b- defN 20-Apr-01 15:06 tensorflow_hub-0.8.0.dist-info/LICENSE.txt
--rw-rw-r--  2.0 unx     1721 b- defN 20-Apr-01 15:06 tensorflow_hub-0.8.0.dist-info/METADATA
--rw-rw-r--  2.0 unx      110 b- defN 20-Apr-01 15:06 tensorflow_hub-0.8.0.dist-info/WHEEL
--rw-rw-r--  2.0 unx      287 b- defN 20-Apr-01 15:06 tensorflow_hub-0.8.0.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx       15 b- defN 20-Apr-01 15:06 tensorflow_hub-0.8.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     3792 b- defN 20-Apr-01 15:06 tensorflow_hub-0.8.0.dist-info/RECORD
-40 files, 288249 bytes uncompressed, 95414 bytes compressed:  66.9%
+Zip file size: 103256 bytes, number of entries: 40
+-rw-rw-r--  2.0 unx     3797 b- defN 20-Aug-19 11:36 tensorflow_hub/__init__.py
+-rw-rw-r--  2.0 unx     4061 b- defN 20-Aug-19 11:36 tensorflow_hub/compressed_module_resolver.py
+-rw-rw-r--  2.0 unx     1392 b- defN 20-Aug-19 11:36 tensorflow_hub/config.py
+-rw-rw-r--  2.0 unx     8498 b- defN 20-Aug-19 11:36 tensorflow_hub/estimator.py
+-rw-rw-r--  2.0 unx    23001 b- defN 20-Aug-19 11:36 tensorflow_hub/feature_column.py
+-rw-rw-r--  2.0 unx     6051 b- defN 20-Aug-19 11:36 tensorflow_hub/feature_column_v2.py
+-rw-rw-r--  2.0 unx     4017 b- defN 20-Aug-19 11:42 tensorflow_hub/image_module_info_pb2.py
+-rw-rw-r--  2.0 unx     5242 b- defN 20-Aug-19 11:36 tensorflow_hub/image_util.py
+-rw-rw-r--  2.0 unx    20094 b- defN 20-Aug-19 11:36 tensorflow_hub/keras_layer.py
+-rw-rw-r--  2.0 unx     5134 b- defN 20-Aug-19 11:36 tensorflow_hub/meta_graph_lib.py
+-rw-rw-r--  2.0 unx    23308 b- defN 20-Aug-19 11:36 tensorflow_hub/module.py
+-rw-rw-r--  2.0 unx     2520 b- defN 20-Aug-19 11:42 tensorflow_hub/module_attachment_pb2.py
+-rw-rw-r--  2.0 unx     3276 b- defN 20-Aug-19 11:42 tensorflow_hub/module_def_pb2.py
+-rw-rw-r--  2.0 unx     2132 b- defN 20-Aug-19 11:36 tensorflow_hub/module_impl.py
+-rw-rw-r--  2.0 unx     7778 b- defN 20-Aug-19 11:36 tensorflow_hub/module_spec.py
+-rw-rw-r--  2.0 unx     4701 b- defN 20-Aug-19 11:36 tensorflow_hub/module_v2.py
+-rw-rw-r--  2.0 unx    47021 b- defN 20-Aug-19 11:36 tensorflow_hub/native_module.py
+-rw-rw-r--  2.0 unx     1977 b- defN 20-Aug-19 11:36 tensorflow_hub/registry.py
+-rw-rw-r--  2.0 unx    18413 b- defN 20-Aug-19 11:36 tensorflow_hub/resolver.py
+-rw-rw-r--  2.0 unx    17936 b- defN 20-Aug-19 11:36 tensorflow_hub/saved_model_lib.py
+-rw-rw-r--  2.0 unx     4101 b- defN 20-Aug-19 11:36 tensorflow_hub/saved_model_module.py
+-rw-rw-r--  2.0 unx     8537 b- defN 20-Aug-19 11:36 tensorflow_hub/tensor_info.py
+-rw-rw-r--  2.0 unx     7887 b- defN 20-Aug-19 11:36 tensorflow_hub/tf_utils.py
+-rw-rw-r--  2.0 unx     3299 b- defN 20-Aug-19 11:36 tensorflow_hub/tf_v1.py
+-rw-rw-r--  2.0 unx      751 b- defN 20-Aug-19 11:36 tensorflow_hub/version.py
+-rw-rw-r--  2.0 unx      693 b- defN 20-Aug-19 11:36 tensorflow_hub/tools/__init__.py
+-rw-rw-r--  2.0 unx        0 b- defN 20-Aug-19 11:42 tensorflow_hub/tools/make_image_classifier/__init__.py
+-rw-rw-r--  2.0 unx    11386 b- defN 20-Aug-19 11:36 tensorflow_hub/tools/make_image_classifier/make_image_classifier.py
+-rw-rw-r--  2.0 unx    12231 b- defN 20-Aug-19 11:36 tensorflow_hub/tools/make_image_classifier/make_image_classifier_lib.py
+-rw-rw-r--  2.0 unx        0 b- defN 20-Aug-19 11:42 tensorflow_hub/tools/make_nearest_neighbour_index/__init__.py
+-rw-rw-r--  2.0 unx     4525 b- defN 20-Aug-19 11:36 tensorflow_hub/tools/make_nearest_neighbour_index/embedding_generator.py
+-rw-rw-r--  2.0 unx     4198 b- defN 20-Aug-19 11:36 tensorflow_hub/tools/make_nearest_neighbour_index/index_builder.py
+-rw-rw-r--  2.0 unx     6048 b- defN 20-Aug-19 11:36 tensorflow_hub/tools/make_nearest_neighbour_index/make_nearest_neighbour_index.py
+-rw-rw-r--  2.0 unx     3113 b- defN 20-Aug-19 11:36 tensorflow_hub/tools/make_nearest_neighbour_index/similarity_finder.py
+-rw-rw-r--  2.0 unx    11358 b- defN 20-Aug-19 11:42 tensorflow_hub-0.9.0.dist-info/LICENSE.txt
+-rw-rw-r--  2.0 unx     1721 b- defN 20-Aug-19 11:42 tensorflow_hub-0.9.0.dist-info/METADATA
+-rw-rw-r--  2.0 unx      110 b- defN 20-Aug-19 11:42 tensorflow_hub-0.9.0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx      287 b- defN 20-Aug-19 11:42 tensorflow_hub-0.9.0.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx       15 b- defN 20-Aug-19 11:42 tensorflow_hub-0.9.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     3793 b- defN 20-Aug-19 11:42 tensorflow_hub-0.9.0.dist-info/RECORD
+40 files, 294402 bytes uncompressed, 97054 bytes compressed:  67.0%
```

## zipnote {}

```diff
@@ -96,26 +96,26 @@
 
 Filename: tensorflow_hub/tools/make_nearest_neighbour_index/make_nearest_neighbour_index.py
 Comment: 
 
 Filename: tensorflow_hub/tools/make_nearest_neighbour_index/similarity_finder.py
 Comment: 
 
-Filename: tensorflow_hub-0.8.0.dist-info/LICENSE.txt
+Filename: tensorflow_hub-0.9.0.dist-info/LICENSE.txt
 Comment: 
 
-Filename: tensorflow_hub-0.8.0.dist-info/METADATA
+Filename: tensorflow_hub-0.9.0.dist-info/METADATA
 Comment: 
 
-Filename: tensorflow_hub-0.8.0.dist-info/WHEEL
+Filename: tensorflow_hub-0.9.0.dist-info/WHEEL
 Comment: 
 
-Filename: tensorflow_hub-0.8.0.dist-info/entry_points.txt
+Filename: tensorflow_hub-0.9.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: tensorflow_hub-0.8.0.dist-info/top_level.txt
+Filename: tensorflow_hub-0.9.0.dist-info/top_level.txt
 Comment: 
 
-Filename: tensorflow_hub-0.8.0.dist-info/RECORD
+Filename: tensorflow_hub-0.9.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## tensorflow_hub/config.py

```diff
@@ -19,27 +19,20 @@
 
 from tensorflow_hub import compressed_module_resolver
 from tensorflow_hub import native_module
 from tensorflow_hub import registry
 from tensorflow_hub import resolver
 
 
-def _get_default_resolvers():
-  return [
-      resolver.FailResolver(),
+def _install_default_resolvers():
+  for impl in [
       resolver.PathResolver(),
       compressed_module_resolver.GcsCompressedFileResolver(),
-      compressed_module_resolver.HttpCompressedFileResolver(),
-  ]
-
-
-def _get_default_loaders():
-  return [
-      native_module.Loader(),
-  ]
+      compressed_module_resolver.HttpCompressedFileResolver()
+  ]:
+    registry.resolver.add_implementation(impl)
 
 
 def _run():
-  for impl in _get_default_resolvers():
-    registry.resolver.add_implementation(impl)
-  for impl in _get_default_loaders():
-    registry.loader.add_implementation(impl)
+  _install_default_resolvers()
+
+  registry.loader.add_implementation(native_module.Loader())
```

## tensorflow_hub/estimator.py

```diff
@@ -33,15 +33,15 @@
 # meta_graphs.
 _EXPORT_MODULES_COLLECTION = ("__tfhub_export_modules",)
 
 
 def register_module_for_export(module, export_name):
   """Register a Module to be exported under `export_name`.
 
-  DEPRECATION NOTE: This belongs to the hub.Module API and file format for TF1.
+  DEPRECATION NOTE: This belongs to the hub.Module API and TF1 Hub format.
 
   This function registers `module` to be exported by `LatestModuleExporter`
   under a subdirectory named `export_name`.
 
   Note that `export_name` must be unique for each module exported from the
   current graph. It only controls the export subdirectory name and it has
   no scope effects such as the `name` parameter during Module instantiation.
@@ -60,15 +60,15 @@
           % export_name)
   tf_v1.add_to_collection(_EXPORT_MODULES_COLLECTION, (export_name, module))
 
 
 class LatestModuleExporter(tf_v1.estimator.Exporter):
   """Regularly exports registered modules into timestamped directories.
 
-  DEPRECATION NOTE: This belongs to the hub.Module API and file format for TF1.
+  DEPRECATION NOTE: This belongs to the hub.Module API and TF1 Hub format.
 
   Modules can be registered to be exported by this class by calling
   `register_module_for_export` when constructing the graph. The
   `export_name` provided determines the subdirectory name used when
   exporting.
 
   In addition to exporting, this class also garbage collects older exports.
```

## tensorflow_hub/feature_column.py

```diff
@@ -240,15 +240,15 @@
 
   @classmethod
   def from_config(cls, config, custom_objects=None, columns_by_name=None):
     copied_config = config.copy()
     return cls(**copied_config)
 
 
-def image_embedding_column(key, module_spec):
+def image_embedding_column(key, module_spec, image_size=None):
   """Uses a Module to get a dense 1-D representation from the pixels of images.
 
   TODO(b/131678043): This does not work yet with TF2.
 
   This feature column can be used on images, represented as float32 tensors of
   RGB pixel data in the range [0,1]. This can be read from a numeric_column()
   if the tf.Example input data happens to have decoded images, all with the
@@ -272,29 +272,41 @@
     input_fn = ...  # Provides "embeddings" with shape [None, height, width, 3].
     estimator.train(input_fn, ...)
   ```
 
   Args:
     key: A string or `_FeatureColumn` identifying the input image data.
     module_spec: A string handle or a `ModuleSpec` identifying the module.
+    image_size: Optional. If specified it should be a tuple of image height and
+        width to use with the module. Note that it depends on the module on
+        whether the default size can be overridden and what the permissible
+        values are.
 
   Returns:
     `_DenseColumn` that converts from pixel data.
 
   Raises:
      ValueError: if module_spec is not suitable for use in this feature column.
   """
-  return _ImageEmbeddingColumn(key=key, module_spec_path=module_spec)
+  # Configuration stored in a feature column should be hashable or user can
+  # get a TypeError when using it with DenseFeatures. If a user passes a list
+  # cast it to a tuple to avoid wasted debugging time.
+  if isinstance(image_size, list):
+    image_size = tuple(image_size)
+  return _ImageEmbeddingColumn(key=key, module_spec_path=module_spec,
+                               image_size=image_size)
 
 
-def _check_module_is_image_embedding(module_spec):
+def _check_module_is_image_embedding(module_spec, check_image_size):
   """Raises ValueError if `module_spec` is not usable as image embedding.
 
   Args:
     module_spec: A `_ModuleSpec` to test.
+    check_image_size: Whether to check for compatibility with
+        get_expected_image_size.
 
   Raises:
     ValueError: if `module_spec` default signature is not compatible with
         mappingan "images" input to a Tensor(float32, shape=(_,K)).
   """
   issues = []
 
@@ -304,15 +316,16 @@
   input_info_dict = module_spec.get_input_info_dict()
   if (list(input_info_dict.keys()) != ["images"] or
       input_info_dict["images"].dtype != tf.float32):
     issues.append("Module 'default' signature must require a single input, "
                   "which must have type float32 and name 'images'.")
   else:
     try:
-      image_util.get_expected_image_size(module_spec)
+      if check_image_size:
+        image_util.get_expected_image_size(module_spec)
     except ValueError as e:
       issues.append("Module does not support hub.get_expected_image_size(); "
                     "original error was:\n" + str(e))  # Raised again below.
 
   # Find issues with "default" signature outputs. We test that the dtype and
   # shape is appropriate for use in input_layer().
   output_info_dict = module_spec.get_output_info_dict()
@@ -328,21 +341,23 @@
 
   if issues:
     raise ValueError("Module is not usable as image embedding: %r" % issues)
 
 
 class _ImageEmbeddingColumn(DenseFeatureColumn,
                             collections.namedtuple("_ImageEmbeddingColumn",
-                                                   ("key", "module_spec_path"))
+                                                   ("key", "module_spec_path",
+                                                    "image_size"))
                            ):
   """Returned by image_embedding_column(). Do not use directly."""
 
-  def __init__(self, key, module_spec_path):
+  def __init__(self, key, module_spec_path, image_size):
     self.module_spec = module.as_module_spec(self.module_spec_path)
-    _check_module_is_image_embedding(self.module_spec)
+    _check_module_is_image_embedding(self.module_spec,
+                                     check_image_size=self.image_size is None)
     super(_ImageEmbeddingColumn, self).__init__()
 
   @property
   def _is_v2_column(self):
     return tf_utils.fc2_implements_resources()
 
   @property
@@ -376,15 +391,18 @@
   def _parse_example_spec(self):
     """Returns a `tf.Example` parsing spec as dict."""
     return self.parse_example_spec
 
   @property
   def parse_example_spec(self):
     """Returns a `tf.Example` parsing spec as dict."""
-    height, width = image_util.get_expected_image_size(self.module_spec)
+    if self.image_size:
+      height, width = self.image_size
+    else:
+      height, width = image_util.get_expected_image_size(self.module_spec)
     input_shape = [height, width, 3]
     return {self.key: tf_v1.FixedLenFeature(input_shape, tf.float32)}
 
   @property
   def _variable_shape(self):
     """`TensorShape` of `_get_dense_tensor`, without batch dimension."""
     return self.variable_shape
```

## tensorflow_hub/image_util.py

```diff
@@ -18,38 +18,38 @@
 from __future__ import division
 from __future__ import print_function
 
 from tensorflow_hub import image_module_info_pb2
 from tensorflow_hub import native_module
 
 
-# hub.Modules for images can provide further information for the utilities
-# in this file by attaching an ImageModuleInfo message under this key.
+# Models in TF1 Hub format for images can provide further information for the
+# utilities in this file by attaching an ImageModuleInfo message under this key.
 IMAGE_MODULE_INFO_KEY = "image_module_info"
 
 
 # The externally visible name of the message is hub.ImageModuleInfo
 ImageModuleInfo = image_module_info_pb2.ImageModuleInfo  # pylint: disable=invalid-name
 
 
 def attach_image_module_info(image_module_info):
   """Attaches an ImageModuleInfo message from within a module_fn.
 
-  DEPRECATION NOTE: This belongs to the hub.Module API and file format for TF1.
+  DEPRECATION NOTE: This belongs to the hub.Module API and TF1 Hub format.
 
   Args:
     image_module_info: an ImageModuleInfo message.
   """
   native_module.attach_message(IMAGE_MODULE_INFO_KEY, image_module_info)
 
 
 def get_image_module_info(module_or_spec, required=False):
   """Returns the module's attached ImageModuleInfo message, or None if missing.
 
-  DEPRECATION NOTE: This belongs to the hub.Module API and file format for TF1.
+  DEPRECATION NOTE: This belongs to the hub.Module API and TF1 Hub format
 
   Args:
     module_or_spec: a hub.Module or module_spec object.
     required: if true, raises KeyError instead of returning None.
   """
   return module_or_spec.get_attached_message(
       IMAGE_MODULE_INFO_KEY, ImageModuleInfo, required=required)
```

## tensorflow_hub/keras_layer.py

```diff
@@ -34,15 +34,15 @@
 from tensorflow.python.framework import smart_cond
 from tensorflow.python.training.tracking import data_structures
 from tensorflow.python.util import tf_inspect
 # pylint: enable=g-direct-tensorflow-import
 
 
 class KerasLayer(tf.keras.layers.Layer):
-  """Wraps a SavedModel (or a legacy Hub.Module) as a Keras Layer.
+  """Wraps a SavedModel (or a legacy TF1 Hub format) as a Keras Layer.
 
   This layer wraps a callable object for use as a Keras layer. The callable
   object can be passed directly, or be specified by a Python string with a
   handle that gets passed to `hub.load()`.
 
   This is the preferred API to load a TF2-style SavedModel from TF Hub
   into a Keras model. Calling this function requires TF 1.15 or newer.
@@ -80,55 +80,68 @@
   Note: This layer can be used inside the model_fn of a TF2 Estimator. See the
   [migration guide]
   (https://www.tensorflow.org/beta/guide/migration_guide#using_a_custom_model_fn)
   for guidance on how to pick up trainable variables, losses and updates
   explicitly from Keras objects instead of relying on graph collections.
   This layer class does not support graph collections.
   Distributed training of the Estimator requires setting the option
-  `session_config.experimental.share_cluster_devices_in_session` within
-  the `tf.estimator.RunConfig`. (It becomes non-experimental in TF2.2.)
+  `session_config.share_cluster_devices_in_session` within the
+  `tf.estimator.RunConfig`. (This option was experimental from TF1.14 to TF2.1.)
 
   Note: The data types used by a saved model have been fixed at saving time.
   Using tf.keras.mixed_precision etc. has no effect on the saved model
   that gets loaded by a hub.KerasLayer.
 
   Attributes:
-    handle: A callable object (subject to the conventions above), or a
-      Python string to load a saved model via hub.load().
-      A string is required to save the Keras config of this Layer.
+    handle: A callable object (subject to the conventions above), or a Python
+      string to load a saved model via hub.load(). A string is required to save
+      the Keras config of this Layer.
     trainable: Optional. A boolean controlling whether this layer is trainable.
       Must not be set to True when using a signature (raises ValueError),
-      including the use of legacy hub.Modules.
-    arguments: Optional. A dict with additional keyword arguments passed
-      to the callable. These must be JSON-serializable to save the Keras config
-      of this layer, and are not tracked as checkpointing dependencies
-      of this layer.
+      including the use of legacy TF1 Hub format.
+    arguments: Optional. A dict with additional keyword arguments passed to the
+      callable. These must be JSON-serializable to save the Keras config of this
+      layer, and are not tracked as checkpointing dependencies of this layer.
     _sentinel: Used to prevent further positional arguments.
     tags: Optional. If set indicates which graph variant to use. For legacy
-      hub.Modules leaving unset means to use the empty tags set.
+      models in TF1 Hub format leaving unset means to use the empty tags set.
     signature: Optional. If set, KerasLayer will use the requested signature.
-      For legacy hub.Modules leaving unset means to use the `default` signature.
-      When using a signature, either signature_outputs_as_dict or output_key
-      have to set.
+      For legacy models in TF1 Hub format leaving unset means to use the
+      `default` signature. When using a signature, either
+      signature_outputs_as_dict or output_key have to set.
     signature_outputs_as_dict: If set to True, the call to this layer returns a
       dict of all the signature outputs. Can only be used if a signature is
-      specified (or default signature is used for legacy Hub.Modules).
+      specified (or default signature is used for legacy models in TF1 Hub
+      format).
     output_key: Name of the output item to return if the layer returns a dict.
-      For legacy hub.Modules leaving unset means to return the `default` output.
-    output_shape: A tuple or a nest of tuples with the
-      (possibly partial) output shapes of the callable *without* leading
-      batch size. This must have the same nesting structure as the output of
-      the callable object and cover all output tensors.
+      For legacy models in TF1 Hub format leaving unset means to return the
+      `default` output.
+    output_shape: A tuple or a nest of tuples with the (possibly partial) output
+      shapes of the callable *without* leading batch size. This must have the
+      same nesting structure as the output of the callable object and cover all
+      output tensors.
+    load_options: Optional, `tf.saved_model.LoadOptions` object that specifies
+      options for loading when a Python string is provided as `handle`. This
+      argument can only be used from TensorFlow 2.3 onwards.
     **kwargs: Forwarded to Keras' base Layer constructor.
   """
 
-  def __init__(self, handle, trainable=False,  # pylint: disable=invalid-name
-               arguments=None, _sentinel=None, tags=None, signature=None,
-               signature_outputs_as_dict=None, output_key=None,
-               output_shape=None, **kwargs):
+  def __init__(
+      self,
+      handle,
+      trainable=False,
+      arguments=None,
+      _sentinel=None,  # pylint: disable=invalid-name
+      tags=None,
+      signature=None,
+      signature_outputs_as_dict=None,
+      output_key=None,
+      output_shape=None,
+      load_options=None,
+      **kwargs):
     # Note: for compatibility with keras-model serialization this layer is
     # json-serializable. If you add or change arguments here, please also update
     # the `get_config` method.
     # The arguments are marked NoDependency to avoid autoconversion to a
     # trackable _DictWrapper, because that upsets json.dumps() when saving
     # the result of get_config().
     self._handle = handle
@@ -139,31 +152,32 @@
     # TODO(b/142213824): Remove setting shapes when shape inference works.
     if output_shape:
       # Autograph chokes on _convert_nest_to_shapes(), so we call it here
       # and not from within call().
       self._output_shape = data_structures.NoDependency(
           _convert_nest_to_shapes(output_shape))
 
-    self._func = load_module(handle, tags)
+    self._load_options = load_options
+    self._func = load_module(handle, tags, self._load_options)
     self._has_training_argument = func_has_training_argument(self._func)
     self._is_hub_module_v1 = getattr(self._func, "_is_hub_module_v1", False)
 
-    # Update with the defaults when using legacy Hub.Module.
+    # Update with the defaults when using legacy TF1 Hub format.
     if self._is_hub_module_v1:
       self._signature = self._signature or "default"
       if not self._signature_outputs_as_dict:
         self._output_key = self._output_key or "default"
     # More validity checks.
     if self._signature and (bool(self._output_key is not None)
                             == bool(self._signature_outputs_as_dict)):
       raise ValueError("When using a signature, either output_key or "
                        "signature_outputs_as_dict=True should be set.")
     if not self._signature and self._signature_outputs_as_dict:
       raise ValueError("signature_outputs_as_dict is only valid if specifying "
-                       "a signature (or using a legacy Hub.Module).")
+                       "a signature (or using a legacy TF1 Hub format).")
 
     self._callable = self._get_callable()
     self._setup_layer(trainable, **kwargs)
 
   def _setup_layer(self, trainable=False, **kwargs):
     """Constructs keras layer with relevant weights and losses."""
     # Initialize an empty layer, then add_weight() etc. as needed.
@@ -248,27 +262,27 @@
   def _check_trainability(self):
     """Raises or logs errors for unuspported uses of trainable=True."""
     if not self.trainable: return  # Nothing to do.
 
     # Training is only supported when calling a reusable TF2 SavedModel through
     # its @tf.function __call__. Trying to train through a signature is likely
     # to go wrong beyond the most simple cases due to a number of pitfalls:
-    # - No good support for train vs inference mode. hub.Modules used
+    # - No good support for train vs inference mode. TF1 Hub format used
     #   graph versions identified by tags, but this was not a general
     #   standard for SavedModels, and TF2 can no longer save with tags.
-    # - No support for update ops. hub.Modules had them in the UPDATE_OPS
+    # - No support for update ops. TF1 Hub format had them in the UPDATE_OPS
     #   collection, but collections are no longer loaded in TF2. General
     #   SavedModel signatures had no support for them.
     # - No support for regularization losses (same story).
     # - A SavedModel without @tf.function __call__ will likely also not
     #   provide a trainable_variables attribute.
     if self._is_hub_module_v1:
       raise ValueError(
           "Setting hub.KerasLayer.trainable = True is unsupported when "
-          "loading from the hub.Module format of TensorFlow 1.")
+          "loading from the TF1 Hub format.")
     elif self._signature:
       raise ValueError(
           "Setting hub.KerasLayer.trainable = True is unsupported when "
           "calling a SavedModel signature.")
     # Having zero trainable variables in an otherwise trainable model
     # is suspicious but may be valid as a boundary case, so we just log,
     # but at most once per layer instance.
@@ -286,15 +300,15 @@
       if self._signature:  # Assuming the user intended to use a signature.
         raise ValueError("Loaded object has no signatures.")
       else:  # Assuming the user intended to use a callable SavedModel.
         raise ValueError(
             "Loaded object is not callable and has no signatures.")
     if self._signature is None:
       raise ValueError("Signature name has to be specified for non-callable "
-                       "saved models (if not legacy Hub.Module).")
+                       "saved models (if not legacy TF1 Hub format).")
     if self._signature not in self._func.signatures:
       raise ValueError("Unknown signature %s in %s (available signatures: %s)."
                        % (self._signature, self._handle, self._func.signatures))
     f = self._func.signatures[self._signature]
     if not callable(f):
       raise ValueError("Internal error: signature %s is not callable in %s" %
                        (self._signature, self._handle))
@@ -349,14 +363,21 @@
     if self._signature:
       config["signature"] = self._signature
     if self._output_key:
       config["output_key"] = self._output_key
     if self._signature_outputs_as_dict:
       config["signature_outputs_as_dict"] = self._signature_outputs_as_dict
 
+    # self._load_options is not stored in the config. Instead, the load
+    # options passed at the time when this layer gets reloaded from its config
+    # are applied to its own loading as well. That is because the only
+    # load option available at this time (July 2020) is
+    # `experimental_io_device`, which relates to the loading environment,
+    # and not to the interpretation of the loaded SavedModel.
+
     return config
 
   @property
   def resolved_object(self):
     """Returns the callable object to which `handle` resolved in `__init__`."""
     return self._func
 
@@ -384,22 +405,32 @@
   """Converts a nest of tf.TensorShape to raw tuples of int or None."""
   def _shape_as_tuple(x):
     assert isinstance(x, tf.TensorShape)
     return tuple(x.as_list())
   return tf.nest.map_structure(_shape_as_tuple, x)
 
 
-def load_module(handle, tags=None):
+def load_module(handle, tags=None, load_options=None):
   if callable(handle):
     if tags is not None:
       raise ValueError("Passing a callable handle is mutually exclusive "
                        "with setting tags.")
+    if load_options is not None:
+      raise ValueError("Passing a callable handle is mutually exclusive "
+                       "with setting load_options.")
     return handle
   else:
-    return module_v2.load(handle, tags=tags)
+    try:
+      # pylint: disable=g-import-not-at-top
+      # pylint: disable=g-direct-tensorflow-import
+      from tensorflow.python.saved_model import load_context
+      set_load_options = load_options or load_context.get_load_options()
+    except ImportError:
+      set_load_options = load_options
+    return module_v2.load(handle, tags=tags, options=set_load_options)
 
 
 def func_has_training_argument(func):
   """Checks whether saved model has a `training` argument."""
   if not callable(func):
     return False
   fullargspec = tf_inspect.getfullargspec(func.__call__)
```

## tensorflow_hub/module.py

```diff
@@ -35,27 +35,32 @@
   elif isinstance(spec, six.string_types):
     return load_module_spec(spec)
   else:
     raise ValueError("Unknown module spec type: %r" % type(spec))
 
 
 def load_module_spec(path):
-  """Loads a ModuleSpec from the filesystem.
+  """Loads a ModuleSpec from a TF Hub service or the filesystem.
 
-  DEPRECATION NOTE: This belongs to the hub.Module API and file format for TF1.
-  For TF2, switch to plain SavedModels and hub.load().
+  DEPRECATION NOTE: This belongs to the hub.Module API and TF1 Hub format.
+  For TF2, switch to plain SavedModels and hub.load(); see also hub.resolve().
 
   Args:
     path: string describing the location of a module. There are several
-          supported path encoding schemes:
-          a) URL location specifying an archived module
-            (e.g. http://domain/module.tgz)
-          b) Any filesystem location of a module directory (e.g. /module_dir
-             for a local filesystem). All filesystems implementations provided
-             by Tensorflow are supported.
+      supported path encoding schemes:
+        a) A URL like "https://tfhub.dev/the/module/1" referring to tfhub.dev or
+           another service implementing https://www.tensorflow.org/hub/hosting.
+        b) A URL like "https://example.com/module.tar.gz" that points to a
+           compressed tarball directly, as long as that web server ignores
+           the query parameters added by https://www.tensorflow.org/hub/hosting.
+        c) Any filesystem location of a module directory (e.g. /module_dir
+           for a local filesystem). All filesystems implementations provided
+           by Tensorflow are supported.
+        d) Private name resolution schemes added by the maintainer of your
+           local installation of the tensorflow_hub library (usually none).
 
   Returns:
     A ModuleSpec.
 
   Raises:
     ValueError: on unexpected values in the module spec.
     tf.errors.OpError: on file handling exceptions.
@@ -79,15 +84,15 @@
 
 
 # Module class provides a unified access to all ModuleSpecs implementations and
 # should not contain specific implementation code in it (e.g. SavedModel code).
 class Module(object):
   """Part of a TensorFlow 1 model that can be transferred between models.
 
-  DEPRECATION NOTE: The hub.Module API and file format works for TF1 only.
+  DEPRECATION NOTE: The hub.Module API works for TF1 only.
   For TF2, switch to plain SavedModels and hub.load().
 
   A Module represents a part of a TensorFlow graph that can be exported to disk
   (based on the SavedModel format) and later re-loaded. A Module has a defined
   interface that allows it to be used in a replaceable way, with little or no
   knowledge of its internals and its serialization format. Example:
 
@@ -481,15 +486,15 @@
     raise TypeError("There is no output named 'default'. Use as_dict=True.")
 
 
 @contextlib.contextmanager
 def eval_function_for_module(spec, tags=None):
   """Context manager that yields a function to directly evaluate a hub.Module.
 
-  DEPRECATION NOTE: This belongs to the hub.Module API and file format for TF1.
+  DEPRECATION NOTE: This belongs to the hub.Module API and TF1 Hub format.
   For TF2, switch to plain SavedModels and hub.load().
   Eager evalutaion in TF2 obviates the need for this helper.
 
   This creates a separate graph, in which all of the signatures of the module
   are instantiated. Then, it creates a session and initializes the module
   variables. Finally, it returns a function which can be used to evaluate the
   module signatures.
```

## tensorflow_hub/module_spec.py

```diff
@@ -20,15 +20,15 @@
 
 import abc
 
 
 class ModuleSpec(object):
   """Represents the contents of a hub.Module before it has been instantiated.
 
-  DEPRECATION NOTE: This belongs to the hub.Module API and file format for TF1.
+  DEPRECATION NOTE: This belongs to the hub.Module API and TF1 Hub format.
   For TF2, switch to plain SavedModels and hub.load().
 
   A ModuleSpec is the blueprint used by `Module` to create one or more instances
   of a specific module in one or more graphs. The details on how to construct
   the Module are internal to the library implementation but methods to inspect
   a Module interface are public.
```

## tensorflow_hub/module_v2.py

```diff
@@ -25,16 +25,16 @@
 from tensorflow_hub import registry
 from tensorflow_hub import tf_v1
 
 
 def resolve(handle):
   """Resolves a module handle into a path.
 
-  This function works both for plain TF2 SavedModels and the older
-  hub.Modules for TF1.
+  This function works both for plain TF2 SavedModels and the legacy TF1 Hub
+  format.
 
   Resolves a module handle into a path by downloading and caching in
   location specified by TF_HUB_CACHE_DIR if needed.
 
   Currently, three types of module handles are supported:
     1) Smart URL resolvers such as tfhub.dev, e.g.:
        https://tfhub.dev/google/nnlm-en-dim128/1.
@@ -49,42 +49,46 @@
 
   Returns:
     A string representing the Module path.
   """
   return registry.resolver(handle)
 
 
-def load(handle, tags=None):
+def load(handle, tags=None, options=None):
   """Resolves a handle and loads the resulting module.
 
   This is the preferred API to load a Hub module in low-level TensorFlow 2.
   Users of higher-level frameworks like Keras should use the framework's
   corresponding wrapper, like hub.KerasLayer.
 
-  This function is roughly equivalent to the TF2 function `tf.save_model.load()`
-  on the result of `hub.resolve(handle)`. Calling this function requires
-  TF 1.14 or newer. It can be called both in eager and graph mode.
+  This function is roughly equivalent to the TF2 function
+  `tf.saved_model.load()` on the result of `hub.resolve(handle)`. Calling this
+  function requires TF 1.14 or newer. It can be called both in eager and graph
+  mode.
 
   Note: Using in a tf.compat.v1.Session with variables placed on parameter
   servers requires setting `experimental.share_cluster_devices_in_session`
   within the `tf.compat.v1.ConfigProto`. (It becomes non-experimental in TF2.2.)
 
-  This function can handle the deprecated hub.Module format to the extent
-  that `tf.save_model.load()` in TF2 does. In particular, the returned object
+  This function can handle the deprecated TF1 Hub format to the extent
+  that `tf.saved_model.load()` in TF2 does. In particular, the returned object
   has attributes
     * `.variables`: a list of variables from the loaded object;
     * `.signatures`: a dict of TF2 ConcreteFunctions, keyed by signature names,
       that take tensor kwargs and return a tensor dict.
   However, the information imported by hub.Module into the collections of a
   tf.Graph is lost (e.g., regularization losses and update ops).
 
   Args:
     handle: (string) the Module handle to resolve; see hub.resolve().
     tags: A set of strings specifying the graph variant to use, if loading from
       a v1 module.
+    options: Optional, `tf.saved_model.LoadOptions` object that specifies
+      options for loading. This argument can only be used from TensorFlow 2.3
+      onwards.
 
   Returns:
     A trackable object (see tf.saved_model.load() documentation for details).
 
   Raises:
     NotImplementedError: If the code is running against incompatible (1.x)
                          version of TF.
@@ -94,11 +98,19 @@
                               "Current version: %s" % tf.__version__)
   if not isinstance(handle, six.string_types):
     raise ValueError("Expected a string, got %s" % handle)
   module_path = resolve(handle)
   is_hub_module_v1 = tf.io.gfile.exists(
       native_module.get_module_proto_path(module_path))
   if tags is None and is_hub_module_v1:
-      tags = []
-  obj = tf_v1.saved_model.load_v2(module_path, tags=tags)
+    tags = []
+
+  if options:
+    if not hasattr(getattr(tf, "saved_model", None), "LoadOptions"):
+      raise NotImplementedError("options are not supported for TF < 2.3.x,"
+                                " Current version: %s" % tf.__version__)
+    obj = tf_v1.saved_model.load_v2(
+        module_path, tags=tags, options=options)
+  else:
+    obj = tf_v1.saved_model.load_v2(module_path, tags=tags)
   obj._is_hub_module_v1 = is_hub_module_v1  # pylint: disable=protected-access
   return obj
```

## tensorflow_hub/native_module.py

```diff
@@ -8,15 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-"""The implementation of deprecated hub.Module backed by custom SavedModels."""
+"""The implementation of deprecated hub.Module backed by TF1 Hub format."""
 
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
 import collections
 import os
@@ -120,49 +120,48 @@
       tf.compat.as_bytes(_MODULE_PROTO_FILENAME_PB))
 
 
 class Loader(object):
   """Loader for Hub modules in the native format."""
 
   def is_supported(self, path):
-    module_def_path = get_module_proto_path(path)
-    if not tf_v1.gfile.Exists(module_def_path):
-      return False
+    return True
 
+  def _get_module_def_proto(self, path):
+    module_def_path = get_module_proto_path(path)
     module_def_proto = module_def_pb2.ModuleDef()
     with tf_v1.gfile.Open(module_def_path, "rb") as f:
       module_def_proto.ParseFromString(f.read())
+    return module_def_proto
 
-    return module_def_proto.format == module_def_pb2.ModuleDef.FORMAT_V3
+  def _module_def_proto_to_module_spec(self, path):
+    saved_model_handler = saved_model_lib.load(path)
+    checkpoint_filename = saved_model_lib.get_variables_path(path)
+    return _ModuleSpec(saved_model_handler, checkpoint_filename)
 
   def __call__(self, path):
-    module_def_path = get_module_proto_path(path)
-    module_def_proto = module_def_pb2.ModuleDef()
-    with tf_v1.gfile.Open(module_def_path, "rb") as f:
-      module_def_proto.ParseFromString(f.read())
+    module_def_proto = self._get_module_def_proto(path)
 
     if module_def_proto.format != module_def_pb2.ModuleDef.FORMAT_V3:
       raise ValueError("Unsupported module def format: %r" %
                        module_def_proto.format)
 
     required_features = set(module_def_proto.required_features)
     unsupported_features = (required_features - _MODULE_V3_SUPPORTED_FEATURES)
 
     if unsupported_features:
       raise ValueError("Unsupported features: %r" % list(unsupported_features))
 
-    saved_model_handler = saved_model_lib.load(path)
-    checkpoint_filename = saved_model_lib.get_variables_path(path)
-    return _ModuleSpec(saved_model_handler, checkpoint_filename)
+    return self._module_def_proto_to_module_spec(path)
 
 
 def create_module_spec(module_fn, tags_and_args=None, drop_collections=None):
   """Creates a ModuleSpec from a function that builds the module's graph.
 
-  DEPRECATION NOTE: This belongs to the hub.Module API and file format for TF1.
+  DEPRECATION NOTE: This belongs to the hub.Module API and TF1 Hub format.
   For TF2, switch to plain SavedModels.
 
   The `module_fn` is called on a new graph (not the current one) to build the
   graph of the module and define its signatures via `hub.add_signature()`.
   Example:
 
   ```python
@@ -234,15 +233,15 @@
 
   return _ModuleSpec(saved_model_handler, checkpoint_variables_path=None)
 
 
 def add_signature(name=None, inputs=None, outputs=None):
   """Adds a signature to the module definition.
 
-  DEPRECATION NOTE: This belongs to the hub.Module API and file format for TF1.
+  DEPRECATION NOTE: This belongs to the hub.Module API and TF1 Hub format.
   For TF2, switch to plain SavedModels.
 
   NOTE: This must be called within a `module_fn` that is defining a hub.Module.
 
   Args:
     name: Signature name as a string. If omitted, it is interpreted as 'default'
       and is the signature used when `Module.__call__` `signature` is not
@@ -273,15 +272,15 @@
   if message: raise ValueError(message)
   saved_model_lib.add_signature(name, inputs, outputs)
 
 
 def attach_message(key, message):
   """Adds an attached message to the module definition.
 
-  DEPRECATION NOTE: This belongs to the hub.Module API and file format for TF1.
+  DEPRECATION NOTE: This belongs to the hub.Module API and TF1 Hub format.
   For TF2, switch to plain SavedModels.
 
   NOTE: This must be called within a `module_fn` that is defining a hub.Module.
 
   See ModuleSpec.get_attached_message() for an introduction to attached messages
   and the API for module consumers.
 
@@ -554,17 +553,18 @@
     elif apply_graph.building_function:
       # Log a warning if a user is using a hub module in function graph.
       # This is only expected to work if the function graph is pruned and
       # not all nodes are executed.
       #
       # E.g. it could work with "tf.compat.v1.wrap_function", but it will not
       # work with defun, Dataset.map_fn, etc...
-      logging.warning("Using `hub.Module` while building a function: %s. This "
-                      "can lead to errors if the function is not pruned.",
-                      apply_graph.name)
+      logging.warning(
+          "Using TF1 Hub format while building a function: %s. "
+          "This can lead to errors if the function is not pruned.",
+          apply_graph.name)
 
     # As state ops in the apply graph are unused, replace them with Placeholders
     # so that in a heirarchical instantiation, apply_graph state ops are
     # ignored.
     replace_apply_state(
         meta_graph,
         list_registered_stateful_ops_without_inputs(meta_graph.graph_def),
```

## tensorflow_hub/registry.py

```diff
@@ -14,14 +14,16 @@
 # ==============================================================================
 """Internal. Registry holds python objects that can be injected."""
 
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+from absl import logging
+
 
 class MultiImplRegister(object):
   """Utility class to inject multiple implementations of methods.
 
   An implementation must implement __call__ and is_supported with the same
   set of arguments. The registered implementations "is_supported" methods are
   called in reverse order under which they are registered. The first to return
@@ -36,14 +38,17 @@
     """Register an implementation."""
     self._impls += [impl]
 
   def __call__(self, *args, **kwargs):
     for impl in reversed(self._impls):
       if impl.is_supported(*args, **kwargs):
         return impl(*args, **kwargs)
+      else:
+        logging.info("%s %s does not support the provided handle.", self._name,
+                     type(impl).__name__)
     raise RuntimeError(
         "Missing implementation that supports: %s(*%r, **%r)" % (
             self._name, args, kwargs))
 
 
 resolver = MultiImplRegister("resolver")
 loader = MultiImplRegister("loader")
```

## tensorflow_hub/resolver.py

```diff
@@ -376,17 +376,20 @@
   try:
     while True:
       try:
         tf_utils.atomic_write_string_to_file(lock_file, lock_contents,
                                              overwrite=False)
         # Must test condition again, since another process could have created
         # the module and deleted the old lock file since last test.
-        if tf_v1.gfile.Exists(module_dir):
+        if (tf_v1.gfile.Exists(module_dir) and
+            tf_v1.gfile.ListDirectory(module_dir)):
           # Lock file will be deleted in the finally-clause.
           return module_dir
+        if tf_v1.gfile.Exists(module_dir):
+          tf_v1.gfile.DeleteRecursively(module_dir)
         break  # Proceed to downloading the module.
       # These errors are believed to be permanent problems with the
       # module_dir that justify failing the download.
       except (tf.errors.NotFoundError,
               tf.errors.PermissionDeniedError,
               tf.errors.UnauthenticatedError,
               tf.errors.ResourceExhaustedError,
@@ -445,18 +448,14 @@
         tf_v1.gfile.Remove(lock_file)
       except tf.errors.NotFoundError:
         pass
 
   return module_dir
 
 
-class UnsupportedHandleError(Exception):
-  """Exception class for incorrectly formatted handles."""
-
-
 class Resolver(object):
   """Resolver base class: all resolvers inherit from this class."""
   __metaclass__ = abc.ABCMeta
 
   @abc.abstractmethod
   def __call__(self, handle):
     """Resolves a handle into a Module path.
@@ -484,30 +483,15 @@
     pass
 
 
 class PathResolver(Resolver):
   """Resolves handles which are absolute paths."""
 
   def is_supported(self, handle):
-    try:
-      return tf_v1.gfile.Exists(handle)
-    except tf.errors.OpError:
-      return False
-
-  def __call__(self, handle):
-    return handle
-
-
-class FailResolver(Resolver):
-  """Always fails to resolve a path."""
-
-  def is_supported(self, handle):
+    # Path resolver is the last Resolver in the chain so __call__ can always be
+    # called.
     return True
 
   def __call__(self, handle):
-    raise UnsupportedHandleError(
-        "unsupported handle format '%s'. No resolvers found that can "
-        "successfully resolve it. If the handle points to the local "
-        "filesystem, the error indicates that the module directory does not "
-        "exist. Supported handle formats: URLs pointing to a TGZ  file "
-        "(e.g. https://address/module.tgz), or Local File System directory "
-        "file (e.g. /tmp/my_local_module)." % handle)
+    if not tf_v1.gfile.Exists(handle):
+      raise IOError("%s does not exist." % handle)
+    return handle
```

## tensorflow_hub/saved_model_lib.py

```diff
@@ -21,14 +21,15 @@
 from __future__ import division
 from __future__ import print_function
 
 import collections
 import os
 import re
 
+from absl import logging
 import tensorflow as tf
 from tensorflow_hub import module_attachment_pb2
 from tensorflow_hub import tf_utils
 from tensorflow_hub import tf_v1
 
 from google.protobuf import message
 from tensorflow.core.protobuf import meta_graph_pb2
@@ -68,14 +69,15 @@
   filename = os.path.join(
       tf.compat.as_bytes(assets_dir),
       tf.compat.as_bytes(asset_filename))
   if not tf_utils.absolute_path(filename).startswith(
       tf_utils.absolute_path(assets_dir)):
     raise ValueError(
         "Asset filename (%s) points outside assets_dir" % asset_filename)
+  logging.debug("Asset filename: %s", filename)
   return filename
 
 
 def _get_saved_model_proto_path(export_dir):
   return os.path.join(
       tf.compat.as_bytes(export_dir),
       tf.compat.as_bytes(tf_v1.saved_model.constants.SAVED_MODEL_FILENAME_PB))
@@ -265,15 +267,15 @@
     if original_filename in asset_filenames:
       return asset_filenames[original_filename]
 
     basename = os.path.basename(original_filename)
     suggestion = basename
     index = 0
     while suggestion in used_asset_filenames:
-      suggestion = "%s%d" % (basename, index)
+      suggestion = tf.compat.as_bytes(basename) + tf.compat.as_bytes(str(index))
       index += 1
     asset_filenames[original_filename] = suggestion
     used_asset_filenames.add(suggestion)
     return suggestion
 
   for meta_graph in saved_model_proto.meta_graphs:
     collection_def = meta_graph.collection_def.get(
@@ -296,14 +298,15 @@
 
     tensor_filename_map = {}
     for node in meta_graph.graph_def.node:
       if node.name in asset_nodes:
         _check_asset_node_def(node)
         filename = node.attr["value"].tensor.string_val[0]
         tensor_filename_map[node.name + ":0"] = filename
+        logging.debug("Found asset node %s pointing to %s", node.name, filename)
         # Clear value to avoid leaking the original path.
         node.attr["value"].tensor.string_val[0] = (
             tf.compat.as_bytes("SAVEDMODEL-ASSET"))
 
     if tensor_filename_map:
       assets_key_collection = meta_graph.collection_def[
           tf_v1.saved_model.constants.ASSETS_KEY]
@@ -424,19 +427,21 @@
       tf_v1.gfile.Copy(source, destination)
 
   def _save_variables(self, path, variables_saver):
     if variables_saver:
       variables_path = get_variables_path(path)
       variables_dir = os.path.dirname(variables_path)
       tf_v1.gfile.MakeDirs(variables_dir)
+      logging.debug("Variables saved in: %s", variables_path)
       variables_saver(variables_path)
 
   def _save_proto(self, path, proto):
     proto_path = _get_saved_model_proto_path(path)
     tf_v1.gfile.MakeDirs(os.path.dirname(proto_path))
+    logging.debug("SavedModel saved in: %s", proto_path)
     tf_utils.atomic_write_string_to_file(proto_path,
                                          proto.SerializeToString(),
                                          overwrite=True)
 
 
 def _parse_saved_model(path):
   """Reads the savedmodel.pb file containing `SavedModel`."""
```

## tensorflow_hub/saved_model_module.py

```diff
@@ -53,15 +53,15 @@
         del meta_graph.collection_def[collection]
 
 
 def create_module_spec_from_saved_model(saved_model_path,
                                         drop_collections=None):
   """Experimental: Create a ModuleSpec out of a SavedModel from TF1.
 
-  DEPRECATION NOTE: This belongs to the hub.Module API and file format for TF1.
+  DEPRECATION NOTE: This belongs to the hub.Module API and TF1 Hub format.
   For TF2, TensorFlow Hub ships plain SavedModels, removing the need for
   conversions like this.
 
   Define a ModuleSpec from a SavedModel. Note that this is not guaranteed to
   work in all cases and it assumes the SavedModel has followed some conventions:
 
   - The serialized SaverDef can be ignored and instead can be reconstructed.
```

## tensorflow_hub/version.py

```diff
@@ -10,8 +10,8 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 """Contains the version string."""
 
-__version__ = "0.8.0"
+__version__ = "0.9.0"
```

## tensorflow_hub/tools/make_image_classifier/make_image_classifier.py

```diff
@@ -16,15 +16,15 @@
 
 This program builds, trains and exports a TensorFlow 2.x model that classifies
 natural images (photos) into a fixed set of classes. The classes are learned
 from a user-supplied dataset of images, stored as a directory of subdirectories
 of JPEG images, each subdirectory representing one class.
 
 The model is built from a pre-trained image feature vector module from
-TensorFlow Hub (in its TF2/SavedModel format, not the older hub.Module format)
+TensorFlow Hub (in its TF2/SavedModel format, not the legacy TF1 Hub format)
 followed by a linear classifier. The linear classifier, and optionally also
 the TF Hub module, are trained on the new dataset. TF Hub offers a variety of
 suitable modules with various size/accuracy tradeoffs.
 
 The resulting model can be exported in TensorFlow's standard SavedModel format
 and as a .tflite file for deployment to mobile devices with TensorFlow Lite.
 TODO(b/139467904): Add support for post-training model optimization.
@@ -76,14 +76,17 @@
     "tflite_output_file", None,
     "The final model is exported as a .tflite flatbuffers file with this name.")
 flags.DEFINE_string(
     "labels_output_file", None,
     "Where to save the labels (that is, names of image subdirectories). "
     "The lines in this file appear in the same order as the predictions "
     "of the model.")
+flags.DEFINE_string(
+    "summaries_dir", None,
+    "Where to save summary logs for TensorBoard.")
 flags.DEFINE_float(
     "assert_accuracy_at_least", None,
     "If set, the program fails if the validation accuracy at the end of "
     "training is less than this number (between 0 and 1), and no export of "
     "the trained model happens.")
 flags.DEFINE_integer(
     "train_epochs", _DEFAULT_HPARAMS.train_epochs,
@@ -110,26 +113,65 @@
 flags.DEFINE_bool(
     "set_memory_growth", False,
     "If flag is set, memory growth functionality flag will be set as true for "
     "all GPUs prior to training. "
     "More details: "
     "https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth"
 )
+flags.DEFINE_float(
+    "l1_regularizer", _DEFAULT_HPARAMS.l1_regularizer,
+    "Coefficient of L1 regularization applied on model weights.")
+flags.DEFINE_float(
+    "l2_regularizer", _DEFAULT_HPARAMS.l2_regularizer,
+    "Coefficient of L2 regularization applied on model weights.")
+flags.DEFINE_float("label_smoothing", _DEFAULT_HPARAMS.label_smoothing,
+                   "Coefficient of label smoothing used in loss function.")
+flags.DEFINE_float("validation_split", _DEFAULT_HPARAMS.validation_split,
+                   "The fractin of the dataset splitted into a validation set")
+flags.DEFINE_bool(
+    "do_data_augmentation", _DEFAULT_HPARAMS.do_data_augmentation,
+    "Whether do data augmentation on training set."
+    "Can use default augmentation params or specifying them.")
+flags.DEFINE_integer("rotation_range", _DEFAULT_HPARAMS.rotation_range,
+                     "Degree range for random rotation.")
+flags.DEFINE_bool("horizontal_flip", _DEFAULT_HPARAMS.horizontal_flip,
+                  "Horizontally flip images.")
+flags.DEFINE_float(
+    "width_shift_range", _DEFAULT_HPARAMS.width_shift_range,
+    "Shift images horizontally by pixels(if >=1) or by ratio(if <1).")
+flags.DEFINE_float(
+    "height_shift_range", _DEFAULT_HPARAMS.height_shift_range,
+    "Shift images vertically by pixels(if >=1) or by ratio(if <1).")
+flags.DEFINE_float("shear_range", _DEFAULT_HPARAMS.shear_range,
+                   "Shear angle in counter-clockwise direction in degrees.")
+flags.DEFINE_float("zoom_range", _DEFAULT_HPARAMS.zoom_range,
+                   "Range for random zoom.")
 FLAGS = flags.FLAGS
 
 
 def _get_hparams_from_flags():
   """Creates dict of hyperparameters from flags."""
   return lib.HParams(
       train_epochs=FLAGS.train_epochs,
       do_fine_tuning=FLAGS.do_fine_tuning,
       batch_size=FLAGS.batch_size,
       learning_rate=FLAGS.learning_rate,
       momentum=FLAGS.momentum,
-      dropout_rate=FLAGS.dropout_rate)
+      dropout_rate=FLAGS.dropout_rate,
+      l1_regularizer=FLAGS.l1_regularizer,
+      l2_regularizer=FLAGS.l2_regularizer,
+      label_smoothing=FLAGS.label_smoothing,
+      validation_split=FLAGS.validation_split,
+      do_data_augmentation=FLAGS.do_data_augmentation,
+      rotation_range=FLAGS.rotation_range,
+      horizontal_flip=FLAGS.horizontal_flip,
+      width_shift_range=FLAGS.width_shift_range,
+      height_shift_range=FLAGS.height_shift_range,
+      shear_range=FLAGS.shear_range,
+      zoom_range=FLAGS.zoom_range)
 
 
 def _check_keras_dependencies():
   """Checks dependencies of tf.keras.preprocessing.image are present.
 
   This function may come to depend on flag values that determine the kind
   of preprocessing being done.
@@ -181,15 +223,16 @@
 
   image_dir = FLAGS.image_dir or lib.get_default_image_dir()
 
   if FLAGS.set_memory_growth:
     _set_gpu_memory_growth()
 
   model, labels, train_result = lib.make_image_classifier(
-      FLAGS.tfhub_module, image_dir, hparams, FLAGS.image_size)
+      FLAGS.tfhub_module, image_dir, hparams, FLAGS.image_size,
+      FLAGS.summaries_dir)
   if FLAGS.assert_accuracy_at_least:
     _assert_accuracy(train_result, FLAGS.assert_accuracy_at_least)
   print("Done with training.")
 
   if FLAGS.labels_output_file:
     with tf.io.gfile.GFile(FLAGS.labels_output_file, "w") as f:
       f.write("\n".join(labels + ("",)))
@@ -215,23 +258,17 @@
   """Ensure running with TensorFlow 2 behavior.
 
   This function is safe to call even before flags have been parsed.
 
   Raises:
     ImportError: If tensorflow is too old for proper TF2 behavior.
   """
-  logging.info("Running with tensorflow %s (git version %s) and hub %s",
-               tf.__version__, tf.__git_version__, hub.__version__)
-  if tf.__version__.startswith("1."):
-    if tf.__git_version__ == "unknown":  # For internal testing use.
-      try:
-        tf.compat.v1.enable_v2_behavior()
-        return
-      except AttributeError:
-        pass  # Fail below for missing enabler function.
+  logging.info("Running with tensorflow %s and hub %s",
+               tf.__version__, hub.__version__)
+  if not tf.executing_eagerly():
     raise ImportError("Sorry, this program needs TensorFlow 2.")
 
 
 def run_main():
   """Entry point equivalent to executing this file."""
   _ensure_tf2()
   app.run(main)
```

## tensorflow_hub/tools/make_image_classifier/make_image_classifier_lib.py

```diff
@@ -34,41 +34,55 @@
   return tf.keras.utils.get_file("flower_photos",
                                  _DEFAULT_IMAGE_URL, untar=True)
 
 
 class HParams(
     collections.namedtuple("HParams", [
         "train_epochs", "do_fine_tuning", "batch_size", "learning_rate",
-        "momentum", "dropout_rate"
+        "momentum", "dropout_rate", "l1_regularizer", "l2_regularizer",
+        "label_smoothing", "validation_split", "do_data_augmentation",
+        "rotation_range", "horizontal_flip", "width_shift_range",
+        "height_shift_range", "shear_range", "zoom_range"
     ])):
   """The hyperparameters for make_image_classifier.
 
   train_epochs: Training will do this many iterations over the dataset.
   do_fine_tuning: If true, the Hub module is trained together with the
     classification layer on top.
   batch_size: Each training step samples a batch of this many images.
   learning_rate: The learning rate to use for gradient descent training.
   momentum: The momentum parameter to use for gradient descent training.
   dropout_rate: The fraction of the input units to drop, used in dropout layer.
-"""
+  """
 
 
 def get_default_hparams():
   """Returns a fresh HParams object initialized to default values."""
   return HParams(
       train_epochs=5,
       do_fine_tuning=False,
       batch_size=32,
       learning_rate=0.005,
       momentum=0.9,
-      dropout_rate=0.2)
+      dropout_rate=0.2,
+      l1_regularizer=0.0,
+      l2_regularizer=0.0001,
+      label_smoothing=0.1,
+      validation_split=0.2,
+      do_data_augmentation=False,
+      rotation_range=40,
+      horizontal_flip=True,
+      width_shift_range=0.2,
+      height_shift_range=0.2,
+      shear_range=0.2,
+      zoom_range=0.2)
 
 
-def _get_data_with_keras(image_dir, image_size, batch_size,
-                         do_data_augmentation=False):
+def _get_data_with_keras(image_dir, image_size, batch_size, validation_split,
+                         do_data_augmentation, augmentation_params):
   """Gets training and validation data via keras_preprocessing.
 
   Args:
     image_dir: A Python string with the name of a directory that contains
       subdirectories of images, one per class.
     image_size: A list or tuple with 2 Python integers specifying
       the fixed height and width to which input images are resized.
@@ -87,30 +101,26 @@
         labels is a float32 Tensor of shape [batch_size, num_classes]
           with one-hot encoded classes.
     train_size, valid_size: Python integers with the numbers of training
       and validation examples, respectively.
     labels: A tuple of strings with the class labels (subdirectory names).
       The index of a label in this tuple is the numeric class id.
   """
-  datagen_kwargs = dict(rescale=1./255,
-                        # TODO(b/139467904): Expose this as a flag.
-                        validation_split=.20)
+  datagen_kwargs = dict(rescale=1. / 255, validation_split=validation_split)
   dataflow_kwargs = dict(target_size=image_size, batch_size=batch_size,
                          interpolation="bilinear")
 
   valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
       **datagen_kwargs)
   valid_generator = valid_datagen.flow_from_directory(
       image_dir, subset="validation", shuffle=False, **dataflow_kwargs)
 
-  if do_data_augmentation:
-    # TODO(b/139467904): Expose the following constants as flags.
+  if do_data_augmentation and len(augmentation_params):
+    datagen_kwargs.update(**augmentation_params)
     train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
-        rotation_range=40, horizontal_flip=True, width_shift_range=0.2,
-        height_shift_range=0.2, shear_range=0.2, zoom_range=0.2,
         **datagen_kwargs)
   else:
     train_datagen = valid_datagen
   train_generator = train_datagen.flow_from_directory(
       image_dir, subset="training", shuffle=True, **dataflow_kwargs)
 
   indexed_labels = [(index, label)
@@ -149,15 +159,16 @@
   if requested_image_size is None:
     if None in module_image_size:
       raise ValueError("Must specify an image size because "
                        "the selected TF Hub module specifies none.")
     else:
       return module_image_size
   else:
-    requested_image_size = tf.TensorShape([requested_image_size, requested_image_size])
+    requested_image_size = tf.TensorShape(
+        [requested_image_size, requested_image_size])
     assert requested_image_size.is_fully_defined()
     if requested_image_size.is_compatible_with(module_image_size):
       return tuple(requested_image_size.as_list())
     else:
       raise ValueError("The selected TF Hub module expects image size {}, "
                        "but size {} is requested".format(
                            module_image_size,
@@ -174,28 +185,29 @@
         layer.
     image_size: The input image size to use with the given module layer.
     num_classes: Number of the classes to be predicted.
 
   Returns:
     The full classifier model.
   """
-  # TODO(b/139467904): Expose the hyperparameters below as flags.
   model = tf.keras.Sequential([
       tf.keras.Input(shape=(image_size[0], image_size[1], 3)), module_layer,
       tf.keras.layers.Dropout(rate=hparams.dropout_rate),
       tf.keras.layers.Dense(
           num_classes,
           activation="softmax",
-          kernel_regularizer=tf.keras.regularizers.l2(0.0001))
+          kernel_regularizer=tf.keras.regularizers.l1_l2(
+              l1=hparams.l1_regularizer, l2=hparams.l2_regularizer))
   ])
   print(model.summary())
   return model
 
 
-def train_model(model, hparams, train_data_and_size, valid_data_and_size):
+def train_model(model, hparams, train_data_and_size, valid_data_and_size,
+                log_dir=None):
   """Trains model with the given data and hyperparameters.
 
   Args:
     model: The tf.keras.Model from _build_model().
     hparams: A namedtuple of hyperparameters. This function expects
       .train_epochs: a Python integer with the number of passes over the
         training dataset;
@@ -205,56 +217,74 @@
         call to the generators.
     train_data_and_size: A (data, size) tuple in which data is training data to
       be fed in tf.keras.Model.fit(), size is a Python integer with the
       numbers of training.
     valid_data_and_size: A (data, size) tuple in which data is validation data
       to be fed in tf.keras.Model.fit(), size is a Python integer with the
       numbers of validation.
+    log_dir: A directory to write logs for TensorBoard into (defaults to None,
+      no logs will then be written).
 
   Returns:
     The tf.keras.callbacks.History object returned by tf.keras.Model.fit().
   """
   train_data, train_size = train_data_and_size
   valid_data, valid_size = valid_data_and_size
-  # TODO(b/139467904): Expose this hyperparameter as a flag.
-  loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)
+  loss = tf.keras.losses.CategoricalCrossentropy(
+      label_smoothing=hparams.label_smoothing)
   model.compile(
       optimizer=tf.keras.optimizers.SGD(
           lr=hparams.learning_rate, momentum=hparams.momentum),
       loss=loss,
       metrics=["accuracy"])
   steps_per_epoch = train_size // hparams.batch_size
   validation_steps = valid_size // hparams.batch_size
+  callbacks = []
+  if log_dir != None:
+    callbacks.append(tf.keras.callbacks.TensorBoard(log_dir=log_dir,
+                                                    histogram_freq=1))
   return model.fit(
       train_data,
       epochs=hparams.train_epochs,
       steps_per_epoch=steps_per_epoch,
       validation_data=valid_data,
-      validation_steps=validation_steps)
+      validation_steps=validation_steps,
+      callbacks=callbacks)
 
 
 def make_image_classifier(tfhub_module, image_dir, hparams,
-                          requested_image_size=None):
+                          requested_image_size=None,
+                          log_dir=None):
   """Builds and trains a TensorFLow model for image classification.
 
   Args:
     tfhub_module: A Python string with the handle of the Hub module.
     image_dir: A Python string naming a directory with subdirectories of images,
       one per class.
     hparams: A HParams object with hyperparameters controlling the training.
     requested_image_size: A Python integer controlling the size of images to
       feed into the Hub module. If the module has a fixed input size, this
       must be omitted or set to that same value.
+    log_dir: A directory to write logs for TensorBoard into (defaults to None,
+      no logs will then be written).
   """
   module_layer = hub.KerasLayer(tfhub_module,
                                 trainable=hparams.do_fine_tuning)
   image_size = _image_size_for_module(module_layer, requested_image_size)
   print("Using module {} with image size {}".format(
       tfhub_module, image_size))
+  augmentation_params = dict(
+      rotation_range=hparams.rotation_range,
+      horizontal_flip=hparams.horizontal_flip,
+      width_shift_range=hparams.width_shift_range,
+      height_shift_range=hparams.height_shift_range,
+      shear_range=hparams.shear_range,
+      zoom_range=hparams.zoom_range)
   train_data_and_size, valid_data_and_size, labels = _get_data_with_keras(
-      image_dir, image_size, hparams.batch_size)
+      image_dir, image_size, hparams.batch_size, hparams.validation_split,
+      hparams.do_data_augmentation, augmentation_params)
   print("Found", len(labels), "classes:", ", ".join(labels))
 
   model = build_model(module_layer, hparams, image_size, len(labels))
   train_result = train_model(model, hparams, train_data_and_size,
-                             valid_data_and_size)
+                             valid_data_and_size, log_dir)
   return model, labels, train_result
```

## Comparing `tensorflow_hub-0.8.0.dist-info/LICENSE.txt` & `tensorflow_hub-0.9.0.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `tensorflow_hub-0.8.0.dist-info/METADATA` & `tensorflow_hub-0.9.0.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: tensorflow-hub
-Version: 0.8.0
+Version: 0.9.0
 Summary: TensorFlow Hub is a library to foster the publication, discovery, and consumption of reusable parts of machine learning models.
 Home-page: https://github.com/tensorflow/hub
 Author: Google LLC
 Author-email: packages@tensorflow.org
 License: Apache 2.0
 Keywords: tensorflow machine learning share module subgraph component hub embedding retraining transfer
 Platform: UNKNOWN
```

## Comparing `tensorflow_hub-0.8.0.dist-info/RECORD` & `tensorflow_hub-0.9.0.dist-info/RECORD`

 * *Files 13% similar despite different names*

```diff
@@ -1,40 +1,40 @@
 tensorflow_hub/__init__.py,sha256=VQXanb3h1btUNsMn88rHJlzqioB7q55vbwBuieSuFEU,3797
 tensorflow_hub/compressed_module_resolver.py,sha256=4yAcvuB_vjpevSz7LN2tHsJC9VwqIj7fT_ZOKKB9FU8,4061
-tensorflow_hub/config.py,sha256=OGlUFO7NF6F_TNBKXHyKcnFhk-Gxy5y1oDSbBwBNJao,1519
-tensorflow_hub/estimator.py,sha256=a6R8GiMmecbdxCMu2_L31dUCYNuDeBkyz3OorNhADZo,8508
-tensorflow_hub/feature_column.py,sha256=c4qdWQl7SWPOfBZ_KW5IS8vjaQmgKWuKKaHRH9cqP0o,22026
+tensorflow_hub/config.py,sha256=O1hZr-He9lzHmr4Dk0FiJyI1FBn6LzJrjVvAKHmxo-o,1392
+tensorflow_hub/estimator.py,sha256=T3xlYcKyzu4paL77ao29UHO3vVmxWWJsa4nGwmNqV9w,8498
+tensorflow_hub/feature_column.py,sha256=0vuxzZnBb4TqCOENPlugieIcVjx-wMzvZXrw402Zww8,23001
 tensorflow_hub/feature_column_v2.py,sha256=86zL9NhqYj0r7th6wtSXpUGwi5GclpClmYP1cUE_tsM,6051
 tensorflow_hub/image_module_info_pb2.py,sha256=k27yvOen7Ow7sAY5sTSYEBa25aGVBENPRRReHSvrmzc,4017
-tensorflow_hub/image_util.py,sha256=ggKkzHi8gl1kNDpICaSp7yOToJ_cn2QyUq7QrKZoMKY,5240
-tensorflow_hub/keras_layer.py,sha256=Z5gdXKX8otjhF5mifveIoWnaCcK7G20Zwa_XGiD-zA4,18788
+tensorflow_hub/image_util.py,sha256=WfFAv_sGJDBlTbHelq7Pueo2Sk5I0DX9DzDjWDeTR1s,5242
+tensorflow_hub/keras_layer.py,sha256=iQOKQZYEC3kcC5L6S-vzWKcceZqfYh2CkuSYGKN3qdc,20094
 tensorflow_hub/meta_graph_lib.py,sha256=iyx4ITlj9rnHYFnp6OSaC-ljdRx7bbfPqPU6YvLXz1g,5134
-tensorflow_hub/module.py,sha256=KNKPa4QPLygef1-q3XhAhbOC3As97BHzAj0ESOaWDWg,22857
+tensorflow_hub/module.py,sha256=BEEtgZ2D47ftoo8GR5pVcR7r701TvGU8SvKy9sSjXbM,23308
 tensorflow_hub/module_attachment_pb2.py,sha256=mI_uXCRWvmlGy3X2jicoK1kUb6D3kX3Q9eV2v_yAMk0,2520
 tensorflow_hub/module_def_pb2.py,sha256=AeuzUF013aGOKTTNgz8fLEXkBCwTRnNs3876WSr5Sa0,3276
 tensorflow_hub/module_impl.py,sha256=gCX3aZj7mZZZiexXYEGpLqWIxmJgpQWzZsGBLOx24Xo,2132
-tensorflow_hub/module_spec.py,sha256=lAa7UUJ9rI5RVUoyb5VV_VxMjztzDmCkJ55NtOpHOK4,7783
-tensorflow_hub/module_v2.py,sha256=P2AyXzLCGGYoAjHrZ8NJRjMT8j_ST0Ccdmjc5n7LQXs,4196
-tensorflow_hub/native_module.py,sha256=xTnLZcgHYCEAQVYFCOBC0A3dzUAfBPVPmKclPuBYsq0,47147
-tensorflow_hub/registry.py,sha256=H5aIXCRUoYKhlXWzPu_sl5NCvFN8uUaH-DfWXmnBWc4,1817
-tensorflow_hub/resolver.py,sha256=fceQZ6vdwX-Nlh011nvQetWANDlhdO1va1vh8WhjzLQ,18873
-tensorflow_hub/saved_model_lib.py,sha256=qWw7lZYyr7MaxNoLMWI6xboU25D7lF3Tr27C6XR9LAA,17628
-tensorflow_hub/saved_model_module.py,sha256=utG4Fpe-QgKgsDrvwjQnyzcbOUwGJmYvA73xPDfkuA8,4106
+tensorflow_hub/module_spec.py,sha256=cYbqEeCNNzCvpeP1Q_yyA7EICJsVCNE9NlqulQtPZSA,7778
+tensorflow_hub/module_v2.py,sha256=qVmK61URf5gRotjd3QmhNE-V0adPu3WhZMt2qFEtUsA,4701
+tensorflow_hub/native_module.py,sha256=9CApl3NnjfXnjNy9kn8DNPtHF9stlHmdQzMz3w7Dptk,47021
+tensorflow_hub/registry.py,sha256=o-Qh8t3_nVnzfBbdglWLS3MLhqbkpXVFp4WXK13hFGw,1977
+tensorflow_hub/resolver.py,sha256=jhTKiHv7prdOoYG9uFs3Itw7UT7sisOTpLtC68Zf2nI,18413
+tensorflow_hub/saved_model_lib.py,sha256=3pF0Ci6QArbW3mpy1rTaJyDzQbJt8_QeBqYKpMyKWzQ,17936
+tensorflow_hub/saved_model_module.py,sha256=unikTsYKqJjPfAPrlj8ztir5LDwyBHi3T7m3nWGOcQQ,4101
 tensorflow_hub/tensor_info.py,sha256=sg-qrlFuwrN1RP5o1-I4b7w5s9J1Xz7kyhVAiPe-F-M,8537
 tensorflow_hub/tf_utils.py,sha256=TFuIKc2xZPmmYISSLmMe-E2UQjSh1OHBVNdhv0zY1i8,7887
 tensorflow_hub/tf_v1.py,sha256=tmZRnNEX_nzVoL37Kib-Q3Jny8Sn_FFu0OMH_xBJ9r0,3299
-tensorflow_hub/version.py,sha256=werwlyiwJrNlhblslJ8RrAqH482l1PAqQqZPlcoDnxM,751
+tensorflow_hub/version.py,sha256=ECcywJ3zj1eYsBpPooW9G6a76Mf89CpeqaupvBMI2iw,751
 tensorflow_hub/tools/__init__.py,sha256=gXvfFdW9UqTEgp7hpMgnqJiEx34D4vfA-CFzRryuANc,693
 tensorflow_hub/tools/make_image_classifier/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-tensorflow_hub/tools/make_image_classifier/make_image_classifier.py,sha256=wj5ILMqifj7YosyuMIE5iqvjUNeutq2NTHqs8Wpc0BY,9449
-tensorflow_hub/tools/make_image_classifier/make_image_classifier_lib.py,sha256=a3n2HDh4CzXGy5jUDNKQGZavx-skvEAuK3ns75KEW5k,10990
+tensorflow_hub/tools/make_image_classifier/make_image_classifier.py,sha256=1IdAUsvgzxOv847gEnvvx1S6M4jJB9FxzI6LvT-sdsc,11386
+tensorflow_hub/tools/make_image_classifier/make_image_classifier_lib.py,sha256=mLHb_Sop65ywap9Gdqz9onXeEuHnj07ltBK3U4k7xSQ,12231
 tensorflow_hub/tools/make_nearest_neighbour_index/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 tensorflow_hub/tools/make_nearest_neighbour_index/embedding_generator.py,sha256=0B75ClHSJxTcA3YbFSzPuNGN95pukP9I2yEsDOWhZDE,4525
 tensorflow_hub/tools/make_nearest_neighbour_index/index_builder.py,sha256=g2k65int6SE5dgLW-ixHiYSHJV3YJIsKaiR2k5zJMPY,4198
 tensorflow_hub/tools/make_nearest_neighbour_index/make_nearest_neighbour_index.py,sha256=k6UKqGDivOyi3yvcLkB7D5aQO_SijM5XU9q--nDNs7k,6048
 tensorflow_hub/tools/make_nearest_neighbour_index/similarity_finder.py,sha256=sBd_vnWzSQk1t6ipl7gLjktFjHX7Tl0NeJDTyV6JNAY,3113
-tensorflow_hub-0.8.0.dist-info/LICENSE.txt,sha256=z8d0m5b2O9McPEK1xHG_dWgUBT6EfBDz6wA0F7xSPTA,11358
-tensorflow_hub-0.8.0.dist-info/METADATA,sha256=nWfVzL7mf2kx57LuIYSv7pA6M7K-mVQVslgFJ5Rl7uU,1721
-tensorflow_hub-0.8.0.dist-info/WHEEL,sha256=kGT74LWyRUZrL4VgLh6_g12IeVl_9u9ZVhadrgXZUEY,110
-tensorflow_hub-0.8.0.dist-info/entry_points.txt,sha256=N7N_1KJHEG9xq6GkGC4qsvZnUADZaPXAm1xKnRXMGOc,287
-tensorflow_hub-0.8.0.dist-info/top_level.txt,sha256=JwGGRTdO5r5e7goUiwkgPpplw8k-YrdmyhvByaYD40w,15
-tensorflow_hub-0.8.0.dist-info/RECORD,,
+tensorflow_hub-0.9.0.dist-info/LICENSE.txt,sha256=z8d0m5b2O9McPEK1xHG_dWgUBT6EfBDz6wA0F7xSPTA,11358
+tensorflow_hub-0.9.0.dist-info/METADATA,sha256=fe5Z-afKECdDFYhVSr96MFPsY4I2Aue6jFlz39jR-T0,1721
+tensorflow_hub-0.9.0.dist-info/WHEEL,sha256=ADKeyaGyKF5DwBNE0sRE5pvW-bSkFMJfBuhzZ3rceP4,110
+tensorflow_hub-0.9.0.dist-info/entry_points.txt,sha256=N7N_1KJHEG9xq6GkGC4qsvZnUADZaPXAm1xKnRXMGOc,287
+tensorflow_hub-0.9.0.dist-info/top_level.txt,sha256=JwGGRTdO5r5e7goUiwkgPpplw8k-YrdmyhvByaYD40w,15
+tensorflow_hub-0.9.0.dist-info/RECORD,,
```

