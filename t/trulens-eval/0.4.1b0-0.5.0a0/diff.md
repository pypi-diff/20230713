# Comparing `tmp/trulens_eval-0.4.1b0-py3-none-any.whl.zip` & `tmp/trulens_eval-0.5.0a0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,37 +1,44 @@
-Zip file size: 98533 bytes, number of entries: 35
--rw-r--r--  2.0 unx     5164 b- defN 23-Jun-26 14:24 trulens_eval/Example_TruBot.py
--rw-r--r--  2.0 unx     3264 b- defN 23-Jun-28 19:32 trulens_eval/Leaderboard.py
--rw-r--r--  2.0 unx     1270 b- defN 23-Jun-30 21:39 trulens_eval/__init__.py
--rw-r--r--  2.0 unx    10971 b- defN 23-Jun-26 15:00 trulens_eval/app.py
--rw-r--r--  2.0 unx     5344 b- defN 23-Jun-26 14:24 trulens_eval/benchmark.py
--rw-r--r--  2.0 unx    21071 b- defN 23-Jun-30 21:32 trulens_eval/db.py
--rw-r--r--  2.0 unx    13905 b- defN 23-Jun-30 15:55 trulens_eval/db_migration.py
--rw-r--r--  2.0 unx    33181 b- defN 23-Jun-28 14:05 trulens_eval/feedback.py
--rw-r--r--  2.0 unx     3443 b- defN 23-Jun-21 15:22 trulens_eval/feedback_prompts.py
--rw-r--r--  2.0 unx    20140 b- defN 23-Jun-26 15:00 trulens_eval/instruments.py
--rw-r--r--  2.0 unx     3118 b- defN 23-Jun-28 20:19 trulens_eval/keys.py
--rw-r--r--  2.0 unx    21343 b- defN 23-Jun-28 20:19 trulens_eval/provider_apis.py
--rw-r--r--  2.0 unx    12293 b- defN 23-Jun-28 14:05 trulens_eval/schema.py
--rw-r--r--  2.0 unx    14827 b- defN 23-Jun-30 21:38 trulens_eval/tru.py
--rw-r--r--  2.0 unx      293 b- defN 23-Jun-26 14:24 trulens_eval/tru_app.py
--rw-r--r--  2.0 unx     7702 b- defN 23-Jun-28 20:19 trulens_eval/tru_chain.py
--rw-r--r--  2.0 unx      288 b- defN 23-Jun-26 14:24 trulens_eval/tru_db.py
--rw-r--r--  2.0 unx      318 b- defN 23-Jun-26 14:24 trulens_eval/tru_feedback.py
--rw-r--r--  2.0 unx     6370 b- defN 23-Jun-26 15:00 trulens_eval/tru_llama.py
--rw-r--r--  2.0 unx    43136 b- defN 23-Jun-28 20:19 trulens_eval/util.py
--rw-r--r--  2.0 unx     9966 b- defN 23-Jun-28 14:05 trulens_eval/pages/Evaluations.py
--rw-r--r--  2.0 unx     1737 b- defN 23-Jun-28 19:32 trulens_eval/pages/Progress.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-21 15:22 trulens_eval/pages/__init__.py
--rw-r--r--  2.0 unx     5551 b- defN 23-Jun-23 14:51 trulens_eval/tests/test_tru_chain.py
--rw-r--r--  2.0 unx     5365 b- defN 23-Jun-26 14:24 trulens_eval/utils/langchain.py
--rw-r--r--  2.0 unx     4774 b- defN 23-Jun-26 14:24 trulens_eval/utils/llama.py
--rw-r--r--  2.0 unx     1001 b- defN 23-Jun-30 21:32 trulens_eval/utils/notebook_utils.py
--rw-r--r--  2.0 unx     1188 b- defN 23-Jun-30 15:04 trulens_eval/ux/add_logo.py
--rw-r--r--  2.0 unx     4260 b- defN 23-Jun-28 14:05 trulens_eval/ux/components.py
--rw-r--r--  2.0 unx     1209 b- defN 23-Jun-26 14:24 trulens_eval/ux/styles.py
--rw-r--r--  2.0 unx    29567 b- defN 23-Jun-21 15:22 trulens_eval/ux/trulens_logo.svg
--rw-r--r--  2.0 unx    14914 b- defN 23-Jun-30 21:39 trulens_eval-0.4.1b0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jun-30 21:39 trulens_eval-0.4.1b0.dist-info/WHEEL
--rw-r--r--  2.0 unx       13 b- defN 23-Jun-30 21:39 trulens_eval-0.4.1b0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     2921 b- defN 23-Jun-30 21:39 trulens_eval-0.4.1b0.dist-info/RECORD
-35 files, 309999 bytes uncompressed, 93893 bytes compressed:  69.7%
+Zip file size: 247739 bytes, number of entries: 42
+-rw-rw-r--  2.0 unx     5164 b- defN 23-Jul-12 14:37 trulens_eval/Example_TruBot.py
+-rw-rw-r--  2.0 unx     3265 b- defN 23-Jul-12 14:37 trulens_eval/Leaderboard.py
+-rw-rw-r--  2.0 unx     1371 b- defN 23-Jul-12 22:46 trulens_eval/__init__.py
+-rw-rw-r--  2.0 unx    11727 b- defN 23-Jul-12 20:22 trulens_eval/app.py
+-rw-rw-r--  2.0 unx     5344 b- defN 23-Jul-12 14:37 trulens_eval/benchmark.py
+-rw-rw-r--  2.0 unx    21071 b- defN 23-Jul-12 14:37 trulens_eval/db.py
+-rw-rw-r--  2.0 unx    14061 b- defN 23-Jul-12 14:37 trulens_eval/db_migration.py
+-rw-rw-r--  2.0 unx    48952 b- defN 23-Jul-12 18:13 trulens_eval/feedback.py
+-rw-rw-r--  2.0 unx     3443 b- defN 23-Jul-12 14:37 trulens_eval/feedback_prompts.py
+-rw-rw-r--  2.0 unx    20145 b- defN 23-Jul-12 18:13 trulens_eval/instruments.py
+-rw-rw-r--  2.0 unx     3147 b- defN 23-Jul-12 14:37 trulens_eval/keys.py
+-rw-rw-r--  2.0 unx    21353 b- defN 23-Jul-12 14:37 trulens_eval/provider_apis.py
+-rw-rw-r--  2.0 unx    13750 b- defN 23-Jul-12 20:22 trulens_eval/schema.py
+-rw-rw-r--  2.0 unx    16226 b- defN 23-Jul-12 14:37 trulens_eval/tru.py
+-rw-rw-r--  2.0 unx      293 b- defN 23-Jul-12 14:37 trulens_eval/tru_app.py
+-rw-rw-r--  2.0 unx     3545 b- defN 23-Jul-12 18:13 trulens_eval/tru_basic_app.py
+-rw-rw-r--  2.0 unx     7819 b- defN 23-Jul-12 20:22 trulens_eval/tru_chain.py
+-rw-rw-r--  2.0 unx      288 b- defN 23-Jul-12 14:37 trulens_eval/tru_db.py
+-rw-rw-r--  2.0 unx      318 b- defN 23-Jul-12 14:37 trulens_eval/tru_feedback.py
+-rw-rw-r--  2.0 unx     6370 b- defN 23-Jul-12 14:37 trulens_eval/tru_llama.py
+-rw-rw-r--  2.0 unx    43449 b- defN 23-Jul-12 20:22 trulens_eval/util.py
+-rw-rw-r--  2.0 unx    12673 b- defN 23-Jul-12 20:22 trulens_eval/pages/Evaluations.py
+-rw-rw-r--  2.0 unx     1775 b- defN 23-Jul-12 14:37 trulens_eval/pages/Progress.py
+-rw-rw-r--  2.0 unx     3294 b- defN 23-Jul-12 20:22 trulens_eval/react_components/record_viewer/__init__.py
+-rw-rw-r--  2.0 unx      411 b- defN 23-Jul-12 22:47 trulens_eval/react_components/record_viewer/dist/index.html
+-rw-rw-r--  2.0 unx   470039 b- defN 23-Jul-12 22:47 trulens_eval/react_components/record_viewer/dist/assets/index-13c9a784.js
+-rw-rw-r--  2.0 unx      779 b- defN 23-Jul-12 22:47 trulens_eval/react_components/record_viewer/dist/assets/index-d4dfd9ae.css
+-rw-rw-r--  2.0 unx     5551 b- defN 23-Jul-12 14:37 trulens_eval/tests/test_tru_chain.py
+-rw-rw-r--  2.0 unx       83 b- defN 23-Jul-12 20:22 trulens_eval/utils/command_line.py
+-rw-rw-r--  2.0 unx     5892 b- defN 23-Jul-12 22:23 trulens_eval/utils/langchain.py
+-rw-rw-r--  2.0 unx     4842 b- defN 23-Jul-12 14:37 trulens_eval/utils/llama.py
+-rw-rw-r--  2.0 unx     1001 b- defN 23-Jul-12 14:37 trulens_eval/utils/notebook_utils.py
+-rw-rw-r--  2.0 unx      927 b- defN 23-Jul-12 20:22 trulens_eval/utils/trulens.py
+-rw-rw-r--  2.0 unx     1212 b- defN 23-Jul-12 14:37 trulens_eval/ux/add_logo.py
+-rw-rw-r--  2.0 unx     5492 b- defN 23-Jul-12 20:22 trulens_eval/ux/components.py
+-rw-rw-r--  2.0 unx     1209 b- defN 23-Jul-12 14:37 trulens_eval/ux/styles.py
+-rw-rw-r--  2.0 unx    29567 b- defN 23-Jul-12 14:37 trulens_eval/ux/trulens_logo.svg
+-rw-rw-r--  2.0 unx    16347 b- defN 23-Jul-12 22:47 trulens_eval-0.5.0a0.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Jul-12 22:47 trulens_eval-0.5.0a0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       70 b- defN 23-Jul-12 22:47 trulens_eval-0.5.0a0.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx       13 b- defN 23-Jul-12 22:47 trulens_eval-0.5.0a0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     3689 b- defN 23-Jul-12 22:47 trulens_eval-0.5.0a0.dist-info/RECORD
+42 files, 816059 bytes uncompressed, 241827 bytes compressed:  70.4%
```

## zipnote {}

```diff
@@ -39,14 +39,17 @@
 
 Filename: trulens_eval/tru.py
 Comment: 
 
 Filename: trulens_eval/tru_app.py
 Comment: 
 
+Filename: trulens_eval/tru_basic_app.py
+Comment: 
+
 Filename: trulens_eval/tru_chain.py
 Comment: 
 
 Filename: trulens_eval/tru_db.py
 Comment: 
 
 Filename: trulens_eval/tru_feedback.py
@@ -60,47 +63,65 @@
 
 Filename: trulens_eval/pages/Evaluations.py
 Comment: 
 
 Filename: trulens_eval/pages/Progress.py
 Comment: 
 
-Filename: trulens_eval/pages/__init__.py
+Filename: trulens_eval/react_components/record_viewer/__init__.py
+Comment: 
+
+Filename: trulens_eval/react_components/record_viewer/dist/index.html
+Comment: 
+
+Filename: trulens_eval/react_components/record_viewer/dist/assets/index-13c9a784.js
+Comment: 
+
+Filename: trulens_eval/react_components/record_viewer/dist/assets/index-d4dfd9ae.css
 Comment: 
 
 Filename: trulens_eval/tests/test_tru_chain.py
 Comment: 
 
+Filename: trulens_eval/utils/command_line.py
+Comment: 
+
 Filename: trulens_eval/utils/langchain.py
 Comment: 
 
 Filename: trulens_eval/utils/llama.py
 Comment: 
 
 Filename: trulens_eval/utils/notebook_utils.py
 Comment: 
 
+Filename: trulens_eval/utils/trulens.py
+Comment: 
+
 Filename: trulens_eval/ux/add_logo.py
 Comment: 
 
 Filename: trulens_eval/ux/components.py
 Comment: 
 
 Filename: trulens_eval/ux/styles.py
 Comment: 
 
 Filename: trulens_eval/ux/trulens_logo.svg
 Comment: 
 
-Filename: trulens_eval-0.4.1b0.dist-info/METADATA
+Filename: trulens_eval-0.5.0a0.dist-info/METADATA
+Comment: 
+
+Filename: trulens_eval-0.5.0a0.dist-info/WHEEL
 Comment: 
 
-Filename: trulens_eval-0.4.1b0.dist-info/WHEEL
+Filename: trulens_eval-0.5.0a0.dist-info/entry_points.txt
 Comment: 
 
-Filename: trulens_eval-0.4.1b0.dist-info/top_level.txt
+Filename: trulens_eval-0.5.0a0.dist-info/top_level.txt
 Comment: 
 
-Filename: trulens_eval-0.4.1b0.dist-info/RECORD
+Filename: trulens_eval-0.5.0a0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## trulens_eval/Example_TruBot.py

 * *Ordering differences only*

```diff
@@ -8,23 +8,23 @@
 from langchain.llms import OpenAI
 from langchain.memory import ConversationSummaryBufferMemory
 from langchain.vectorstores import Pinecone
 import numpy as np
 import pinecone
 import streamlit as st
 
+from trulens_eval import feedback
 from trulens_eval import Query
 from trulens_eval import tru
 from trulens_eval import tru_chain
-from trulens_eval import feedback
+from trulens_eval.db import Record
+from trulens_eval.feedback import Feedback
 from trulens_eval.keys import *
 from trulens_eval.keys import PINECONE_API_KEY
 from trulens_eval.keys import PINECONE_ENV
-from trulens_eval.db import Record
-from trulens_eval.feedback import Feedback
 
 # Set up GPT-3 model
 model_name = "gpt-3.5-turbo"
 
 app_id = "TruBot"
 # app_id = "TruBot_langprompt"
 # app_id = "TruBot_relevance"
```

## trulens_eval/Leaderboard.py

```diff
@@ -1,13 +1,14 @@
 import math
 
 from millify import millify
 import numpy as np
 import streamlit as st
 from streamlit_extras.switch_page_button import switch_page
+
 from trulens_eval.db_migration import MIGRATION_UNKNOWN_STR
 
 st.runtime.legacy_caching.clear_cache()
 
 from trulens_eval import db
 from trulens_eval import Tru
 from trulens_eval.feedback import default_pass_fail_color_threshold
```

## trulens_eval/__init__.py

```diff
@@ -29,29 +29,32 @@
     - `provider_apis.py` `feedback_prompts.py`
 
     - `schema.py`
 
     - `util.py` `keys.py`
 """
 
-__version__ = "0.4.1b"
+__version__ = "0.5.0a"
 
-from trulens_eval.schema import FeedbackMode
-from trulens_eval.schema import Query, Select
-from trulens_eval.tru import Tru
-from trulens_eval.tru_chain import TruChain
 from trulens_eval.feedback import Feedback
 from trulens_eval.feedback import Huggingface
 from trulens_eval.feedback import OpenAI
 from trulens_eval.feedback import Provider
+from trulens_eval.schema import FeedbackMode
+from trulens_eval.schema import Query
+from trulens_eval.schema import Select
+from trulens_eval.tru import Tru
+from trulens_eval.tru_basic_app import TruBasicApp
+from trulens_eval.tru_chain import TruChain
 from trulens_eval.tru_llama import TruLlama
 from trulens_eval.util import TP
 
 __all__ = [
     'Tru',
+    'TruBasicApp',
     'TruChain',
     'TruLlama',
     'Feedback',
     'OpenAI',
     'Huggingface',
     'FeedbackMode',
     'Provider',
```

## trulens_eval/app.py

```diff
@@ -1,36 +1,40 @@
 """
 Generalized root type for various libraries like llama_index and langchain .
 """
 
-from abc import ABC, abstractmethod
+from abc import ABC
+from abc import abstractmethod
 import logging
 from pprint import PrettyPrinter
-from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, Set, Tuple
+from typing import (
+    Any, Callable, Dict, Iterable, List, Optional, Sequence, Set, Tuple
+)
 
-from pydantic import Field
 import pydantic
+from pydantic import Field
 
+from trulens_eval.db import DB
+from trulens_eval.feedback import Feedback
 from trulens_eval.instruments import Instrument
+from trulens_eval.schema import AppDefinition
 from trulens_eval.schema import Cost
 from trulens_eval.schema import FeedbackMode
 from trulens_eval.schema import FeedbackResult
-from trulens_eval.schema import AppDefinition
 from trulens_eval.schema import Perf
-from trulens_eval.schema import Select
 from trulens_eval.schema import Record
+from trulens_eval.schema import Select
 from trulens_eval.tru import Tru
-from trulens_eval.db import DB
-from trulens_eval.feedback import Feedback
-from trulens_eval.util import GetItemOrAttribute
 from trulens_eval.util import all_objects
-from trulens_eval.util import JSON_BASES_T
-from trulens_eval.util import CLASS_INFO
-from trulens_eval.util import JSON, JSON_BASES
 from trulens_eval.util import Class
+from trulens_eval.util import CLASS_INFO
+from trulens_eval.util import GetItemOrAttribute
+from trulens_eval.util import JSON
+from trulens_eval.util import JSON_BASES
+from trulens_eval.util import JSON_BASES_T
 from trulens_eval.util import json_str_of_obj
 from trulens_eval.util import jsonify
 from trulens_eval.util import JSONPath
 from trulens_eval.util import SerialModel
 from trulens_eval.util import TP
 
 logger = logging.getLogger(__name__)
@@ -64,14 +68,16 @@
 
         cls = Class.of_json(json)
 
         if LangChainComponent.class_is(cls):
             return LangChainComponent.of_json(json)
         elif LlamaIndexComponent.class_is(cls):
             return LlamaIndexComponent.of_json(json)
+        elif TrulensComponent.class_is(cls):
+            return TrulensComponent.of_json(json)
         else:
             raise TypeError(f"Unhandled component type with class {cls}")
 
     @staticmethod
     @abstractmethod
     def class_is(cls: Class) -> bool:
         """
@@ -97,17 +103,17 @@
 class LangChainComponent(ComponentView):
 
     @staticmethod
     def class_is(cls: Class) -> bool:
         if cls.module.module_name.startswith("langchain."):
             return True
 
-        if any(base.module.module_name.startswith("langchain.")
-               for base in cls.bases):
-            return True
+        #if any(base.module.module_name.startswith("langchain.")
+        #       for base in cls.bases):
+        #    return True
 
         return False
 
     @staticmethod
     def of_json(json: JSON) -> 'LangChainComponent':
         from trulens_eval.utils.langchain import component_of_json
         return component_of_json(json)
@@ -116,26 +122,46 @@
 class LlamaIndexComponent(ComponentView):
 
     @staticmethod
     def class_is(cls: Class) -> bool:
         if cls.module.module_name.startswith("llama_index."):
             return True
 
-        if any(base.module.module_name.startswith("llama_index.")
-               for base in cls.bases):
-            return True
+        #if any(base.module.module_name.startswith("llama_index.")
+        #       for base in cls.bases):
+        #    return True
 
         return False
 
     @staticmethod
     def of_json(json: JSON) -> 'LlamaIndexComponent':
         from trulens_eval.utils.llama import component_of_json
         return component_of_json(json)
 
 
+class TrulensComponent(ComponentView):
+    """
+    Components provided in trulens.
+    """
+
+    def class_is(cls: Class) -> bool:
+        if cls.module.module_name.startswith("trulens_eval."):
+            return True
+
+        #if any(base.module.module_name.startswith("trulens.") for base in cls.bases):
+        #    return True
+
+        return False
+
+    @staticmethod
+    def of_json(json: JSON) -> 'TrulensComponent':
+        from trulens_eval.utils.trulens import component_of_json
+        return component_of_json(json)
+
+
 class Prompt(ComponentView):
     # langchain.prompts.base.BasePromptTemplate
     # llama_index.prompts.base.Prompt
 
     @property
     @abstractmethod
     def template(self) -> str:
@@ -272,14 +298,15 @@
         """
 
         ret_record_args['main_error'] = str(error)
         ret_record_args['calls'] = record
         ret_record_args['cost'] = cost
         ret_record_args['perf'] = Perf(start_time=start_time, end_time=end_time)
         ret_record_args['app_id'] = self.app_id
+        ret_record_args['tags'] = self.tags
 
         ret_record = Record(**ret_record_args)
 
         if error is not None:
             if self.feedback_mode == FeedbackMode.WITH_APP:
                 self._handle_error(record=ret_record, error=error)
 
@@ -354,15 +381,15 @@
     def print_instrumented(self) -> None:
         """
         Print instrumented components and their categories.
         """
 
         print(
             "\n".join(
-                f"{t[1].__class__.__name__} component: "
+                f"{t[1].__class__.__name__} of {t[1].__class__.__module__} component: "
                 f"{str(t[0])}" for t in self.instrumented()
             )
         )
 
 
 class TruApp(App):
```

## trulens_eval/db.py

 * *Ordering differences only*

```diff
@@ -9,28 +9,28 @@
 
 from merkle_json import MerkleJson
 import numpy as np
 import pandas as pd
 import pydantic
 
 from trulens_eval import __version__
+from trulens_eval import db_migration
+from trulens_eval.db_migration import MIGRATION_UNKNOWN_STR
 from trulens_eval.feedback import Feedback
 from trulens_eval.schema import AppDefinition
 from trulens_eval.schema import AppID
 from trulens_eval.schema import Cost
 from trulens_eval.schema import FeedbackDefinition
 from trulens_eval.schema import FeedbackDefinitionID
 from trulens_eval.schema import FeedbackResult
 from trulens_eval.schema import FeedbackResultID
 from trulens_eval.schema import FeedbackResultStatus
 from trulens_eval.schema import Perf
 from trulens_eval.schema import Record
 from trulens_eval.schema import RecordID
-from trulens_eval import db_migration
-from trulens_eval.db_migration import MIGRATION_UNKNOWN_STR
 from trulens_eval.util import JSON
 from trulens_eval.util import json_str_of_obj
 from trulens_eval.util import SerialModel
 from trulens_eval.util import UNICODE_CHECK
 from trulens_eval.util import UNICODE_CLOCK
 
 mj = MerkleJson()
```

## trulens_eval/db_migration.py

```diff
@@ -1,14 +1,20 @@
+import json
 import shutil
+import traceback
 import uuid
+
 from tqdm import tqdm
-import json
-import traceback
 
-from trulens_eval.schema import Record, Cost, Perf, FeedbackDefinition, AppDefinition, FeedbackCall
+from trulens_eval.schema import AppDefinition
+from trulens_eval.schema import Cost
+from trulens_eval.schema import FeedbackCall
+from trulens_eval.schema import FeedbackDefinition
+from trulens_eval.schema import Perf
+from trulens_eval.schema import Record
 from trulens_eval.util import FunctionOrMethod
 
 
 class VersionException(Exception):
     pass
```

## trulens_eval/feedback.py

```diff
@@ -1,18 +1,408 @@
 """
 # Feedback Functions
+
+The `Feedback` class contains the starting point for feedback function
+specification and evaluation. A typical use-case looks like this:
+
+```python 
+from trulens_eval import feedback, Select, Feedback
+
+openai = feedback.OpenAI()
+
+f_lang_match = Feedback(openai.language_match)
+    .on_input_output()
+```
+
+The components of this specifications are:
+
+- **Provider classes** -- `feedback.OpenAI` contains feedback function
+  implementations like `qs_relevance`. Other classes subtyping
+  `feedback.Provider` include `Huggingface` and `Cohere`.
+
+- **Feedback implementations** -- `openai.qs_relevance` is a feedback function
+  implementation. Feedback implementations are simple callables that can be run
+  on any arguments matching their signatures. In the example, the implementation
+  has the following signature: 
+
+    ```python
+    def language_match(self, text1: str, text2: str) -> float:
+    ```
+
+  That is, `language_match` is a plain python method that accepts two pieces
+  of text, both strings, and produces a float (assumed to be between 0.0 and
+  1.0).
+
+- **Feedback constructor** -- The line `Feedback(openai.language_match)`
+  constructs a Feedback object with a feedback implementation. 
+
+- **Argument specification** -- The next line, `on_input_output`, specifies how
+  the `language_match` arguments are to be determined from an app record or app
+  definition. The general form of this specification is done using `on` but
+  several shorthands are provided. `on_input_output` states that the first two
+  argument to `language_match` (`text1` and `text2`) are to be the main app
+  input and the main output, respectively.
+
+  Several utility methods starting with `.on` provide shorthands:
+
+    - `on_input(arg) == on_prompt(arg: Optional[str])` -- both specify that the next
+    unspecified argument or `arg` should be the main app input.
+
+    - `on_output(arg) == on_response(arg: Optional[str])` -- specify that the next
+    argument or `arg` should be the main app output.
+
+    - `on_input_output() == on_input().on_output()` -- specifies that the first
+    two arguments of implementation should be the main app input and main app
+    output, respectively.
+
+    - `on_default()` -- depending on signature of implementation uses either
+    `on_output()` if it has a single argument, or `on_input_output` if it has
+    two arguments.
+
+    Some wrappers include additional shorthands:
+
+    ### llama_index-specific selectors
+
+    - `TruLlama.select_source_nodes()` -- outputs the selector of the source
+        documents part of the engine output.
+
+## Fine-grained Selection and Aggregation
+
+For more advanced control on the feedback function operation, we allow data
+selection and aggregation. Consider this feedback example:
+
+```python
+f_qs_relevance = Feedback(openai.qs_relevance)
+    .on_input()
+    .on(Select.Record.app.combine_docs_chain._call.args.inputs.input_documents[:].page_content)
+    .aggregate(numpy.min)
+
+# Implementation signature:
+# def qs_relevance(self, question: str, statement: str) -> float:
+```
+
+- **Argument Selection specification ** -- Where we previously set,
+  `on_input_output` , the `on(Select...)` line enables specification of where
+  the statement argument to the implementation comes from. The form of the
+  specification will be discussed in further details in the Specifying Arguments
+  section.
+
+- **Aggregation specification** -- The last line `aggregate(numpy.min)` specifies
+  how feedback outputs are to be aggregated. This only applies to cases where
+  the argument specification names more than one value for an input. The second
+  specification, for `statement` was of this type. The input to `aggregate` must
+  be a method which can be imported globally. This requirement is further
+  elaborated in the next section. This function is called on the `float` results
+  of feedback function evaluations to produce a single float. The default is
+  `numpy.mean`.
+
+The result of these lines is that `f_qs_relevance` can be now be run on
+app/records and will automatically select the specified components of those
+apps/records:
+
+```python
+record: Record = ...
+app: App = ...
+
+feedback_result: FeedbackResult = f_qs_relevance.run(app=app, record=record)
+```
+
+The object can also be provided to an app wrapper for automatic evaluation:
+
+```python
+app: App = tru.Chain(...., feedbacks=[f_qs_relevance])
+```
+
+## Specifying Implementation Function and Aggregate
+
+The function or method provided to the `Feedback` constructor is the
+implementation of the feedback function which does the actual work of producing
+a float indicating some quantity of interest. 
+
+**Note regarding FeedbackMode.DEFERRED** -- Any callable can be provided here
+but there are additional requirements if your app uses the "deferred" feedback
+evaluation mode (when `feedback_mode=FeedbackMode.DEFERRED` are specified to app
+constructor). In those cases the callables must be methods that are globally
+importable (see the next section for details). The function/method performing
+the aggregation has the same requirements.
+
+### Global import requirement (DEFERRED feedback mode only)
+
+If using deferred evaluation, the feedback function implementations and
+aggregation implementations must be methods from a class that is globally
+importable. That is, the callables must be accessible were you to evaluate this
+code:
+
+```python
+import somepackage.[...].someclass 
+# [...] means optionally further package specifications
+
+provider = someclass(...) # constructor arguments can be included
+feedback_implementation = provider.somemethod
+```
+
+For provided feedback functions, `somepackage` is `trulens_eval.feedback` and
+`someclass` is `OpenAI` or one of the other `Provider` subclasses. Custom
+feedback functions likewise need to belong to a package that can be imported.
+Critically, classes defined locally in a notebook will not be importable this
+way.
+
+## Specifying Arguments
+
+The mapping between app/records to feedback implementation arguments is
+specified by the `on...` methods of the `Feedback` objects. The general form is:
+
+```python
+feedback: Feedback = feedback.on(argname1=selector1, argname2=selector2, ...)
+```
+
+That is, `Feedback.on(...)` returns a new `Feedback` object with additional
+argument mappings, the source of `argname1` is `selector1` and so on for further
+argument names. The types of `selector1` is `JSONPath` which we elaborate on in
+the "Selector Details".
+
+If argument names are ommitted, they are taken from the feedback function
+implementation signature in order. That is, 
+
+```python
+...on(argname1=selector1, argname2=selector2)
+```
+
+and
+
+```python
+...on(selector1, selector2)
+```
+
+are equivalent assuming the feedback implementation has two arguments,
+`argname1` and `argname2`, in that order.
+
+### Running Feedback
+
+Feedback implementations are simple callables that can be run on any arguments
+matching their signatures. However, once wrapped with `Feedback`, they are meant
+to be run on outputs of app evaluation (the "Records"). Specifically,
+`Feedback.run` has this definition:
+
+```python
+def run(self, 
+    app: Union[AppDefinition, JSON], 
+    record: Record
+) -> FeedbackResult:
+```
+
+That is, the context of a Feedback evaluation is an app (either as
+`AppDefinition` or a JSON-like object) and a `Record` of the execution of the
+aforementioned app. Both objects are indexable using "Selectors". By indexable
+here we mean that their internal components can be specified by a Selector and
+subsequently that internal component can be extracted using that selector.
+Selectors for Feedback start by specifying whether they are indexing into an App
+or a Record via the `__app__` and `__record__` special
+attributes (see **Selectors** section below).
+
+### Selector Details
+
+Selectors are of type `JSONPath` defined in `util.py` but are also aliased in
+`schema.py` as `Select.Query`. Objects of this type specify paths into JSON-like
+structures (enumerating `Record` or `App` contents). 
+
+By JSON-like structures we mean python objects that can be converted into JSON
+or are base types. This includes:
+
+- base types: strings, integers, dates, etc.
+
+- sequences
+
+- dictionaries with string keys
+
+Additionally, JSONPath also index into general python objects like
+`AppDefinition` or `Record` though each of these can be converted to JSON-like.
+
+When used to index json-like objects, JSONPath are used as generators: the path
+can be used to iterate over items from within the object:
+
+```python
+class JSONPath...
+    ...
+    def __call__(self, obj: Any) -> Iterable[Any]:
+    ...
+```
+
+In most cases, the generator produces only a single item but paths can also
+address multiple items (as opposed to a single item containing multiple).
+
+The syntax of this specification mirrors the syntax one would use with
+instantiations of JSON-like objects. For every `obj` generated by `query: JSONPath`:
+
+- `query[somekey]` generates the `somekey` element of `obj` assuming it is a
+    dictionary with key `somekey`.
+
+- `query[someindex]` generates the index `someindex` of `obj` assuming it is
+    a sequence.
+
+- `query[slice]` generates the __multiple__ elements of `obj` assuming it is a
+    sequence. Slices include `:` or in general `startindex:endindex:step`.
+
+- `query[somekey1, somekey2, ...]` generates __multiple__ elements of `obj`
+    assuming `obj` is a dictionary and `somekey1`... are its keys.
+
+- `query[someindex1, someindex2, ...]` generates __multiple__ elements
+    indexed by `someindex1`... from a sequence `obj`.
+
+- `query.someattr` depends on type of `obj`. If `obj` is a dictionary, then
+    `query.someattr` is an alias for `query[someattr]`. Otherwise if
+    `someattr` is an attribute of a python object `obj`, then `query.someattr`
+    generates the named attribute.
+
+For feedback argument specification, the selectors should start with either
+`__record__` or `__app__` indicating which of the two JSON-like structures to
+select from (Records or Apps). `Select.Record` and `Select.App` are defined as
+`Query().__record__` and `Query().__app__` and thus can stand in for the start of a
+selector specification that wishes to select from a Record or App, respectively.
+The full set of Query aliases are as follows:
+
+- `Record = Query().__record__` -- points to the Record.
+
+- App = Query().__app__ -- points to the App.
+
+- `RecordInput = Record.main_input` -- points to the main input part of a
+    Record. This is the first argument to the root method of an app (for
+    langchain Chains this is the `__call__` method).
+
+- `RecordOutput = Record.main_output` -- points to the main output part of a
+    Record. This is the output of the root method of an app (i.e. `__call__`
+    for langchain Chains).
+
+- `RecordCalls = Record.app` -- points to the root of the app-structured
+    mirror of calls in a record. See **App-organized Calls** Section above.
+
+## Multiple Inputs Per Argument
+
+As in the `f_qs_relevance` example, a selector for a _single_ argument may point
+to more than one aspect of a record/app. These are specified using the slice or
+lists in key/index poisitions. In that case, the feedback function is evaluated
+multiple times, its outputs collected, and finally aggregated into a main
+feedback result.
+
+The collection of values for each argument of feedback implementation is
+collected and every combination of argument-to-value mapping is evaluated with a
+feedback definition. This may produce a large number of evaluations if more than
+one argument names multiple values. In the dashboard, all individual invocations
+of a feedback implementation are shown alongside the final aggregate result.
+
+## App/Record Organization (What can be selected)
+
+Apps are serialized into JSON-like structures which are indexed via selectors.
+The exact makeup of this structure is app-dependent though always start with
+`app`, that is, the trulens wrappers (subtypes of `App`) contain the wrapped app
+in the attribute `app`:
+
+```python
+# app.py:
+class App(AppDefinition, SerialModel):
+    ...
+    # The wrapped app.
+    app: Any = Field(exclude=True)
+    ...
+```
+
+For your app, you can inspect the JSON-like structure by using the `dict`
+method:
+
+```python
+tru = ... # your app, extending App
+print(tru.dict())
+```
+
+The other non-excluded fields accessible outside of the wrapped app are listed
+in the `AppDefinition` class in `schema.py`:
+
+```python
+class AppDefinition(SerialModel, WithClassInfo, ABC):
+    ...
+
+    app_id: AppID
+
+    feedback_definitions: Sequence[FeedbackDefinition] = []
+
+    feedback_mode: FeedbackMode = FeedbackMode.WITH_APP_THREAD
+
+    root_class: Class
+
+    root_callable: ClassVar[FunctionOrMethod]
+
+    app: JSON
+```
+
+Note that `app` is in both classes. This distinction between `App` and
+`AppDefinition` here is that one corresponds to potentially non-serializable
+python objects (`App`) and their serializable versions (`AppDefinition`).
+Feedbacks should expect to be run with `AppDefinition`. Fields of `App` that are
+not part of `AppDefinition` may not be available.
+
+You can inspect the data available for feedback definitions in the dashboard by
+clicking on the "See full app json" button on the bottom of the page after
+selecting a record from a table.
+
+The other piece of context to Feedback evaluation are records. These contain the
+inputs/outputs and other information collected during the execution of an app:
+
+```python
+class Record(SerialModel):
+    record_id: RecordID
+    app_id: AppID
+
+    cost: Optional[Cost] = None
+    perf: Optional[Perf] = None
+
+    ts: datetime = pydantic.Field(default_factory=lambda: datetime.now())
+
+    tags: str = ""
+
+    main_input: Optional[JSON] = None
+    main_output: Optional[JSON] = None  # if no error
+    main_error: Optional[JSON] = None  # if error
+
+    # The collection of calls recorded. Note that these can be converted into a
+    # json structure with the same paths as the app that generated this record
+    # via `layout_calls_as_app`.
+    calls: Sequence[RecordAppCall] = []
+```
+
+A listing of a record can be seen in the dashboard by clicking the "see full
+record json" button on the bottom of the page after selecting a record from the
+table.
+
+### Calls made by App Components
+
+When evaluating a feedback function, Records are augmented with
+app/component calls in app layout in the attribute `app`. By this we mean that
+in addition to the fields listed in the class definition above, the `app` field
+will contain the same information as `calls` but organized in a manner mirroring
+the organization of the app structure. For example, if the instrumented app
+contains a component `combine_docs_chain` then `app.combine_docs_chain` will
+contain calls to methods of this component. In the example at the top of this
+docstring, `_call` was an example of such a method. Thus
+`app.combine_docs_chain._call` further contains a `RecordAppCall` (see
+schema.py) structure with information about the inputs/outputs/metadata
+regarding the `_call` call to that component. Selecting this information is the
+reason behind the `Select.RecordCalls` alias (see next section).
+
+You can inspect the components making up your app via the `App` method
+`print_instrumented`.
 """
 
 from datetime import datetime
 from inspect import Signature
 from inspect import signature
 import itertools
 import logging
 from multiprocessing.pool import AsyncResult
 import re
+import traceback
 from typing import Any, Callable, Dict, Iterable, Optional, Type, Union
 
 import numpy as np
 import openai
 import pydantic
 
 from trulens_eval import feedback_prompts
@@ -31,16 +421,16 @@
 from trulens_eval.schema import Select
 from trulens_eval.util import FunctionOrMethod
 from trulens_eval.util import JSON
 from trulens_eval.util import jsonify
 from trulens_eval.util import SerialModel
 from trulens_eval.util import TP
 from trulens_eval.util import UNICODE_CHECK
-from trulens_eval.util import UNICODE_YIELD
 from trulens_eval.util import UNICODE_CLOCK
+from trulens_eval.util import UNICODE_YIELD
 
 PROVIDER_CLASS_NAMES = ['OpenAI', 'Huggingface', 'Cohere']
 
 default_pass_fail_color_threshold = 0.5
 
 logger = logging.getLogger(__name__)
 
@@ -154,23 +544,27 @@
         )
 
         if len(par_names) == 1:
             # A single argument remaining. Assume it is record output.
             selectors = {par_names[0]: Select.RecordOutput}
             self._print_guessed_selector(par_names[0], Select.RecordOutput)
 
+            # TODO: replace with on_output ?
+
         elif len(par_names) == 2:
             # Two arguments remaining. Assume they are record input and output
             # respectively.
             selectors = {
                 par_names[0]: Select.RecordInput,
                 par_names[1]: Select.RecordOutput
             }
             self._print_guessed_selector(par_names[0], Select.RecordInput)
             self._print_guessed_selector(par_names[1], Select.RecordOutput)
+
+            # TODO: replace on_input_output ?
         else:
             # Otherwise give up.
 
             raise RuntimeError(
                 f"Cannot determine default paths for feedback function arguments. "
                 f"The feedback function has signature {sig}."
             )
@@ -263,14 +657,19 @@
 
     def _next_unselected_arg_name(self):
         if self.imp is not None:
             sig = signature(self.imp)
             par_names = list(
                 k for k in sig.parameters.keys() if k not in self.selectors
             )
+            if "self" in par_names:
+                logger.warning(
+                    f"Feedback function `{self.imp.__name__}` has `self` as argument. "
+                    "Perhaps it is static method or its Provider class was not initialized?"
+                )
             return par_names[0]
         else:
             raise RuntimeError(
                 "Cannot determine name of feedback function parameter without its definition."
             )
 
     def on_prompt(self, arg: Optional[str] = None):
@@ -381,16 +780,21 @@
                 status=FeedbackResultStatus.DONE,
                 cost=cost,
                 calls=feedback_calls
             )
 
             return feedback_result
 
-        except Exception as e:
-            raise e
+        except:
+            exc_tb = traceback.format_exc()
+            logger.warning(f"Feedback Function Exception Caught: {exc_tb}")
+            feedback_result.update(
+                error=exc_tb, status=FeedbackResultStatus.FAILED
+            )
+            return feedback_result
 
     def run_and_log(
         self,
         record: Record,
         tru: 'Tru',
         app: Union[AppDefinition, JSON] = None,
         feedback_result_id: Optional[FeedbackResultID] = None
@@ -419,17 +823,18 @@
             )
 
             feedback_result = self.run(
                 app=app, record=record
             ).update(feedback_result_id=feedback_result_id)
 
         except Exception as e:
+            exc_tb = traceback.format_exc()
             db.insert_feedback(
                 feedback_result.update(
-                    error=str(e), status=FeedbackResultStatus.FAILED
+                    error=exc_tb, status=FeedbackResultStatus.FAILED
                 )
             )
             return
 
         # Otherwise update based on what Feedback.run produced (could be success or failure).
         db.insert_feedback(feedback_result)
```

## trulens_eval/instruments.py

```diff
@@ -236,23 +236,23 @@
 from datetime import datetime
 from inspect import BoundArguments
 from inspect import signature
 import logging
 import os
 from pprint import PrettyPrinter
 import threading as th
-from typing import (Callable, Dict, Iterable, Optional, Sequence, Set)
+from typing import Callable, Dict, Iterable, Optional, Sequence, Set
 
 from pydantic import BaseModel
 
+from trulens_eval.feedback import Feedback
 from trulens_eval.schema import Perf
 from trulens_eval.schema import Query
 from trulens_eval.schema import RecordAppCall
 from trulens_eval.schema import RecordAppCallMethod
-from trulens_eval.feedback import Feedback
 from trulens_eval.util import _safe_getattr
 from trulens_eval.util import get_local_in_call_stack
 from trulens_eval.util import jsonify
 from trulens_eval.util import Method
 
 logger = logging.getLogger(__name__)
 pp = PrettyPrinter()
@@ -384,15 +384,15 @@
 
             # If a wrapped method was called in this call stack, get the prior
             # calls from this variable. Otherwise create a new chain stack.
             stack = get_local_in_call_stack(
                 key="stack", func=find_instrumented, offset=1
             ) or ()
             frame_ident = RecordAppCallMethod(
-                path=query, method=Method.of_method(func, obj=obj)
+                path=query, method=Method.of_method(func, obj=obj, cls=cls)
             )
             stack = stack + (frame_ident,)
 
             start_time = None
             end_time = None
 
             try:
@@ -420,15 +420,14 @@
                 perf=Perf(start_time=start_time, end_time=end_time),
                 pid=os.getpid(),
                 tid=th.get_native_id(),
                 stack=stack,
                 rets=rets,
                 error=error_str if error is not None else None
             )
-
             row = RecordAppCall(**row_args)
             record.append(row)
 
             if error is not None:
                 raise error
 
             return rets
@@ -475,23 +474,22 @@
             # the very base classes such as object:
             if not self.to_instrument_module(base.__module__):
                 continue
 
             logger.debug(f"\t{query}: instrumenting base {base.__name__}")
 
             for method_name in self.methods:
+
                 if hasattr(base, method_name):
                     check_class = self.methods[method_name]
                     if not check_class(obj):
                         continue
-
                     original_fun = getattr(base, method_name)
 
                     logger.debug(f"\t\t{query}: instrumenting {method_name}")
-
                     setattr(
                         base, method_name,
                         self.instrument_tracked_method(
                             query=query,
                             func=original_fun,
                             method_name=method_name,
                             cls=base,
```

## trulens_eval/keys.py

```diff
@@ -11,17 +11,18 @@
 
 import logging
 import os
 from pathlib import Path
 
 import cohere
 import dotenv
-from trulens_eval.util import UNICODE_CHECK, UNICODE_STOP
 
 from trulens_eval.util import caller_frame
+from trulens_eval.util import UNICODE_CHECK
+from trulens_eval.util import UNICODE_STOP
 
 logger = logging.getLogger(__name__)
 
 
 def get_config():
     for path in [Path().cwd(), *Path.cwd().parents]:
         file = path / ".env"
```

## trulens_eval/provider_apis.py

```diff
@@ -1,18 +1,18 @@
 import inspect
 import json
 import logging
+from pprint import PrettyPrinter
 from queue import Queue
 from threading import Thread
 from time import sleep
 from types import ModuleType
 from typing import (
     Any, Callable, Dict, Optional, Sequence, Tuple, Type, TypeVar
 )
-from pprint import PrettyPrinter
 
 from langchain.callbacks.openai_info import OpenAICallbackHandler
 from langchain.schema import LLMResult
 import pydantic
 import requests
 
 from trulens_eval.keys import get_huggingface_headers
@@ -240,15 +240,15 @@
             try:
                 self.pace_me()
                 ret = thunk()
                 return ret
             except Exception as e:
                 retries -= 1
                 logger.error(
-                    f"{self.name} request failed {type(e)}={e}. Retries={retries}."
+                    f"{self.name} request failed {type(e)}={e}. Retries remaining={retries}."
                 )
                 if retries > 0:
                     sleep(retry_delay)
                     retry_delay *= 2
 
         raise RuntimeError(
             f"API {self.name} request failed {self.retries+1} time(s)."
```

## trulens_eval/schema.py

```diff
@@ -16,26 +16,27 @@
 AppDefinition.app is the JSONized version of a wrapped app while App.app is the
 actual wrapped app. We can thus inspect the contents of a wrapped app without
 having to construct it. Additionally, JSONized objects like AppDefinition.app
 feature information about the encoded object types in the dictionary under the
 util.py:CLASS_INFO key.
 """
 
-from abc import ABC, abstractmethod
+from abc import ABC
+from abc import abstractmethod
 from datetime import datetime
 from enum import Enum
-
-from typing import (Any, ClassVar, Dict, Optional, Sequence, TypeVar, Union)
 import logging
+from typing import Any, ClassVar, Dict, Optional, Sequence, TypeVar, Union
+
 from munch import Munch as Bunch
 import pydantic
-from trulens_eval.util import FunctionOrMethod
 
 from trulens_eval.util import Class
 from trulens_eval.util import Function
+from trulens_eval.util import FunctionOrMethod
 from trulens_eval.util import GetItemOrAttribute
 from trulens_eval.util import JSON
 from trulens_eval.util import JSONPath
 from trulens_eval.util import Method
 from trulens_eval.util import obj_id_of_obj
 from trulens_eval.util import SerialModel
 from trulens_eval.util import WithClassInfo
@@ -44,14 +45,15 @@
 
 logger = logging.getLogger(__name__)
 
 # Identifier types.
 
 RecordID = str
 AppID = str
+Tags = str
 FeedbackDefinitionID = str
 FeedbackResultID = str
 
 # Serialization of python objects/methods. Not using pickling here so we can
 # inspect the contents a little better before unserializaing.
 
 # Record related:
@@ -147,27 +149,27 @@
     app_id: AppID
 
     cost: Optional[Cost] = None  # pydantic.Field(default_factory=Cost)
     perf: Optional[Perf] = None  # pydantic.Field(default_factory=Perf)
 
     ts: datetime = pydantic.Field(default_factory=lambda: datetime.now())
 
-    tags: str = ""
+    tags: Optional[str] = ""
 
     main_input: Optional[JSON] = None
     main_output: Optional[JSON] = None  # if no error
     main_error: Optional[JSON] = None  # if error
 
     # The collection of calls recorded. Note that these can be converted into a
     # json structure with the same paths as the app that generated this record
     # via `layout_calls_as_app`.
     calls: Sequence[RecordAppCall] = []
 
     def __init__(self, record_id: Optional[RecordID] = None, **kwargs):
-        super().__init__(record_id="temporay", **kwargs)
+        super().__init__(record_id="temporary", **kwargs)
 
         if record_id is None:
             record_id = obj_id_of_obj(self.dict(), prefix="record")
 
         self.record_id = record_id
 
     def layout_calls_as_app(self) -> JSON:
@@ -218,14 +220,55 @@
     # A App's main input and main output.
     # TODO: App input/output generalization.
     RecordInput: Query = Record.main_input
     RecordOutput: Query = Record.main_output
 
     RecordCalls: Query = Record.app
 
+    def for_record(query: Query) -> Query:
+        return Select.Query(path=Select.Record.path + query.path)
+
+    def for_app(query: Query) -> Query:
+        return Select.Query(path=Select.App.path + query.path)
+
+    def render_for_dashboard(query: Query) -> str:
+        """
+        Render the given query for use in dashboard to help user specify
+        feedback functions.
+        """
+
+        if len(query) == 0:
+            return "Select.Query()"
+
+        ret = ""
+        rest = None
+
+        if query.path[0:2] == Select.RecordInput.path:
+            ret = "Select.RecordInput"
+            rest = query.path[2:]
+        elif query.path[0:2] == Select.RecordOutput.path:
+            ret = "Select.RecordOutput"
+            rest = query.path[2:]
+        elif query.path[0:2] == Select.RecordCalls.path:
+            ret = "Select.RecordCalls"
+            rest = query.path[2:]
+        elif query.path[0] == Select.Record.path[0]:
+            ret = "Select.Record"
+            rest = query.path[1:]
+        elif query.path[0] == Select.App.path[0]:
+            ret = "Select.App"
+            rest = query.path[1:]
+        else:
+            rest = query.path
+
+        for step in rest:
+            ret += repr(step)
+
+        return f"{ret}"
+
 
 # To deprecate in 1.0.0:
 Query = Select
 
 
 class FeedbackResultStatus(Enum):
     NONE = "none"
@@ -249,16 +292,14 @@
     # "last timestamp"
     last_ts: datetime = pydantic.Field(default_factory=lambda: datetime.now())
 
     status: FeedbackResultStatus = FeedbackResultStatus.NONE
 
     cost: Cost = pydantic.Field(default_factory=Cost)
 
-    tags: str = ""
-
     name: str
 
     calls: Sequence[FeedbackCall] = []
     result: Optional[
         float] = None  # final result, potentially aggregating multiple calls
     error: Optional[str] = None  # if there was an error
 
@@ -357,14 +398,15 @@
     # Serialized fields here whereas app.py:App contains
     # non-serialized fields.
 
     class Config:
         arbitrary_types_allowed = True
 
     app_id: AppID
+    tags: Tags
 
     # Feedback functions to evaluate on each record. Unlike the above, these are
     # meant to be serialized.
     feedback_definitions: Sequence[FeedbackDefinition] = []
 
     # NOTE: Custom feedback functions cannot be run deferred and will be run as
     # if "withappthread" was set.
@@ -379,32 +421,38 @@
 
     # Wrapped app in jsonized form.
     app: JSON
 
     def __init__(
         self,
         app_id: Optional[AppID] = None,
+        tags: Optional[Tags] = None,
         feedback_mode: FeedbackMode = FeedbackMode.WITH_APP_THREAD,
         **kwargs
     ):
 
         # for us:
         kwargs['app_id'] = "temporary"  # will be adjusted below
         kwargs['feedback_mode'] = feedback_mode
+        kwargs['tags'] = ""
 
         # for WithClassInfo:
         kwargs['obj'] = self
 
         super().__init__(**kwargs)
 
         if app_id is None:
             app_id = obj_id_of_obj(obj=self.dict(), prefix="app")
 
         self.app_id = app_id
 
+        if tags is None:
+            tags = "-"  # Set tags to a "-" if None is provided
+        self.tags = tags
+
     @classmethod
     def select_inputs(cls) -> JSONPath:
         """
         Get the path to the main app's call inputs.
         """
 
         return getattr(
```

## trulens_eval/tru.py

```diff
@@ -13,17 +13,18 @@
 
 from trulens_eval.db import JSON
 from trulens_eval.db import LocalSQLite
 from trulens_eval.feedback import Feedback
 from trulens_eval.schema import AppDefinition
 from trulens_eval.schema import FeedbackResult
 from trulens_eval.schema import Record
-from trulens_eval.util import UNICODE_CHECK, UNICODE_YIELD
 from trulens_eval.util import SingletonPerName
 from trulens_eval.util import TP
+from trulens_eval.util import UNICODE_CHECK
+from trulens_eval.util import UNICODE_YIELD
 from trulens_eval.utils.notebook_utils import is_notebook
 from trulens_eval.utils.notebook_utils import setup_widget_stdout_stderr
 
 logger = logging.getLogger(__name__)
 
 # How long to wait (seconds) for streamlit to print out url when starting the
 # dashboard.
@@ -404,58 +405,87 @@
             stdout=subprocess.PIPE,
             stderr=subprocess.PIPE,
             text=True,
             **env_opts
         )
 
         started = threading.Event()
+        tunnel_started = threading.Event()
         if is_notebook():
             out_stdout, out_stderr = setup_widget_stdout_stderr()
         else:
             out_stdout = None
             out_stderr = None
 
-        tunnel_proc = subprocess.Popen(
-            ["npx", "localtunnel", "--port", "8501"],
-            stdout=subprocess.PIPE,
-            stderr=subprocess.PIPE,
-            text=True,
-            **env_opts
-        )
-        def listen_to_tunnel(proc: subprocess.Popen, pipe, out, started):
-            while proc.poll() is None:
-                started.set()
-                line = pipe.readline()
-                if out is not None:
-                    out.append_stdout(line)
-                else:
-                    print(line)
-        Tru.tunnel_listener_stdout = Thread(
-            target=listen_to_tunnel,
-            args=(tunnel_proc, tunnel_proc.stdout, out_stdout, started)
-        )
-        Tru.tunnel_listener_stderr = Thread(
-            target=listen_to_tunnel,
-            args=(tunnel_proc, tunnel_proc.stderr, out_stderr, started)
-        )
-        Tru.tunnel_listener_stdout.start()
-        Tru.tunnel_listener_stderr.start()
+        IN_COLAB = 'google.colab' in sys.modules
+        if IN_COLAB:
+            tunnel_proc = subprocess.Popen(
+                ["npx", "localtunnel", "--port", "8501"],
+                stdout=subprocess.PIPE,
+                stderr=subprocess.PIPE,
+                text=True,
+                **env_opts
+            )
+
+            def listen_to_tunnel(proc: subprocess.Popen, pipe, out, started):
+                while proc.poll() is None:
+
+                    line = pipe.readline()
+                    if "url" in line:
+                        started.set()
+                        line = "Go to this url and submit the ip given here. " + line
+
+                    if out is not None:
+                        out.append_stdout(line)
+
+                    else:
+                        print(line)
+
+            Tru.tunnel_listener_stdout = Thread(
+                target=listen_to_tunnel,
+                args=(
+                    tunnel_proc, tunnel_proc.stdout, out_stdout, tunnel_started
+                )
+            )
+            Tru.tunnel_listener_stderr = Thread(
+                target=listen_to_tunnel,
+                args=(
+                    tunnel_proc, tunnel_proc.stderr, out_stderr, tunnel_started
+                )
+            )
+            Tru.tunnel_listener_stdout.start()
+            Tru.tunnel_listener_stderr.start()
+            if not tunnel_started.wait(timeout=DASHBOARD_START_TIMEOUT
+                                      ):  # This might not work on windows.
+                raise RuntimeError("Tunnel failed to start in time. ")
 
         def listen_to_dashboard(proc: subprocess.Popen, pipe, out, started):
             while proc.poll() is None:
                 line = pipe.readline()
-                if "Network URL: " in line:
-                    url = line.split(": ")[1]
-                    url = url.rstrip()
-                    print(f"Dashboard started at {url} .")
-                    started.set()
-                if out is not None:
-                    out.append_stdout(line)
+                if IN_COLAB:
+                    if "External URL: " in line:
+                        started.set()
+                        line = line.replace(
+                            "External URL: http://", "Submit this IP Address: "
+                        )
+                        line = line.replace(":8501", "")
+                        if out is not None:
+                            out.append_stdout(line)
+                        else:
+                            print(line)
                 else:
-                    print(line)
+                    if "Network URL: " in line:
+                        url = line.split(": ")[1]
+                        url = url.rstrip()
+                        print(f"Dashboard started at {url} .")
+                        started.set()
+                    if out is not None:
+                        out.append_stdout(line)
+                    else:
+                        print(line)
             if out is not None:
                 out.append_stdout("Dashboard closed.")
             else:
                 print("Dashboard closed.")
 
         Tru.dashboard_listener_stdout = Thread(
             target=listen_to_dashboard,
@@ -466,15 +496,19 @@
             args=(proc, proc.stderr, out_stderr, started)
         )
         Tru.dashboard_listener_stdout.start()
         Tru.dashboard_listener_stderr.start()
 
         Tru.dashboard_proc = proc
 
-        if not started.wait(timeout=DASHBOARD_START_TIMEOUT
+        wait_period = DASHBOARD_START_TIMEOUT
+        if IN_COLAB:
+            # Need more time to setup 2 processes tunnel and dashboard
+            wait_period = wait_period * 3
+        if not started.wait(timeout=wait_period
                            ):  # This might not work on windows.
             raise RuntimeError(
                 "Dashboard failed to start in time. "
                 "Please inspect dashboard logs for additional information."
             )
 
         return proc
```

## trulens_eval/tru_app.py

 * *Ordering differences only*

```diff
@@ -1,11 +1,11 @@
-from trulens_eval import app
-
 import logging
 
+from trulens_eval import app
+
 logger = logging.getLogger(__name__)
 
 for attr in dir(app):
     if not attr.startswith("_"):
         globals()[attr] = getattr(app, attr)
 
 # Since 0.2.0
```

## trulens_eval/tru_chain.py

```diff
@@ -43,15 +43,17 @@
             BaseLLM, langchain.prompts.base.BasePromptTemplate, langchain.schema
             .BaseMemory, langchain.schema.BaseChatMessageHistory
         }
 
         # Instrument only methods with these names and of these classes.
         METHODS = {
             "_call": lambda o: isinstance(o, langchain.chains.base.Chain),
-            "get_relevant_documents": lambda o: True,  # VectorStoreRetriever
+            # "get_relevant_documents": lambda o: True,  # VectorStoreRetriever
+            "_get_relevant_documents":
+                lambda o: True,  # VectorStoreRetriever, langchain >= 0.230
         }
 
     def __init__(self):
         super().__init__(
             root_method=TruChain.call_with_record,
             modules=LangChainInstrument.Default.MODULES,
             classes=LangChainInstrument.Default.CLASSES(),
```

## trulens_eval/tru_db.py

 * *Ordering differences only*

```diff
@@ -1,11 +1,11 @@
-from trulens_eval import db
-
 import logging
 
+from trulens_eval import db
+
 logger = logging.getLogger(__name__)
 
 for attr in dir(db):
     if not attr.startswith("_"):
         globals()[attr] = getattr(db, attr)
 
 # Since 0.2.0
```

## trulens_eval/tru_feedback.py

 * *Ordering differences only*

```diff
@@ -1,11 +1,11 @@
-from trulens_eval import feedback
-
 import logging
 
+from trulens_eval import feedback
+
 logger = logging.getLogger(__name__)
 
 for attr in dir(feedback):
     if not attr.startswith("_"):
         globals()[attr] = getattr(feedback, attr)
 
 # Since 0.2.0
```

## trulens_eval/tru_llama.py

 * *Ordering differences only*

```diff
@@ -5,26 +5,26 @@
 from datetime import datetime
 import logging
 from pprint import PrettyPrinter
 from typing import ClassVar, Sequence, Tuple
 
 from pydantic import Field
 
-from trulens_eval.instruments import Instrument
-from trulens_eval.schema import Record
-from trulens_eval.schema import RecordAppCall
 from trulens_eval.app import App
+from trulens_eval.instruments import Instrument
+from trulens_eval.provider_apis import Endpoint
 from trulens_eval.provider_apis import OpenAIEndpoint
 from trulens_eval.schema import Cost
-from trulens_eval.provider_apis import Endpoint
+from trulens_eval.schema import Record
+from trulens_eval.schema import RecordAppCall
+from trulens_eval.util import Class
+from trulens_eval.util import dict_set_with
 from trulens_eval.util import FunctionOrMethod
 from trulens_eval.util import JSONPath
 from trulens_eval.util import Method
-from trulens_eval.util import dict_set_with
-from trulens_eval.util import Class
 from trulens_eval.util import OptionalImports
 from trulens_eval.util import REQUIREMENT_LLAMA
 
 logger = logging.getLogger(__name__)
 
 pp = PrettyPrinter()
```

## trulens_eval/util.py

```diff
@@ -19,14 +19,15 @@
   reconstruct the object whereas Obj does not. This information is its
   constructor arguments.
 """
 
 from __future__ import annotations
 
 import builtins
+from concurrent.futures import ThreadPoolExecutor as fThreadPoolExecutor
 from enum import Enum
 import importlib
 import inspect
 from inspect import stack
 import itertools
 import json
 import logging
@@ -57,21 +58,23 @@
 UNICODE_CHECK = ""
 UNICODE_YIELD = ""
 UNICODE_HOURGLASS = ""
 UNICODE_CLOCK = ""
 
 # Optional requirements.
 
+langchain_version = "0.0.230"
+
 REQUIREMENT_LLAMA = (
     "llama_index 0.6.24 or above is required for instrumenting llama_index apps. "
     "Please install it before use: `pip install llama_index>=0.6.24`."
 )
 REQUIREMENT_LANGCHAIN = (
-    "langchain 0.0.170 or above is required for instrumenting langchain apps. "
-    "Please install it before use: `pip install langchain>=0.0.170`."
+    f"langchain {langchain_version} or above is required for instrumenting langchain apps. "
+    f"Please install it before use: `pip install langchain>={langchain_version}`."
 )
 
 
 class Dummy(object):
     """
     Class to pretend to be a module or some other imported object. Will raise an
     error if accessed in any way.
@@ -741,15 +744,15 @@
 
 class GetItemOrAttribute(Step):
     # For item/attribute agnostic addressing.
 
     item_or_attribute: str  # distinct from "item" for deserialization
 
     def __hash__(self):
-        return hash(self.item)
+        return hash(self.item_or_attribute)
 
     def __call__(self, obj: Dict[str, T]) -> Iterable[T]:
         if isinstance(obj, Dict):
             if self.item_or_attribute in obj:
                 yield obj[self.item_or_attribute]
             else:
                 raise KeyError(
@@ -1033,14 +1036,37 @@
 
         return SingletonPerName.instances[k]
 
 
 # Threading utilities
 
 
+def _future_target_wrapper(stack, func, *args, **kwargs):
+    """
+    Wrapper for a function that is started by threads. This is needed to
+    record the call stack prior to thread creation as in python threads do
+    not inherit the stack. Our instrumentation, however, relies on walking
+    the stack and need to do this to the frames prior to thread starts.
+    """
+
+    # Keep this for looking up via get_local_in_call_stack .
+    pre_start_stack = stack
+
+    return func(*args, **kwargs)
+
+
+class ThreadPoolExecutor(fThreadPoolExecutor):
+
+    def submit(self, fn, /, *args, **kwargs):
+        present_stack = stack()
+        return super().submit(
+            _future_target_wrapper, present_stack, fn, *args, **kwargs
+        )
+
+
 class TP(SingletonPerName):  # "thread processing"
 
     # Store here stacks of calls to various thread starting methods so that we can retrieve
     # the trace of calls that caused a thread to start.
     # pre_run_stacks = dict()
 
     def __init__(self):
@@ -1059,33 +1085,19 @@
         def runner():
             while True:
                 func(*args, **kwargs)
                 sleep(60 / rpm)
 
         self.runlater(runner)
 
-    @staticmethod
-    def _thread_target_wrapper(stack, func, *args, **kwargs):
-        """
-        Wrapper for a function that is started by threads. This is needed to
-        record the call stack prior to thread creation as in python threads do
-        not inherit the stack. Our instrumentation, however, relies on walking
-        the stack and need to do this to the frames prior to thread starts.
-        """
-
-        # Keep this for looking up via get_local_in_call_stack .
-        pre_start_stack = stack
-
-        return func(*args, **kwargs)
-
     def _thread_starter(self, func, args, kwargs):
         present_stack = stack()
 
         prom = self.thread_pool.apply_async(
-            self._thread_target_wrapper,
+            _future_target_wrapper,
             args=(present_stack, func) + args,
             kwds=kwargs
         )
         return prom
 
     def runlater(self, func: Callable, *args, **kwargs) -> None:
         prom = self._thread_starter(func, args, kwargs)
@@ -1160,15 +1172,15 @@
         q.put(f)
 
     while not q.empty():
         fi = q.get()
 
         logger.debug(f"{fi.frame.f_code}")
 
-        if id(fi.frame.f_code) == id(TP()._thread_target_wrapper.__code__):
+        if id(fi.frame.f_code) == id(_future_target_wrapper.__code__):
             logger.debug(
                 "Found thread starter frame. "
                 "Will walk over frames prior to thread start."
             )
             locs = fi.frame.f_locals
             assert "pre_start_stack" in locs, "Pre thread start stack expected but not found."
             for f in locs['pre_start_stack']:
@@ -1277,15 +1289,15 @@
                 return True
 
         return False
 
 
 # inspect.signature does not work on builtin type constructors but they are used
 # like this method. Use it to create a signature of a builtin constructor.
-def builtin_init_dummy(self, /, *args, **kwargs):
+def builtin_init_dummy(self, *args, **kwargs):
     pass
 
 
 builtin_init_sig = inspect.signature(builtin_init_dummy)
 
 
 class Obj(SerialModel):
```

## trulens_eval/pages/Evaluations.py

```diff
@@ -4,29 +4,34 @@
 import matplotlib.pyplot as plt
 import pandas as pd
 from st_aggrid import AgGrid
 from st_aggrid.grid_options_builder import GridOptionsBuilder
 from st_aggrid.shared import GridUpdateMode
 from st_aggrid.shared import JsCode
 import streamlit as st
+from streamlit_javascript import st_javascript
 from ux.add_logo import add_logo
 from ux.styles import default_pass_fail_color_threshold
 
 from trulens_eval import Tru
 from trulens_eval.app import ComponentView
 from trulens_eval.app import instrumented_component_views
 from trulens_eval.app import LLM
 from trulens_eval.app import Other
 from trulens_eval.app import Prompt
+from trulens_eval.react_components.record_viewer import record_viewer
 from trulens_eval.schema import Record
+from trulens_eval.schema import Select
 from trulens_eval.util import jsonify
 from trulens_eval.util import JSONPath
 from trulens_eval.ux.components import draw_call
 from trulens_eval.ux.components import draw_llm_info
 from trulens_eval.ux.components import draw_prompt_info
+from trulens_eval.ux.components import draw_selector_button
+from trulens_eval.ux.components import render_selector_markdown
 from trulens_eval.ux.components import write_or_json
 from trulens_eval.ux.styles import cellstyle_jscode
 
 st.set_page_config(page_title="Evaluations", layout="wide")
 
 st.title("Evaluations")
 
@@ -35,14 +40,65 @@
 add_logo()
 
 tru = Tru()
 lms = tru.db
 
 df_results, feedback_cols = lms.get_records_and_feedback([])
 
+state = st.session_state
+
+if "clipboard" not in state:
+    state.clipboard = "nothing"
+
+if state.clipboard:
+    ret = st_javascript(
+        f"""navigator.clipboard.writeText("{state.clipboard}")
+    .then(
+        function() {{
+            console.log('success?')
+        }},
+        function(err) {{
+            console.error("Async: Could not copy text: ", err)
+        }}
+    )
+"""
+    )
+
+
+def render_component(query, component, header=True):
+    # Draw the accessor/path within the wrapped app of the component.
+    if header:
+        st.subheader(
+            f"Component {render_selector_markdown(Select.for_app(query))}"
+        )
+
+    # Draw the python class information of this component.
+    cls = component.cls
+    base_cls = cls.base_class()
+    label = f"__{repr(cls)}__"
+    if str(base_cls) != str(cls):
+        label += f" < __{repr(base_cls)}__"
+    st.write("Python class: " + label)
+
+    # Per-component-type drawing routines.
+    if isinstance(component, LLM):
+        draw_llm_info(component=component, query=query)
+
+    elif isinstance(component, Prompt):
+        draw_prompt_info(component=component, query=query)
+
+    elif isinstance(component, Other):
+        with st.expander("Uncategorized Component Details:"):
+            st.json(jsonify(component.json, skip_specials=True))
+
+    else:
+        with st.expander("Unhandled Component Details:"):
+            st.json(jsonify(component.json, skip_specials=True))
+
+
 if df_results.empty:
     st.write("No records yet...")
 
 else:
     apps = list(df_results.app_id.unique())
     if 'app' in st.session_state:
         app = st.session_state.app
@@ -128,18 +184,22 @@
         else:
             st.header(f"Selected LLM Application: {selected_rows['app_id'][0]}")
             st.text(f"Selected Record ID: {selected_rows['record_id'][0]}")
 
             prompt = selected_rows['input'][0]
             response = selected_rows['output'][0]
 
-            with st.expander("Input", expanded=True):
+            with st.expander(
+                    f"Input {render_selector_markdown(Select.RecordInput)}",
+                    expanded=True):
                 write_or_json(st, obj=prompt)
 
-            with st.expander("Response", expanded=True):
+            with st.expander(
+                    f"Response {render_selector_markdown(Select.RecordOutput)}",
+                    expanded=True):
                 write_or_json(st, obj=response)
 
             row = selected_rows.head().iloc[0]
 
             st.header("Feedback")
             for fcol in feedback_cols:
                 feedback_name = fcol
@@ -178,60 +238,78 @@
             record = Record(**record_json)
 
             details = selected_rows['app_json'][0]
             app_json = json.loads(
                 details
             )  # apps may not be deserializable, don't try to, keep it json.
 
-            classes: Iterable[Tuple[JSONPath, List[ComponentView]]
-                             ] = instrumented_component_views(app_json)
-
-            st.header("Components")
-
-            for query, component in classes:
-
-                if len(query.path) == 0:
-                    # Skip App, will still list A.app under "app" below.
-                    continue
-
-                # Draw the accessor/path within the wrapped app of the component.
-                st.subheader(f"{query}")
-
-                # Draw the python class information of this component.
-                cls = component.cls
-                base_cls = cls.base_class()
-                label = f"`{repr(cls)}`"
-                if str(base_cls) != str(cls):
-                    label += f" < `{repr(base_cls)}`"
-                st.write(label)
-
-                # Per-component-type drawing routines.
-                if isinstance(component, LLM):
-                    draw_llm_info(component=component, query=query)
-
-                elif isinstance(component, Prompt):
-                    draw_prompt_info(component=component, query=query)
-
-                elif isinstance(component, Other):
-                    with st.expander("Uncategorized Component Details:"):
-                        st.json(jsonify(component.json, skip_specials=True))
+            classes: Iterable[Tuple[JSONPath, ComponentView]
+                             ] = list(instrumented_component_views(app_json))
+            classes_map = {path: view for path, view in classes}
+
+            st.header('Timeline')
+            val = record_viewer(record_json, app_json)
+
+            match_query = None
+            if val != "":
+                match = None
+                for call in record.calls:
+                    if call.perf.start_time.isoformat() == val:
+                        match = call
+                        break
+
+                if match:
+                    length = len(match.stack)
+                    app_call = match.stack[length - 1]
+
+                    match_query = match.top().path
+
+                    st.subheader(
+                        f"{app_call.method.obj.cls.name} {render_selector_markdown(Select.for_app(match_query))}"
+                    )
+
+                    draw_call(match)
+                    # with st.expander("Call Details:"):
+                    #     st.json(jsonify(match, skip_specials=True))
+
+                    view = classes_map.get(match_query)
+                    if view is not None:
+                        render_component(
+                            query=match_query, component=view, header=False
+                        )
+                    else:
+                        st.write(
+                            f"Call by {match_query} was not associated with any instrumented component."
+                        )
+                        # Look up whether there was any data at that path even if not an instrumented component:
+                        app_component_json = list(match_query(app_json))[0]
+                        if app_component_json is not None:
+                            with st.expander(
+                                    "Uninstrumented app component details."):
+                                st.json(app_component_json)
 
                 else:
-                    with st.expander("Unhandled Component Details:"):
-                        st.json(jsonify(component.json, skip_specials=True))
+                    st.text('No match found')
+            else:
+                st.subheader(f"App {render_selector_markdown(Select.App)}")
+                with st.expander("App Details:"):
+                    st.json(jsonify(app_json, skip_specials=True))
+
+            if match_query is not None:
+                st.header("Subcomponents:")
+
+                for query, component in classes:
+                    if not match_query.is_immediate_prefix_of(query):
+                        continue
+
+                    if len(query.path) == 0:
+                        # Skip App, will still list App.app under "app".
+                        continue
 
-                # Draw the calls issued to component.
-                calls = [
-                    call for call in record.calls
-                    if query == call.stack[-1].path
-                ]
-                if len(calls) > 0:
-                    st.subheader("Calls to component:")
-                    for call in calls:
-                        draw_call(call)
+                    render_component(query, component)
 
             st.header("More options:")
 
             if st.button("Display full app json"):
 
                 st.write(jsonify(app_json, skip_specials=True))
```

## trulens_eval/pages/Progress.py

```diff
@@ -4,24 +4,25 @@
 
 import pandas as pd
 from st_aggrid import AgGrid
 from st_aggrid.grid_options_builder import GridOptionsBuilder
 from st_aggrid.shared import GridUpdateMode
 from st_aggrid.shared import JsCode
 import streamlit as st
-from trulens_eval.provider_apis import HuggingfaceEndpoint, OpenAIEndpoint
 from ux.add_logo import add_logo
 
-from trulens_eval import Tru
 from trulens_eval import db
+from trulens_eval import Tru
+from trulens_eval.db import DB
+from trulens_eval.feedback import Feedback
 from trulens_eval.keys import *
 from trulens_eval.provider_apis import Endpoint
+from trulens_eval.provider_apis import HuggingfaceEndpoint
+from trulens_eval.provider_apis import OpenAIEndpoint
 from trulens_eval.schema import FeedbackResultStatus
-from trulens_eval.db import DB
-from trulens_eval.feedback import Feedback
 from trulens_eval.util import is_empty
 from trulens_eval.util import is_noserio
 from trulens_eval.util import TP
 
 st.set_page_config(page_title="Feedback Progress", layout="wide")
 
 st.title("Feedback Progress")
```

## trulens_eval/utils/langchain.py

```diff
@@ -1,18 +1,19 @@
 from typing import Iterable, List, Type
 
-from trulens_eval.feedback import Feedback
-from trulens_eval.app import COMPONENT_CATEGORY
 from trulens_eval import app
-from trulens_eval.util import JSON
+from trulens_eval.app import COMPONENT_CATEGORY
+from trulens_eval.feedback import Feedback
 from trulens_eval.util import Class
 from trulens_eval.util import first
+from trulens_eval.util import JSON
 from trulens_eval.util import OptionalImports
 from trulens_eval.util import REQUIREMENT_LANGCHAIN
 from trulens_eval.util import second
+from trulens_eval.util import ThreadPoolExecutor
 from trulens_eval.util import TP
 
 with OptionalImports(message=REQUIREMENT_LANGCHAIN):
     import langchain
     from langchain.schema import Document
     from langchain.vectorstores.base import VectorStoreRetriever
 
@@ -27,15 +28,18 @@
         return super().unsorted_parameters(skip=set(['template']))
 
     @staticmethod
     def class_is(cls: Class) -> bool:
         return cls.noserio_issubclass(
             module_name="langchain.prompts.base",
             class_name="BasePromptTemplate"
-        )
+        ) or cls.noserio_issubclass(
+            module_name="langchain.schema.prompt_template",
+            class_name="BasePromptTemplate"
+        )  # langchain >= 0.230
 
 
 class LLM(app.LLM, app.LangChainComponent):
 
     @property
     def model_name(self) -> str:
         return self.json['model_name']
@@ -154,30 +158,39 @@
           at least this threshold.
         """
 
         super().__init__(
             feedback=feedback, threshold=threshold, *args, **kwargs
         )
 
-    def get_relevant_documents(self, query: str) -> List[Document]:
+    # Signature must match
+    # langchain.schema.retriever.BaseRetriever._get_relevant_documents .
+    def _get_relevant_documents(self, query: str, *,
+                                run_manager) -> List[Document]:
         # Get relevant docs using super class:
-        docs = super().get_relevant_documents(query)
+        docs = super()._get_relevant_documents(query, run_manager=run_manager)
 
         # Evaluate the filter on each, in parallel.
-        promises = (
+        ex = ThreadPoolExecutor(max_workers=max(1, len(docs)))
+
+        futures = list(
             (
-                doc, TP().promise(
-                    lambda doc, query: self.feedback(query, doc.page_content) >
-                    self.threshold,
+                doc,
+                ex.submit(
+                    (
+                        lambda doc, query: self.
+                        feedback(query, doc.page_content) > self.threshold
+                    ),
                     query=query,
                     doc=doc
                 )
             ) for doc in docs
         )
-        results = ((doc, promise.get()) for (doc, promise) in promises)
+
+        results = list((doc, promise.result()) for (doc, promise) in futures)
         filtered = map(first, filter(second, results))
 
         # Return only the filtered ones.
         return list(filtered)
 
     @staticmethod
     def of_retriever(retriever: VectorStoreRetriever, **kwargs):
```

## trulens_eval/utils/llama.py

```diff
@@ -1,23 +1,26 @@
 from typing import Iterable, List, Type
 
+from trulens_eval import app
 from trulens_eval import Feedback
-from trulens_eval.feedback import Feedback
 from trulens_eval.app import COMPONENT_CATEGORY
-from trulens_eval import app
+from trulens_eval.feedback import Feedback
+from trulens_eval.util import Class
+from trulens_eval.util import first
 from trulens_eval.util import JSON
-from trulens_eval.util import Class, first, second
 from trulens_eval.util import OptionalImports
 from trulens_eval.util import REQUIREMENT_LLAMA
+from trulens_eval.util import second
 from trulens_eval.util import TP
 
 with OptionalImports(message=REQUIREMENT_LLAMA):
     from llama_index.data_structs.node import NodeWithScore
     from llama_index.indices.query.schema import QueryBundle
-    from llama_index.indices.vector_store.retrievers import VectorIndexRetriever
+    from llama_index.indices.vector_store.retrievers import \
+        VectorIndexRetriever
 
 
 class Prompt(app.Prompt, app.LangChainComponent):
 
     @property
     def template(self) -> str:
         return self.json['template']
```

## trulens_eval/ux/add_logo.py

```diff
@@ -1,14 +1,15 @@
 import base64
 
 import pkg_resources
 import streamlit as st
 
+from trulens_eval import __package__
+from trulens_eval import __version__
 
-from trulens_eval import __version__, __package__
 
 def add_logo():
     logo = open(
         pkg_resources.resource_filename('trulens_eval', 'ux/trulens_logo.svg'),
         "rb"
     ).read()
```

## trulens_eval/ux/components.py

```diff
@@ -1,21 +1,25 @@
 import json
+import random
 from typing import Dict, List
 
 import pandas as pd
 import streamlit as st
+from streamlit_javascript import st_javascript
 
+from trulens_eval.app import ComponentView
 from trulens_eval.schema import Record
 from trulens_eval.schema import RecordAppCall
-from trulens_eval.app import ComponentView
-from trulens_eval.util import jsonify
-from trulens_eval.util import JSONPath
+from trulens_eval.schema import Select
 from trulens_eval.util import CLASS_INFO
+from trulens_eval.util import GetItemOrAttribute
 from trulens_eval.util import is_empty
 from trulens_eval.util import is_noserio
+from trulens_eval.util import jsonify
+from trulens_eval.util import JSONPath
 
 
 def write_or_json(st, obj):
     """
     Dispatch either st.json or st.write depending on content of `obj`. If it is
     a string that can parses into strictly json (dict), use st.json, otherwise
     use st.write.
@@ -29,39 +33,64 @@
             else:
                 st.write(content)
 
         except BaseException:
             st.write(obj)
 
 
-def render_call_frame(frame: RecordAppCall) -> str:  # markdown
+def copy_to_clipboard(path, *args, **kwargs):
+    st.session_state.clipboard = str(path)
+
+
+def draw_selector_button(path) -> None:
+    st.button(
+        key=str(random.random()),
+        label=f"{Select.render_for_dashboard(path)}",
+        on_click=copy_to_clipboard,
+        args=(path,)
+    )
+
+
+def render_selector_markdown(path) -> str:
+    return f"[`{Select.render_for_dashboard(path)}`]"
+
+
+def render_call_frame(frame: RecordAppCall, path=None) -> str:  # markdown
+    path = path or frame.path
 
     return (
-        f"{frame.path}.___{frame.method.name}___\n"
-        f"(`{frame.method.obj.cls.module.module_name}.{frame.method.obj.cls.name}`)"
+        f"__{frame.method.name}__ (__{frame.method.obj.cls.module.module_name}.{frame.method.obj.cls.name}__)"
     )
 
 
-def draw_call(call) -> None:
+def draw_call(call: RecordAppCall) -> None:
     top = call.stack[-1]
 
-    with st.expander(label=render_call_frame(top)):
+    path = Select.for_record(
+        top.path._append(
+            step=GetItemOrAttribute(item_or_attribute=top.method.name)
+        )
+    )
+
+    with st.expander(label=f"Call " + render_call_frame(top, path=path) + " " +
+                     render_selector_markdown(path)):
+
         args = call.args
         rets = call.rets
 
-        for frame in call.stack[0:-2]:
-            st.write("Via " + render_call_frame(frame))
+        for frame in call.stack[::-1][1:]:
+            st.write("Via " + render_call_frame(frame, path=path))
 
-        st.subheader(f"Inputs:")
+        st.subheader(f"Inputs {render_selector_markdown(path.args)}")
         if isinstance(args, Dict):
             st.json(args)
         else:
             st.write(args)
 
-        st.subheader(f"Outputs:")
+        st.subheader(f"Outputs {render_selector_markdown(path.rets)}")
         if isinstance(rets, Dict):
             st.json(rets)
         else:
             st.write(rets)
 
 
 def draw_calls(record: Record, index: int) -> None:
@@ -81,24 +110,28 @@
 
         draw_call(call)
 
 
 def draw_prompt_info(query: JSONPath, component: ComponentView) -> None:
     prompt_details_json = jsonify(component.json, skip_specials=True)
 
-    path_str = str(query)
     st.subheader(f"*Prompt Details*")
 
+    path = Select.for_app(query)
+
     prompt_types = {
         k: v for k, v in prompt_details_json.items() if (v is not None) and
         not is_empty(v) and not is_noserio(v) and k != CLASS_INFO
     }
 
     for key, value in prompt_types.items():
-        with st.expander(key.capitalize(), expanded=True):
+        with st.expander(key.capitalize() + " " +
+                         render_selector_markdown(getattr(path, key)),
+                         expanded=True):
+
             if isinstance(value, (Dict, List)):
                 st.write(value)
             else:
                 if isinstance(value, str) and len(value) > 32:
                     st.text(value)
                 else:
                     st.write(value)
@@ -122,25 +155,37 @@
                 tbody th {display:none}
                 </style>
                 """
     df = pd.DataFrame.from_dict(llm_kv, orient='index').transpose()
 
     # Iterate over each column of the DataFrame
     for column in df.columns:
+        path = getattr(Select.for_app(query), str(column))
         # Check if any cell in the column is a dictionary
+
         if any(isinstance(cell, dict) for cell in df[column]):
             # Create new columns for each key in the dictionary
             new_columns = df[column].apply(
                 lambda x: pd.Series(x) if isinstance(x, dict) else pd.Series()
             )
-            new_columns.columns = [f"{key}" for key in new_columns.columns]
+            new_columns.columns = [
+                f"{key} {render_selector_markdown(path)}"
+                for key in new_columns.columns
+            ]
 
             # Remove extra zeros after the decimal point
             new_columns = new_columns.applymap(
                 lambda x: '{0:g}'.format(x) if isinstance(x, float) else x
             )
 
             # Add the new columns to the original DataFrame
             df = pd.concat([df.drop(column, axis=1), new_columns], axis=1)
+
+        else:
+            # TODO: add selectors to the output here
+
+            pass
+
     # Inject CSS with Markdown
+
     st.markdown(hide_table_row_index, unsafe_allow_html=True)
     st.table(df)
```

## Comparing `trulens_eval-0.4.1b0.dist-info/METADATA` & `trulens_eval-0.5.0a0.dist-info/METADATA`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: trulens-eval
-Version: 0.4.1b0
+Version: 0.5.0a0
 Summary: Library with langchain instrumentation to evaluate LLM based applications.
 Home-page: https://www.trulens.org
 Author: Truera Inc
 Author-email: all@truera.com
 License: MIT
 Classifier: Programming Language :: Python :: 3
 Classifier: Operating System :: OS Independent
@@ -12,27 +12,28 @@
 Classifier: License :: OSI Approved :: MIT License
 Requires-Python: >=3.8
 Description-Content-Type: text/markdown
 Requires-Dist: cohere (>=4.4.1)
 Requires-Dist: datasets (>=2.12.0)
 Requires-Dist: python-dotenv (>=1.0.0)
 Requires-Dist: kaggle (>=1.5.13)
-Requires-Dist: langchain (>=0.0.170)
+Requires-Dist: langchain (>=0.0.230)
 Requires-Dist: llama-index (>=0.6.24)
 Requires-Dist: merkle-json (>=1.0.0)
 Requires-Dist: millify (>=0.1.1)
 Requires-Dist: openai (>=0.27.6)
 Requires-Dist: pinecone-client (>=2.2.1)
 Requires-Dist: pydantic (>=1.10.7)
 Requires-Dist: requests (>=2.30.0)
 Requires-Dist: slack-bolt (>=1.18.0)
 Requires-Dist: slack-sdk (>=3.21.3)
 Requires-Dist: streamlit (>=1.13.0)
 Requires-Dist: streamlit-aggrid (>=0.3.4.post3)
 Requires-Dist: streamlit-extras (>=0.2.7)
+Requires-Dist: streamlit-javascript (>=0.1.5)
 Requires-Dist: transformers (>=4.10.0)
 Requires-Dist: typing-inspect (==0.8.0)
 Requires-Dist: typing-extensions (==4.5.0)
 Requires-Dist: frozendict (>=2.3.8)
 Requires-Dist: munch (>=3.0.0)
 Requires-Dist: ipywidgets (>=8.0.6)
 Requires-Dist: numpy (>=1.23.5)
@@ -47,23 +48,38 @@
 
 ![Architecture Diagram](https://www.trulens.org/Assets/image/TruLens_Architecture.png)
 
 ## Quick Usage
 
 To quickly play around with the TruLens Eval library:
 
-[langchain_quickstart.ipynb](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.4.0/trulens_eval/examples/quickstart.ipynb).
+Langchain:
 
-[langchain_quickstart.py](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.4.0/trulens_eval/examples/quickstart.py).
+[langchain_quickstart.ipynb](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.5.0/trulens_eval/examples/quickstart.ipynb).
+[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/releases/rc-trulens-eval-0.5.0/trulens_eval/examples/colab/quickstarts/langchain_quickstart_colab.ipynb)
 
-[llamaindex_quickstart.ipynb](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.4.0/trulens_eval/examples/frameworks/llama_index/llama_index_quickstart.ipynb).
+[langchain_quickstart.py](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.5.0/trulens_eval/examples/quickstart.py).
 
-[llamaindex_quickstart.py](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.4.0/trulens_eval/examples/llama_index_quickstart.py)
+Llama Index: 
 
+[llama_index_quickstart.ipynb](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.5.0/trulens_eval/examples/frameworks/llama_index/llama_index_quickstart.ipynb).
+[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/releases/rc-trulens-eval-0.5.0/trulens_eval/examples/colab/quickstarts/llama_index_quickstart_colab.ipynb)
 
+[llama_index_quickstart.py](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.5.0/trulens_eval/examples/llama_index_quickstart.py)
+
+No Framework: 
+
+[no_framework_quickstart.ipynb](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.5.0/trulens_eval/examples/no_framework_quickstart.ipynb).
+[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/releases/rc-trulens-eval-0.5.0/trulens_eval/examples/colab/quickstarts/no_framework_quickstart_colab.ipynb)
+
+[no_framework_quickstart.py](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.5.0/trulens_eval/examples/no_framework_quickstart.py)
+
+###  Contributing
+
+Interested in contributing? See our [contribution guide](https://github.com/truera/trulens/tree/main/trulens_eval/CONTRIBUTING.md) for more details.
 
 ## Installation and Setup
 
 Install the trulens-eval pip package from PyPI.
 
 ```bash
     pip install trulens-eval
@@ -170,16 +186,17 @@
 ```
 
 ## Instrument chain for logging with TruLens
 
 
 ```python
 truchain = TruChain(chain,
-    app_id='Chain3_ChatApplication',
-    feedbacks=[f_lang_match])
+    app_id='Chain1_ChatApplication',
+    feedbacks=[f_lang_match],
+    tags = "prototype")
 ```
 
 
 ```python
 # Instrumented chain can operate like the original:
 llm_response = truchain(prompt_input)
 
@@ -191,14 +208,16 @@
 
 ```python
 tru.run_dashboard() # open a local streamlit app to explore
 
 # tru.stop_dashboard() # stop if needed
 ```
 
+Alternatively, you can run `trulens-eval` from a command line in the same folder to start the dashboard.
+
 ### Chain Leaderboard
 
 Understand how your LLM application is performing at a glance. Once you've set up logging and evaluation in your application, you can view key performance statistics including cost and average feedback value across all of your LLM apps using the chain leaderboard. As you iterate new versions of your LLM application, you can compare their performance across all of the different quality metrics you've set up.
 
 Note: Average feedback values are returned and displayed in a range from 0 (worst) to 1 (best).
 
 ![Chain Leaderboard](https://www.trulens.org/Assets/image/Leaderboard.png)
```

## Comparing `trulens_eval-0.4.1b0.dist-info/RECORD` & `trulens_eval-0.5.0a0.dist-info/RECORD`

 * *Files 22% similar despite different names*

```diff
@@ -1,35 +1,42 @@
-trulens_eval/Example_TruBot.py,sha256=h09iD3TO_Bje86_0PeF1_Q_y7OOehz26NudYOTC1c0U,5164
-trulens_eval/Leaderboard.py,sha256=OnloNNlXQXp2PebTSPpduiLrpCSTi1Sa3VQew6xsc30,3264
-trulens_eval/__init__.py,sha256=QkP3gDVDSxAqY9mFKuLo-gXxE0PONLylyHrOEvcEBSU,1270
-trulens_eval/app.py,sha256=jzHiUoQ3s6pZ5xyf9oENb2m7rqrHc3UXiGM7TF88DIo,10971
+trulens_eval/Example_TruBot.py,sha256=rbomfkDmYOFMJgrFLghHh9XsjhOR-TzZaGVO9_NWdZ0,5164
+trulens_eval/Leaderboard.py,sha256=_NxvxxZ1ZdVwQ5Nz-l27Qfc2IEZWEE5zYxBz5eshrNA,3265
+trulens_eval/__init__.py,sha256=eOQbVBxFVIE-LtUrYgujUoTBrFdKG84NLQHoD3kIipc,1371
+trulens_eval/app.py,sha256=EGOj-dlgOP3vMrnYd4YvUBUe4kibFZMl2OKfzw44SBQ,11727
 trulens_eval/benchmark.py,sha256=GI-JmBBr8KswuMaXVoJ2Rm6eAtjtwhYThcNheZakQNo,5344
-trulens_eval/db.py,sha256=kJ3IN-zL5fHH6enyGDYFQR1AE81THtJxOkEjAWR6QT8,21071
-trulens_eval/db_migration.py,sha256=6yJTMIqoXJC8FHcpH4-cB_co3EpsGZgBPpECvnEEqos,13905
-trulens_eval/feedback.py,sha256=T0yVzxFw5Fntzl_Wo1J4PovYSNbUUe-n1UYIOT3-Y0A,33181
+trulens_eval/db.py,sha256=W3ScGso6ya3n6CI2zTJzx6Rt4E7b_e39zJxUDw_nCdQ,21071
+trulens_eval/db_migration.py,sha256=nDWmwAq2H0JmZ_I9TdK4xa2U2TA3IrwrHDCeE00cFvM,14061
+trulens_eval/feedback.py,sha256=dgemyCfFaBUMgxQn_03zRDTs_6kf2rklSQhhHCkVMCE,48952
 trulens_eval/feedback_prompts.py,sha256=DgW4_f_4g018tYNwca1D1taJhhdwaf2fDR9J8s0Upls,3443
-trulens_eval/instruments.py,sha256=lysx4kcwtl42Rh2_S_2oLHUp8RrtCvSuiRHls9zsJbs,20140
-trulens_eval/keys.py,sha256=CUuUNaw3Ug_dxwF9kGRkPGT_abB9LO5TveYHFHrWr3U,3118
-trulens_eval/provider_apis.py,sha256=QvXNSt75CBFcuezCnIDs22Sr4fuoaH3y5jcU4d6IDlI,21343
-trulens_eval/schema.py,sha256=QgKYFdjKCU7hDLa13mfnxwPdfz29C7_S78FLchYaLXk,12293
-trulens_eval/tru.py,sha256=yudWEBOQudio-hlLiMxHoZCdy5EC5hbxSPVPkP0r5vo,14827
-trulens_eval/tru_app.py,sha256=H8AiWlNE1u1ERwD1VOqwJOmahZGALodGtFV3zsOVEw0,293
-trulens_eval/tru_chain.py,sha256=46szx74y0yj1uJAegneeslNNVIXzLcZqHz9z4O2G2Wg,7702
-trulens_eval/tru_db.py,sha256=QPmyCbrYAmKJ6vi3SDwgSH4YX5rEGwq4jcIvBvj4Jfw,288
-trulens_eval/tru_feedback.py,sha256=qRaN_pwdGBSlqsUc4xjQDRyVAJfinA9Jn4M2KwyqNs0,318
-trulens_eval/tru_llama.py,sha256=TwDNk54GUbtrJvgyXTcN8guJB7A1mfeQ1hvA4g_tdbQ,6370
-trulens_eval/util.py,sha256=9v-vaJd7aeO4ous5PKdpsYqih4pzB1s8j-9Qmwyc6jg,43136
-trulens_eval/pages/Evaluations.py,sha256=vfMAhSZUhDPm4At0Uo4oWlNPKxR5xG8t3Qm0hyvnHzs,9966
-trulens_eval/pages/Progress.py,sha256=Q_1FLFlyIPqWleZo8hOcuzUPz7x9iT6ZNxYwwZ6ZBvo,1737
-trulens_eval/pages/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+trulens_eval/instruments.py,sha256=SwHs25qeNXOU87xCSxeaPKn9JXthaO4RYc-DrlwmjWI,20145
+trulens_eval/keys.py,sha256=PzUiX6-9jKij2kfAbl-4AHYxDIPqWhpVoIMQH6zIvvs,3147
+trulens_eval/provider_apis.py,sha256=cvgbjTk1wav6zlXDp48S59poh6mbW4Sasgk_pFY6ydM,21353
+trulens_eval/schema.py,sha256=DrbjApbW__kQ-71YFYCOypNIMFKDR6S2laZSTyBiuQc,13750
+trulens_eval/tru.py,sha256=wR6t2NzbVCJTsvRvx1f3Bt6-PZC1XFAY_9PGQ8KVMKo,16226
+trulens_eval/tru_app.py,sha256=QhY0tbhif5MfkzeMzv_uwi2Dqr_POVljg-9woBq2Ep8,293
+trulens_eval/tru_basic_app.py,sha256=01sBriY4MmZ0CrogkNa6WEVaSrHj38GtpocMiCSeFCE,3545
+trulens_eval/tru_chain.py,sha256=E-96_SbJoAmpW5MDuqNBLxol0zQ1B_NOk1FOdSyBUcs,7819
+trulens_eval/tru_db.py,sha256=_S9gtV_bizaQBaEDbayBVB6ns2j11Fi-hl_Bvy6_SXg,288
+trulens_eval/tru_feedback.py,sha256=Shc6KX33QfVZjqEF2iXg6wFmXD9ANd-l6BwhMkVR8gM,318
+trulens_eval/tru_llama.py,sha256=5O3Fx_ogk0btwdxZ_Wrkkyx7mot1fKPqRZSwZJyMZAE,6370
+trulens_eval/util.py,sha256=8h6Q-IeuHFl_H0m7oId9YVP6MqARmrbU8OWJ0GZmDR0,43449
+trulens_eval/pages/Evaluations.py,sha256=cilJnesFx0iz06XBxFu8tWkJbTufM0rG4ssqIkvOprU,12673
+trulens_eval/pages/Progress.py,sha256=Nkq_-cd2U7mo7PN6_mmnQGNaSmAD26c1E9_Q77HndmY,1775
+trulens_eval/react_components/record_viewer/__init__.py,sha256=HC-OeLHjf2ULICAnIVrWP1iO4xfgZDqJKrvzIKUyV7Y,3294
+trulens_eval/react_components/record_viewer/dist/index.html,sha256=Q1dH3M-Se_PZSy3aRyN8DnbKhbH7uvq6ReclnnStxDA,411
+trulens_eval/react_components/record_viewer/dist/assets/index-13c9a784.js,sha256=03PozYu-zkT-qFuWYWLnqolOOYJS0pgJdA4sf29BlCw,470039
+trulens_eval/react_components/record_viewer/dist/assets/index-d4dfd9ae.css,sha256=1N_ZroI2walrlJwnxw5c7RzyO8z6sPS0sWT_D1lmqkM,779
 trulens_eval/tests/test_tru_chain.py,sha256=KK_yj5wVSIaRnK-LamW3g40n_g3-Ga8CC48sTB7I7yM,5551
-trulens_eval/utils/langchain.py,sha256=3NPeUtDqHtDnEx3MRz-_hx9OFfVQjVKsh4-vioccgYI,5365
-trulens_eval/utils/llama.py,sha256=O8nranBcDrFapRlZpm46NfORZ3WyCI0QCIiiF1hicvw,4774
+trulens_eval/utils/command_line.py,sha256=K2vRIMp03IJhnevkxDnlXbn5Rux4TMXzIDgkqcePLTU,83
+trulens_eval/utils/langchain.py,sha256=y3by0IT0dYEIEqZVt3Px4deknj2YA9-jKapRd95ywMg,5892
+trulens_eval/utils/llama.py,sha256=CrqhVGNz19B29CZG2AFXoZIX9SXmxtGl_IlkGgaLll4,4842
 trulens_eval/utils/notebook_utils.py,sha256=QTB2tedjSNF5d25sfrJEg20aqcK3Kx3MfcteeWcRzxQ,1001
-trulens_eval/ux/add_logo.py,sha256=nsxAKPQ-2WXkAZvCCIWXBl_qMNpd9v-Y2tlxh8FHqLw,1188
-trulens_eval/ux/components.py,sha256=ehI8kyiRrOH7G-R8T4LCyhj23RX9pDy2zT3wmM_mSLo,4260
+trulens_eval/utils/trulens.py,sha256=9NQOctB0_qRONaKModZq3bPWOORMLvysAEjwt4BA1l8,927
+trulens_eval/ux/add_logo.py,sha256=lIpLNwqSGbfSP2Td6VEE6s0A1-9x4vFfXpJUxY7BFdQ,1212
+trulens_eval/ux/components.py,sha256=1NZw65TxlwR4OFNikp8Dnf8LTnZUoNqF2X_iTPMg9xM,5492
 trulens_eval/ux/styles.py,sha256=WYdJIsvUwPla1oZepc1FsmUJNPkT0vW0xt5sCSMs2qA,1209
 trulens_eval/ux/trulens_logo.svg,sha256=92RLTgG0YDPEtZcQWWI7aXTYZAW4wAOAkIIgKUbTiW8,29567
-trulens_eval-0.4.1b0.dist-info/METADATA,sha256=kIjjZeR84yY16VY2ZeVystg4b2MTmdemJvtp_cdh57k,14914
-trulens_eval-0.4.1b0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-trulens_eval-0.4.1b0.dist-info/top_level.txt,sha256=AKIBExe5S-v-TbsBrhe1ctF06PubKoYmWNS9rJ1Rb_o,13
-trulens_eval-0.4.1b0.dist-info/RECORD,,
+trulens_eval-0.5.0a0.dist-info/METADATA,sha256=1n6kr0MT9k2R2hhhiJRb2ujvAC2IaLp0E2BCMykmJV0,16347
+trulens_eval-0.5.0a0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+trulens_eval-0.5.0a0.dist-info/entry_points.txt,sha256=EpSmkbk1fF0UH-djUia4lE2hzg1oMt1QvaVxA7SfZmo,70
+trulens_eval-0.5.0a0.dist-info/top_level.txt,sha256=AKIBExe5S-v-TbsBrhe1ctF06PubKoYmWNS9rJ1Rb_o,13
+trulens_eval-0.5.0a0.dist-info/RECORD,,
```

