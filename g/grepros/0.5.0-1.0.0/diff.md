# Comparing `tmp/grepros-0.5.0-py2.py3-none-any.whl.zip` & `tmp/grepros-1.0.0-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,33 +1,34 @@
-Zip file size: 134988 bytes, number of entries: 31
--rw-rw-rw-  2.0 fat      476 b- defN 22-Oct-30 12:49 grepros/__init__.py
--rw-rw-rw-  2.0 fat      452 b- defN 21-Dec-12 17:20 grepros/__main__.py
--rw-rw-rw-  2.0 fat    33237 b- defN 22-Oct-30 12:49 grepros/common.py
--rw-rw-rw-  2.0 fat    38736 b- defN 22-Oct-30 12:49 grepros/inputs.py
--rw-rw-rw-  2.0 fat    23668 b- defN 22-Oct-30 12:49 grepros/main.py
--rw-rw-rw-  2.0 fat    26754 b- defN 22-Oct-30 12:49 grepros/outputs.py
--rw-rw-rw-  2.0 fat    20128 b- defN 22-Oct-30 12:49 grepros/ros1.py
--rw-rw-rw-  2.0 fat    31524 b- defN 22-Oct-30 12:49 grepros/ros2.py
--rw-rw-rw-  2.0 fat    26654 b- defN 22-Oct-30 12:49 grepros/rosapi.py
--rw-rw-rw-  2.0 fat    14793 b- defN 22-Oct-30 12:49 grepros/search.py
--rw-rw-rw-  2.0 fat     8546 b- defN 22-May-26 17:30 grepros/plugins/__init__.py
--rw-rw-rw-  2.0 fat     8138 b- defN 22-Oct-30 12:49 grepros/plugins/embag.py
--rw-rw-rw-  2.0 fat    18317 b- defN 22-Oct-30 12:49 grepros/plugins/mcap.py
--rw-rw-rw-  2.0 fat    14962 b- defN 22-Oct-30 12:49 grepros/plugins/parquet.py
--rw-rw-rw-  2.0 fat    10524 b- defN 22-Feb-09 20:24 grepros/plugins/sql.py
--rw-rw-rw-  2.0 fat      426 b- defN 21-Dec-28 09:10 grepros/plugins/auto/__init__.py
--rw-rw-rw-  2.0 fat     9788 b- defN 22-May-26 17:30 grepros/plugins/auto/csv.py
--rw-rw-rw-  2.0 fat    15784 b- defN 22-Feb-06 20:59 grepros/plugins/auto/dbbase.py
--rw-rw-rw-  2.0 fat     8059 b- defN 22-Mar-17 18:19 grepros/plugins/auto/html.py
--rw-rw-rw-  2.0 fat    42323 b- defN 22-May-26 17:30 grepros/plugins/auto/html.tpl
--rw-rw-rw-  2.0 fat     9442 b- defN 22-Feb-09 20:24 grepros/plugins/auto/postgres.py
--rw-rw-rw-  2.0 fat    32302 b- defN 22-Oct-30 12:49 grepros/plugins/auto/sqlbase.py
--rw-rw-rw-  2.0 fat     8754 b- defN 22-Feb-09 20:24 grepros/plugins/auto/sqlite.py
+Zip file size: 166385 bytes, number of entries: 32
+-rw-rw-rw-  2.0 fat      533 b- defN 23-Jul-13 17:46 grepros/__init__.py
+-rw-rw-rw-  2.0 fat      452 b- defN 23-Jun-25 19:19 grepros/__main__.py
+-rw-rw-rw-  2.0 fat    46566 b- defN 23-Jul-13 17:45 grepros/api.py
+-rw-rw-rw-  2.0 fat    42514 b- defN 23-Jul-13 17:45 grepros/common.py
+-rw-rw-rw-  2.0 fat    53136 b- defN 23-Jul-13 17:45 grepros/inputs.py
+-rw-rw-rw-  2.0 fat    18891 b- defN 23-Jul-13 17:45 grepros/library.py
+-rw-rw-rw-  2.0 fat    25233 b- defN 23-Jul-13 17:45 grepros/main.py
+-rw-rw-rw-  2.0 fat    36173 b- defN 23-Jul-13 17:45 grepros/outputs.py
+-rw-rw-rw-  2.0 fat    30697 b- defN 23-Jul-13 17:45 grepros/ros1.py
+-rw-rw-rw-  2.0 fat    43615 b- defN 23-Jul-13 17:45 grepros/ros2.py
+-rw-rw-rw-  2.0 fat    21881 b- defN 23-Jul-13 17:45 grepros/search.py
+-rw-rw-rw-  2.0 fat    11620 b- defN 23-Jul-13 17:45 grepros/plugins/__init__.py
+-rw-rw-rw-  2.0 fat    12525 b- defN 23-Jul-13 17:45 grepros/plugins/embag.py
+-rw-rw-rw-  2.0 fat    31324 b- defN 23-Jul-13 17:45 grepros/plugins/mcap.py
+-rw-rw-rw-  2.0 fat    25014 b- defN 23-Jul-13 17:45 grepros/plugins/parquet.py
+-rw-rw-rw-  2.0 fat    12063 b- defN 23-Jul-13 17:45 grepros/plugins/sql.py
+-rw-rw-rw-  2.0 fat      426 b- defN 23-Jul-05 13:08 grepros/plugins/auto/__init__.py
+-rw-rw-rw-  2.0 fat    11900 b- defN 23-Jul-13 17:45 grepros/plugins/auto/csv.py
+-rw-rw-rw-  2.0 fat    16591 b- defN 23-Jul-13 17:45 grepros/plugins/auto/dbbase.py
+-rw-rw-rw-  2.0 fat    11073 b- defN 23-Jul-13 17:45 grepros/plugins/auto/html.py
+-rw-rw-rw-  2.0 fat    42482 b- defN 23-Jul-13 17:45 grepros/plugins/auto/html.tpl
+-rw-rw-rw-  2.0 fat    10151 b- defN 23-Jul-13 17:45 grepros/plugins/auto/postgres.py
+-rw-rw-rw-  2.0 fat    32550 b- defN 23-Jul-13 17:45 grepros/plugins/auto/sqlbase.py
+-rw-rw-rw-  2.0 fat     9694 b- defN 23-Jul-13 17:45 grepros/plugins/auto/sqlite.py
 -rw-rw-rw-  2.0 fat        0 b- defN 21-Dec-12 17:18 grepros/vendor/__init__.py
 -rw-rw-rw-  2.0 fat     7903 b- defN 21-Dec-12 17:18 grepros/vendor/step.py
--rw-rw-rw-  2.0 fat     1523 b- defN 22-Oct-30 12:54 grepros-0.5.0.dist-info/LICENSE.md
--rw-rw-rw-  2.0 fat    47262 b- defN 22-Oct-30 12:54 grepros-0.5.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat      116 b- defN 22-Oct-30 12:54 grepros-0.5.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       46 b- defN 22-Oct-30 12:54 grepros-0.5.0.dist-info/entry_points.txt
--rw-rw-rw-  2.0 fat        8 b- defN 22-Oct-30 12:54 grepros-0.5.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     2496 b- defN 22-Oct-30 12:54 grepros-0.5.0.dist-info/RECORD
-31 files, 463141 bytes uncompressed, 131050 bytes compressed:  71.7%
+-rw-rw-rw-  2.0 fat     1523 b- defN 23-Jul-13 17:47 grepros-1.0.0.dist-info/LICENSE.md
+-rw-rw-rw-  2.0 fat    38013 b- defN 23-Jul-13 17:47 grepros-1.0.0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      110 b- defN 23-Jul-13 17:47 grepros-1.0.0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       45 b- defN 23-Jul-13 17:47 grepros-1.0.0.dist-info/entry_points.txt
+-rw-rw-rw-  2.0 fat        8 b- defN 23-Jul-13 17:47 grepros-1.0.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     2574 b- defN 23-Jul-13 17:47 grepros-1.0.0.dist-info/RECORD
+32 files, 597280 bytes uncompressed, 162341 bytes compressed:  72.8%
```

## zipnote {}

```diff
@@ -1,34 +1,37 @@
 Filename: grepros/__init__.py
 Comment: 
 
 Filename: grepros/__main__.py
 Comment: 
 
+Filename: grepros/api.py
+Comment: 
+
 Filename: grepros/common.py
 Comment: 
 
 Filename: grepros/inputs.py
 Comment: 
 
+Filename: grepros/library.py
+Comment: 
+
 Filename: grepros/main.py
 Comment: 
 
 Filename: grepros/outputs.py
 Comment: 
 
 Filename: grepros/ros1.py
 Comment: 
 
 Filename: grepros/ros2.py
 Comment: 
 
-Filename: grepros/rosapi.py
-Comment: 
-
 Filename: grepros/search.py
 Comment: 
 
 Filename: grepros/plugins/__init__.py
 Comment: 
 
 Filename: grepros/plugins/embag.py
@@ -69,26 +72,26 @@
 
 Filename: grepros/vendor/__init__.py
 Comment: 
 
 Filename: grepros/vendor/step.py
 Comment: 
 
-Filename: grepros-0.5.0.dist-info/LICENSE.md
+Filename: grepros-1.0.0.dist-info/LICENSE.md
 Comment: 
 
-Filename: grepros-0.5.0.dist-info/METADATA
+Filename: grepros-1.0.0.dist-info/METADATA
 Comment: 
 
-Filename: grepros-0.5.0.dist-info/WHEEL
+Filename: grepros-1.0.0.dist-info/WHEEL
 Comment: 
 
-Filename: grepros-0.5.0.dist-info/entry_points.txt
+Filename: grepros-1.0.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: grepros-0.5.0.dist-info/top_level.txt
+Filename: grepros-1.0.0.dist-info/top_level.txt
 Comment: 
 
-Filename: grepros-0.5.0.dist-info/RECORD
+Filename: grepros-1.0.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## grepros/__init__.py

```diff
@@ -2,14 +2,17 @@
 """
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     31.10.2021
-@modified    30.10.2022
+@modified    13.07.2023
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros
 __title__        = "grepros"
-__version__      = "0.5.0"
-__version_info__ = (0, 5, 0)
+__version__      = "1.0.0"
+__version_info__ = (1, 0, 0)
+__version_date__ = "13.07.2023"
+
+from . library import *
```

## grepros/common.py

```diff
@@ -1,93 +1,153 @@
 # -*- coding: utf-8 -*-
-## @namespace grepros.common
 """
 Common utilities.
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS1 bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     23.10.2021
-@modified    20.10.2022
+@modified    02.07.2023
 ------------------------------------------------------------------------------
 """
+## @namespace grepros.common
 from __future__ import print_function
+import argparse
+import copy
 import datetime
 import functools
 import glob
 import importlib
+import inspect
+import io
 import itertools
+import logging
 import math
 import os
-import random
 import re
 import shutil
 import sys
 import threading
 import time
 try: import curses
 except ImportError: curses = None
 
+import six
 try: import zstandard
 except ImportError: zstandard = None
 
 
+## Python types for filesystem paths
+PATH_TYPES = (six.binary_type, six.text_type)
+if six.PY34: PATH_TYPES += (importlib.import_module("pathlib").Path, )
+## Python types for both byte strings and text strings
+STRING_TYPES = (six.binary_type, six.text_type)
+## Python types for text strings
+TEXT_TYPES = (six.binary_type, six.text_type) if six.PY2 else (six.text_type, )
+
+
 class MatchMarkers(object):
     """Highlight markers for matches in message values."""
 
-    ID    = "%08x" % random.randint(1, 1E9)  ## Unique marker for match highlight replacements
-    START = "<%s>"  % ID                     ## Temporary placeholder in front of match
-    END   = "</%s>" % ID                     ## Temporary placeholder at end of match
-    EMPTY = START + END                      ## Temporary placeholder for empty string match
-    EMPTY_REPL = "%s''%s" % (START, END)     ## Replacement for empty string match
+    ## Unique marker for match highlight replacements
+    ID    = "matching"
+    ## Placeholder in front of match
+    START = "<%s>"  % ID
+    ## Placeholder at end of match
+    END   = "</%s>" % ID
+    ## Placeholder for empty string match
+    EMPTY = START + END
+    ## Replacement for empty string match
+    EMPTY_REPL = "%s''%s" % (START, END)
+
+    @classmethod
+    def populate(cls, value):
+        """Populates highlight markers with specified value."""
+        cls.ID    = str(value)
+        cls.START = "<%s>"  % cls.ID
+        cls.END   = "</%s>" % cls.ID
+        cls.EMPTY = cls.START + cls.END
+        cls.EMPTY_REPL = "%s''%s" % (cls.START, cls.END)
+
 
 
 class ConsolePrinter(object):
-    """Prints to console, supports color output."""
+    """
+    Prints to console, supports color output.
 
-    STYLE_RESET     = "\x1b(B\x1b[m"            ## Default color+weight
-    STYLE_HIGHLIGHT = "\x1b[31m"                ## Red
-    STYLE_LOWLIGHT  = "\x1b[38;2;105;105;105m"  ## Dim gray
-    STYLE_SPECIAL   = "\x1b[35m"                ## Purple
-    STYLE_SPECIAL2  = "\x1b[36m"                ## Cyan
-    STYLE_WARN      = "\x1b[33m"                ## Yellow
-    STYLE_ERROR     = "\x1b[31m\x1b[2m"         ## Dim red
+    If configured with `apimode=True`, logs debugs and warnings to logger and raises errors.
+    """
 
-    DEBUG_START, DEBUG_END = STYLE_LOWLIGHT, STYLE_RESET  ## Metainfo wrappers
-    WARN_START,  WARN_END =  STYLE_WARN,     STYLE_RESET  ## Warning message wrappers
-    ERROR_START, ERROR_END = STYLE_ERROR,    STYLE_RESET  ## Error message wrappers
+    STYLE_RESET     = "\x1b(B\x1b[m"            # Default color+weight
+    STYLE_HIGHLIGHT = "\x1b[31m"                # Red
+    STYLE_LOWLIGHT  = "\x1b[38;2;105;105;105m"  # Dim gray
+    STYLE_SPECIAL   = "\x1b[35m"                # Purple
+    STYLE_SPECIAL2  = "\x1b[36m"                # Cyan
+    STYLE_WARN      = "\x1b[33m"                # Yellow
+    STYLE_ERROR     = "\x1b[31m\x1b[2m"         # Dim red
+
+    DEBUG_START, DEBUG_END = STYLE_LOWLIGHT, STYLE_RESET  # Metainfo wrappers
+    WARN_START,  WARN_END =  STYLE_WARN,     STYLE_RESET  # Warning message wrappers
+    ERROR_START, ERROR_END = STYLE_ERROR,    STYLE_RESET  # Error message wrappers
+
+    ## Whether using colors in output
+    COLOR = None
+
+    ## Console width in characters, updated from shutil and curses
+    WIDTH = 80
 
-    COLOR = None       ## Whether using colors in output
+    ## {sys.stdout: number of texts printed, sys.stderr: ..}
+    PRINTS = {}
 
-    WIDTH = 80         ## Console width in characters, updated from shutil and curses
+    ## Whether logging debugs and warnings and raising errors, instead of printing
+    APIMODE = False
 
-    PRINTS = {}        ## {sys.stdout: number of texts printed, sys.stderr: ..}
+    _COLORFLAG = None  ## Color flag set in configure()
 
     _LINEOPEN = False  ## Whether last print was without linefeed
 
     _UNIQUES = set()   ## Unique texts printed with `__once=True`
 
     @classmethod
-    def configure(cls, color):
+    def configure(cls, color=True, apimode=False):
         """
-        Initializes terminal for color output, or disables color output if unsupported.
+        Initializes printer, for terminal output or library mode.
 
-        @param   color  True / False / None for auto-detect from TTY support
+        For terminal output, initializes terminal colors, or disables colors if unsupported.
+
+        @param   color    True / False / None for auto-detect from TTY support;
+                          will be disabled if terminal does not support colors
+        @param   apimode  whether to log debugs and warnings to logger and raise errors,
+                          instead of printing
         """
+        cls.APIMODE    = bool(apimode)
+        cls._COLORFLAG = color
+        if apimode:
+            cls.DEBUG_START, cls.DEBUG_END = "", ""
+            cls.WARN_START,  cls.WARN_END  = "", ""
+            cls.ERROR_START, cls.ERROR_END = "", ""
+        else: cls.init_terminal()
+
+
+    @classmethod
+    def init_terminal(cls):
+        """Initializes terminal for color output, or disables color output if unsupported."""
+        if cls.COLOR is not None: return
+
         try: cls.WIDTH = shutil.get_terminal_size().columns  # Py3
         except Exception: pass  # Py2
-        cls.COLOR = (color is not False)
+        cls.COLOR = (cls._COLORFLAG is not False)
         try:
             curses.setupterm()
             if cls.COLOR and not sys.stdout.isatty():
                 raise Exception()
         except Exception:
-            cls.COLOR = bool(color)
+            cls.COLOR = bool(cls._COLORFLAG)
         try:
             if sys.stdout.isatty() or cls.COLOR:
                 cls.WIDTH = curses.initscr().getmaxyx()[1]
             curses.endwin()
         except Exception: pass
 
         if cls.COLOR:
@@ -106,70 +166,120 @@
         Prints text, formatted with args and kwargs.
 
         @param   __file   file object to print to if not sys.stdout
         @param   __end    line end to use if not linefeed "\n"
         @param   __once   whether text should be printed only once
                           and discarded on any further calls (applies to unformatted text)
         """
-        text, fmted = str(text), False
+        text = str(text)
         if kwargs.pop("__once", False):
             if text in cls._UNIQUES: return
             cls._UNIQUES.add(text)
         fileobj, end = kwargs.pop("__file", sys.stdout), kwargs.pop("__end", "\n")
         pref, suff = kwargs.pop("__prefix", ""), kwargs.pop("__suffix", "")
-        try: text, fmted = (text % args if args else text), bool(args)
-        except Exception: pass
-        try: text, fmted = (text % kwargs if kwargs else text), fmted or bool(kwargs)
-        except Exception: pass
-        try: text = text.format(*args, **kwargs) if not fmted and (args or kwargs) else text
-        except Exception: pass
         if cls._LINEOPEN and "\n" in end: pref = "\n" + pref  # Add linefeed to end open line
+        text = cls._format(text, *args, **kwargs)
 
         cls.PRINTS[fileobj] = cls.PRINTS.get(fileobj, 0) + 1
         cls._LINEOPEN = "\n" not in end
+        cls.init_terminal()
         print(pref + text + suff, end=end, file=fileobj)
         not fileobj.isatty() and fileobj.flush()
 
 
     @classmethod
     def error(cls, text="", *args, **kwargs):
-        """Prints error to stderr, formatted with args and kwargs, in error colors if supported."""
+        """
+        Prints error to stderr, formatted with args and kwargs, in error colors if supported.
+
+        Raises exception instead if APIMODE.
+        """
+        if cls.APIMODE:
+            raise Exception(cls._format(text, *args, __once=False, **kwargs))
         KWS = dict(__file=sys.stderr, __prefix=cls.ERROR_START, __suffix=cls.ERROR_END)
         cls.print(text, *args, **dict(kwargs, **KWS))
 
 
     @classmethod
     def warn(cls, text="", *args, **kwargs):
         """
-        Prints warning to stderr.
+        Prints warning to stderr, or logs to logger if APIMODE.
 
-        Formatted with args and kwargs, in warning colors if supported.
+        Text is formatted with args and kwargs, in warning colors if supported.
         """
+        if cls.APIMODE:
+            text = cls._format(text, *args, **kwargs)
+            if text: logging.getLogger(__name__).warning(text)
+            return
         KWS = dict(__file=sys.stderr, __prefix=cls.WARN_START, __suffix=cls.WARN_END)
         cls.print(text, *args, **dict(kwargs, **KWS))
 
 
     @classmethod
     def debug(cls, text="", *args, **kwargs):
         """
-        Prints debug text to stderr.
+        Prints debug text to stderr, or logs to logger if APIMODE.
 
-        Formatted with args and kwargs, in lowlight colors if supported.
+        Text is formatted with args and kwargs, in warning colors if supported.
         """
+        if cls.APIMODE:
+            text = cls._format(text, *args, **kwargs)
+            if text: logging.getLogger(__name__).debug(text)
+            return
         KWS = dict(__file=sys.stderr, __prefix=cls.DEBUG_START, __suffix=cls.DEBUG_END)
         cls.print(text, *args, **dict(kwargs, **KWS))
 
 
     @classmethod
+    def log(cls, level, text="", *args, **kwargs):
+        """
+        Prints text to stderr, or logs to logger if APIMODE.
+
+        Text is formatted with args and kwargs, in level colors if supported.
+
+        @param   level  logging level like `logging.ERROR` or "ERROR"
+        """
+        if cls.APIMODE:
+            text = cls._format(text, *args, **kwargs)
+            if text: logging.getLogger(__name__).log(level, text)
+            return
+        level = logging.getLevelName(level)
+        if not isinstance(level, TEXT_TYPES): level = logging.getLevelName(level)
+        func = {"DEBUG": cls.debug, "WARNING": cls.warn, "ERROR": cls.error}.get(level, cls.print)
+        func(text, *args, **dict(kwargs, __file=sys.stderr))
+
+
+    @classmethod
     def flush(cls):
         """Ends current open line, if any."""
         if cls._LINEOPEN: print()
         cls._LINEOPEN = False
 
 
+    @classmethod
+    def _format(cls, text="", *args, **kwargs):
+        """
+        Returns text formatted with printf-style or format() arguments.
+
+        @param  __once  registers text, returns "" if text not unique
+        """
+        text, fmted = str(text), False
+        if kwargs.get("__once"):
+            if text in cls._UNIQUES: return ""
+            cls._UNIQUES.add(text)
+        for k in ("__file", "__end", "__once", "__prefix", "__suffix"): kwargs.pop(k, None)
+        try: text, fmted = (text % args if args else text), bool(args)
+        except Exception: pass
+        try: text, fmted = (text % kwargs if kwargs else text), fmted or bool(kwargs)
+        except Exception: pass
+        try: text = text.format(*args, **kwargs) if not fmted and (args or kwargs) else text
+        except Exception: pass
+        return text
+
+
 class Decompressor(object):
     """Decompresses zstandard archives."""
 
     ## Supported archive extensions
     EXTENSIONS = (".zst", ".zstd")
 
     ## zstd file header magic start bytes
@@ -211,15 +321,15 @@
 
 
     @classmethod
     def is_compressed(cls, path):
         """Returns whether file is a recognized archive."""
         result = os.path.isfile(path)
         if result:
-            result = any(path.lower().endswith(x) for x in cls.EXTENSIONS)
+            result = any(str(path).lower().endswith(x) for x in cls.EXTENSIONS)
         if result:
             with open(path, "rb") as f:
                 result = (f.read(len(cls.ZSTD_MAGIC)) == cls.ZSTD_MAGIC)
         return result
 
 
     @classmethod
@@ -257,15 +367,14 @@
         @param   width          progress bar width (in characters)
         @param   forechar       character used for filling the progress bar
         @param   backchar       character used for filling the background
         @param   foreword       text in front of progress bar
         @param   afterword      text after progress bar
         @param   interval       ticker thread interval, in seconds
         @param   pulse          ignore value-min-max, use constant pulse instead
-        @param   counts         print value and nax afterword
         @param   aftertemplate  afterword format() template, populated with vars(self)
         """
         threading.Thread.__init__(self)
         for k, v in locals().items(): setattr(self, k, v) if "self" != k else 0
         afterword = aftertemplate.format(**vars(self))
         self.daemon    = True   # Daemon threads do not keep application running
         self.percent   = None   # Current progress ratio in per cent
@@ -347,14 +456,33 @@
 
 
     def stop(self):
         self.is_running = False
 
 
 
+class LenIterable(object):
+    """Wrapper for iterable value with specified fixed length."""
+
+    def __init__(self, iterable, count):
+        """
+        @param   iterable  any iterable value
+        @param   count     value to return for len(self), or callable to return value from
+        """
+        self._iterer = iter(iterable)
+        self._count  = count
+
+    def __iter__(self): return self
+
+    def __next__(self): return next(self._iterer)
+
+    def __len__(self):  return self._count() if callable(self._count) else self._count
+
+
+
 class TextWrapper(object):
     """
     TextWrapper that supports custom substring widths in line width calculation.
 
     Intended for wrapping text containing ANSI control codes.
     Heavily refactored from Python standard library textwrap.TextWrapper.
     """
@@ -414,15 +542,15 @@
                 result[-1] += self.placeholder
         if len(self.lencache)  > self.LENCACHEMAX:  self.lencache.clear()
         return result
 
 
     def reserve_width(self, reserved=""):
         """Decreases the configured width by given amount (number or string)."""
-        reserved = self.strlen(reserved) if isinstance(reserved, str) else reserved
+        reserved = self.strlen(reserved) if isinstance(reserved, TEXT_TYPES) else reserved
         self.width = max(self.minwidth, self.realwidth - reserved)
 
 
     def strlen(self, v):
         """Returns length of string, using custom substring widths."""
         if v not in self.lencache:
             self.lencache[v] = len(v) - sum(v.count(s) * ld for s, ld in self.custom_lens)
@@ -523,14 +651,39 @@
 def ellipsize(text, limit, ellipsis=".."):
     """Returns text ellipsized if beyond limit."""
     if limit <= 0 or len(text) < limit:
         return text
     return text[:max(0, limit - len(ellipsis))] + ellipsis
 
 
+def ensure_namespace(val, defaults=None, **kwargs):
+    """
+    Returns a copy of value as `argparse.Namespace`, with all keys uppercase.
+
+    Arguments with list/tuple values in defaults are ensured to have list/tuple values.
+
+    @param  val       `argparse.Namespace` or dictionary or `None`
+    @param  defaults  additional arguments to set to namespace if missing
+    @param  kwargs    any and all argument overrides as keyword overrides
+    """
+    if val is None or isinstance(val, dict): val = argparse.Namespace(**val or {})
+    else: val = structcopy(val)
+    for k, v in vars(val).items():
+        if not k.isupper():
+            delattr(val, k)
+            setattr(val, k.upper(), v)
+    for k, v in ((k.upper(), v) for k, v in (defaults.items() if defaults else ())):
+        if not hasattr(val, k): setattr(val, k, structcopy(v))
+    for k, v in ((k.upper(), v) for k, v in kwargs.items()): setattr(val, k, v)
+    for k, v in ((k.upper(), v) for k, v in (defaults.items() if defaults else ())):
+        if isinstance(v, (tuple, list)) and not isinstance(getattr(val, k), (tuple, list)):
+            setattr(val, k, [getattr(val, k)])
+    return val
+
+
 def filter_dict(dct, keys=(), values=(), reverse=False):
     """
     Filters string dictionary by keys and values. Dictionary values may be
     additional lists; keys with emptied lists are dropped.
 
     Retains only entries that find a match (supports * wildcards);
     if reverse, retains only entries that do not find a match.
@@ -549,65 +702,41 @@
         for v in (vv if is_array else [vv]):
             if  (k not in keys   and not any(p.match(k) for p in kpatterns)) \
             and (v not in values and not any(p.match(v) for p in vpatterns)):
                 result.setdefault(k, []).append(v) if is_array else result.update({k: v})
     return result
 
 
-def filter_fields(fieldmap, top=(), include=(), exclude=()):
-    """
-    Returns fieldmap filtered by include and exclude patterns.
-
-    @param   fieldmap  {field name: field type name}
-    @param   top       parent path as (rootattr, ..)
-    @param   include   [((nested, path), re.Pattern())] to require in parent path
-    @param   exclude   [((nested, path), re.Pattern())] to reject in parent path
-    """
-    result = type(fieldmap)() if include or exclude else fieldmap
-    for k, v in fieldmap.items() if not result else ():
-        trail, trailstr = top + (k, ), ".".join(top + (k, ))
-        for is_exclude, patterns in enumerate((include, exclude)):
-            matches = any(p[:len(trail)] == trail[:len(p)] or r.match(trailstr)
-                          for p, r in patterns)  # Match by beginning or wildcard pattern
-            if patterns and (not matches if is_exclude else matches):
-                result[k] = v
-            elif patterns and is_exclude and matches:
-                result.pop(k, None)
-            if include and exclude and k not in result:  # Failing to include takes precedence
-                break  # for is_exclude
-    return result
-
-
 def find_files(names=(), paths=(), extensions=(), skip_extensions=(), recurse=False):
     """
-    Yields filenames from current directory or given paths, .
+    Yields filenames from current directory or given paths.
 
     Seeks only files with given extensions if names not given.
-    Prints errors for names and paths not found.
+    Logs errors for names and paths not found.
 
     @param   names            list of specific files to return (supports * wildcards)
     @param   paths            list of paths to look under, if not using current directory
     @param   extensions       list of extensions to select if not using names, as (".ext1", ..)
     @param   skip_extensions  list of extensions to skip if not using names, as (".ext1", ..)
     @param   recurse          whether to recurse into subdirectories
     """
     namesfound, pathsfound = set(), set()
     def iter_files(directory):
         """Yields matching filenames from path."""
         if os.path.isfile(directory):
-            ConsolePrinter.error("%s: Is a file", directory)
+            ConsolePrinter.log(logging.ERROR, "%s: Is a file", directory)
             return
         for path in sorted(glob.glob(directory)):  # Expand * wildcards, if any
             pathsfound.add(directory)
             for n in names:
                 p = n if not paths or os.path.isabs(n) else os.path.join(path, n)
                 for f in (f for f in glob.glob(p) if "*" not in n
                           or not any(map(f.endswith, skip_extensions))):
                     if os.path.isdir(f):
-                        ConsolePrinter.error("%s: Is a directory", f)
+                        ConsolePrinter.log(logging.ERROR, "%s: Is a directory", f)
                         continue  # for n
                     namesfound.add(n)
                     yield f
             for root, _, files in os.walk(path) if not names else ():
                 for f in (os.path.join(root, f) for f in sorted(files)
                           if (not extensions or any(map(f.endswith, extensions)))
                           and not any(map(f.endswith, skip_extensions))):
@@ -620,17 +749,17 @@
         if os.path.abspath(f) not in processed:
             processed.add(os.path.abspath(f))
             if not paths and f == os.path.join(".", os.path.basename(f)):
                 f = os.path.basename(f)  # Strip leading "./"
             yield f
 
     for path in (p for p in paths if p not in pathsfound):
-        ConsolePrinter.error("%s: No such directory", path)
+        ConsolePrinter.log(logging.ERROR, "%s: No such directory", path)
     for name in (n for n in names if n not in namesfound):
-        ConsolePrinter.error("%s: No such file", name)
+        ConsolePrinter.log(logging.ERROR, "%s: No such file", name)
 
 
 def format_timedelta(delta):
     """Formats the datetime.timedelta as "3d 40h 23min 23.1sec"."""
     dd, rem = divmod(delta.total_seconds(), 24*3600)
     hh, rem = divmod(rem, 3600)
     mm, ss  = divmod(rem, 60)
@@ -654,14 +783,41 @@
 
 
 def format_stamp(stamp):
     """Returns ISO datetime from UNIX timestamp."""
     return datetime.datetime.fromtimestamp(stamp).isoformat(sep=" ")
 
 
+def get_name(obj):
+    """
+    Returns the fully namespaced name for a Python module, class, function or object.
+
+    E.g. "my.thing" or "my.module.MyCls" or "my.module.MyCls.my_method"
+    or "my.module.MyCls<0x1234abcd>" or "my.module.MyCls<0x1234abcd>.my_method".
+    """
+    namer = lambda x: getattr(x, "__qualname__", getattr(x, "__name__", ""))
+    if inspect.ismodule(obj): return namer(obj)
+    if inspect.isclass(obj):  return ".".join((obj.__module__, namer(obj)))
+    if inspect.isroutine(obj):
+        parts, self = [], six.get_method_self(obj)
+        if self is not None:           parts.extend((get_name(self), obj.__name__))
+        elif hasattr(obj, "im_class"): parts.extend((get_name(obj.im_class), namer(obj)))  # Py2
+        else:                          parts.extend((obj.__module__, namer(obj)))          # Py3
+        return ".".join(parts)
+    cls = type(obj)
+    return "%s.%s<0x%x>" % (cls.__module__, namer(cls), id(obj))
+
+
+def has_arg(func, name):
+    """Returns whether function supports taking specified argument by name."""
+    spec = getattr(inspect, "getfullargspec", inspect.getargspec)(func)  # Py3/Py2
+    return name in spec.args or name in getattr(spec, "kwonlyargs", ()) or \
+           getattr(spec, "varkw", None) or getattr(spec, "keywords", None)
+
+
 def import_item(name):
     """
     Returns imported module, or identifier from imported namespace; raises on error.
 
     @param   name  Python module name like "my.module"
                    or module namespace identifier like "my.module.Class"
     """
@@ -674,29 +830,59 @@
             try: result, success = getattr(result, item), True
             except AttributeError: pass
         if not success:
             raise ImportError("No module or identifier named %r" % path)
     return result
 
 
+def is_iterable(value):
+    """Returns whether value is iterable."""
+    try: iter(value)
+    except Exception: return False
+    return True
+
+
+def is_stream(value):
+    """Returns whether value is a file-like object."""
+    try: return isinstance(value, (file, io.IOBase))       # Py2
+    except NameError: return isinstance(value, io.IOBase)  # Py3
+
+
 def makedirs(path):
     """Creates directory structure for path if not already existing."""
-    parts, accum = list(filter(bool, path.split(os.sep))), []
+    parts, accum = list(filter(bool, os.path.realpath(path).split(os.sep))), []
     while parts:
         accum.append(parts.pop(0))
-        curpath = (os.sep if path.startswith(os.sep) else "") + os.path.join(*accum)
+        curpath = os.path.join(os.sep, accum[0] + os.sep, *accum[1:])  # Windows drive letter thing
         if not os.path.exists(curpath):
             os.mkdir(curpath)
 
 
+def structcopy(value):
+    """
+    Returns a deep copy of a standard data structure (dict, list, set, tuple),
+    other object types reused instead of copied.
+    """
+    COLLECTIONS = (dict, list, set, tuple)
+    memo = {}
+    def collect(x):  # Walk structure and collect objects to skip copying
+        if isinstance(x, argparse.Namespace): x = vars(x)
+        if not isinstance(x, COLLECTIONS): return memo.update([(id(x), x)])
+        for y in sum(map(list, x.items()), []) if isinstance(x, dict) else x: collect(y)
+    collect(value)
+    return copy.deepcopy(value, memo)
+
+
 def memoize(func):
-    """Returns a results-caching wrapper for the function."""
+    """Returns a results-caching wrapper for the function, cache used if arguments hashable."""
     cache = {}
     def inner(*args, **kwargs):
         key = args + sum(kwargs.items(), ())
+        try: hash(key)
+        except Exception: return func(*args, **kwargs)
         if key not in cache:
             cache[key] = func(*args, **kwargs)
         return cache[key]
     return functools.update_wrapper(inner, func)
 
 
 def merge_dicts(d1, d2):
@@ -770,19 +956,19 @@
 
     If a file or directory with the same name already exists, returns a unique
     version (e.g. "/tmp/my.2.file" if ""/tmp/my.file" already exists).
 
     @param   empty_ok  whether to ignore existence if file is empty
     """
     result = pathname
-    if "linux2" == sys.platform and sys.version_info < (3, 0) \
-    and isinstance(result, unicode) and "utf-8" != sys.getfilesystemencoding():
+    if "linux2" == sys.platform and six.PY2 and isinstance(result, six.text_type) \
+    and "utf-8" != sys.getfilesystemencoding():
         result = result.encode("utf-8") # Linux has trouble if locale not UTF-8
     if os.path.isfile(result) and empty_ok and not os.path.getsize(result):
-        return result
+        return result if isinstance(result, STRING_TYPES) else str(result)
     path, name = os.path.split(result)
     base, ext = os.path.splitext(name)
     if len(name) > 255: # Filesystem limitation
         name = base[:255 - len(ext) - 2] + ".." + ext
         result = os.path.join(path, name)
     counter = 2
     while os.path.exists(result):
@@ -791,15 +977,86 @@
         if len(name) > 255:
             name = base[:255 - len(suffix) - 2] + ".." + suffix
         result = os.path.join(path, name)
         counter += 1
     return result
 
 
+def verify_io(f, mode):
+    """
+    Returns whether stream or file path can be read from and/or written to as binary.
+
+    Prints or raises error if not.
+
+    Tries to open file in append mode if verifying path writability,
+    auto-creating missing directories if any, will delete any file or directory created.
+
+    @param   f     file path, or stream
+    @param   mode  "r" for readable, "w" for writable, "a" for readable and writable
+    """
+    result, op = True, ""
+    if is_stream(f):
+        try:
+            pos = f.tell()
+            if mode in ("r", "a"):
+                op = " reading from"
+                result = isinstance(f.read(1), bytes)
+            if result and mode in ("w", "a"):
+                op = " writing to"
+                result, _ = True, f.write(b"")
+            f.seek(pos)
+            return result
+        except Exception as e:
+            ConsolePrinter.log(logging.ERROR, "Error%s %s: %s", op, type(f).__name__, e)
+            return False
+
+    present, paths_created = os.path.exists(f), []
+    try:
+        if not present and mode in ("w", "a"):
+            op = " writing to"
+            path = os.path.realpath(os.path.dirname(f))
+            parts, accum = [x for x in path.split(os.sep) if x], []
+            while parts:
+                accum.append(parts.pop(0))
+                curpath = os.path.join(os.sep, accum[0] + os.sep, *accum[1:])  # Windows drive letter thing
+                if not os.path.exists(curpath):
+                    os.mkdir(curpath)
+                    paths_created.append(curpath)
+        elif not present and "r" == mode:
+            return False
+        with open(f, {"r": "rb", "w": "ab", "a": "ab+"}[mode]) as g:
+            if mode in ("r", "a"):
+                op = " reading from"
+                result = isinstance(g.read(1), bytes)
+            if result and mode in ("w", "a"):
+                op = " writing to"
+                result, _ = True, g.write(b"")
+            return result
+    except Exception as e:
+        ConsolePrinter.log(logging.ERROR, "Error%s %s: %s", f, e)
+        return False
+    finally:
+        if not present:
+            try: os.remove(f)
+            except Exception: pass
+            for path in paths_created[::-1]:
+                try: os.rmdir(path)
+                except Exception: pass
+
+
 def wildcard_to_regex(text, end=False):
     """
     Returns plain wildcard like "foo*bar" as re.Pattern("foo.*bar", re.I).
 
     @param   end  whether pattern should match until end (adds $)
     """
     suff = "$" if end else ""
     return re.compile(".*".join(map(re.escape, text.split("*"))) + suff, re.I)
+
+
+__all__ = [
+    "PATH_TYPES", "ConsolePrinter", "Decompressor", "MatchMarkers", "ProgressBar", "TextWrapper",
+    "drop_zeros", "ellipsize", "ensure_namespace", "filter_dict", "find_files",
+    "format_bytes", "format_stamp", "format_timedelta", "get_name", "has_arg", "import_item",
+    "is_iterable", "is_stream", "makedirs", "memoize", "merge_dicts", "merge_spans",
+    "parse_datetime", "plural", "unique_path", "verify_io", "wildcard_to_regex",
+]
```

## grepros/inputs.py

```diff
@@ -1,87 +1,109 @@
 # -*- coding: utf-8 -*-
 """
-Input sources for search content.
+Input sources for ROS messages.
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     23.10.2021
-@modified    16.10.2022
+@modified    29.06.2023
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.inputs
 from __future__ import print_function
-import copy
 import collections
 import datetime
 import functools
 import itertools
 import os
 try: import queue  # Py3
 except ImportError: import Queue as queue  # Py2
 import re
 import threading
 import time
 
-from . common import ConsolePrinter, Decompressor, ProgressBar, drop_zeros, filter_dict, \
-                     find_files, format_bytes, format_stamp, format_timedelta, plural, \
-                     wildcard_to_regex
-from . import rosapi
+from . import api
+from . import common
+from . common import ConsolePrinter, ensure_namespace, drop_zeros
 
 
-class SourceBase(object):
+class Source(object):
     """Message producer base class."""
 
+    ## Returned from read() as (topic name, ROS message, ROS timestamp object).
+    class SourceMessage(api.Bag.BagMessage): pass
+
     ## Template for message metainfo line
     MESSAGE_META_TEMPLATE = "{topic} #{index} ({type}  {dt}  {stamp})"
 
-    def __init__(self, args):
+    ## Constructor argument defaults
+    DEFAULT_ARGS = dict(START_TIME=None, END_TIME=None, UNIQUE=False, SELECT_FIELD=(),
+                        NOSELECT_FIELD=(), NTH_MESSAGE=1, NTH_INTERVAL=0)
+
+    def __init__(self, args=None, **kwargs):
         """
-        @param   args                    arguments object like argparse.Namespace
-        @param   args.START_TIME         earliest timestamp of messages to scan
-        @param   args.END_TIME           latest timestamp of messages to scan
-        @param   args.UNIQUE             emit messages that are unique in topic
-        @param   args.SELECT_FIELDS      message fields to use for uniqueness if not all
-        @param   args.NOSELECT_FIELDS    message fields to skip for uniqueness
-        @param   args.NTH_MESSAGE        scan every Nth message in topic
-        @param   args.NTH_INTERVAL       minimum time interval between messages in topic
+        @param   args                   arguments as namespace or dictionary, case-insensitive
+        @param   args.start_time        earliest timestamp of messages to read
+        @param   args.end_time          latest timestamp of messages to read
+        @param   args.unique            emit messages that are unique in topic
+        @param   args.select_field      message fields to use for uniqueness if not all
+        @param   args.noselect_field    message fields to skip for uniqueness
+        @param   args.nth_message       read every Nth message in topic
+        @param   args.nth_interval      minimum time interval between messages in topic
+        @param   kwargs                 any and all arguments as keyword overrides, case-insensitive
         """
         # {key: [(() if any field else ('nested', 'path') or re.Pattern, re.Pattern), ]}
         self._patterns = {}
         # {topic: ["pkg/MsgType", ]} searched in current source
         self._topics = collections.defaultdict(list)
         self._counts = collections.Counter()  # {(topic, typename, typehash): count processed}
         # {(topic, typename, typehash): (message hash over all fields used in matching)}
         self._hashes = collections.defaultdict(set)
         self._processables = {}  # {(topic, typename, typehash): (index, stamp) of last processable}
 
-        self.args = copy.deepcopy(args)
-        ## outputs.SinkBase instance bound to this source
+        self.args = ensure_namespace(args, Source.DEFAULT_ARGS, **kwargs)
+        ## outputs.Sink instance bound to this source
         self.sink = None
         ## All topics in source, as {(topic, typenane, typehash): total message count or None}
         self.topics = {}
         ## ProgressBar instance, if any
         self.bar = None
+        ## Result of validate()
+        self.valid = None
+        ## Apply all filter arguments when reading, not only topic and type
+        self.preprocess = True
 
         self._parse_patterns()
-        rosapi.TypeMeta.SOURCE = self
+
+    def __iter__(self):
+        """Yields messages from source, as (topic, msg, ROS time)."""
+        return self.read()
+
+    def __enter__(self):
+        """Context manager entry."""
+        return self
+
+    def __exit__(self, exc_type, exc_value, traceback):
+        """Context manager exit, closes source."""
+        self.close()
 
     def read(self):
         """Yields messages from source, as (topic, msg, ROS time)."""
 
     def bind(self, sink):
         """Attaches sink to source"""
         self.sink = sink
 
     def validate(self):
         """Returns whether source prerequisites are met (like ROS environment for TopicSource)."""
-        return True
+        if self.valid is None: self.valid = True
+        return self.valid
 
     def close(self):
         """Shuts down input, closing any files or connections."""
         self.topics.clear()
         self._topics.clear()
         self._counts.clear()
         self._hashes.clear()
@@ -95,79 +117,81 @@
         """Shuts down input batch if any (like bagfile), else all input."""
         self.close()
 
     def format_meta(self):
         """Returns source metainfo string."""
         return ""
 
-    def format_message_meta(self, topic, index, stamp, msg):
+    def format_message_meta(self, topic, msg, stamp, index=None):
         """Returns message metainfo string."""
-        return self.MESSAGE_META_TEMPLATE.format(**self.get_message_meta(topic, index, stamp, msg))
+        meta = self.get_message_meta(topic, msg, stamp, index)
+        meta = {k: "" if v is None else v for k, v in meta.items()}
+        return self.MESSAGE_META_TEMPLATE.format(**meta)
 
     def get_batch(self):
         """Returns source batch identifier if any (like bagfile name if BagSource)."""
 
     def get_meta(self):
         """Returns source metainfo data dict."""
         return {}
 
-    def get_message_meta(self, topic, index, stamp, msg):
+    def get_message_meta(self, topic, msg, stamp, index=None):
         """Returns message metainfo data dict."""
-        with rosapi.TypeMeta.make(msg, topic) as m:
-            return dict(topic=topic, type=m.typename, index=index, hash=m.typehash,
-                        dt=drop_zeros(format_stamp(rosapi.to_sec(stamp)), " "),
-                        stamp=drop_zeros(rosapi.to_sec(stamp)), schema=m.definition)
+        with api.TypeMeta.make(msg, topic) as m:
+            return dict(topic=topic, type=m.typename, stamp=drop_zeros(api.to_sec(stamp)),
+                        index=index, dt=drop_zeros(common.format_stamp(api.to_sec(stamp)), " "),
+                        hash=m.typehash, schema=m.definition)
 
     def get_message_class(self, typename, typehash=None):
         """Returns message type class."""
-        return rosapi.get_message_class(typename)
+        return api.get_message_class(typename)
 
     def get_message_definition(self, msg_or_type):
         """Returns ROS message type definition full text, including subtype definitions."""
-        return rosapi.get_message_definition(msg_or_type)
+        return api.get_message_definition(msg_or_type)
 
     def get_message_type_hash(self, msg_or_type):
         """Returns ROS message type MD5 hash."""
-        return rosapi.get_message_type_hash(msg_or_type)
+        return api.get_message_type_hash(msg_or_type)
 
-    def is_processable(self, topic, index, stamp, msg):
-        """Returns whether specified message in topic is in acceptable time range."""
+    def is_processable(self, topic, msg, stamp, index=None):
+        """Returns whether message passes source filters."""
         if self.args.START_TIME and stamp < self.args.START_TIME:
             return False
         if self.args.END_TIME and stamp > self.args.END_TIME:
             return False
         if self.args.UNIQUE or self.args.NTH_MESSAGE > 1 or self.args.NTH_INTERVAL > 0:
-            topickey = rosapi.TypeMeta.make(msg, topic).topickey
+            topickey = api.TypeMeta.make(msg, topic).topickey
             last_accepted = self._processables.get(topickey)
-        if self.args.NTH_MESSAGE > 1 and last_accepted:
+        if self.args.NTH_MESSAGE > 1 and last_accepted and index is not None:
             if (index - 1) % self.args.NTH_MESSAGE:
                 return False
-        if self.args.NTH_INTERVAL > 0 and last_accepted:
-            if rosapi.to_sec(stamp - last_accepted[1]) < self.args.NTH_INTERVAL:
+        if self.args.NTH_INTERVAL > 0 and last_accepted and stamp is not None:
+            if api.to_sec(stamp - last_accepted[1]) < self.args.NTH_INTERVAL:
                 return False
         if self.args.UNIQUE:
             include, exclude = self._patterns["select"], self._patterns["noselect"]
-            msghash = rosapi.make_message_hash(msg, include, exclude)
+            msghash = api.make_message_hash(msg, include, exclude)
             if msghash in self._hashes[topickey]:
                 return False
             self._hashes[topickey].add(msghash)
         return True
 
     def notify(self, status):
         """Reports match status of last produced message."""
 
     def thread_excepthook(self, text, exc):
         """Handles exception, used by background threads."""
         ConsolePrinter.error(text)
 
     def _parse_patterns(self):
         """Parses pattern arguments into re.Patterns."""
-        selects, noselects = self.args.SELECT_FIELDS, self.args.NOSELECT_FIELDS
+        selects, noselects = self.args.SELECT_FIELD, self.args.NOSELECT_FIELD
         for key, vals in [("select", selects), ("noselect", noselects)]:
-            self._patterns[key] = [(tuple(v.split(".")), wildcard_to_regex(v)) for v in vals]
+            self._patterns[key] = [(tuple(v.split(".")), common.wildcard_to_regex(v)) for v in vals]
 
 
 class ConditionMixin(object):
     """
     Provides topic conditions evaluation.
 
     Evaluates a set of Python expressions, with a namespace of:
@@ -191,14 +215,17 @@
 
     Example condition: `<topic */control_enable>.data and <topic */cmd_vel>.linear.x > 0`
                        `and <topic */cmd_vel>.angular.z < 0.02`.
     """
 
     TOPIC_RGX = re.compile(r"<topic\s+([^\s><]+)\s*>")  # "<topic /some/thing>"
 
+    ## Constructor argument defaults
+    DEFAULT_ARGS = dict(CONDITION=())
+
     class NoMessageException(Exception): pass
 
 
     class Topic(object):
         """
         Object for <topic x> replacements in condition expressions.
 
@@ -231,36 +258,37 @@
     class Empty(object):
         """Placeholder falsy object that raises NoMessageException on attribute access."""
         def __getattr__(self, name): raise ConditionMixin.NoMessageException()
         def __bool__(self):          return False
         def __nonzero__(self):       return False
 
 
-    def __init__(self, args):
+    def __init__(self, args=None, **kwargs):
         """
-        @param   args              arguments object like argparse.Namespace
-        @param   args.CONDITIONS   Python expressions that must evaluate as true
-                                   for message to be processable
+        @param   args             arguments as namespace or dictionary, case-insensitive
+        @param   args.condition   Python expressions that must evaluate as true
+                                  for message to be processable, see ConditionMixin
+        @param   kwargs           any and all arguments as keyword overrides, case-insensitive
         """
         self._topic_states         = {}  # {topic: whether only used for condition, not matching}
         self._topics_per_condition = []  # [[topics in 1st condition], ]
         self._wildcard_topics      = {}  # {"/my/*/topic": re.Pattern}
         # {(topic, typename, typehash): [1st, 2nd, ..]}
         self._firstmsgs = collections.defaultdict(collections.deque)
         # {(topic, typename, typehash): [.., last]}
         self._lastmsgs  = collections.defaultdict(collections.deque)
         # {topic: (max positive index + 1, max abs(negative index) or 1)}
         self._topic_limits = collections.defaultdict(lambda: [1, 1])
 
         ## {condition with <topic x> as get_topic("x"): compiled code object}
         self._conditions = collections.OrderedDict()
-        self._configure_conditions(args)
+        self._configure_conditions(ensure_namespace(args, ConditionMixin.DEFAULT_ARGS, **kwargs))
 
-    def is_processable(self, topic, index, stamp, msg):
-        """Returns whether current state passes conditions, if any."""
+    def is_processable(self, topic, msg, stamp, index=None):
+        """Returns whether message passes passes current state conditions, if any."""
         result = True
         if not self._conditions:
             return result
         for i, (expr, code) in enumerate(self._conditions.items()):
             topics = self._topics_per_condition[i]
             wildcarded = [t for t in topics if t in self._wildcard_topics]
             realcarded = {wt: [(t, n, h) for (t, n, h) in self._lastmsgs if p.match(t)]
@@ -313,15 +341,15 @@
         """Sets whether topic is purely used for conditions not matching."""
         if topic in self._topic_states:
             self._topic_states[topic] = pure
 
     def conditions_register_message(self, topic, msg):
         """Retains message for condition evaluation if in condition topic."""
         if self.is_conditions_topic(topic, pure=False):
-            topickey = rosapi.TypeMeta.make(msg, topic).topickey
+            topickey = api.TypeMeta.make(msg, topic).topickey
             self._lastmsgs[topickey].append(msg)
             if len(self._lastmsgs[topickey]) > self._topic_limits[topic][-1]:
                 self._lastmsgs[topickey].popleft()
             if len(self._firstmsgs[topickey]) < self._topic_limits[topic][0]:
                 self._firstmsgs[topickey].append(msg)
 
     def _get_topic_instance(self, topic, remap=None):
@@ -337,513 +365,755 @@
         if topickey not in self._counts:
             return self.Empty()
         c, f, l = (d[topickey] for d in (self._counts, self._firstmsgs, self._lastmsgs))
         return self.Topic(c, f, l)
 
     def _configure_conditions(self, args):
         """Parses condition expressions and populates local structures."""
-        for v in args.CONDITIONS:
+        for v in args.CONDITION:
             topics = list(set(self.TOPIC_RGX.findall(v)))
             self._topic_states.update({t: True for t in topics})
             self._topics_per_condition.append(topics)
             for t in (t for t in topics if "*" in t):
-                self._wildcard_topics[t] = wildcard_to_regex(t, end=True)
+                self._wildcard_topics[t] = common.wildcard_to_regex(t, end=True)
             expr = self.TOPIC_RGX.sub(r'get_topic("\1")', v)
             self._conditions[expr] = compile(expr, "", "eval")
 
-        for v in args.CONDITIONS:  # Set history length from <topic x>[index]
+        for v in args.CONDITION:  # Set history length from <topic x>[index]
             indexexprs = re.findall(self.TOPIC_RGX.pattern + r"\s*\[([^\]]+)\]", v)
             for topic, indexexpr in indexexprs:
                 limits = self._topic_limits[topic]
                 try:
                     index = eval(indexexpr)  # If integer, set history limits
                     limits[index < 0] = max(limits[index < 0], abs(index) + (index >= 0))
                 except Exception: continue  # for topic
 
 
-
-class BagSource(SourceBase, ConditionMixin):
+class BagSource(Source, ConditionMixin):
     """Produces messages from ROS bagfiles."""
 
     ## Template for message metainfo line
     MESSAGE_META_TEMPLATE = "{topic} {index}/{total} ({type}  {dt}  {stamp})"
+
     ## Template for bag metainfo header
     META_TEMPLATE         = "\nFile {file} ({size}), {tcount} topics, {mcount:,d} messages\n" \
                             "File period {startdt} - {enddt}\n" \
                             "File span {delta} ({start} - {end})"
 
-    def __init__(self, args):
+    ## Constructor argument defaults
+    DEFAULT_ARGS = dict(BAG=(), FILE=(), PATH=(), RECURSE=False, TOPIC=(), TYPE=(),
+                        SKIP_TOPIC=(), SKIP_TYPE=(), START_TIME=None, END_TIME=None,
+                        START_INDEX=None, END_INDEX=None, CONDITION=(), AFTER=0, ORDERBY=None,
+                        DECOMPRESS=False, REINDEX=False, WRITE=(), PROGRESS=False,
+                        STOP_ON_ERROR=False)
+
+    def __init__(self, args=None, **kwargs):
         """
-        @param   args               arguments object like argparse.Namespace
-        @param   args.FILES         names of ROS bagfiles to scan if not all in directory
-        @param   args.PATHS         paths to scan if not current directory
-        @param   args.RECURSE       recurse into subdirectories when looking for bagfiles
-        @param   args.TOPICS        ROS topics to scan if not all
-        @param   args.TYPES         ROS message types to scan if not all
-        @param   args.SKIP_TOPICS   ROS topics to skip
-        @param   args.SKIP_TYPES    ROS message types to skip
-        @param   args.START_TIME    earliest timestamp of messages to scan
-        @param   args.END_TIME      latest timestamp of messages to scan
-        @param   args.START_INDEX   message index within topic to start from
-        @param   args.END_INDEX     message index within topic to stop at
-        @param   args.CONDITIONS    Python expressions that must evaluate as true
-                                    for message to be processable
-        @param   args.AFTER         emit NUM messages of trailing context after match
-        @param   args.ORDERBY       "topic" or "type" if any to group results by
-        @param   args.DECOMPRESS    decompress archived bags to file directory
-        @param   args.REINDEX       make a copy of unindexed bags and reindex them (ROS1 only)
-        @param   args.WRITE         outputs, to skip in input files
-        @param   args.PROGRESS      whether to print progress bar
+        @param   args                   arguments as namespace or dictionary, case-insensitive;
+                                        or a single path as the ROS bagfile to read,
+                                        or a stream to read from,
+                                        or one or more {@link grepros.api.Bag Bag} instances
+        <!--sep-->
+
+        Bag-specific arguments:
+        @param   args.file              names of ROS bagfiles to read if not all in directory,
+                                        or a stream to read from;
+                                        or one or more {@link grepros.api.Bag Bag} instances
+        @param   args.path              paths to scan if not current directory
+        @param   args.recurse           recurse into subdirectories when looking for bagfiles
+        @param   args.orderby           "topic" or "type" if any to group results by
+        @param   args.decompress        decompress archived bags to file directory
+        @param   args.reindex           make a copy of unindexed bags and reindex them (ROS1 only)
+        @param   args.write             outputs, to skip in input files
+        @param   args.bag               one or more {@link grepros.api.Bag Bag} instances
+        <!--sep-->
+
+        General arguments:
+        @param   args.topic             ROS topics to read if not all
+        @param   args.type              ROS message types to read if not all
+        @param   args.skip_topic        ROS topics to skip
+        @param   args.skip_type         ROS message types to skip
+        @param   args.start_time        earliest timestamp of messages to read
+        @param   args.end_time          latest timestamp of messages to read
+        @param   args.start_index       message index within topic to start from
+        @param   args.end_index         message index within topic to stop at
+        @param   args.unique            emit messages that are unique in topic
+        @param   args.select_field      message fields to use for uniqueness if not all
+        @param   args.noselect_field    message fields to skip for uniqueness
+        @param   args.nth_message       read every Nth message in topic
+        @param   args.nth_interval      minimum time interval between messages in topic
+        @param   args.condition         Python expressions that must evaluate as true
+                                        for message to be processable, see ConditionMixin
+        @param   args.progress          whether to print progress bar
+        @param   args.stop_on_error     stop execution on any error like unknown message type
+        @param   kwargs                 any and all arguments as keyword overrides, case-insensitive
         """
+        args0 = args
+        is_bag = isinstance(args, api.Bag) or \
+                 common.is_iterable(args) and all(isinstance(x, api.Bag) for x in args)
+        args = {"FILE": str(args)} if isinstance(args, common.PATH_TYPES) else \
+               {"FILE": args} if common.is_stream(args) else {} if is_bag else args
+        args = ensure_namespace(args, BagSource.DEFAULT_ARGS, **kwargs)
         super(BagSource, self).__init__(args)
         ConditionMixin.__init__(self, args)
-        self._args0     = copy.deepcopy(args)  # Original arguments
+        self._args0     = common.structcopy(self.args)  # Original arguments
         self._status    = None   # Match status of last produced message
-        self._sticky    = False  # Scanning a single topic until all after-context emitted
-        self._totals_ok = False  # Whether message count totals have been retrieved
+        self._sticky    = False  # Reading a single topic until all after-context emitted
+        self._totals_ok = False  # Whether message count totals have been retrieved (ROS2 optimize)
+        self._types_ok  = False  # Whether type definitions have been retrieved (ROS2 optimize)
         self._running   = False
         self._bag       = None   # Current bag object instance
         self._filename  = None   # Current bagfile path
         self._meta      = None   # Cached get_meta()
+        self._bag0      = ([args0] if isinstance(args0, api.Bag) else args0) if is_bag else None
 
     def read(self):
         """Yields messages from ROS bagfiles, as (topic, msg, ROS time)."""
+        if not self.validate(): raise Exception("invalid")
         self._running = True
-        names, paths = self.args.FILES, self.args.PATHS
-        exts, skip_exts = rosapi.BAG_EXTENSIONS, rosapi.SKIP_EXTENSIONS
-        exts = list(exts) + ["%s%s" % (a, b) for a in exts for b in Decompressor.EXTENSIONS]
 
-        encountereds = set()
-        for filename in find_files(names, paths, exts, skip_exts, self.args.RECURSE):
+        for _ in self._produce_bags():
             if not self._running:
-                continue  # for filename
-
-            fullname = os.path.realpath(os.path.abspath(filename))
-            skip = Decompressor.make_decompressed_name(fullname) in encountereds
-            encountereds.add(fullname)
-
-            if skip or not self._configure(filename):
-                continue  # for filename
+                break  # for _
 
             topicsets = [self._topics]
             if "topic" == self.args.ORDERBY:  # Group output by sorted topic names
                 topicsets = [{n: tt} for n, tt in sorted(self._topics.items())]
             elif "type" == self.args.ORDERBY:  # Group output by sorted type names
                 typetopics = {}
                 for n, tt in self._topics.items():
                     for t in tt: typetopics.setdefault(t, []).append(n)
                 topicsets = [{n: [t] for n in nn} for t, nn in sorted(typetopics.items())]
 
+            self._types_ok = False
             self._init_progress()
             for topics in topicsets:
-                for topic, msg, stamp in self._produce(topics) if topics else ():
+                for topic, msg, stamp, index in self._produce(topics) if topics else ():
                     self.conditions_register_message(topic, msg)
-                    if not self.is_conditions_topic(topic, pure=True):
-                        yield topic, msg, stamp
+                    if not self.is_conditions_topic(topic, pure=True) \
+                    and (not self.preprocess or self.is_processable(topic, msg, stamp, index)):
+                        yield self.SourceMessage(topic, msg, stamp)
                 if not self._running:
                     break  # for topics
-            self._counts and self.sink.flush()
+            self._counts and self.sink and self.sink.flush()
             self.close_batch()
         self._running = False
 
     def validate(self):
-        """Returns whether ROS environment is set, prints error if not."""
-        result = rosapi.validate()
+        """Returns whether ROS environment is set and arguments valid, prints error if not."""
+        if self.valid is not None: return self.valid
+        self.valid = api.validate()
+        if not self._bag0 and self.args.FILE and os.path.isfile(self.args.FILE[0]) \
+        and not common.verify_io(self.args.FILE[0], "r"):
+            ConsolePrinter.error("File not readable.")
+            self.valid = False
+        if not self._bag0 and common.is_stream(self.args.FILE) \
+        and not any(c.STREAMABLE for c in api.Bag.READER_CLASSES):
+            ConsolePrinter.error("Bag format does not support reading streams.")
+            self.valid = False
+        if self._bag0 and not any(x.mode in ("r", "a") for x in self._bag0):
+            ConsolePrinter.error("Bag not in read mode.")
+            self.valid = False
         if self.args.ORDERBY and self.conditions_get_topics():
             ConsolePrinter.error("Cannot use topics in conditions and bag order by %s.",
                                  self.args.ORDERBY)
-            result = False
-        return result
+            self.valid = False
+        return self.valid
 
     def close(self):
         """Closes current bag, if any."""
         self._running = False
-        self._bag and self._bag.close()
+        if self._bag and not self._bag0: self._bag.close()
         ConditionMixin.close_batch(self)
         super(BagSource, self).close()
 
     def close_batch(self):
         """Closes current bag, if any."""
-        self._bag and self._bag.close()
+        if self._bag0: self._running = False
+        elif self._bag: self._bag.close()
         self._bag = None
         if self.bar:
             self.bar.update(flush=True)
             self.bar = None
         ConditionMixin.close_batch(self)
 
     def format_meta(self):
         """Returns bagfile metainfo string."""
         return self.META_TEMPLATE.format(**self.get_meta())
 
-    def format_message_meta(self, topic, index, stamp, msg):
+    def format_message_meta(self, topic, msg, stamp, index=None):
         """Returns message metainfo string."""
-        return self.MESSAGE_META_TEMPLATE.format(**self.get_message_meta(topic, index, stamp, msg))
+        meta = self.get_message_meta(topic, msg, stamp, index)
+        meta = {k: "" if v is None else v for k, v in meta.items()}
+        return self.MESSAGE_META_TEMPLATE.format(**meta)
 
     def get_batch(self):
-        """Returns name of current bagfile."""
-        return self._filename
+        """Returns name of current bagfile, or self if reading stream."""
+        return self._filename if self._filename is not None else self
 
     def get_meta(self):
         """Returns bagfile metainfo data dict."""
         if self._meta is not None:
             return self._meta
         mcount = self._bag.get_message_count()
         start, end = (self._bag.get_start_time(), self._bag.get_end_time()) if mcount else ("", "")
-        delta = format_timedelta(datetime.timedelta(seconds=(end or 0) - (start or 0)))
-        self._meta = dict(file=self._filename, size=format_bytes(self._bag.size),
-                          mcount=mcount, tcount=len(self.topics),
+        delta = common.format_timedelta(datetime.timedelta(seconds=(end or 0) - (start or 0)))
+        self._meta = dict(file=self._filename, size=common.format_bytes(self._bag.size),
+                          mcount=mcount, tcount=len(self.topics), delta=delta,
                           start=drop_zeros(start), end=drop_zeros(end),
-                          startdt=drop_zeros(format_stamp(start)) if start != "" else "",
-                          enddt=drop_zeros(format_stamp(end)) if end != "" else "", delta=delta)
+                          startdt=drop_zeros(common.format_stamp(start)) if start != "" else "",
+                          enddt=drop_zeros(common.format_stamp(end)) if end != "" else "")
         return self._meta
 
-    def get_message_meta(self, topic, index, stamp, msg):
+    def get_message_meta(self, topic, msg, stamp, index=None):
         """Returns message metainfo data dict."""
         self._ensure_totals()
-        result = super(BagSource, self).get_message_meta(topic, index, stamp, msg)
+        result = super(BagSource, self).get_message_meta(topic, msg, stamp, index)
         result.update(total=self.topics[(topic, result["type"], result["hash"])])
         if callable(getattr(self._bag, "get_qoses", None)):
             result.update(qoses=self._bag.get_qoses(topic, result["type"]))
         return result
 
     def get_message_class(self, typename, typehash=None):
         """Returns ROS message type class."""
         return self._bag.get_message_class(typename, typehash) or \
-               rosapi.get_message_class(typename)
+               api.get_message_class(typename)
 
     def get_message_definition(self, msg_or_type):
         """Returns ROS message type definition full text, including subtype definitions."""
         return self._bag.get_message_definition(msg_or_type) or \
-               rosapi.get_message_definition(msg_or_type)
+               api.get_message_definition(msg_or_type)
 
     def get_message_type_hash(self, msg_or_type):
         """Returns ROS message type MD5 hash."""
         return self._bag.get_message_type_hash(msg_or_type) or \
-               rosapi.get_message_type_hash(msg_or_type)
+               api.get_message_type_hash(msg_or_type)
 
     def notify(self, status):
         """Reports match status of last produced message."""
         self._status = bool(status)
         if status and not self._totals_ok:
             self._ensure_totals()
 
-    def is_processable(self, topic, index, stamp, msg):
-        """
-        Returns whether specified message in topic is in acceptable range,
-        and all conditions, if any, evaluate as true.
-        """
-        topickey = rosapi.TypeMeta.make(msg, topic).topickey
-        if self.args.START_INDEX:
+    def is_processable(self, topic, msg, stamp, index=None):
+        """Returns whether message passes source filters."""
+        topickey = api.TypeMeta.make(msg, topic).topickey
+        if self.args.START_INDEX and index is not None:
             self._ensure_totals()
             START = self.args.START_INDEX
             MIN = max(0, START + (self.topics[topickey] if START < 0 else 0))
             if MIN >= index:
                 return False
-        if self.args.END_INDEX:
+        if self.args.END_INDEX and index is not None:
             self._ensure_totals()
             END = self.args.END_INDEX
             MAX = END + (self.topics[topickey] if END < 0 else 0)
             if MAX < index:
                 return False
-        if not super(BagSource, self).is_processable(topic, index, stamp, msg):
+        if not super(BagSource, self).is_processable(topic, msg, stamp, index):
             return False
-        return ConditionMixin.is_processable(self, topic, index, stamp, msg)
+        return ConditionMixin.is_processable(self, topic, msg, stamp, index)
 
     def _produce(self, topics, start_time=None):
-        """Yields messages from current ROS bagfile, as (topic, msg, ROS time)."""
+        """
+        Yields messages from current ROS bagfile, as (topic, msg, ROS time, index in topic).
+
+        @param   topics  {topic: [typename, ]}
+        """
+        if not self._running or not self._bag: return
         counts = collections.Counter()
         for topic, msg, stamp in self._bag.read_messages(list(topics), start_time):
             if not self._running or not self._bag:
-                break  # for topic
-            typename = rosapi.get_message_type(msg)
+                break  # for topic, 
+            typename = api.get_message_type(msg)
             if topics and typename not in topics[topic]:
                 continue  # for topic
+            if api.ROS2 and not self._types_ok:
+                self.topics, self._types_ok = self._bag.get_topic_info(counts=False), True
 
-            topickey = rosapi.TypeMeta.make(msg, topic).topickey
+            topickey = api.TypeMeta.make(msg, topic, self).topickey
             counts[topickey] += 1; self._counts[topickey] += 1
             # Skip messages already processed during sticky
             if not self._sticky and counts[topickey] != self._counts[topickey]:
                 continue  # for topic
 
             self._status = None
             self.bar and self.bar.update(value=sum(self._counts.values()))
-            yield topic, msg, stamp
+            yield topic, msg, stamp, self._counts[topickey]
 
             if self.args.NTH_MESSAGE > 1 or self.args.NTH_INTERVAL > 0:
                 self._processables[topickey] = (self._counts[topickey], stamp)
             if self._status and self.args.AFTER and not self._sticky \
             and not self.has_conditions() \
             and (len(self._topics) > 1 or len(next(iter(self._topics.values()))) > 1):
                 # Stick to one topic until trailing messages have been emitted
                 self._sticky = True
-                continue_from = stamp + rosapi.make_duration(nsecs=1)
+                continue_from = stamp + api.make_duration(nsecs=1)
                 for entry in self._produce({topic: typename}, continue_from):
                     yield entry
                 self._sticky = False
+            if not self._running or not self._bag:
+                break  # for topic
+
+    def _produce_bags(self):
+        """Yields Bag instances from configured arguments."""
+        if self._bag0:
+            for bag in self._bag0:
+                if self._configure(bag=bag):
+                    yield self._bag
+            return
+
+        names, paths = self.args.FILE, self.args.PATH
+        exts, skip_exts = api.BAG_EXTENSIONS, api.SKIP_EXTENSIONS
+        exts = list(exts) + ["%s%s" % (a, b) for a in exts for b in common.Decompressor.EXTENSIONS]
+
+        encountereds = set()
+        for filename in common.find_files(names, paths, exts, skip_exts, self.args.RECURSE):
+            if not self._running:
+                break  # for filename
+
+            fullname = os.path.realpath(os.path.abspath(filename))
+            skip = common.Decompressor.make_decompressed_name(fullname) in encountereds
+            encountereds.add(fullname)
+
+            if skip or not self._configure(filename):
+                continue  # for filename
+
+            encountereds.add(self._bag.filename)
+            yield self._bag
 
     def _init_progress(self):
         """Initializes progress bar, if any, for current bag."""
         if self.args.PROGRESS and not self.bar:
             self._ensure_totals()
-            self.bar = ProgressBar(aftertemplate=" {afterword} ({value:,d}/{max:,d})")
-            self.bar.afterword = os.path.basename(self._filename)
+            self.bar = common.ProgressBar(aftertemplate=" {afterword} ({value:,d}/{max:,d})")
+            self.bar.afterword = os.path.basename(self._filename or "<stream>")
             self.bar.max = sum(sum(c for (t, n, _), c in self.topics.items()
                                    if c and t == t_ and n in nn)
                                for t_, nn in self._topics.items())
             self.bar.update(value=0)
 
     def _ensure_totals(self):
         """Retrieves total message counts if not retrieved."""
-        if not self._totals_ok:  # Must be ros2.Bag
-            for (t, n, h), c in self._bag.get_topic_info(counts=True).items():
+        if not self._totals_ok:  # ROS2 bag probably
+            has_ensure = common.has_arg(self._bag.get_topic_info, "ensure_types")
+            kws = dict(ensure_types=False) if has_ensure else {}
+            for (t, n, h), c in self._bag.get_topic_info(**kws).items():
                 self.topics[(t, n, h)] = c
             self._totals_ok = True
 
-    def _configure(self, filename):
+    def _configure(self, filename=None, bag=None):
         """Opens bag and populates bag-specific argument state, returns success."""
         self._meta      = None
         self._bag       = None
         self._filename  = None
         self._sticky    = False
         self._totals_ok = False
         self._counts.clear()
         self._processables.clear()
         self._hashes.clear()
         self.topics.clear()
-        if self.args.WRITE \
+
+        if bag is not None and bag.mode not in ("r", "a"):
+            ConsolePrinter.warn("Cannot read %s: bag in write mode.", bag)
+            return False
+
+        if filename and self.args.WRITE \
         and any(os.path.realpath(x[0]) == os.path.realpath(filename)
                 for x in self.args.WRITE):
             return False
         try:
-            bag = rosapi.Bag(filename, mode="r", decompress=self.args.DECOMPRESS,
-                             reindex=self.args.REINDEX, progress=self.args.PROGRESS)
+            if filename and common.Decompressor.is_compressed(filename):
+                if self.args.DECOMPRESS:
+                    filename = common.Decompressor.decompress(filename, self.args.PROGRESS)
+                else: raise Exception("decompression not enabled")
+            bag = api.Bag(filename, mode="r", reindex=self.args.REINDEX,
+                          progress=self.args.PROGRESS) if bag is None else bag
+            bag.stop_on_error = self.args.STOP_ON_ERROR
+            bag.open()
         except Exception as e:
-            ConsolePrinter.error("\nError opening %r: %s", filename, e)
+            ConsolePrinter.error("\nError opening %r: %s", filename or bag, e)
+            if self.args.STOP_ON_ERROR: raise
             return False
 
         self._bag      = bag
         self._filename = bag.filename
 
         dct = fulldct = {}  # {topic: [typename, ]}
-        for (t, n, h), c in bag.get_topic_info().items():
+        kws = dict(ensure_types=False) if common.has_arg(bag.get_topic_info, "ensure_types") else {}
+        for (t, n, h), c in bag.get_topic_info(counts=False, **kws).items():
             dct.setdefault(t, []).append(n)
             self.topics[(t, n, h)] = c
         self._totals_ok = not any(v is None for v in self.topics.values())
         for topic in self.conditions_get_topics():
             self.conditions_set_topic_state(topic, True)
 
-        dct = filter_dict(dct, self.args.TOPICS, self.args.TYPES)
-        dct = filter_dict(dct, self.args.SKIP_TOPICS, self.args.SKIP_TYPES, reverse=True)
+        dct = common.filter_dict(dct, self.args.TOPIC, self.args.TYPE)
+        dct = common.filter_dict(dct, self.args.SKIP_TOPIC, self.args.SKIP_TYPE, reverse=True)
         for topic in self.conditions_get_topics():  # Add topics used in conditions
-            matches = [t for p in [wildcard_to_regex(topic, end=True)] for t in fulldct
+            matches = [t for p in [common.wildcard_to_regex(topic, end=True)] for t in fulldct
                        if t == topic or "*" in topic and p.match(t)]
             for topic in matches:
                 dct.setdefault(topic, fulldct[topic])
                 self.conditions_set_topic_state(topic, topic not in dct)
         self._topics = dct
         self._meta   = self.get_meta()
 
-        args = self.args = copy.deepcopy(self._args0)
+        args = self.args = common.structcopy(self._args0)
         if args.START_TIME is not None:
-            args.START_TIME = rosapi.make_bag_time(args.START_TIME, bag)
+            args.START_TIME = api.make_bag_time(args.START_TIME, bag)
         if args.END_TIME is not None:
-            args.END_TIME = rosapi.make_bag_time(args.END_TIME, bag)
+            args.END_TIME = api.make_bag_time(args.END_TIME, bag)
         return True
 
 
-class TopicSource(SourceBase, ConditionMixin):
+class TopicSource(Source, ConditionMixin):
     """Produces messages from live ROS topics."""
 
     ## Seconds between refreshing available topics from ROS master.
     MASTER_INTERVAL = 2
 
-    def __init__(self, args):
+    ## Constructor argument defaults
+    DEFAULT_ARGS = dict(TOPIC=(), TYPE=(), SKIP_TOPIC=(), SKIP_TYPE=(), START_TIME=None,
+                        END_TIME=None, START_INDEX=None, END_INDEX=None, CONDITION=(),
+                        QUEUE_SIZE_IN=10, ROS_TIME_IN=False, PROGRESS=False, STOP_ON_ERROR=False)
+
+    def __init__(self, args=None, **kwargs):
         """
-        @param   args                 arguments object like argparse.Namespace
-        @param   args.TOPICS          ROS topics to scan if not all
-        @param   args.TYPES           ROS message types to scan if not all
-        @param   args.SKIP_TOPICS     ROS topics to skip
-        @param   args.SKIP_TYPES      ROS message types to skip
-        @param   args.START_TIME      earliest timestamp of messages to scan
-        @param   args.END_TIME        latest timestamp of messages to scan
-        @param   args.START_INDEX     message index within topic to start from
-        @param   args.END_INDEX       message index within topic to stop at
-        @param   args.CONDITIONS      Python expressions that must evaluate as true
-                                      for message to be processable
-        @param   args.QUEUE_SIZE_IN   subscriber queue size
-        @param   args.ROS_TIME_IN     stamp messages with ROS time instead of wall time
-        @param   args.PROGRESS        whether to print progress bar
+        @param   args                   arguments as namespace or dictionary, case-insensitive
+        @param   args.topic             ROS topics to read if not all
+        @param   args.type              ROS message types to read if not all
+        @param   args.skip_topic        ROS topics to skip
+        @param   args.skip_type         ROS message types to skip
+        @param   args.start_time        earliest timestamp of messages to read
+        @param   args.end_time          latest timestamp of messages to read
+        @param   args.start_index       message index within topic to start from
+        @param   args.end_index         message index within topic to stop at
+        @param   args.unique            emit messages that are unique in topic
+        @param   args.select_field      message fields to use for uniqueness if not all
+        @param   args.noselect_field    message fields to skip for uniqueness
+        @param   args.nth_message       read every Nth message in topic
+        @param   args.nth_interval      minimum time interval between messages in topic
+        @param   args.condition         Python expressions that must evaluate as true
+                                        for message to be processable, see ConditionMixin
+        @param   args.queue_size_in     subscriber queue size (default 10)
+        @param   args.ros_time_in       stamp messages with ROS time instead of wall time
+        @param   args.progress          whether to print progress bar
+        @param   args.stop_on_error     stop execution on any error like unknown message type
+        @param   kwargs                 any and all arguments as keyword overrides, case-insensitive
         """
+        args = ensure_namespace(args, TopicSource.DEFAULT_ARGS, **kwargs)
         super(TopicSource, self).__init__(args)
         ConditionMixin.__init__(self, args)
         self._running = False  # Whether is in process of yielding messages from topics
         self._queue   = None   # [(topic, msg, ROS time)]
         self._subs    = {}     # {(topic, typename, typehash): ROS subscriber}
 
         self._configure()
 
     def read(self):
         """Yields messages from subscribed ROS topics, as (topic, msg, ROS time)."""
         if not self._running:
+            if not self.validate(): raise Exception("invalid")
+            api.init_node()
             self._running = True
             self._queue = queue.Queue()
             self.refresh_topics()
             t = threading.Thread(target=self._run_refresh)
             t.daemon = True
             t.start()
 
         total = 0
         self._init_progress()
         while self._running:
             topic, msg, stamp = self._queue.get()
             total += bool(topic)
             self._update_progress(total, running=self._running and bool(topic))
             if topic:
-                topickey = rosapi.TypeMeta.make(msg, topic).topickey
+                topickey = api.TypeMeta.make(msg, topic, self).topickey
                 self._counts[topickey] += 1
                 self.conditions_register_message(topic, msg)
                 if self.is_conditions_topic(topic, pure=True): continue  # while
 
-                yield topic, msg, stamp
+                if not self.preprocess \
+                or self.is_processable(topic, msg, stamp, self._counts[topickey]):
+                    yield self.SourceMessage(topic, msg, stamp)
                 if self.args.NTH_MESSAGE > 1 or self.args.NTH_INTERVAL > 0:
                     self._processables[topickey] = (self._counts[topickey], stamp)
         self._queue = None
         self._running = False
 
     def bind(self, sink):
         """Attaches sink to source and blocks until connected to ROS live."""
-        SourceBase.bind(self, sink)
-        rosapi.init_node()
+        if not self.validate(): raise Exception("invalid")
+        super(TopicSource, self).bind(sink)
+        api.init_node()
 
     def validate(self):
         """Returns whether ROS environment is set, prints error if not."""
-        return rosapi.validate(live=True)
+        if self.valid is None: self.valid = api.validate(live=True)
+        return self.valid
 
     def close(self):
         """Shuts down subscribers and stops producing messages."""
         self._running = False
         for k in list(self._subs):
             self._subs.pop(k).unregister()
         self._queue and self._queue.put((None, None, None))  # Wake up iterator
         self._queue = None
         ConditionMixin.close_batch(self)
         super(TopicSource, self).close()
-        rosapi.shutdown_node()
 
     def get_meta(self):
         """Returns source metainfo data dict."""
         ENV = {k: os.getenv(k) for k in ("ROS_MASTER_URI", "ROS_DOMAIN_ID") if os.getenv(k)}
         return dict(ENV, tcount=len(self.topics))
 
-    def get_message_meta(self, topic, index, stamp, msg):
+    def get_message_meta(self, topic, msg, stamp, index=None):
         """Returns message metainfo data dict."""
-        result = super(TopicSource, self).get_message_meta(topic, index, stamp, msg)
+        result = super(TopicSource, self).get_message_meta(topic, msg, stamp, index)
         topickey = (topic, result["type"], result["hash"])
         if topickey in self._subs:
             result.update(qoses=self._subs[topickey].get_qoses())
         return result
 
     def get_message_class(self, typename, typehash=None):
         """Returns message type class, from active subscription if available."""
         sub = next((s for (t, n, h), s in self._subs.items()
                     if n == typename and typehash in (s.get_message_type_hash(), None)), None)
-        return sub and sub.get_message_class() or rosapi.get_message_class(typename)
+        return sub and sub.get_message_class() or api.get_message_class(typename)
 
     def get_message_definition(self, msg_or_type):
         """Returns ROS message type definition full text, including subtype definitions."""
-        if rosapi.is_ros_message(msg_or_type):
-            return rosapi.get_message_definition(msg_or_type)
+        if api.is_ros_message(msg_or_type):
+            return api.get_message_definition(msg_or_type)
         sub = next((s for (t, n, h), s in self._subs.items() if n == msg_or_type), None)
-        return sub and sub.get_message_definition() or rosapi.get_message_definition(msg_or_type)
+        return sub and sub.get_message_definition() or api.get_message_definition(msg_or_type)
 
     def get_message_type_hash(self, msg_or_type):
         """Returns ROS message type MD5 hash."""
-        if rosapi.is_ros_message(msg_or_type):
-            return rosapi.get_message_type_hash(msg_or_type)
+        if api.is_ros_message(msg_or_type):
+            return api.get_message_type_hash(msg_or_type)
         sub = next((s for (t, n, h), s in self._subs.items() if n == msg_or_type), None)
-        return sub and sub.get_message_type_hash() or rosapi.get_message_type_hash(msg_or_type)
+        return sub and sub.get_message_type_hash() or api.get_message_type_hash(msg_or_type)
 
     def format_meta(self):
         """Returns source metainfo string."""
         metadata = self.get_meta()
-        result = "\nROS%s live" % os.getenv("ROS_VERSION")
+        result = "\nROS%s live" % api.ROS_VERSION
         if "ROS_MASTER_URI" in metadata:
             result += ", ROS master %s" % metadata["ROS_MASTER_URI"]
         if "ROS_DOMAIN_ID" in metadata:
             result += ", ROS domain ID %s" % metadata["ROS_DOMAIN_ID"]
-        result += ", %s initially" % plural("topic", metadata["tcount"])
+        result += ", %s initially" % common.plural("topic", metadata["tcount"])
         return result
 
-    def is_processable(self, topic, index, stamp, msg):
-        """Returns whether specified message in topic is in acceptable range."""
-        if self.args.START_INDEX:
+    def is_processable(self, topic, msg, stamp, index=None):
+        """Returns whether message passes source filters."""
+        if self.args.START_INDEX and index is not None:
             if max(0, self.args.START_INDEX) >= index:
                 return False
-        if self.args.END_INDEX:
+        if self.args.END_INDEX and index is not None:
             if 0 < self.args.END_INDEX < index:
                 return False
-        if not super(TopicSource, self).is_processable(topic, index, stamp, msg):
+        if not super(TopicSource, self).is_processable(topic, msg, stamp, index):
             return False
-        return ConditionMixin.is_processable(self, topic, index, stamp, msg)
+        return ConditionMixin.is_processable(self, topic, msg, stamp, index)
 
     def refresh_topics(self):
         """Refreshes topics and subscriptions from ROS live."""
-        for topic, typename in rosapi.get_topic_types():
-            dct = filter_dict({topic: [typename]}, self.args.TOPICS, self.args.TYPES)
-            if not filter_dict(dct, self.args.SKIP_TOPICS, self.args.SKIP_TYPES, reverse=True):
+        for topic, typename in api.get_topic_types():
+            dct = common.filter_dict({topic: [typename]}, self.args.TOPIC, self.args.TYPE)
+            if not common.filter_dict(dct, self.args.SKIP_TOPIC, self.args.SKIP_TYPE, reverse=True):
                 continue  # for topic, typename
-            try: rosapi.get_message_class(typename)  # Raises error in ROS2
-            except Exception as e:
-                ConsolePrinter.warn("Error loading type %s in topic %s: %%s" %
-                                    (typename, topic), e, __once=True)
+            if api.get_message_class(typename) is None:
+                msg = "Error loading type %s in topic %s." % (typename, topic)
+                if self.args.STOP_ON_ERROR: raise Exception(msg)
+                ConsolePrinter.warn(msg, __once=True)
                 continue  # for topic, typename
             topickey = (topic, typename, None)
             if topickey in self.topics:
                 continue  # for topic, typename
 
             handler = functools.partial(self._on_message, topic)
             try:
-                sub = rosapi.create_subscriber(topic, typename, handler,
-                                               queue_size=self.args.QUEUE_SIZE_IN)
+                sub = api.create_subscriber(topic, typename, handler,
+                                            queue_size=self.args.QUEUE_SIZE_IN)
             except Exception as e:
                 ConsolePrinter.warn("Error subscribing to topic %s: %%r" % topic,
                                     e, __once=True)
+                if self.args.STOP_ON_ERROR: raise
                 continue  # for topic, typename
             self._subs[topickey] = sub
             self.topics[topickey] = None
 
     def _init_progress(self):
         """Initializes progress bar, if any."""
         if self.args.PROGRESS and not self.bar:
-            self.bar = ProgressBar(afterword="ROS%s live" % os.getenv("ROS_VERSION"),
-                                   aftertemplate=" {afterword}", pulse=True)
+            self.bar = common.ProgressBar(afterword="ROS%s live" % api.ROS_VERSION,
+                                          aftertemplate=" {afterword}", pulse=True)
             self.bar.start()
 
     def _update_progress(self, count, running=True):
         """Updates progress bar, if any."""
         if self.bar:
-            afterword = "ROS%s live, %s" % (os.getenv("ROS_VERSION"), plural("message", count))
+            afterword = "ROS%s live, %s" % (api.ROS_VERSION, common.plural("message", count))
             self.bar.afterword, self.bar.max = afterword, count
             if not running:
                 self.bar.pause, self.bar.pulse_pos = True, None
             self.bar.update(count)
 
     def _configure(self):
         """Adjusts start/end time filter values to current time."""
         if self.args.START_TIME is not None:
-            self.args.START_TIME = rosapi.make_live_time(self.args.START_TIME)
+            self.args.START_TIME = api.make_live_time(self.args.START_TIME)
         if self.args.END_TIME is not None:
-            self.args.END_TIME = rosapi.make_live_time(self.args.END_TIME)
+            self.args.END_TIME = api.make_live_time(self.args.END_TIME)
 
     def _run_refresh(self):
         """Periodically refreshes topics and subscriptions from ROS live."""
         time.sleep(self.MASTER_INTERVAL)
         while self._running:
             try: self.refresh_topics()
             except Exception as e: self.thread_excepthook("Error refreshing live topics: %r" % e, e)
             time.sleep(self.MASTER_INTERVAL)
 
     def _on_message(self, topic, msg):
         """Subscription callback handler, queues message for yielding."""
-        stamp = rosapi.get_rostime() if self.args.ROS_TIME_IN else \
-                rosapi.make_time(time.time())
+        stamp = api.get_rostime() if self.args.ROS_TIME_IN else api.make_time(time.time())
         self._queue and self._queue.put((topic, msg, stamp))
+
+
+class AppSource(Source, ConditionMixin):
+    """Produces messages from iterable or pushed data."""
+
+    ## Constructor argument defaults
+    DEFAULT_ARGS = dict(TOPIC=(), TYPE=(), SKIP_TOPIC=(), SKIP_TYPE=(), START_TIME=None,
+                        END_TIME=None, START_INDEX=None, END_INDEX=None, UNIQUE=False,
+                        SELECT_FIELD=(), NOSELECT_FIELD=(), NTH_MESSAGE=1, NTH_INTERVAL=0,
+                        CONDITION=(), ITERABLE=None)
+
+    def __init__(self, args=None, **kwargs):
+        """
+        @param   args                  arguments as namespace or dictionary, case-insensitive;
+                                       or iterable yielding messages
+        @param   args.topic            ROS topics to read if not all
+        @param   args.type             ROS message types to read if not all
+        @param   args.skip_topic       ROS topics to skip
+        @param   args.skip_type        ROS message types to skip
+        @param   args.start_time       earliest timestamp of messages to read
+        @param   args.end_time         latest timestamp of messages to read
+        @param   args.start_index      message index within topic to start from
+        @param   args.end_index        message index within topic to stop at
+        @param   args.unique           emit messages that are unique in topic
+        @param   args.select_field     message fields to use for uniqueness if not all
+        @param   args.noselect_field   message fields to skip for uniqueness
+        @param   args.nth_message      read every Nth message in topic
+        @param   args.nth_interval     minimum time interval between messages in topic
+        @param   args.condition        Python expressions that must evaluate as true
+                                       for message to be processable, see ConditionMixin
+        @param   args.iterable         iterable yielding (topic, msg, stamp) or (topic, msg);
+                                       yielding `None` signals end of content
+        @param   kwargs                any and all arguments as keyword overrides, case-insensitive
+        """
+        if common.is_iterable(args) and not isinstance(args, dict):
+            args = ensure_namespace(None, iterable=args)
+        args = ensure_namespace(args, AppSource.DEFAULT_ARGS, **kwargs)
+        super(AppSource, self).__init__(args)
+        ConditionMixin.__init__(self, args)
+        self._queue = queue.Queue()  # [(topic, msg, ROS time)]
+        self._reading = False
+
+        self._configure()
+
+    def read(self):
+        """
+        Yields messages from iterable or pushed data, as (topic, msg, ROS timestamp).
+
+        Blocks until a message is available, or source is closed.
+        """
+        def generate(iterable):
+            for x in iterable: yield x
+        feeder = generate(self.args.ITERABLE) if self.args.ITERABLE else None
+        self._reading = True
+        while self._reading:
+            item = self._queue.get() if not feeder or self._queue.qsize() else next(feeder, None)
+            if item is None: break  # while
+
+            if len(item) > 2: topic, msg, stamp = item[:3]
+            else: (topic, msg), stamp = item[:2], api.get_rostime(fallback=True)
+            topickey = api.TypeMeta.make(msg, topic, self).topickey
+            self._counts[topickey] += 1
+            self.conditions_register_message(topic, msg)
+            if self.is_conditions_topic(topic, pure=True): continue  # while
+
+            if not self.preprocess \
+            or self.is_processable(topic, msg, stamp, self._counts[topickey]):
+                yield self.SourceMessage(topic, msg, stamp)
+            if self.args.NTH_MESSAGE > 1 or self.args.NTH_INTERVAL > 0:
+                self._processables[topickey] = (self._counts[topickey], stamp)
+        self._reading = False
+
+    def close(self):
+        """Closes current read() yielding, if any."""
+        if self._reading:
+            self._reading = False
+            self._queue.put(None)
+
+    def read_queue(self):
+        """
+        Returns (topic, msg, stamp) from push queue, or `None` if no queue
+        or message in queue is condition topic only.
+        """
+        item = None
+        try: item = self._queue.get(block=False)
+        except queue.Empty: pass
+        if item is None: return None
+
+        topic, msg, stamp = item
+        topickey = api.TypeMeta.make(msg, topic, self).topickey
+        self._counts[topickey] += 1
+        self.conditions_register_message(topic, msg)
+        return None if self.is_conditions_topic(topic, pure=True) else (topic, msg, stamp)
+
+    def mark_queue(self, topic, msg, stamp):
+        """Registers message produced from read_queue()."""
+        if self.args.NTH_MESSAGE > 1 or self.args.NTH_INTERVAL > 0:
+            topickey = api.TypeMeta.make(msg, topic).topickey
+            self._processables[topickey] = (self._counts[topickey], stamp)
+
+    def push(self, topic, msg=None, stamp=None):
+        """
+        Pushes a message to be yielded from read().
+
+        @param   topic  topic name, or `None` to signal end of content
+        @param   msg    ROS message
+        @param   stamp  message ROS timestamp, defaults to current wall time if `None`
+        """
+        if topic is None: self._queue.put(None)
+        else: self._queue.put((topic, msg, stamp or api.get_rostime(fallback=True)))
+
+    def is_processable(self, topic, msg, stamp, index=None):
+        """Returns whether message passes source filters."""
+        dct = common.filter_dict({topic: [api.get_message_type(msg)]},
+                                 self.args.TOPIC, self.args.TYPE)
+        if not common.filter_dict(dct, self.args.SKIP_TOPIC, self.args.SKIP_TYPE, reverse=True):
+            return False
+        if self.args.START_INDEX and index is not None:
+            if max(0, self.args.START_INDEX) >= index:
+                return False
+        if self.args.END_INDEX and index is not None:
+            if 0 < self.args.END_INDEX < index:
+                return False
+        if not super(AppSource, self).is_processable(topic, msg, stamp, index):
+            return False
+        return ConditionMixin.is_processable(self, topic, msg, stamp, index)
+
+    def _configure(self):
+        """Adjusts start/end time filter values to current time."""
+        if self.args.START_TIME is not None:
+            self.args.START_TIME = api.make_live_time(self.args.START_TIME)
+        if self.args.END_TIME is not None:
+            self.args.END_TIME = api.make_live_time(self.args.END_TIME)
+
+
+__all__ = ["AppSource", "BagSource", "ConditionMixin", "Source", "TopicSource"]
```

## grepros/main.py

```diff
@@ -4,27 +4,31 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     23.10.2021
-@modified    14.10.2022
+@modified    05.07.2023
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.main
 import argparse
 import atexit
 import collections
+import logging
 import os
+import random
 import re
 import sys
 
-from . import __version__, inputs, outputs, search
-from . common import ConsolePrinter, parse_datetime
+import six
+
+from . import __title__, __version__, __version_date__, api, inputs, outputs, search
+from . common import ConsolePrinter, MatchMarkers, parse_datetime
 from . import plugins
 
 
 
 ## Configuration for argparse, as {description, epilog, args: [..], groups: {name: [..]}}
 ARGUMENTS = {
     "description": "Searches through messages in ROS bag files or live topics.",
@@ -33,76 +37,77 @@
 * wildcards use simple globbing as zero or more characters,
 target matches if any value matches.
  
 
 Example usage:
 
 Search for "my text" in all bags under current directory and subdirectories:
-    grepros -r "my text"
+    %(title)s -r "my text"
 
 Print 30 lines of the first message from each live ROS topic:
-    grepros --max-per-topic 1 --lines-per-message 30 --live
+    %(title)s --max-per-topic 1 --lines-per-message 30 --live
 
 Find first message containing "future" (case-sensitive) in my.bag:
-    grepros future -I --max-count 1 --name my.bag
+    %(title)s future -I --max-count 1 --name my.bag
 
 Find 10 messages, from geometry_msgs package, in "map" frame,
 from bags in current directory, reindexing any unindexed bags:
-    grepros frame_id=map --type geometry_msgs/* --max-count 10  --reindex-if-unindexed
+    %(title)s frame_id=map --type geometry_msgs/* --max-count 10  --reindex-if-unindexed
 
 Pipe all diagnostics messages with "CPU usage" from live ROS topics to my.bag:
-    grepros "CPU usage" --type *DiagnosticArray --no-console-output --write my.bag
+    %(title)s "CPU usage" --type *DiagnosticArray --no-console-output --write my.bag
 
 Find messages with field "key" containing "0xA002",
 in topics ending with "diagnostics", in bags under "/tmp":
-    grepros key=0xA002 --topic *diagnostics --path /tmp
+    %(title)s key=0xA002 --topic *diagnostics --path /tmp
 
 Find diagnostics_msgs messages in bags in current directory,
 containing "navigation" in fields "name" or "message",
 print only header stamp and values:
-    grepros --type diagnostic_msgs/* --select-field name message \\
-            --print-field header.stamp status.values -- navigation
+    %(title)s --type diagnostic_msgs/* --select-field name message \\
+            --emit-field header.stamp status.values -- navigation
 
-Print first message from each lidar topic on host 1.2.3.4:
+Print first message from each lidar topic on host 1.2.3.4, without highlight:
     ROS_MASTER_URI=http://1.2.3.4::11311 \\
-    grepros --live --topic *lidar* --max-per-topic 1
+    %(title)s --live --topic *lidar* --max-per-topic 1 --no-highlight
 
 Export all bag messages to SQLite and Postgres, print only export progress:
-    grepros -n my.bag --write my.bag.sqlite --no-console-output --no-verbose --progress
+    %(title)s -n my.bag --write my.bag.sqlite --no-console-output --no-verbose --progress
 
-    grepros -n my.bag --write postgresql://user@host/dbname \\
+    %(title)s -n my.bag --write postgresql://user@host/dbname \\
             --no-console-output --no-verbose --progress
-    """,
+    """ % dict(title=__title__),
 
     "arguments": [
-        dict(args=["PATTERNS"], nargs="*", metavar="PATTERN",
+        dict(args=["PATTERN"], nargs="*", default=[],
              help="pattern(s) to find in message field values,\n"
                   "all messages match if not given,\n"
                   "can specify message field as NAME=PATTERN\n"
                   "(supports nested.paths and * wildcards)"),
 
         dict(args=["-h", "--help"],
              dest="HELP", action="store_true",
              help="show this help message and exit"),
 
         dict(args=["-F", "--fixed-strings"],
-             dest="RAW", action="store_true",
+             dest="FIXED_STRING", action="store_true",
              help="PATTERNs are ordinary strings, not regular expressions"),
 
         dict(args=["-I", "--no-ignore-case"],
              dest="CASE", action="store_true",
              help="use case-sensitive matching in PATTERNs"),
 
         dict(args=["-v", "--invert-match"],
              dest="INVERT", action="store_true",
-             help="select non-matching messages"),
+             help="select messages not matching PATTERN"),
 
         dict(args=["--version"],
              dest="VERSION", action="version",
-             version="grepros (grep for ROS bag files and live topics) %s" % __version__,
+             version="%s: grep for ROS bag files and live topics, v%s (%s)" %
+                     (__title__, __version__, __version_date__),
              help="display version information and exit"),
 
         dict(args=["--live"],
              dest="LIVE", action="store_true",
              help="read messages from live ROS topics instead of bagfiles"),
 
         dict(args=["--publish"],
@@ -117,57 +122,61 @@
                   "Bag or database will be appended to if it already exists.\n"
                   "Keyword arguments are given to output writer."),
 
         dict(args=["--write-options"],  # Will be populated from --write by MultiSink
              dest="WRITE_OPTIONS", default=argparse.SUPPRESS, help=argparse.SUPPRESS),
 
         dict(args=["--plugin"],
-             dest="PLUGINS", metavar="PLUGIN", nargs="+", default=[], action="append",
+             dest="PLUGIN", nargs="+", default=[], action="append",
              help="load a Python module or class as plugin"),
+
+        dict(args=["--stop-on-error"],
+             dest="STOP_ON_ERROR", action="store_true",
+             help="stop further execution on any error like unknown message type"),
     ],
 
     "groups": {"Filtering": [
 
         dict(args=["-t", "--topic"],
-             dest="TOPICS", metavar="TOPIC", nargs="+", default=[], action="append",
-             help="ROS topics to scan if not all (supports * wildcards)"),
+             dest="TOPIC", nargs="+", default=[], action="append",
+             help="ROS topics to read if not all (supports * wildcards)"),
 
         dict(args=["-nt", "--no-topic"],
-             dest="SKIP_TOPICS", metavar="TOPIC", nargs="+", default=[], action="append",
+             dest="SKIP_TOPIC", metavar="TOPIC", nargs="+", default=[], action="append",
              help="ROS topics to skip (supports * wildcards)"),
 
         dict(args=["-d", "--type"],
-             dest="TYPES", metavar="TYPE", nargs="+", default=[], action="append",
-             help="ROS message types to scan if not all (supports * wildcards)"),
+             dest="TYPE", nargs="+", default=[], action="append",
+             help="ROS message types to read if not all (supports * wildcards)"),
 
         dict(args=["-nd", "--no-type"],
-             dest="SKIP_TYPES", metavar="TYPE", nargs="+", default=[], action="append",
+             dest="SKIP_TYPE", metavar="TYPE", nargs="+", default=[], action="append",
              help="ROS message types to skip (supports * wildcards)"),
 
         dict(args=["--condition"],
-             dest="CONDITIONS", metavar="CONDITION", nargs="+", default=[], action="append",
+             dest="CONDITION", nargs="+", default=[], action="append",
              help="extra conditions to require for matching messages,\n"
                   "as ordinary Python expressions, can refer to last messages\n"
-                  "in topics as {topic /my/topic}; topic name can contain wildcards.\n"
-                  'E.g. --condition "{topic /robot/enabled}.data" matches\n'
+                  "in topics as <topic /my/topic>; topic name can contain wildcards.\n"
+                  'E.g. --condition "<topic /robot/enabled>.data" matches\n'
                   "messages only while last message in '/robot/enabled' has data=true."),
 
         dict(args=["-t0", "--start-time"],
              dest="START_TIME", metavar="TIME",
-             help="earliest timestamp of messages to scan\n"
+             help="earliest timestamp of messages to read\n"
                   "as relative seconds if signed,\n"
                   "or epoch timestamp or ISO datetime\n"
                   "(for bag input, relative to bag start time\n"
                   "if positive or end time if negative,\n"
                   "for live input relative to system time,\n"
                   "datetime may be partial like 2021-10-14T12)"),
 
         dict(args=["-t1", "--end-time"],
              dest="END_TIME", metavar="TIME",
-             help="latest timestamp of messages to scan\n"
+             help="latest timestamp of messages to read\n"
                   "as relative seconds if signed,\n"
                   "or epoch timestamp or ISO datetime\n"
                   "(for bag input, relative to bag start time\n"
                   "if positive or end time if negative,\n"
                   "for live input relative to system time,\n"
                   "datetime may be partial like 2021-10-14T12)"),
 
@@ -179,40 +188,40 @@
         dict(args=["-n1", "--end-index"],
              dest="END_INDEX", metavar="INDEX", type=int,
              help="message index within topic to stop at\n"
                   "(1-based if positive, counts back from bag total if negative)"),
 
         dict(args=["--every-nth-message"],
              dest="NTH_MESSAGE", metavar="NUM", type=int, default=1,
-             help="scan every Nth message within topic"),
+             help="read every Nth message within topic"),
 
         dict(args=["--every-nth-interval"],
              dest="NTH_INTERVAL", metavar="SECONDS", type=int, default=0,
-             help="scan messages at least N seconds apart within topic"),
+             help="read messages at least N seconds apart within topic"),
 
         dict(args=["--every-nth-match"],
              dest="NTH_MATCH", metavar="NUM", type=int, default=1,
              help="emit every Nth match in topic"),
 
         dict(args=["-sf", "--select-field"],
-             dest="SELECT_FIELDS", metavar="FIELD", nargs="+", default=[], action="append",
+             dest="SELECT_FIELD", metavar="FIELD", nargs="+", default=[], action="append",
              help="message fields to use in matching if not all\n"
                   "(supports nested.paths and * wildcards)"),
 
         dict(args=["-ns", "--no-select-field"],
-             dest="NOSELECT_FIELDS", metavar="FIELD", nargs="+", default=[], action="append",
+             dest="NOSELECT_FIELD", metavar="FIELD", nargs="+", default=[], action="append",
              help="message fields to skip in matching\n"
                   "(supports nested.paths and * wildcards)"),
 
         dict(args=["-m", "--max-count"],
-             dest="MAX_MATCHES", metavar="NUM", default=0, type=int,
+             dest="MAX_COUNT", metavar="NUM", default=0, type=int,
              help="number of matched messages to emit (per each file if bag input)"),
 
         dict(args=["--max-per-topic"],
-             dest="MAX_TOPIC_MATCHES", metavar="NUM", default=0, type=int,
+             dest="MAX_PER_TOPIC", metavar="NUM", default=0, type=int,
              help="number of matched messages to emit from each topic\n"
                   "(per each file if bag input)"),
 
         dict(args=["--max-topics"],
              dest="MAX_TOPICS", metavar="NUM", default=0, type=int,
              help="number of topics to emit matches from (per each file if bag input)"),
 
@@ -233,73 +242,76 @@
              help="emit NUM messages of trailing context after match"),
 
         dict(args=["-C", "--context"],
              dest="CONTEXT", metavar="NUM", default=0, type=int,
              help="emit NUM messages of leading and trailing context\n"
                   "around match"),
 
-        dict(args=["-pf", "--print-field"],
-             dest="PRINT_FIELDS", metavar="FIELD", nargs="+", default=[], action="append",
-             help="message fields to print in console output if not all\n"
+        dict(args=["-ef", "--emit-field"],
+             dest="EMIT_FIELD", metavar="FIELD", nargs="+", default=[], action="append",
+             help="message fields to emit in console output if not all\n"
                   "(supports nested.paths and * wildcards)"),
 
-        dict(args=["-np", "--no-print-field"],
-             dest="NOPRINT_FIELDS", metavar="FIELD", nargs="+", default=[], action="append",
+        dict(args=["-nf", "--no-emit-field"],
+             dest="NOEMIT_FIELD", metavar="FIELD", nargs="+", default=[], action="append",
              help="message fields to skip in console output\n"
                   "(supports nested.paths and * wildcards)"),
 
         dict(args=["-mo", "--matched-fields-only"],
              dest="MATCHED_FIELDS_ONLY", action="store_true",
-             help="print only the fields where PATTERNs find a match"),
+             help="emit only the fields where PATTERNs find a match in console output"),
 
         dict(args=["-la", "--lines-around-match"],
              dest="LINES_AROUND_MATCH", metavar="NUM", type=int,
-             help="print only matched fields and NUM message lines\n"
-                  "around match"),
+             help="emit only matched fields and NUM message lines\n"
+                  "around match in console output"),
 
         dict(args=["-lf", "--lines-per-field"],
              dest="MAX_FIELD_LINES", metavar="NUM", type=int,
-             help="maximum number of lines to print per field"),
+             help="maximum number of lines to emit per field in console output"),
 
         dict(args=["-l0", "--start-line"],
              dest="START_LINE", metavar="NUM", type=int,
-             help="message line number to start printing from\n"
+             help="message line number to start emitting from in console output\n"
                   "(1-based if positive, counts back from total if negative)"),
 
         dict(args=["-l1", "--end-line"],
              dest="END_LINE", metavar="NUM", type=int,
-             help="message line number to stop printing at\n"
+             help="message line number to stop emitting at in console output\n"
                   "(1-based if positive, counts back from total if negative)"),
 
         dict(args=["-lm", "--lines-per-message"],
              dest="MAX_MESSAGE_LINES", metavar="NUM", type=int,
-             help="maximum number of lines to print per message"),
+             help="maximum number of lines to emit per message in console output"),
 
         dict(args=["--match-wrapper"],
              dest="MATCH_WRAPPER", metavar="STR", nargs="*",
-             help="string to wrap around matched values,\n"
+             help="string to wrap around matched values in console output,\n"
                   "both sides if one value, start and end if more than one,\n"
                   "or no wrapping if zero values\n"
                   '(default "**" in colorless output)'),
 
         dict(args=["--wrap-width"],
              dest="WRAP_WIDTH", metavar="NUM", type=int,
-             help="character width to wrap message YAML output at,\n"
+             help="character width to wrap message YAML console output at,\n"
                   "0 disables (defaults to detected terminal width)"),
 
         dict(args=["--color"], dest="COLOR",
              choices=["auto", "always", "never"], default="always",
              help='use color output in console (default "always")'),
 
         dict(args=["--no-meta"], dest="META", action="store_false",
              help="do not print source and message metainfo to console"),
 
         dict(args=["--no-filename"], dest="LINE_PREFIX", action="store_false",
              help="do not print bag filename prefix on each console message line"),
 
+        dict(args=["--no-highlight"], dest="HIGHLIGHT", action="store_false",
+             help="do not highlight matched values"),
+
         dict(args=["--no-console-output"], dest="CONSOLE", action="store_false",
              help="do not print matches to console"),
 
         dict(args=["--progress"], dest="PROGRESS", action="store_true",
              help="show progress bar when not printing matches to console"),
 
         dict(args=["--verbose"], dest="VERBOSE", action="store_true",
@@ -309,20 +321,20 @@
         dict(args=["--no-verbose"], dest="SKIP_VERBOSE", action="store_true",
              help="do not print status messages during console output\n"
                   "for publishing and writing"),
 
     ], "Bag input control": [
 
         dict(args=["-n", "--filename"],
-             dest="FILES", metavar="FILE", nargs="+", default=[], action="append",
-             help="names of ROS bagfiles to scan if not all in directory\n"
+             dest="FILE", nargs="+", default=[], action="append",
+             help="names of ROS bagfiles to read if not all in directory\n"
                   "(supports * wildcards)"),
 
         dict(args=["-p", "--path"],
-             dest="PATHS", metavar="PATH", nargs="+", default=[], action="append",
+             dest="PATH", nargs="+", default=[], action="append",
              help="paths to scan if not current directory\n"
                   "(supports * wildcards)"),
 
         dict(args=["-r", "--recursive"],
              dest="RECURSE", action="store_true",
              help="recurse into subdirectories when looking for bagfiles"),
 
@@ -365,14 +377,17 @@
              dest="ROS_TIME_IN", action="store_true",
              help="use ROS time instead of system time for incoming message\n"
                   "timestamps from subsribed live ROS topics"),
 
     ]},
 }
 
+## List of command-line arguments the program was invoked with
+CLI_ARGS = None
+
 
 class HelpFormatter(argparse.RawTextHelpFormatter):
     """RawTextHelpFormatter returning custom metavar for WRITE."""
 
     def _format_action_invocation(self, action):
         """Returns formatted invocation."""
         if "WRITE" == action.dest:
@@ -392,41 +407,49 @@
         for arg in map(dict, groupargs):
             grouper.add_argument(*arg.pop("args"), **arg)
     return argparser
 
 
 def process_args(args):
     """
-    Converts or combines arguments where necessary, returns args.
+    Converts or combines arguments where necessary, returns full args.
 
     @param   args  arguments object like argparse.Namespace
     """
+    for arg in sum(ARGUMENTS.get("groups", {}).values(), ARGUMENTS["arguments"][:]):
+        name = arg.get("dest") or arg["args"][0]
+        if "version" != arg.get("action") and argparse.SUPPRESS != arg.get("default") \
+        and "HELP" != name and not hasattr(args, name):
+            value = False if arg.get("store_true") else True if arg.get("store_false") else None
+            setattr(args, name, arg.get("default", value))
+
     if args.CONTEXT:
         args.BEFORE = args.AFTER = args.CONTEXT
 
     # Default to printing metadata for publish/write if no console output
-    args.VERBOSE = False if args.SKIP_VERBOSE else (args.VERBOSE or not args.CONSOLE)
+    args.VERBOSE = False if args.SKIP_VERBOSE else \
+                   (args.VERBOSE or not args.CONSOLE and bool(CLI_ARGS))
 
     # Show progress bar only if no console output
     args.PROGRESS = args.PROGRESS and not args.CONSOLE
 
     # Print filename prefix on each console message line if not single specific file
-    args.LINE_PREFIX = args.LINE_PREFIX and (args.RECURSE or len(args.FILES) != 1
-                                             or args.PATHS or any("*" in x for x in args.FILES))
+    args.LINE_PREFIX = args.LINE_PREFIX and (args.RECURSE or len(args.FILE) != 1
+                                             or args.PATH or any("*" in x for x in args.FILE))
 
     for k, v in vars(args).items():  # Flatten lists of lists and drop duplicates
         if k != "WRITE" and isinstance(v, list):
             here = set()
             setattr(args, k, [x for xx in v for x in (xx if isinstance(xx, list) else [xx])
                               if not (x in here or here.add(x))])
 
     for n, v in [("START_TIME", args.START_TIME), ("END_TIME", args.END_TIME)]:
-        if v is None: continue  # for v, n
+        if not isinstance(v, (six.binary_type, six.text_type)): continue  # for v, n
         try: v = float(v)
-        except Exception: pass
+        except Exception: pass  # If numeric, leave as string for source to process as relative time
         try: not isinstance(v, float) and setattr(args, n, parse_datetime(v))
         except Exception: pass
 
     return  args
 
 
 def validate_args(args):
@@ -451,70 +474,73 @@
         if v is None: continue  # for v, n
         try: v = float(v)
         except Exception: pass
         try: not isinstance(v, float) and setattr(args, n, parse_datetime(v))
         except Exception: errors[""].append("Invalid ISO datetime for %s: %s" %
                                             (n.lower().replace("_", " "), v))
 
-    for v in args.PATTERNS if not args.RAW else ():
+    for v in args.PATTERN if not args.FIXED_STRING else ():
         split = v.find("=", 1, -1)  # May be "PATTERN" or "attribute=PATTERN"
         v = v[split + 1:] if split > 0 else v
-        try: re.compile(re.escape(v) if args.RAW else v)
+        try: re.compile(re.escape(v) if args.FIXED_STRING else v)
         except Exception as e:
             errors["Invalid regular expression"].append("'%s': %s" % (v, e))
 
-    for v in args.CONDITIONS:
+    for v in args.CONDITION:
         v = inputs.ConditionMixin.TOPIC_RGX.sub("dummy", v)
         try: compile(v, "", "eval")
         except SyntaxError as e:
             errors["Invalid condition"].append("'%s': %s at %schar %s" %
                 (v, e.msg, "line %s " % e.lineno if e.lineno > 1 else "", e.offset))
         except Exception as e:
             errors["Invalid condition"].append("'%s': %s" % (v, e))
 
     for err in errors.get("", []):
-        ConsolePrinter.error(err)
+        ConsolePrinter.log(logging.ERROR, err)
     for category in filter(bool, errors):
-        ConsolePrinter.error(category)
+        ConsolePrinter.log(logging.ERROR, category)
         for err in errors[category]:
-            ConsolePrinter.error("  %s" % err)
+            ConsolePrinter.log(logging.ERROR, "  %s" % err)
     return not errors
 
 
 def flush_stdout():
     """Writes a linefeed to sdtout if nothing has been printed to it so far."""
     if not ConsolePrinter.PRINTS.get(sys.stdout) and not sys.stdout.isatty():
         try: print()  # Piping cursed output to `more` remains paging if nothing is printed
         except (Exception, KeyboardInterrupt): pass
 
 
 def preload_plugins():
     """Imports and initializes plugins from auto-load folder and from arguments."""
     plugins.add_write_format("bag", outputs.BagSink, "bag", [
-        ("overwrite=true|false",   "overwrite existing file in bag output\n"
+        ("overwrite=true|false",   "overwrite existing file\nin bag output\n"
                                    "instead of appending to if bag or database\n"
                                    "or appending unique counter to file name\n"
                                    "(default false)")
 
     ])
-    args = make_parser().parse_known_args()[0] if "--plugin" in sys.argv else None
+    args = make_parser().parse_known_args(CLI_ARGS)[0] if "--plugin" in CLI_ARGS else None
     try: plugins.init(process_args(args) if args else None)
     except ImportWarning: sys.exit(1)
 
 
 def run():
     """Parses command-line arguments and runs search."""
+    global CLI_ARGS
+    CLI_ARGS = sys.argv[1:]
+    MatchMarkers.populate("%08x" % random.randint(1, 1E9))
     preload_plugins()
     argparser = make_parser()
-    if len(sys.argv) < 2:
+    if not CLI_ARGS:
         argparser.print_usage()
         return
 
     atexit.register(flush_stdout)
-    args, _ = argparser.parse_known_args()
+    args, _ = argparser.parse_known_args(CLI_ARGS)
     if args.HELP:
         argparser.print_help()
         return
 
     BREAK_EXS = (KeyboardInterrupt, )
     try: BREAK_EXS += (BrokenPipeError, )  # Py3
     except NameError: pass  # Py2
@@ -532,25 +558,33 @@
         sink = outputs.MultiSink(args)
         sink.sinks.extend(filter(bool, plugins.load("sink", args, collect=True)))
         if not sink.validate():
             sys.exit(1)
 
         thread_excepthook = lambda t, e: (ConsolePrinter.error(t), sys.exit(1))
         source.thread_excepthook = sink.thread_excepthook = thread_excepthook
-        searcher = plugins.load("search", args) or search.Searcher(args)
-        searcher.search(source, sink)
+        grepper = plugins.load("scan", args) or search.Scanner(args)
+        grepper.work(source, sink)
     except BREAK_EXS:
         try: source and source.close()
         except (Exception, KeyboardInterrupt): pass
         try: sink and sink.close()
         except (Exception, KeyboardInterrupt): pass
         # Redirect remaining output to devnull to avoid another BrokenPipeError
         try: os.dup2(os.open(os.devnull, os.O_WRONLY), sys.stdout.fileno())
         except (Exception, KeyboardInterrupt): pass
         sys.exit()
     finally:
         sink and sink.close()
         source and source.close()
+        api.shutdown_node()
+
+
+__all__ = [
+    "ARGUMENTS", "CLI_ARGS", "HelpFormatter",
+    "make_parser", "process_args", "validate_args", "flush_stdout", "preload_plugins", "run",
+]
+
 
 
 if "__main__" == __name__:
     run()
```

## grepros/outputs.py

```diff
@@ -1,80 +1,97 @@
 # -*- coding: utf-8 -*-
 """
-Main outputs for search results.
+Main outputs for emitting messages.
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     23.10.2021
-@modified    16.10.2022
+@modified    03.07.2023
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.outputs
 from __future__ import print_function
 import atexit
 import collections
-import copy
 import os
 import re
 import sys
 
+import six
 import yaml
 
-from . common import ConsolePrinter, MatchMarkers, TextWrapper, filter_fields, \
-                     format_bytes, makedirs, merge_spans, plural, wildcard_to_regex
-from . import rosapi
+from . import api
+from . import common
+from . common import ConsolePrinter, MatchMarkers
+from . inputs import Source
 
 
-class SinkBase(object):
+class Sink(object):
     """Output base class."""
 
     ## Auto-detection file extensions for subclasses, as (".ext", )
     FILE_EXTENSIONS = ()
 
-    def __init__(self, args):
+    ## Constructor argument defaults
+    DEFAULT_ARGS = dict(META=False)
+
+    def __init__(self, args=None, **kwargs):
         """
-        @param   args        arguments object like argparse.Namespace
-        @param   args.META   whether to print metainfo
+        @param   args        arguments as namespace or dictionary, case-insensitive
+        @param   args.meta   whether to emit metainfo
+        @param   kwargs      any and all arguments as keyword overrides, case-insensitive
         """
         self._batch_meta = {}  # {source batch: "source metadata"}
         self._counts     = {}  # {(topic, typename, typehash): count}
 
-        self.args = copy.deepcopy(args)
-        ## inputs.SourceBase instance bound to this sink
-        self.source = None
+        self.args = common.ensure_namespace(args, Sink.DEFAULT_ARGS, **kwargs)
+        ## Result of validate()
+        self.valid = None
+        ## inputs.Source instance bound to this sink
+        self.source = Source(self.args)
+
+    def __enter__(self):
+        """Context manager entry."""
+        return self
+
+    def __exit__(self, exc_type, exc_value, traceback):
+        """Context manager exit, closes sink."""
+        self.close()
 
     def emit_meta(self):
-        """Prints source metainfo like bag header as debug stream, if not already printed."""
+        """Outputs source metainfo like bag header as debug stream, if not already emitted."""
         batch = self.args.META and self.source.get_batch()
         if self.args.META and batch not in self._batch_meta:
             meta = self._batch_meta[batch] = self.source.format_meta()
             meta and ConsolePrinter.debug(meta)
 
-    def emit(self, topic, index, stamp, msg, match):
+    def emit(self, topic, msg, stamp=None, match=None, index=None):
         """
         Outputs ROS message.
 
         @param   topic  full name of ROS topic the message is from
-        @param   index  message index in topic
         @param   msg    ROS message
+        @param   stamp  message ROS timestamp, if not current ROS time
         @param   match  ROS message with values tagged with match markers if matched, else None
+        @param   index  message index in topic, if any
         """
-        topickey = rosapi.TypeMeta.make(msg, topic).topickey
+        topickey = api.TypeMeta.make(msg, topic).topickey
         self._counts[topickey] = self._counts.get(topickey, 0) + 1
 
     def bind(self, source):
         """Attaches source to sink."""
         self.source = source
 
     def validate(self):
         """Returns whether sink prerequisites are met (like ROS environment set if TopicSink)."""
-        return True
+        if self.valid is None: self.valid = True
+        return self.valid
 
     def close(self):
         """Shuts down output, closing any files or connections."""
         self._batch_meta.clear()
         self._counts.clear()
 
     def flush(self):
@@ -90,52 +107,66 @@
 
     @classmethod
     def autodetect(cls, target):
         """Returns true if target is recognizable as output for this sink class."""
         ext = os.path.splitext(target or "")[-1].lower()
         return ext in cls.FILE_EXTENSIONS
 
+    def _ensure_stamp_index(self, topic, msg, stamp=None, index=None):
+        """Returns (stamp, index) populated with current ROS time and topic index if `None`."""
+        if stamp is None: stamp = api.get_rostime(fallback=True)
+        if index is None: index = self._counts.get(api.TypeMeta.make(msg, topic).topickey, 0) + 1
+        return stamp, index
+
 
 class TextSinkMixin(object):
     """Provides message formatting as text."""
 
     ## Default highlight wrappers if not color output
     NOCOLOR_HIGHLIGHT_WRAPPERS = "**", "**"
 
-
-    def __init__(self, args):
-        """
-        @param   args                       arguments object like argparse.Namespace
-        @param   args.COLOR                 "never" for not using colors in replacements
-        @param   args.PRINT_FIELDS          message fields to use in output if not all
-        @param   args.NOPRINT_FIELDS        message fields to skip in output
-        @param   args.MAX_FIELD_LINES       maximum number of lines to output per field
-        @param   args.START_LINE            message line number to start output from
-        @param   args.END_LINE              message line number to stop output at
-        @param   args.MAX_MESSAGE_LINES     maximum number of lines to output per message
-        @param   args.LINES_AROUND_MATCH    number of message lines around matched fields to output
-        @param   args.MATCHED_FIELDS_ONLY   output only the fields where match was found
-        @param   args.WRAP_WIDTH            character width to wrap message YAML output at
-        @param   args.MATCH_WRAPPER         string to wrap around matched values,
+    ## Constructor argument defaults
+    DEFAULT_ARGS = dict(COLOR=True, EMIT_FIELD=(), NOEMIT_FIELD=(), HIGHLIGHT=True,
+                        MAX_FIELD_LINES=None, START_LINE=None, END_LINE=None,
+                        MAX_MESSAGE_LINES=None, LINES_AROUND_MATCH=None, MATCHED_FIELDS_ONLY=False,
+                        WRAP_WIDTH=None, MATCH_WRAPPER=None)
+
+    def __init__(self, args=None, **kwargs):
+        """
+        @param   args                       arguments as namespace or dictionary, case-insensitive
+        @param   args.color                 False or "never" for not using colors in replacements
+        @param   args.highlight             highlight matched values (default true)
+        @param   args.emit_field            message fields to emit if not all
+        @param   args.noemit_field          message fields to skip in output
+        @param   args.max_field_lines       maximum number of lines to output per field
+        @param   args.start_line            message line number to start output from
+        @param   args.end_line              message line number to stop output at
+        @param   args.max_message_lines     maximum number of lines to output per message
+        @param   args.lines_around_match    number of message lines around matched fields to output
+        @param   args.matched_fields_only   output only the fields where match was found
+        @param   args.wrap_width            character width to wrap message YAML output at
+        @param   args.match_wrapper         string to wrap around matched values,
                                             both sides if one value, start and end if more than one,
                                             or no wrapping if zero values
+        @param   kwargs                     any and all arguments as keyword overrides, case-insensitive
         """
         self._prefix       = ""    # Put before each message line (filename if grepping 1+ files)
         self._wrapper      = None  # TextWrapper instance
         self._patterns     = {}    # {key: [(() if any field else ('path', ), re.Pattern), ]}
         self._format_repls = {}    # {text to replace if highlight: replacement text}
         self._styles = collections.defaultdict(str)  # {label: ANSI code string}
 
-        self._configure(args)
+        self._configure(common.ensure_namespace(args, TextSinkMixin.DEFAULT_ARGS, **kwargs))
 
 
     def format_message(self, msg, highlight=False):
-        """Returns message as formatted string, optionally highlighted for matches."""
+        """Returns message as formatted string, optionally highlighted for matches if configured."""
         text = self.message_to_yaml(msg).rstrip("\n")
 
+        highlight = highlight and self.args.HIGHLIGHT
         if self._prefix or self.args.START_LINE or self.args.END_LINE \
         or self.args.MAX_MESSAGE_LINES or (self.args.LINES_AROUND_MATCH and highlight):
             lines = text.splitlines()
 
             if self.args.START_LINE or self.args.END_LINE or self.args.MAX_MESSAGE_LINES:
                 start = self.args.START_LINE or 0
                 start = max(start, -len(lines)) - (start > 0)  # <0 to sanity, >0 to 0-base
@@ -146,15 +177,15 @@
                 spans, NUM = [], self.args.LINES_AROUND_MATCH
                 for i, l in enumerate(lines):
                     if MatchMarkers.START in l:
                         spans.append([max(0, i - NUM), min(i + NUM + 1, len(lines))])
                     if MatchMarkers.END in l and spans:
                         spans[-1][1] = min(i + NUM + 1, len(lines))
                 lines = sum((lines[a:b - 1] + [lines[b - 1] + self._styles["rst"]]
-                             for a, b in merge_spans(spans)), [])
+                             for a, b in common.merge_spans(spans)), [])
 
             if self._prefix:
                 lines = [self._prefix + l for l in lines]
 
             text = "\n".join(lines)
 
         for a, b in self._format_repls.items() if highlight else ():
@@ -205,117 +236,131 @@
             MAX_CHARS = MAX_LEN = MAX_LINES * MIN_CHARS_PER_LINE * self._wrapper.width + 100
             if bytelen > MAX_CHARS:  # Use worst-case max length plus some extra
                 if isinstance(v, (list, tuple)): MAX_LEN = MAX_CHARS // 3
                 v = v[:MAX_LEN]
             return v
 
         indent = "  " * len(top)
-        if isinstance(val, (int, float, bool)):
+        if isinstance(val, six.integer_types + (float, bool)):
             return str(val)
-        if isinstance(val, str):
+        if isinstance(val, common.TEXT_TYPES):
             if val in ("", MatchMarkers.EMPTY):
                 return MatchMarkers.EMPTY_REPL if val else "''"
             # default_style='"' avoids trailing "...\n"
             return yaml.safe_dump(truncate(val), default_style='"', width=sys.maxsize).rstrip("\n")
         if isinstance(val, (list, tuple)):
             if not val:
                 return "[]"
-            if rosapi.scalar(typename) in rosapi.ROS_STRING_TYPES:
+            if api.scalar(typename) in api.ROS_STRING_TYPES:
                 yaml_str = yaml.safe_dump(truncate(val)).rstrip('\n')
                 return "\n" + "\n".join(indent + line for line in yaml_str.splitlines())
             vals = [x for v in truncate(val) for x in [self.message_to_yaml(v, top, typename)] if x]
-            if rosapi.scalar(typename) in rosapi.ROS_NUMERIC_TYPES:
+            if api.scalar(typename) in api.ROS_NUMERIC_TYPES:
                 return "[%s]" % ", ".join(unquote(str(v)) for v in vals)
             return ("\n" + "\n".join(indent + "- " + v for v in vals)) if vals else ""
-        if rosapi.is_ros_message(val):
+        if api.is_ros_message(val):
             MATCHED_ONLY = self.args.MATCHED_FIELDS_ONLY and not self.args.LINES_AROUND_MATCH
-            vals, fieldmap = [], rosapi.get_message_fields(val)
+            vals, fieldmap = [], api.get_message_fields(val)
             prints, noprints = self._patterns["print"], self._patterns["noprint"]
-            fieldmap = filter_fields(fieldmap, top, include=prints, exclude=noprints)
+            fieldmap = api.filter_fields(fieldmap, top, include=prints, exclude=noprints)
             for k, t in fieldmap.items():
-                v = self.message_to_yaml(rosapi.get_message_value(val, k, t), top + (k, ), t)
+                v = self.message_to_yaml(api.get_message_value(val, k, t), top + (k, ), t)
                 if not v or MATCHED_ONLY and MatchMarkers.START not in v:
                     continue  # for k, t
 
-                if t not in rosapi.ROS_STRING_TYPES: v = unquote(v)
-                if rosapi.scalar(t) in rosapi.ROS_BUILTIN_TYPES:
-                    is_strlist = t.endswith("]") and rosapi.scalar(t) in rosapi.ROS_STRING_TYPES
-                    is_num = rosapi.scalar(t) in rosapi.ROS_NUMERIC_TYPES
+                if t not in api.ROS_STRING_TYPES: v = unquote(v)
+                if api.scalar(t) in api.ROS_BUILTIN_TYPES:
+                    is_strlist = t.endswith("]") and api.scalar(t) in api.ROS_STRING_TYPES
+                    is_num = api.scalar(t) in api.ROS_NUMERIC_TYPES
                     extra_indent = indent if is_strlist else " " * len(indent + k + ": ")
                     self._wrapper.reserve_width(self._prefix + extra_indent)
                     self._wrapper.drop_whitespace = t.endswith("]") and not is_strlist
                     self._wrapper.break_long_words = not is_num
                     v = ("\n" + extra_indent).join(retag_match_lines(self._wrapper.wrap(v)))
                     if is_strlist and self._wrapper.strip(v) != "[]": v = "\n" + v
-                vals.append("%s%s: %s" % (indent, k, rosapi.format_message_value(val, k, v)))
+                vals.append("%s%s: %s" % (indent, k, api.format_message_value(val, k, v)))
             return ("\n" if indent and vals else "") + "\n".join(vals)
 
         return str(val)
 
 
     def _configure(self, args):
         """Initializes output settings."""
-        prints, noprints = args.PRINT_FIELDS, args.NOPRINT_FIELDS
+        prints, noprints = args.EMIT_FIELD, args.NOEMIT_FIELD
         for key, vals in [("print", prints), ("noprint", noprints)]:
-            self._patterns[key] = [(tuple(v.split(".")), wildcard_to_regex(v)) for v in vals]
+            self._patterns[key] = [(tuple(v.split(".")), common.wildcard_to_regex(v)) for v in vals]
 
-        if "never" != args.COLOR:
-            self._styles.update({"hl0":  ConsolePrinter.STYLE_HIGHLIGHT,
+        if args.COLOR not in ("never", False):
+            self._styles.update({"hl0":  ConsolePrinter.STYLE_HIGHLIGHT if self.args.HIGHLIGHT
+                                         else "",
                                  "ll0":  ConsolePrinter.STYLE_LOWLIGHT,
                                  "pfx0": ConsolePrinter.STYLE_SPECIAL,  # Content line prefix start
                                  "sep0": ConsolePrinter.STYLE_SPECIAL2})
             self._styles.default_factory = lambda: ConsolePrinter.STYLE_RESET
 
-        WRAPS = args.MATCH_WRAPPER
-        if WRAPS is None and "never" == args.COLOR: WRAPS = self.NOCOLOR_HIGHLIGHT_WRAPPERS
+        WRAPS = args.MATCH_WRAPPER if self.args.HIGHLIGHT else ""
+        if WRAPS is None and args.COLOR in ("never", False): WRAPS = self.NOCOLOR_HIGHLIGHT_WRAPPERS
         WRAPS = ((WRAPS or [""]) * 2)[:2]
         self._styles["hl0"] = self._styles["hl0"] + WRAPS[0]
         self._styles["hl1"] = WRAPS[1] + self._styles["hl1"]
 
         custom_widths = {MatchMarkers.START: len(WRAPS[0]), MatchMarkers.END:     len(WRAPS[1]),
                          self._styles["ll0"]:            0, self._styles["ll1"]:  0,
                          self._styles["pfx0"]:           0, self._styles["pfx1"]: 0,
                          self._styles["sep0"]:           0, self._styles["sep1"]: 0}
         wrapargs = dict(max_lines=args.MAX_FIELD_LINES,
                         placeholder="%s ...%s" % (self._styles["ll0"], self._styles["ll1"]))
         if args.WRAP_WIDTH is not None: wrapargs.update(width=args.WRAP_WIDTH)
-        self._wrapper = TextWrapper(custom_widths=custom_widths, **wrapargs)
+        self._wrapper = common.TextWrapper(custom_widths=custom_widths, **wrapargs)
         self._format_repls = {MatchMarkers.START: self._styles["hl0"],
                               MatchMarkers.END:   self._styles["hl1"]}
 
 
 
-class ConsoleSink(SinkBase, TextSinkMixin):
+class ConsoleSink(Sink, TextSinkMixin):
     """Prints messages to console."""
 
     META_LINE_TEMPLATE   = "{ll0}{sep} {line}{ll1}"
     MESSAGE_SEP_TEMPLATE = "{ll0}{sep}{ll1}"
     PREFIX_TEMPLATE      = "{pfx0}{batch}{pfx1}{sep0}{sep}{sep1}"
     MATCH_PREFIX_SEP     = ":"    # Printed after bag filename for matched message lines
     CONTEXT_PREFIX_SEP   = "-"    # Printed after bag filename for context message lines
     SEP                  = "---"  # Prefix of message separators and metainfo lines
 
-
-    def __init__(self, args):
-        """
-        @param   args                       arguments object like argparse.Namespace
-        @param   args.META                  whether to print metainfo
-        @param   args.PRINT_FIELDS          message fields to print in output if not all
-        @param   args.NOPRINT_FIELDS        message fields to skip in output
-        @param   args.LINE_PREFIX           print source prefix like bag filename on each message line
-        @param   args.MAX_FIELD_LINES       maximum number of lines to print per field
-        @param   args.START_LINE            message line number to start output from
-        @param   args.END_LINE              message line number to stop output at
-        @param   args.MAX_MESSAGE_LINES     maximum number of lines to output per message
-        @param   args.LINES_AROUND_MATCH    number of message lines around matched fields to output
-        @param   args.MATCHED_FIELDS_ONLY   output only the fields where match was found
-        @param   args.WRAP_WIDTH            character width to wrap message YAML output at
+    ## Constructor argument defaults
+    DEFAULT_ARGS = dict(COLOR=True, EMIT_FIELD=(), NOEMIT_FIELD=(), HIGHLIGHT=True, META=False,
+                        LINE_PREFIX=True, MAX_FIELD_LINES=None, START_LINE=None,
+                        END_LINE=None, MAX_MESSAGE_LINES=None, LINES_AROUND_MATCH=None,
+                        MATCHED_FIELDS_ONLY=False, WRAP_WIDTH=None, MATCH_WRAPPER=None)
+
+
+    def __init__(self, args=None, **kwargs):
+        """
+        @param   args                       arguments as namespace or dictionary, case-insensitive
+        @param   args.color                 False or "never" for not using colors in replacements
+        @param   args.highlight             highlight matched values (default true)
+        @param   args.meta                  whether to print metainfo
+        @param   args.emit_field            message fields to emit if not all
+        @param   args.noemit_field          message fields to skip in output
+        @param   args.line_prefix           print source prefix like bag filename on each message line
+        @param   args.max_field_lines       maximum number of lines to print per field
+        @param   args.start_line            message line number to start output from
+        @param   args.end_line              message line number to stop output at
+        @param   args.max_message_lines     maximum number of lines to output per message
+        @param   args.lines_around_match    number of message lines around matched fields to output
+        @param   args.matched_fields_only   output only the fields where match was found
+        @param   args.wrap_width            character width to wrap message YAML output at
+        @param   args.match_wrapper         string to wrap around matched values,
+                                            both sides if one value, start and end if more than one,
+                                            or no wrapping if zero values
+        @param   kwargs                     any and all arguments as keyword overrides, case-insensitive
         """
+        args = common.ensure_namespace(args, ConsoleSink.DEFAULT_ARGS, **kwargs)
         if args.WRAP_WIDTH is None:
-            args = copy.deepcopy(args)
+            args = common.structcopy(args)
             args.WRAP_WIDTH = ConsolePrinter.WIDTH
 
         super(ConsoleSink, self).__init__(args)
         TextSinkMixin.__init__(self, args)
 
 
     def emit_meta(self):
@@ -325,245 +370,362 @@
             meta = self._batch_meta[batch] = self.source.format_meta()
             kws = dict(self._styles, sep=self.SEP)
             meta = "\n".join(x and self.META_LINE_TEMPLATE.format(**dict(kws, line=x))
                              for x in meta.splitlines())
             meta and ConsolePrinter.print(meta)
 
 
-    def emit(self, topic, index, stamp, msg, match):
+    def emit(self, topic, msg, stamp=None, match=None, index=None):
         """Prints separator line and message text."""
         self._prefix = ""
+        stamp, index = self._ensure_stamp_index(topic, msg, stamp, index)
         if self.args.LINE_PREFIX and self.source.get_batch():
             sep = self.MATCH_PREFIX_SEP if match else self.CONTEXT_PREFIX_SEP
             kws = dict(self._styles, sep=sep, batch=self.source.get_batch())
             self._prefix = self.PREFIX_TEMPLATE.format(**kws)
         kws = dict(self._styles, sep=self.SEP)
         if self.args.META:
-            meta = self.source.format_message_meta(topic, index, stamp, msg)
+            meta = self.source.format_message_meta(topic, msg, stamp, index)
             meta = "\n".join(x and self.META_LINE_TEMPLATE.format(**dict(kws, line=x))
                              for x in meta.splitlines())
             meta and ConsolePrinter.print(meta)
         elif self._counts:
             sep = self.MESSAGE_SEP_TEMPLATE.format(**kws)
             sep and ConsolePrinter.print(sep)
         ConsolePrinter.print(self.format_message(match or msg, highlight=bool(match)))
-        super(ConsoleSink, self).emit(topic, index, stamp, msg, match)
+        super(ConsoleSink, self).emit(topic, msg, stamp, match, index)
 
 
     def is_highlighting(self):
-        """Returns True (requires highlighted matches)."""
-        return True
+        """Returns True if sink is configured to highlight matched values."""
+        return bool(self.args.HIGHLIGHT)
 
 
 
-class BagSink(SinkBase):
+class BagSink(Sink):
     """Writes messages to bagfile."""
 
-    def __init__(self, args):
-        """
-        @param   args                 arguments object like argparse.Namespace
-        @param   args.META            whether to print metainfo
-        @param   args.WRITE           name of ROS bagfile to create or append to
-        @param   args.WRITE_OPTIONS   {"overwrite": whether to overwrite existing file
-                                                     (default false)}
-        @param   args.VERBOSE         whether to print debug information
+    ## Constructor argument defaults
+    DEFAULT_ARGS = dict(META=False, WRITE_OPTIONS={}, VERBOSE=False)
+
+    def __init__(self, args=None, **kwargs):
         """
+        @param   args                 arguments as namespace or dictionary, case-insensitive;
+                                      or a single path as the ROS bagfile to write,
+                                      or a stream or {@link grepros.api.Bag Bag} instance to write to
+        @param   args.write           name of ROS bagfile to create or append to,
+                                      or a stream to write to
+        @param   args.write_options   {"overwrite": whether to overwrite existing file
+                                                    (default false)}
+        @param   args.meta            whether to emit metainfo
+        @param   args.verbose         whether to emit debug information
+        @param   kwargs               any and all arguments as keyword overrides, case-insensitive
+        """
+
+        args0 = args
+        args = {"WRITE": str(args)} if isinstance(args, common.PATH_TYPES) else \
+               {"WRITE": args} if common.is_stream(args) else \
+               {} if isinstance(args, api.Bag) else args
+        args = common.ensure_namespace(args, BagSink.DEFAULT_ARGS, **kwargs)
         super(BagSink, self).__init__(args)
-        self._bag = None
-        self._overwrite = (args.WRITE_OPTIONS.get("overwrite") == "true")
+        self._bag = args0 if isinstance(args0, api.Bag) else None
+        self._overwrite = (args.WRITE_OPTIONS.get("overwrite") in ("true", True))
         self._close_printed = False
 
         atexit.register(self.close)
 
-    def emit(self, topic, index, stamp, msg, match):
+    def emit(self, topic, msg, stamp=None, match=None, index=None):
         """Writes message to output bagfile."""
-        if not self._bag:
-            if self.args.VERBOSE:
-                sz = os.path.isfile(self.args.WRITE) and os.path.getsize(self.args.WRITE)
-                ConsolePrinter.debug("%s %s%s.",
-                                     "Overwriting" if sz and self._overwrite else
-                                     "Appending to" if sz else "Creating",
-                                     self.args.WRITE, (" (%s)" % format_bytes(sz)) if sz else "")
-            makedirs(os.path.dirname(self.args.WRITE))
-            self._bag = rosapi.Bag(self.args.WRITE, mode="w" if self._overwrite else "a")
-
-        topickey = rosapi.TypeMeta.make(msg, topic).topickey
+        if not self.validate(): raise Exception("invalid")
+        self._ensure_open()
+        stamp, index = self._ensure_stamp_index(topic, msg, stamp, index)
+        topickey = api.TypeMeta.make(msg, topic).topickey
         if topickey not in self._counts and self.args.VERBOSE:
             ConsolePrinter.debug("Adding topic %s in bag output.", topic)
 
-        self._bag.write(topic, msg, stamp, self.source.get_message_meta(topic, index, stamp, msg))
-        super(BagSink, self).emit(topic, index, stamp, msg, match)
+        qoses = self.source.get_message_meta(topic, msg, stamp).get("qoses")
+        self._bag.write(topic, msg, stamp, qoses=qoses)
+        super(BagSink, self).emit(topic, msg, stamp, match, index)
 
     def validate(self):
-        """Returns whether ROS environment is set, prints error if not."""
+        """Returns whether write options are valid and ROS environment set, emits error if not."""
+        if self.valid is not None: return self.valid
         result = True
-        if self.args.WRITE_OPTIONS.get("overwrite") not in (None, "true", "false"):
+        if self.args.WRITE_OPTIONS.get("overwrite") not in (None, True, False, "true", "false"):
             ConsolePrinter.error("Invalid overwrite option for bag: %r. "
                                  "Choose one of {true, false}.",
                                  self.args.WRITE_OPTIONS["overwrite"])
             result = False
-        return rosapi.validate() and result
+        if not self._bag and not common.verify_io(self.args.WRITE, "w"):
+            ConsolePrinter.error("File not writable.")
+            result = False
+        if not self._bag and common.is_stream(self.args.WRITE) \
+        and not any(c.STREAMABLE for c in api.Bag.WRITER_CLASSES):
+            ConsolePrinter.error("Bag format does not support writing streams.")
+            result = False
+        if self._bag and self._bag.mode not in ("a", "w"):
+            ConsolePrinter.error("Bag not in write mode.")
+            result = False
+        self.valid = api.validate() and result
+        return self.valid
 
     def close(self):
-        """Closes output bagfile, if any."""
-        self._bag and self._bag.close()
-        if not self._close_printed and self._counts:
+        """Closes output bag, if any."""
+        if not self._close_printed and self._counts and self._bag:
             self._close_printed = True
+            self._bag.close()
+            try: sz = common.format_bytes(os.path.getsize(self._bag.filename)
+                                          if self._bag.filename else self._bag.size or 0)
+            except Exception as e:
+                ConsolePrinter.warn("Error getting size of %s: %s", self._bag.filename, e)
+                sz = "error getting size"
             ConsolePrinter.debug("Wrote %s in %s to %s (%s).",
-                                 plural("message", sum(self._counts.values())),
-                                 plural("topic", self._counts), self.args.WRITE,
-                                 format_bytes(os.path.getsize(self.args.WRITE)))
+                                 common.plural("message", sum(self._counts.values())),
+                                 common.plural("topic", self._counts),
+                                 self._bag.filename or "<stream>", sz)
+        self._bag  and self._bag.close()
         super(BagSink, self).close()
 
+    def _ensure_open(self):
+        """Opens output file if not already open."""
+        if self._bag is not None:
+            self._bag.open()
+            return
+        if common.is_stream(self.args.WRITE):
+            self._bag = api.Bag(self.args.WRITE, mode=getattr(self.args.WRITE, "mode", "w"))
+            return
+        filename = self.args.WRITE
+        if not self._overwrite and os.path.isfile(filename) and os.path.getsize(filename):
+            cls = api.Bag.autodetect(filename)
+            if cls and "a" not in getattr(cls, "MODES", ("a", )):
+                filename = common.unique_path(filename)
+                if self.args.VERBOSE:
+                    ConsolePrinter.debug("Making unique filename %r, as %s does not support "
+                                         "appending.", filename, cls.__name___)
+        if self.args.VERBOSE:
+            sz = os.path.isfile(filename) and os.path.getsize(filename)
+            ConsolePrinter.debug("%s %s%s.",
+                                 "Overwriting" if sz and self._overwrite else
+                                 "Appending to" if sz else "Creating",
+                                 filename, (" (%s)" % common.format_bytes(sz)) if sz else "")
+        common.makedirs(os.path.dirname(filename))
+        self._bag = api.Bag(filename, mode="w" if self._overwrite else "a")
+
     @classmethod
     def autodetect(cls, target):
         """Returns true if target is recognizable as a ROS bag."""
         ext = os.path.splitext(target or "")[-1].lower()
-        return ext in rosapi.BAG_EXTENSIONS
+        return ext in api.BAG_EXTENSIONS
 
 
-class TopicSink(SinkBase):
+class TopicSink(Sink):
     """Publishes messages to ROS topics."""
 
-    def __init__(self, args):
-        """
-        @param   args                   arguments object like argparse.Namespace
-        @param   args.LIVE              whether reading messages from live ROS topics
-        @param   args.META              whether to print metainfo
-        @param   args.QUEUE_SIZE_OUT    publisher queue size
-        @param   args.PUBLISH_PREFIX    output topic prefix, prepended to input topic
-        @param   args.PUBLISH_SUFFIX    output topic suffix, appended to output topic
-        @param   args.PUBLISH_FIXNAME   single output topic name to publish to,
+    ## Constructor argument defaults
+    DEFAULT_ARGS = dict(LIVE=False, META=False, QUEUE_SIZE_OUT=10, PUBLISH_PREFIX="",
+                        PUBLISH_SUFFIX="", PUBLISH_FIXNAME="", VERBOSE=False)
+
+    def __init__(self, args=None, **kwargs):
+        """
+        @param   args                   arguments as namespace or dictionary, case-insensitive
+        @param   args.live              whether reading messages from live ROS topics
+        @param   args.queue_size_out    publisher queue size (default 10)
+        @param   args.publish_prefix    output topic prefix, prepended to input topic
+        @param   args.publish_suffix    output topic suffix, appended to output topic
+        @param   args.publish_fixname   single output topic name to publish to,
                                         overrides prefix and suffix if given
-        @param   args.VERBOSE           whether to print debug information
+        @param   args.meta              whether to emit metainfo
+        @param   args.verbose           whether to emit debug information
+        @param   kwargs                 any and all arguments as keyword overrides, case-insensitive
         """
+        args = common.ensure_namespace(args, TopicSink.DEFAULT_ARGS, **kwargs)
         super(TopicSink, self).__init__(args)
         self._pubs = {}  # {(intopic, typename, typehash): ROS publisher}
         self._close_printed = False
 
-    def emit(self, topic, index, stamp, msg, match):
+    def emit(self, topic, msg, stamp=None, match=None, index=None):
         """Publishes message to output topic."""
-        with rosapi.TypeMeta.make(msg, topic) as m:
+        if not self.validate(): raise Exception("invalid")
+        api.init_node()
+        with api.TypeMeta.make(msg, topic) as m:
             topickey, cls = (m.topickey, m.typeclass)
         if topickey not in self._pubs:
             topic2 = self.args.PUBLISH_PREFIX + topic + self.args.PUBLISH_SUFFIX
             topic2 = self.args.PUBLISH_FIXNAME or topic2
             if self.args.VERBOSE:
                 ConsolePrinter.debug("Publishing from %s to %s.", topic, topic2)
 
             pub = None
             if self.args.PUBLISH_FIXNAME:
                 pub = next((v for (_, c), v in self._pubs.items() if c == cls), None)
-            pub = pub or rosapi.create_publisher(topic2, cls, queue_size=self.args.QUEUE_SIZE_OUT)
+            pub = pub or api.create_publisher(topic2, cls, queue_size=self.args.QUEUE_SIZE_OUT)
             self._pubs[topickey] = pub
 
         self._pubs[topickey].publish(msg)
-        super(TopicSink, self).emit(topic, index, stamp, msg, match)
+        super(TopicSink, self).emit(topic, msg, stamp, match, index)
 
     def bind(self, source):
         """Attaches source to sink and blocks until connected to ROS."""
-        SinkBase.bind(self, source)
-        rosapi.init_node()
+        if not self.validate(): raise Exception("invalid")
+        super(TopicSink, self).bind(source)
+        api.init_node()
 
     def validate(self):
         """
         Returns whether ROS environment is set for publishing,
-        and output topic configuration is valid, prints error if not.
+        and output topic configuration is valid, emits error if not.
         """
-        result = rosapi.validate(live=True)
+        if self.valid is not None: return self.valid
+        result = api.validate(live=True)
         config_ok = True
         if self.args.LIVE and not any((self.args.PUBLISH_PREFIX, self.args.PUBLISH_SUFFIX,
                                         self.args.PUBLISH_FIXNAME)):
             ConsolePrinter.error("Need topic prefix or suffix or fixname "
                                  "when republishing messages from live ROS topics.")
             config_ok = False
-        return result and config_ok
+        self.valid = result and config_ok
+        return self.valid
 
     def close(self):
         """Shuts down publishers."""
         if not self._close_printed and self._counts:
             self._close_printed = True
             ConsolePrinter.debug("Published %s to %s.",
-                                 plural("message", sum(self._counts.values())),
-                                 plural("topic", self._pubs))
+                                 common.plural("message", sum(self._counts.values())),
+                                 common.plural("topic", self._pubs))
         for k in list(self._pubs):
-            self._pubs.pop(k).unregister()
+            try: self._pubs.pop(k).unregister()
+            except Exception as e:
+                if self.args.VERBOSE:
+                    ConsolePrinter.warn("Error closing publisher on topic %r: %s", k[0], e)
         super(TopicSink, self).close()
-        rosapi.shutdown_node()
 
 
-class MultiSink(SinkBase):
+class AppSink(Sink):
+    """Provides messages to callback function."""
+
+    ## Constructor argument defaults
+    DEFAULT_ARGS = dict(EMIT=None, METAEMIT=None, HIGHLIGHT=False)
+
+    def __init__(self, args=None, **kwargs):
+        """
+        @param   args             arguments as namespace or dictionary, case-insensitive;
+                                  or emit callback
+        @param   args.emit        callback(topic, msg, stamp, highlighted msg, index in topic), if any
+        @param   args.metaemit    callback(metadata dict) if any, invoked before first emit from source batch
+        @param   args.highlight   whether to expect highlighted matching fields from source messages
+        @param   kwargs           any and all arguments as keyword overrides, case-insensitive
+        """
+        if callable(args): args = common.ensure_namespace(None, emit=args)
+        args = common.ensure_namespace(args, AppSink.DEFAULT_ARGS, **kwargs)
+        super(AppSink, self).__init__(args)
+
+    def emit_meta(self):
+        """Invokes registered metaemit callback, if any, and not already invoked."""
+        if not self.source: return
+        batch = self.source.get_batch() if self.args.METAEMIT else None
+        if self.args.METAEMIT and batch not in self._batch_meta:
+            meta = self._batch_meta[batch] = self.source.get_meta()
+            self.args.METAEMIT(meta)
+
+    def emit(self, topic, msg, stamp=None, match=None, index=None):
+        """Registers message and invokes registered emit callback, if any."""
+        stamp, index = self._ensure_stamp_index(topic, msg, stamp, index)
+        super(AppSink, self).emit(topic, msg, stamp, match, index)
+        if self.args.EMIT: self.args.EMIT(topic, msg, stamp, match, index)
+
+    def is_highlighting(self):
+        """Returns whether emitted matches are highlighted."""
+        return self.args.HIGHLIGHT
+
+
+class MultiSink(Sink):
     """Combines any number of sinks."""
 
     ## Autobinding between argument flags and sink classes
-    FLAG_CLASSES = {"PUBLISH": TopicSink, "CONSOLE": ConsoleSink}
+    FLAG_CLASSES = {"PUBLISH": TopicSink, "CONSOLE": ConsoleSink, "APP": AppSink}
 
-    ## Autobinding between `--write .. format=FORMAT` and sink classes
+    ## Autobinding between `--write TARGET format=FORMAT` and sink classes
     FORMAT_CLASSES = {"bag": BagSink}
 
-    def __init__(self, args):
+    def __init__(self, args=None, sinks=(), **kwargs):
         """
-        @param   args           arguments object like argparse.Namespace
-        @param   args.CONSOLE   print matches to console
-        @param   args.WRITE     [[target, format=FORMAT, key=value, ], ]
-        @param   args.PUBLISH   publish matches to live topics
+        Accepts more arguments, given to the real sinks constructed.
+
+        @param   args           arguments as namespace or dictionary, case-insensitive
+        @param   args.console   print matches to console
+        @param   args.write     [[target, format=FORMAT, key=value, ], ]
+        @param   args.publish   publish matches to live topics
+        @param   args.app       provide messages to given callback function
+        @param   sinks          pre-created sinks, arguments will be ignored
+        @param   kwargs         any and all arguments as keyword overrides, case-insensitive
         """
+        args = common.ensure_namespace(args, **kwargs)
         super(MultiSink, self).__init__(args)
-        self._valid = True
+        self.valid = True
 
         ## List of all combined sinks
         self.sinks = [cls(args) for flag, cls in self.FLAG_CLASSES.items()
-                      if getattr(args, flag, None)]
+                      if getattr(args, flag, None)] if not sinks else list(sinks)
 
-        for dumpopts in args.WRITE:
-            target, kwargs = dumpopts[0], dict(x.split("=", 1) for x in dumpopts[1:])
-            cls = self.FORMAT_CLASSES.get(kwargs.pop("format", None))
+        for dumpopts in getattr(args, "WRITE", []) if not sinks else ():
+            kwargs = dict(x.split("=", 1) for x in dumpopts[1:] if isinstance(x, common.TEXT_TYPES))
+            kwargs.update(kv for x in dumpopts[1:] if isinstance(x, dict) for kv in x.items())
+            target, cls = dumpopts[0], self.FORMAT_CLASSES.get(kwargs.pop("format", None))
             if not cls:
-                cls = next((c for c in self.FORMAT_CLASSES.values()
+                cls = next((c for c in sorted(self.FORMAT_CLASSES.values(),
+                                              key=lambda x: x is BagSink)
                             if callable(getattr(c, "autodetect", None))
                             and c.autodetect(target)), None)
             if not cls:
-                ConsolePrinter.error('Unknown output format in "%s"' % " ".join(dumpopts))
-                self._valid = False
+                ConsolePrinter.error('Unknown output format in "%s"' % " ".join(map(str, dumpopts)))
+                self.valid = False
                 continue  # for dumpopts
-            clsargs = copy.deepcopy(args)
+            clsargs = common.structcopy(args)
             clsargs.WRITE, clsargs.WRITE_OPTIONS = target, kwargs
             self.sinks += [cls(clsargs)]
 
     def emit_meta(self):
         """Outputs source metainfo in one sink, if not already emitted."""
         sink = next((s for s in self.sinks if isinstance(s, ConsoleSink)), None)
-        # Print meta in one sink only, prefer console
+        # Emit meta in one sink only, prefer console
         sink = sink or self.sinks[0] if self.sinks else None
         sink and sink.emit_meta()
 
-    def emit(self, topic, index, stamp, msg, match):
+    def emit(self, topic, msg, stamp=None, match=None, index=None):
         """Outputs ROS message to all sinks."""
+        stamp, index = self._ensure_stamp_index(topic, msg, stamp, index)
         for sink in self.sinks:
-            sink.emit(topic, index, stamp, msg, match)
+            sink.emit(topic, msg, stamp, match, index)
+        super(MultiSink, self).emit(topic, msg, stamp, match, index)
 
     def bind(self, source):
         """Attaches source to all sinks, sets thread_excepthook on all sinks."""
-        SinkBase.bind(self, source)
+        super(MultiSink, self).bind(source)
         for sink in self.sinks:
             sink.bind(source)
             sink.thread_excepthook = self.thread_excepthook
 
     def validate(self):
         """Returns whether prerequisites are met for all sinks."""
         if not self.sinks:
             ConsolePrinter.error("No output configured.")
-        return bool(self.sinks) and all([sink.validate() for sink in self.sinks]) and self._valid
+        return bool(self.sinks) and all([sink.validate() for sink in self.sinks]) and self.valid
 
     def close(self):
         """Closes all sinks."""
         for sink in self.sinks:
             sink.close()
 
     def flush(self):
         """Flushes all sinks."""
         for sink in self.sinks:
             sink.flush()
 
     def is_highlighting(self):
         """Returns whether any sink requires highlighted matches."""
         return any(s.is_highlighting() for s in self.sinks)
+
+
+__all__ = [
+    "AppSink", "BagSink", "ConsoleSink", "MultiSink", "Sink", "TextSinkMixin", "TopicSink"
+]
```

## grepros/ros1.py

```diff
@@ -4,32 +4,39 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     01.11.2021
-@modified    27.10.2022
+@modified    03.07.2023
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.ros1
 import collections
+import datetime
+import decimal
+import inspect
+import logging
 import io
 import os
 import shutil
 import time
 
 import genpy
 import genpy.dynamic
 import rosbag
 import roslib
 import rospy
+import six
 
-from . common import ConsolePrinter, MatchMarkers, ProgressBar, format_bytes, memoize
-from . rosapi import TypeMeta, calculate_definition_hash, parse_definition_subtypes
+from . import api
+from . import common
+from . api import TypeMeta, calculate_definition_hash, parse_definition_subtypes
+from . common import ConsolePrinter, memoize
 
 
 ## Bagfile extensions to seek
 BAG_EXTENSIONS  = (".bag", ".bag.active")
 
 ## Bagfile extensions to skip
 SKIP_EXTENSIONS = (".bag.orig.active", )
@@ -49,48 +56,85 @@
 ## Seconds between checking whether ROS master is available.
 SLEEP_INTERVAL = 0.5
 
 ## rospy.MasterProxy instance
 master = None
 
 
-class Bag(rosbag.Bag):
-    """Add message type getters and grepros-specific write() to rosbag.Bag."""
+class ROS1Bag(rosbag.Bag, api.BaseBag):
+    """
+    ROS1 bag reader and writer.
+
+    Extends `rosbag.Bag` with more conveniences, smooths over the rosbag bug of ignoring
+    topic and time filters in format v1.2, and smooths over the rosbag bug
+    of yielding messages of wrong type, if message types in different topics
+    have different packages but identical fields and hashes.
+
+    Does **not** smooth over the rosbag bug of writing different types to one topic.
+
+    rosbag does allow writing messages of different types to one topic,
+    just like live ROS topics can have multiple message types published
+    to one topic. And their serialized bytes will actually be in the bag,
+    but rosbag will only register the first type for this topic (unless it is
+    explicitly given another connection header with metadata on the other type).
+
+    All messages yielded will be deserialized by rosbag as that first type,
+    and whether reading will raise an exception or not depends on whether
+    the other type has enough bytes to be deserialized as that first type.
+    """
 
     # {(typename, typehash): message type class}
     __TYPES    = {}
 
     ## {(typename, typehash): type definition text}
     __TYPEDEFS = {}
 
     # {(typename, typehash): whether subtype definitions parsed}
     __PARSEDS = {}
 
 
     def __init__(self, *args, **kwargs):
         """
+        @param   f           bag file path, or a stream object
         @param   mode        mode to open bag in, defaults to "r" (read mode)
         @param   reindex     if true and bag is unindexed, make a copy
                              of the file (unless unindexed format) and reindex original
         @param   progress    show progress bar with reindexing status
+        @param   kwargs      additional keyword arguments for `rosbag.Bag`, like `compression`
         """
+        self.__topics = {}    # {(topic, typename, typehash): message count}
+        self.__iterer = None  # Generator from read_messages() for next()
+        f,    args = (args[0] if args else kwargs.pop("f")), args[1:]
+        mode, args = (args[0] if args else kwargs.pop("mode", "r")), args[1:]
+        if mode not in self.MODES: raise ValueError("invalid mode %r" % mode)
+
         kwargs.setdefault("skip_index", True)
         reindex, progress = (kwargs.pop(k, False) for k in ("reindex", "progress"))
-        filename, args = (args[0] if args else kwargs.pop("f")), args[1:]
-        mode,     args = (args[0] if args else kwargs.pop("mode", "r")), args[1:]
+        getargspec = getattr(inspect, "getfullargspec", inspect.getargspec)
+        for n in set(kwargs) - set(getargspec(rosbag.Bag).args): kwargs.pop(n)
+
+        if common.is_stream(f):
+            if not common.verify_io(f, mode):
+                raise io.UnsupportedOperation({"r": "read", "w": "write", "a": "append"}[mode])
+            super(ROS1Bag, self).__init__(f, mode, *args, **kwargs)
+            self._populate_meta()
+            return
+
+        f = str(f)
+        if "a" == mode and (not os.path.exists(f) or not os.path.getsize(f)):
+            mode = "w"  # rosbag raises error on append if no file or empty file
+            os.path.exists(f) and os.remove(f)
+
         try:
-            super(Bag, self).__init__(filename, mode, *args, **kwargs)
+            super(ROS1Bag, self).__init__(f, mode, *args, **kwargs)
         except rosbag.ROSBagUnindexedException:
             if not reindex: raise
-            Bag.reindex_file(filename, progress, *args, **kwargs)
-            super(Bag, self).__init__(filename, mode, *args, **kwargs)
-
-        self.__topics = {}  # {(topic, typename, typehash): message count}
-
-        self.__populate_meta()
+            Bag.reindex_file(f, progress, *args, **kwargs)
+            super(ROS1Bag, self).__init__(f, mode, *args, **kwargs)
+        self._populate_meta()
 
 
     def get_message_definition(self, msg_or_type):
         """Returns ROS1 message type definition full text from bag, including subtype definitions."""
         if is_ros_message(msg_or_type):
             return self.__TYPEDEFS.get((msg_or_type._type, msg_or_type._md5sum))
         typename = msg_or_type
@@ -101,113 +145,221 @@
         """
         Returns rospy message class for typename, or None if unknown type.
 
         Generates class dynamically if not already generated.
 
         @param   typehash  message type definition hash, if any
         """
-        self.__ensure_typedef(typename, typehash)
+        if (typename, typehash) not in self.__TYPES: self._ensure_typedef(typename, typehash)
         typehash = typehash or next((h for n, h in self.__TYPEDEFS if n == typename), None)
         typekey = (typename, typehash)
         if typekey not in self.__TYPES and typekey in self.__TYPEDEFS:
             for n, c in genpy.dynamic.generate_dynamic(typename, self.__TYPEDEFS[typekey]).items():
                 self.__TYPES[(n, c._md5sum)] = c
-        return self.__TYPES.get(typekey) or get_message_class(typename)
+        return self.__TYPES.get(typekey)
 
 
     def get_message_type_hash(self, msg_or_type):
-        """Returns ROS1 message type MD5 hash."""
+        """Returns ROS1 message type MD5 hash, or None if unknown type."""
         if is_ros_message(msg_or_type): return msg_or_type._md5sum
         typename = msg_or_type
         typehash = next((h for n, h in self.__TYPEDEFS if n == typename), None)
         if not typehash:
-            self.__ensure_typedef(typename)
+            self._ensure_typedef(typename)
             typehash = next((h for n, h in self.__TYPEDEFS if n == typename), None)
-        return typehash or get_message_type_hash(typename)
+        return typehash
+
+
+    def get_start_time(self):
+        """Returns the start time of the bag, as UNIX timestamp, or None if bag empty."""
+        try: return super(ROS1Bag, self).get_start_time()
+        except Exception: return None
 
 
-    def get_qoses(self, topic, typename):
-        """Returns None."""
-        return None
+    def get_end_time(self):
+        """Returns the end time of the bag, as UNIX timestamp, or None if bag empty."""
+        try: return super(ROS1Bag, self).get_end_time()
+        except Exception: return None
 
 
-    def get_topic_info(self):
+    def get_topic_info(self, *_, **__):
         """Returns topic and message type metainfo as {(topic, typename, typehash): count}."""
         return dict(self.__topics)
 
 
-    def read_messages(self, topics=None, start_time=None, end_time=None, connection_filter=None, raw=False):
+    def read_messages(self, topics=None, start_time=None, end_time=None,
+                      raw=False, connection_filter=None, **__):
         """
         Yields messages from the bag, optionally filtered by topic, timestamp and connection details.
 
         @param   topics             list of topics or a single topic.
                                     If an empty list is given, all topics will be read.
-        @param   start_time         earliest timestamp of messages to return
-        @param   end_time           latest timestamp of messages to return
+        @param   start_time         earliest timestamp of messages to return,
+                                    as ROS time or convertible (int/float/duration/datetime/decimal)
+        @param   end_time           latest timestamp of messages to return,
+                                    as ROS time or convertible (int/float/duration/datetime/decimal)
         @param   connection_filter  function to filter connections to include
-        @param   raw                if True, then generate tuples of
-                                    (datatype, (data, md5sum, position), pytype)
-        @return                     generator of (topic, message, timestamp) tuples
+        @param   raw                if true, then returned messages are tuples of
+                                    (typename, bytes, typehash, typeclass)
+                                    or (typename, bytes, typehash, position, typeclass),
+                                    depending on file format
+        @return                     BagMessage namedtuples of
+                                    (topic, message, timestamp as rospy.Time)
         """
+        if self.closed: raise ValueError("I/O operation on closed file.")
+        if "w" == self._mode: raise io.UnsupportedOperation("read")
+
         hashtypes = {}
         for n, h in self.__TYPEDEFS: hashtypes.setdefault(h, []).append(n)
-        read_topics = topics if isinstance(topics, list) else [topics] if topics else None
+        read_topics = topics if isinstance(topics, (dict, list, set, tuple)) else \
+                      [topics] if topics else None
         dupes = {t: (n, h) for t, n, h in self.__topics
                  if (read_topics is None or t in read_topics) and len(hashtypes.get(h, [])) > 1}
 
+        # Workaround for rosbag.Bag ignoring topic and time filters in format v1.2
+        if self.version != 102 or (not topics and start_time is None and end_time is None):
+            in_range = lambda *_: True
+        else: in_range = lambda t, s: ((not read_topics or t in read_topics) and
+                                       (start_time is None or s >= start_time) and
+                                       (end_time is None or s <= end_time))
+
+        start_time, end_time = map(to_time, (start_time, end_time))
         kwargs = dict(topics=topics, start_time=start_time, end_time=end_time,
                       connection_filter=connection_filter, raw=raw)
         if not dupes:
-            for topic, msg, stamp in super(Bag, self).read_messages(**kwargs):
-                yield topic, msg, stamp
+            for topic, msg, stamp in super(ROS1Bag, self).read_messages(**kwargs):
+                if in_range(topic, stamp):
+                    TypeMeta.make(msg, topic, self)
+                    yield self.BagMessage(topic, msg, stamp)
+                if self.closed: break  # for topic
             return
 
-        for topic, msg, stamp in super(Bag, self).read_messages(**kwargs):
+        for topic, msg, stamp in super(ROS1Bag, self).read_messages(**kwargs):
+            if not in_range(topic, stamp): continue  # for topic
+
             # Workaround for rosbag bug of using wrong type for identical type hashes
             if topic in dupes:
                 typename, typehash = (msg[0], msg[2]) if raw else (msg._type, msg._md5sum)
                 if dupes[topic] != (typename, typehash):
                     if raw:
                         msg = msg[:-1] + (self.get_message_class(typename, typehash), )
                     else:
-                        msg = self.__convert_message(msg, *dupes[topic])
-            yield topic, msg, stamp
+                        msg = self._convert_message(msg, *dupes[topic])
+            TypeMeta.make(msg, topic, self)
+            yield self.BagMessage(topic, msg, stamp)
+            if self.closed: break  # for topic
+
+
+    def write(self, topic, msg, t=None, raw=False, connection_header=None, **__):
+        """
+        Writes a message to the bag.
 
+        Populates connection header if topic already in bag but with a different message type.
 
-    def write(self, topic, msg, stamp, *_, **__):
-        """Writes a message to the bag."""
-        return super(Bag, self).write(topic, msg, stamp)
+        @param   topic              name of topic
+        @param   msg                ROS1 message
+        @param   t                  message timestamp if not using current wall time,
+                                    as ROS time or convertible (int/float/duration/datetime/decimal)
+        @param   raw                if true, `msg` is in raw format,
+                                    (typename, bytes, typehash, typeclass)
+        @param   connection_header  custom connection record for topic,
+                                    as {"topic", "type", "md5sum", "message_definition"}
+        """
+        if self.closed: raise ValueError("I/O operation on closed file.")
+        if "r" == self._mode: raise io.UnsupportedOperation("write")
+        self._register_write(topic, msg, raw, connection_header)
+        return super(ROS1Bag, self).write(topic, msg, to_time(t), raw, connection_header)
+
+
+    def open(self):
+        """Opens the bag file if not already open."""
+        if not self._file:
+            self._open(self.filename, self.mode, allow_unindexed=True)
+
+
+    def close(self):
+        """Closes the bag file."""
+        if self._file:
+            super(ROS1Bag, self).close()
+            self._iterer = None
+            self._clear_index()
+
+
+    def __contains__(self, key):
+        """Returns whether bag contains given topic."""
+        return any(key == t for t, _, _ in self.__topics)
+
+
+    def __next__(self):
+        """Retrieves next message from bag as (topic, message, timestamp)."""
+        if self.closed: raise ValueError("I/O operation on closed file.")
+        if self.__iterer is None: self.__iterer = self.read_messages()
+        return next(self.__iterer)
 
 
-    def __convert_message(self, msg, typename2, typehash2=None):
+    @property
+    def topics(self):
+        """Returns the list of topics in bag, in alphabetic order."""
+        return sorted((t for t, _, _ in self.__topics), key=str.lower)
+
+
+    @property
+    def closed(self):
+        """Returns whether file is closed."""
+        return not self._file
+
+
+    def _convert_message(self, msg, typename2, typehash2=None):
         """Returns message converted to given type; fields must match."""
         msg2 = self.get_message_class(typename2, typehash2)()
         fields2 = get_message_fields(msg2)
         for fname, ftypename in get_message_fields(msg).items():
             v1 = v2 = getattr(msg, fname)
             if ftypename != fields2.get(fname, ftypename):
-                v2 = self.__convert_message(v1, fields2[fname])
+                v2 = self._convert_message(v1, fields2[fname])
             setattr(msg2, fname, v2)
         return msg2
 
 
-    def __populate_meta(self):
+    def _populate_meta(self):
         """Populates topics and message type definitions and hashes."""
         result = collections.Counter()  # {(topic, typename, typehash): count}
         counts = collections.Counter()  # {connection ID: count}
         for c in self._chunks:
             for c_id, count in c.connection_counts.items():
                 counts[c_id] += count
         for c in self._connections.values():
             result[(c.topic, c.datatype, c.md5sum)] += counts[c.id]
             self.__TYPEDEFS[(c.datatype, c.md5sum)] = c.msg_def
         self.__topics = dict(result)
 
 
-    def __ensure_typedef(self, typename, typehash=None):
+    def _register_write(self, topic, msg, raw=False, connection_header=None):
+        """Registers message in local metadata, writes connection header if new type for topic."""
+        if raw: typename, typehash, typeclass = msg[0], msg[2], msg[3]
+        else:   typename, typehash, typeclass = msg._type, msg._md5sum, type(msg)
+        topickey, typekey = (topic, typename, typehash), (typename, typehash)
+
+        if topickey not in self.__topics \
+        and any(topic == t and typekey != (n, h) for t, n, h in self.__topics):
+            # Write connection header if topic already present but with different type
+            if not connection_header:
+                connection_header = {"topic": topic, "type": typename, "md5sum": typehash,
+                                     "message_definition": typeclass._full_text}
+            conn_id, bagself = len(self._connections), super(ROS1Bag, self)
+            connection_info = rosbag.bag._ConnectionInfo(conn_id, topic, connection_header)
+            bagself._write_connection_record(connection_info, encrypt=False)
+            bagself._connections[conn_id] = bagself._topic_connections[topic] = connection_info
+
+        self.__TYPES.setdefault(typekey, typeclass)
+        self.__TYPEDEFS.setdefault(typekey, typeclass._full_text)
+        self.__topics[topickey] = self.__topics.get(topickey, 0) + 1
+
+
+    def _ensure_typedef(self, typename, typehash=None):
         """Parses subtype definition from any full definition where available, if not loaded."""
         typehash = typehash or next((h for n, h in self.__TYPEDEFS if n == typename), None)
         typekey = (typename, typehash)
         if typekey not in self.__TYPEDEFS:
             for (roottype, roothash), rootdef in list(self.__TYPEDEFS.items()):
                 rootkey = (roottype, roothash)
                 if self.__PARSEDS.get(rootkey): continue  # for (roottype, roothash)
@@ -230,18 +382,18 @@
         @param   progress  show progress bar for reindexing status
         """
         KWS = ["mode", "compression", "chunk_threshold",
                "allow_unindexed", "options", "skip_index"]
         kwargs.update(zip(KWS, args), allow_unindexed=True)
         copied, bar, f2 = False, None, None
         if progress:
-            fmt = lambda s: format_bytes(s, strip=False)
+            fmt = lambda s: common.format_bytes(s, strip=False)
             name, size = os.path.basename(f), os.path.getsize(f)
             aftertemplate = " Reindexing %s (%s): {afterword}" % (name, fmt(size))
-            bar = ProgressBar(size, interval=0.1, pulse=True, aftertemplate=aftertemplate)
+            bar = common.ProgressBar(size, interval=0.1, pulse=True, aftertemplate=aftertemplate)
 
         ConsolePrinter.warn("Unindexed bag %s, reindexing.", f)
         bar and bar.update(0).start()  # Start progress pulse
         try:
             with rosbag.Bag(f, **kwargs) as inbag:
                 inplace = (inbag.version > 102)
 
@@ -268,29 +420,30 @@
 
     @staticmethod
     def _run_reindex(inbag, outbag, bar=None):
         """Runs reindexing, showing optional progress bar."""
         update_bar = noop = lambda s: None
         indexbag, writebag = (inbag, outbag) if inbag.version == 102 else (outbag, None)
         if bar:
-            fmt = lambda s: format_bytes(s, strip=False)
+            fmt = lambda s: common.format_bytes(s, strip=False)
             update_bar = lambda s: (setattr(bar, "afterword", fmt(s)),
                                     setattr(bar, "pulse", False), bar.update(s).stop())
         # v102: build index from inbag, write all messages to outbag.
         # Other versions: re-build index in outbag file in-place.
         progress = update_bar if not writebag else noop  # Incremental progress during re-build
         for offset in indexbag.reindex():
             progress(offset)
         if not writebag:
             return
 
         progress = update_bar if bar else noop  # Incremental progress during re-write
         for (topic, msg, t, header) in indexbag.read_messages(return_connection_header=True):
             writebag.write(topic, msg, t, connection_header=header)
             progress(indexbag._file.tell())
+Bag = ROS1Bag
 
 
 
 def init_node(name):
     """
     Initializes a ROS1 node if not already initialized.
 
@@ -300,21 +453,24 @@
         return
 
     global master
     if not master:
         master = rospy.client.get_master()
         available = None
         while not available:
-            try: master.getSystemState()
+            try: uri = master.getUri()[-1]
             except Exception:
                 if available is None:
-                    ConsolePrinter.error("Unable to register with master. Will keep trying.")
+                    ConsolePrinter.log(logging.ERROR,
+                                       "Unable to register with master. Will keep trying.")
                 available = False
                 time.sleep(SLEEP_INTERVAL)
-            else: available = True
+            else:
+                ConsolePrinter.debug("Connected to ROS master at %s.", uri)
+                available = True
     try: rospy.get_rostime()
     except Exception:  # Init node only if not already inited
         rospy.init_node(name, anonymous=True, disable_signals=True)
 
 
 def shutdown_node():
     """Shuts down live ROS1 node."""
@@ -337,22 +493,36 @@
                              ", ".join(sorted(missing)))
     if "1" != os.getenv("ROS_VERSION", "1"):
         ConsolePrinter.error("ROS environment not supported: need ROS_VERSION=1.")
         missing = True
     return not missing
 
 
+@memoize
+def canonical(typename, unbounded=False):
+    """
+    Returns "pkg/Type" for "pkg/subdir/Type".
+
+    @param  unbounded  drop array bounds, e.g. returning "uint8[]" for "uint8[10]"
+    """
+    if typename and typename.count("/") > 1:
+        typename = "%s/%s" % tuple((x[0], x[-1]) for x in [typename.split("/")])[0]
+    if unbounded and typename and "[" in typename:
+        typename = typename[:typename.index("[")] + "[]"
+    return typename
+
+
 def create_publisher(topic, cls_or_typename, queue_size):
     """Returns a rospy.Publisher."""
     def pub_unregister():
         # ROS1 prints errors when closing a publisher with subscribers
         if not pub.get_num_connections(): super(rospy.Publisher, pub).unregister()
 
     cls = cls_or_typename
-    if isinstance(cls, str): cls = get_message_class(cls)
+    if isinstance(cls, common.TEXT_TYPES): cls = get_message_class(cls)
     pub = rospy.Publisher(topic, cls, queue_size=queue_size)
     pub.unregister = pub_unregister
     return pub
 
 
 def create_subscriber(topic, typename, handler, queue_size):
     """
@@ -396,67 +566,83 @@
     (aligning seconds and nanoseconds).
     """
     LENS = {"secs": 10, "nsecs": 9}
     v = "%s" % (value, )
     if not isinstance(msg, genpy.TVal) or name not in LENS:
         return v
 
-    EXTRA = sum(v.count(x) * len(x) for x in (MatchMarkers.START, MatchMarkers.END))
+    EXTRA = sum(v.count(x) * len(x) for x in (common.MatchMarkers.START, common.MatchMarkers.END))
     return ("%%%ds" % (LENS[name] + EXTRA)) % v  # Default %10s/%9s for secs/nsecs
 
 
 @memoize
 def get_message_class(typename):
     """Returns ROS1 message class."""
-    return roslib.message.get_message_class(typename)
-
-
-def get_message_data(msg):
-    """Returns ROS1 message as a serialized binary."""
-    with TypeMeta.make(msg) as m:
-        if m.data is not None: return m.data
-    buf = io.BytesIO()
-    msg.serialize(buf)
-    return buf.getvalue()
+    if typename in ROS_TIME_TYPES:
+        return next(k for k, v in ROS_TIME_CLASSES.items() if v == typename)
+    if typename in ("genpy/Time", "rospy/Time"):
+        return rospy.Time
+    if typename in ("genpy/Duration", "rospy/Duration"):
+        return rospy.Duration
+    try: return roslib.message.get_message_class(typename)
+    except Exception: return None
 
 
 def get_message_definition(msg_or_type):
-    """Returns ROS1 message type definition full text, including subtype definitions."""
+    """
+    Returns ROS1 message type definition full text, including subtype definitions.
+
+    Returns None if unknown type.
+    """
     msg_or_cls = msg_or_type if is_ros_message(msg_or_type) else get_message_class(msg_or_type)
-    return msg_or_cls._full_text
+    return getattr(msg_or_cls, "_full_text", None)
 
 
 def get_message_type_hash(msg_or_type):
-    """Returns ROS message type MD5 hash."""
+    """Returns ROS message type MD5 hash, or "" if unknown type."""
     msg_or_cls = msg_or_type if is_ros_message(msg_or_type) else get_message_class(msg_or_type)
-    return msg_or_cls._md5sum
+    return getattr(msg_or_cls, "_md5sum", "")
 
 
 def get_message_fields(val):
-    """Returns OrderedDict({field name: field type name}) if ROS1 message, else {}."""
-    names = getattr(val, "__slots__", [])
-    if isinstance(val, tuple(ROS_TIME_CLASSES)):  # Empty __slots__
-        names = genpy.TVal.__slots__
-    return collections.OrderedDict(zip(names, getattr(val, "_slot_types", [])))
+    """
+    Returns OrderedDict({field name: field type name}) if ROS1 message, else {}.
+
+    @param   val  ROS1 message class or instance
+    """
+    names, types = (getattr(val, k, []) for k in ("__slots__", "_slot_types"))
+    # Bug in genpy: class slot types defined as "int32", but everywhere else types use "uint32"
+    if isinstance(val, genpy.TVal): names, types = genpy.TVal.__slots__, ["uint32", "uint32"]
+    return collections.OrderedDict(zip(names, types))
 
 
 def get_message_type(msg_or_cls):
     """Returns ROS1 message type name, like "std_msgs/Header"."""
+    if is_ros_time(msg_or_cls):
+        cls = msg_or_cls if inspect.isclass(msg_or_cls) else type(msg_or_cls)
+        return "duration" if "duration" in cls.__name__.lower() else "time"
     return msg_or_cls._type
 
 
 def get_message_value(msg, name, typename):
     """Returns object attribute value, with numeric arrays converted to lists."""
     v = getattr(msg, name)
     return list(v) if typename.startswith("uint8[") and isinstance(v, bytes) else v
 
 
-def get_rostime():
-    """Returns current ROS1 time, as rospy.Time."""
-    return rospy.get_rostime()
+def get_rostime(fallback=False):
+    """
+    Returns current ROS1 time, as rospy.Time.
+
+    @param   fallback  use wall time if node not initialized
+    """
+    try: return rospy.get_rostime()
+    except Exception:
+        if fallback: return make_time(time.time())
+        raise
 
 
 def get_topic_types():
     """
     Returns currently available ROS1 topics, as [(topicname, typename)].
 
     Omits topics that the current ROS1 node itself has published.
@@ -468,44 +654,61 @@
         if topic not in mypubs:
             result.append((topic, typename))
     return result
 
 
 def is_ros_message(val, ignore_time=False):
     """
-    Returns whether value is a ROS1 message or special like ROS1 time/duration.
+    Returns whether value is a ROS1 message or special like ROS1 time/duration class or instance.
 
     @param  ignore_time  whether to ignore ROS1 time/duration types
     """
-    return isinstance(val, genpy.Message if ignore_time else (genpy.Message, genpy.TVal))
+    isfunc = issubclass if inspect.isclass(val) else isinstance
+    return isfunc(val, genpy.Message if ignore_time else (genpy.Message, genpy.TVal))
 
 
 def is_ros_time(val):
-    """Returns whether value is a ROS1 time/duration."""
-    return isinstance(val, genpy.TVal)
+    """Returns whether value is a ROS1 time/duration class or instance."""
+    return issubclass(val, genpy.TVal) if inspect.isclass(val) else isinstance(val, genpy.TVal)
 
 
 def make_duration(secs=0, nsecs=0):
     """Returns a ROS1 duration, as rospy.Duration."""
     return rospy.Duration(secs=secs, nsecs=nsecs)
 
 
 def make_time(secs=0, nsecs=0):
     """Returns a ROS1 time, as rospy.Time."""
     return rospy.Time(secs=secs, nsecs=nsecs)
 
 
+def serialize_message(msg):
+    """Returns ROS1 message as a serialized binary."""
+    with TypeMeta.make(msg) as m:
+        if m.data is not None: return m.data
+    buf = io.BytesIO()
+    msg.serialize(buf)
+    return buf.getvalue()
+
+
+def deserialize_message(raw, cls_or_typename):
+    """Returns ROS1 message or service request/response instantiated from serialized binary."""
+    cls = cls_or_typename
+    if isinstance(cls, common.TEXT_TYPES): cls = get_message_class(cls)
+    return cls().deserialize(raw)
+
+
 @memoize
 def scalar(typename):
     """
     Returns scalar type from ROS message data type, like "uint8" from "uint8[100]".
 
     Returns type unchanged if already a scalar.
     """
-    return typename[:typename.index("[")] if "[" in typename else typename
+    return typename[:typename.index("[")] if typename and "[" in typename else typename
 
 
 def set_message_value(obj, name, value):
     """Sets message or object attribute value."""
     setattr(obj, name, value)
 
 
@@ -518,7 +721,33 @@
     """Returns value in seconds if value is ROS1 time/duration, else value."""
     return val.to_sec() if isinstance(val, genpy.TVal) else val
 
 
 def to_sec_nsec(val):
     """Returns value as (seconds, nanoseconds) if value is ROS1 time/duration, else value."""
     return (val.secs, val.nsecs) if isinstance(val, genpy.TVal) else val
+
+
+def to_time(val):
+    """Returns value as ROS1 time if convertible (int/float/duration/datetime/decimal), else value."""
+    result = val
+    if isinstance(val, decimal.Decimal):
+        result = rospy.Time(int(val), float(val % 1) * 10**9)
+    elif isinstance(val, datetime.datetime):
+        result = rospy.Time(int(val.timestamp()), 1000 * val.microsecond)
+    elif isinstance(val, six.integer_types + (float, )):
+        result = rospy.Time(val)
+    elif isinstance(val, genpy.Duration):
+        result = rospy.Time(val.secs, val.nsecs)
+    return result
+
+
+__all__ = [
+    "BAG_EXTENSIONS", "ROS_ALIAS_TYPES", "ROS_TIME_CLASSES", "ROS_TIME_TYPES", "SKIP_EXTENSIONS",
+    "SLEEP_INTERVAL", "TYPECLASSES", "Bag", "ROS1Bag", "master",
+    "canonical", "create_publisher", "create_subscriber", "deserialize_message",
+    "format_message_value", "get_message_class", "get_message_definition", "get_message_fields",
+    "get_message_type", "get_message_type_hash", "get_message_value", "get_rostime",
+    "get_topic_types", "init_node", "is_ros_message", "is_ros_time", "make_duration", "make_time",
+    "scalar", "serialize_message", "set_message_value", "shutdown_node", "to_nsec", "to_sec",
+    "to_sec_nsec", "to_time", "validate",
+]
```

## grepros/ros2.py

```diff
@@ -4,41 +4,47 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     02.11.2021
-@modified    27.10.2022
+@modified    03.07.2023
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.ros2
 import array
 import collections
+import datetime
+import decimal
 import enum
 import inspect
+import io
 import os
 import re
 import sqlite3
 import threading
 import time
 
 import builtin_interfaces.msg
+try: import numpy
+except Exception: numpy = None
 import rclpy
+import rclpy.clock
 import rclpy.duration
 import rclpy.executors
 import rclpy.serialization
 import rclpy.time
 import rosidl_parser.parser
 import rosidl_parser.definition
 import rosidl_runtime_py.utilities
 import yaml
 
-from . common import ConsolePrinter, MatchMarkers, memoize
-from . import rosapi
+from . import api
+from . common import PATH_TYPES, ConsolePrinter, MatchMarkers, memoize
 
 
 ## Bagfile extensions to seek
 BAG_EXTENSIONS  = (".db3", )
 
 ## Bagfile extensions to skip
 SKIP_EXTENSIONS = ()
@@ -48,14 +54,18 @@
 
 ## ROS2 time/duration types and message types mapped to type names
 ROS_TIME_CLASSES = {rclpy.time.Time:                 "builtin_interfaces/Time",
                     builtin_interfaces.msg.Time:     "builtin_interfaces/Time",
                     rclpy.duration.Duration:         "builtin_interfaces/Duration",
                     builtin_interfaces.msg.Duration: "builtin_interfaces/Duration"}
 
+## ROS2 time/duration types mapped to message types
+ROS_TIME_MESSAGES = {rclpy.time.Time:          builtin_interfaces.msg.Time,
+                     rclpy.duration.Duration:  builtin_interfaces.msg.Duration}
+
 ## Mapping between type aliases and real types, like {"byte": "uint8"}
 ROS_ALIAS_TYPES = {"byte": "uint8", "char": "int8"}
 
 ## Data Distribution Service types to ROS builtins
 DDS_TYPES = {"boolean":             "bool",
              "float":               "float32",
              "double":              "float64",
@@ -74,16 +84,19 @@
 context = None
 
 ## rclpy.executors.Executor instance
 executor = None
 
 
 
-class Bag(object):
-    """ROS2 bag interface, partially mimicking rosbag.Bag."""
+class ROS2Bag(api.BaseBag):
+    """ROS2 bag reader and writer (SQLite format), providing most of rosbag.Bag interface."""
+
+    ## Whether bag supports reading or writing stream objects, overridden in subclasses
+    STREAMABLE = False
 
     ## ROS2 bag SQLite schema
     CREATE_SQL = """
 CREATE TABLE IF NOT EXISTS messages (
   id        INTEGER PRIMARY KEY,
   topic_id  INTEGER NOT NULL,
   timestamp INTEGER NOT NULL,
@@ -106,79 +119,124 @@
 
 
     def __init__(self, filename, mode="a", *_, **__):
         """
         @param   filename  bag file path to open
         @param   mode      file will be overwritten if "w"
         """
+        if not isinstance(filename, PATH_TYPES):
+            raise ValueError("invalid filename %r" % type(filename))
+        if mode not in self.MODES: raise ValueError("invalid mode %r" % mode)
+
         self._db     = None  # sqlite3.Connection instance
         self._mode   = mode
         self._topics = {}    # {(topic, typename): {id, name, type}}
         self._counts = {}    # {(topic, typename, typehash): message count}
         self._qoses  = {}    # {(topic, typename): [{qos profile dict}]}
+        self._iterer = None  # Generator from read_messages() for next()
+        self._ttinfo = None  # Cached result for get_type_and_topic_info()
+        self._filename = str(filename)
+        self._stop_on_error = True
 
-        ## Bagfile path
-        self.filename = filename
+        self._ensure_open(populate=("r" != mode))
 
 
-    def get_message_count(self):
-        """Returns the number of messages in the bag."""
-        self._ensure_open()
-        if self._has_table("messages"):
-            row = self._db.execute("SELECT COUNT(*) AS count FROM messages").fetchone()
-            return row["count"]
+    def get_message_count(self, topic_filters=None):
+        """
+        Returns the number of messages in the bag.
+
+        @param   topic_filters  list of topics or a single topic to filter by, if any
+        """
+        if self._db and self._has_table("messages"):
+            sql, where = "SELECT COUNT(*) AS count FROM messages", ""
+            if topic_filters:
+                self._ensure_topics()
+                topics = topic_filters
+                topics = topics if isinstance(topics, (dict, list, set, tuple)) else [topics]
+                topic_ids = [x["id"] for (topic, _), x in self._topics.items() if topic in topics]
+                where = " WHERE topic_id IN (%s)" % ", ".join(map(str, topic_ids))
+            return self._db.execute(sql + where).fetchone()["count"]
         return None
 
 
     def get_start_time(self):
-        """Returns the start time of the bag, as UNIX timestamp."""
-        self._ensure_open()
-        if self._has_table("messages"):
+        """Returns the start time of the bag, as UNIX timestamp, or None if bag empty."""
+        if self._db and self._has_table("messages"):
             row = self._db.execute("SELECT MIN(timestamp) AS val FROM messages").fetchone()
             if row["val"] is None: return None
             secs, nsecs = divmod(row["val"], 10**9)
             return secs + nsecs / 1E9
         return None
 
 
     def get_end_time(self):
-        """Returns the end time of the bag, as UNIX timestamp."""
-        self._ensure_open()
-        if self._has_table("messages"):
+        """Returns the end time of the bag, as UNIX timestamp, or None if bag empty."""
+        if self._db and self._has_table("messages"):
             row = self._db.execute("SELECT MAX(timestamp) AS val FROM messages").fetchone()
             if row["val"] is None: return None
             secs, nsecs = divmod(row["val"], 10**9)
             return secs + nsecs / 1E9
         return None
 
 
-    def get_topic_info(self, counts=False):
+    def get_topic_info(self, counts=True, ensure_types=True):
         """
         Returns topic and message type metainfo as {(topic, typename, typehash): count}.
 
-        @param   counts  whether to return actual message counts instead of None
+        Can skip retrieving message counts, as this requires a full table scan.
+        Can skip looking up message type classes, as those might be unavailable in ROS2 environment.
+
+        @param   counts        whether to return actual message counts instead of None
+        @param   ensure_types  whether to look up type classes instead of returning typehash as None
         """
-        self._ensure_open()
-        DEFAULTCOUNT = 0 if counts else None
-        if not self._counts and self._has_table("topics"):
-            for row in self._db.execute("SELECT * FROM topics ORDER BY id").fetchall():
-                topic, typename = row["name"], canonical(row["type"])
-                typehash = get_message_type_hash(typename)
-                self._topics[(topic, typename)] = row
-                self._counts[(topic, typename, typehash)] = DEFAULTCOUNT
+        self._ensure_topics()
+        if counts: self._ensure_counts()
+        if ensure_types: self._ensure_types()
+        return dict(self._counts)
 
-        if counts and self._has_table("messages") and not any(self._counts.values()):
-            topickeys = {v["id"]: (t, n, get_message_type_hash(n))
-                         for (t, n), v in self._topics.items()}
-            for row in self._db.execute("SELECT topic_id, COUNT(*) AS count FROM messages "
-                                        "GROUP BY topic_id").fetchall():
-                if row["topic_id"] in topickeys:
-                    self._counts[topickeys[row["topic_id"]]] = row["count"]
 
-        return dict(self._counts)
+    def get_type_and_topic_info(self, topic_filters=None):
+        """
+        Returns thorough metainfo on topic and message types.
+
+        @param   topic_filters  list of topics or a single topic to filter returned topics-dict by,
+                                if any
+        @return                 TypesAndTopicsTuple(msg_types, topics) namedtuple,
+                                msg_types as dict of {typename: typehash},
+                                topics as a dict of {topic: TopicTuple() namedtuple}.
+        """
+        topics = topic_filters
+        topics = topics if isinstance(topics, (list, set, tuple)) else [topics] if topics else []
+        if self._ttinfo and (not topics or set(topics) == set(t for t, _, _ in self._counts)):
+            return self._ttinfo
+        if self.closed: raise ValueError("I/O operation on closed file.")
+
+        counts = self.get_topic_info()
+        msgtypes = {n: h for t, n, h in counts}
+        topicdict = {}
+
+        def median(vals):
+            """Returns median value from given sorted numbers."""
+            vlen = len(vals)
+            return None if not vlen else vals[vlen // 2] if vlen % 2 else \
+                   float(vals[vlen // 2 - 1] + vals[vlen // 2]) / 2
+
+        for (t, n, _), c in sorted(counts.items(), key=lambda x: x[0][:2]):
+            if topics and t not in topics: continue  # for
+            mymedian = None
+            if c > 1:
+                args = (self._topics[(t, n)]["id"], )
+                cursor = self._db.execute("SELECT timestamp FROM messages WHERE topic_id = ?", args)
+                stamps = sorted(x["timestamp"] / 1E9 for x in cursor)
+                mymedian = median(sorted(s1 - s0 for s1, s0 in zip(stamps[1:], stamps[:-1])))
+            freq = 1.0 / mymedian if mymedian else None
+            topicdict[t] = self.TopicTuple(n, c, len(self.get_qoses(t, n) or []), freq)
+        result = self.TypesAndTopicsTuple(msgtypes, topicdict)
+        if not topics or set(topics) == set(t for t, _, _ in self._counts): self._ttinfo = result
+        return result
 
 
     def get_qoses(self, topic, typename):
         """Returns topic Quality-of-Service profiles as a list of dicts, or None if not available."""
         topickey = (topic, typename)
         if topickey not in self._qoses and topickey in self._topics:
             topicrow = self._topics[topickey]
@@ -188,154 +246,278 @@
             except Exception as e:
                 ConsolePrinter.warn("Error parsing quality of service for topic %r: %r", topic, e)
         self._qoses.setdefault(topickey, None)
         return self._qoses[topickey]
 
 
     def get_message_class(self, typename, typehash=None):
-        """Returns ROS2 message type class."""
-        return get_message_class(typename)
+        """Returns ROS2 message type class, or None if unknown message type for bag."""
+        self._ensure_topics()
+        if any(n == typename for _, n, in self._topics) and typehash is not None \
+        and not any((n, h) == (typename, typehash) for _, n, h in self._counts):
+            self._ensure_types([t for t, n in self._topics if n == typename])
+        if any((typename, typehash) in [(n, h), (n, None)] for _, n, h in self._counts):
+            return get_message_class(typename)
+        return None
 
 
     def get_message_definition(self, msg_or_type):
-        """Returns ROS2 message type definition full text, including subtype definitions."""
-        return get_message_definition(msg_or_type)
+        """
+        Returns ROS2 message type definition full text, including subtype definitions.
+
+        Returns None if unknown message type for bag.
+        """
+        self._ensure_topics()
+        typename = msg_or_type if isinstance(msg_or_type, str) else get_message_type(msg_or_type)
+        if any(n == typename for _, n, _ in self._counts):
+            return get_message_definition(msg_or_type)
+        return None
 
 
     def get_message_type_hash(self, msg_or_type):
-        """Returns ROS2 message type MD5 hash."""
-        return get_message_type_hash(msg_or_type)
+        """Returns ROS2 message type MD5 hash, or None if unknown message type for bag."""
+        typename = msg_or_type if isinstance(msg_or_type, str) else get_message_type(msg_or_type)
+        self._ensure_types([t for t, n in self._topics if n == typename])
+        return next((h for _, n, h in self._counts if n == typename), None)
 
 
-    def read_messages(self, topics=None, start_time=None, end_time=None, raw=False, *_, **__):
+    def read_messages(self, topics=None, start_time=None, end_time=None, raw=False, **__):
         """
         Yields messages from the bag, optionally filtered by topic and timestamp.
 
-        @param   topics      list of topics or a single topic to filter by, if at all
-        @param   start_time  earliest timestamp of message to return, as UNIX timestamp
-        @param   end_time    latest timestamp of message to return, as UNIX timestamp
+        @param   topics      list of topics or a single topic to filter by, if any
+        @param   start_time  earliest timestamp of message to return, as ROS time or convertible
+                             (int/float/duration/datetime/decimal)
+        @param   end_time    latest timestamp of message to return, as ROS time or convertible
+                             (int/float/duration/datetime/decimal)
         @param   raw         if True, then returned messages are tuples of
-                             (typename, bytes, typehash, typeclass)
-        @return              generator of (topic, message, rclpy.time.Time) tuples
+                             (typename, bytes, typehash, typeclass).
+                             If message type unavailable, returns None for hash and class.
+        @return              BagMessage namedtuples of
+                             (topic, message, timestamp as rclpy.time.Time)
         """
-        self.get_topic_info()
+        if self.closed: raise ValueError("I/O operation on closed file.")
+        if "w" == self._mode: raise io.UnsupportedOperation("read")
+
+        self._ensure_topics()
         if not self._topics or (topics is not None and not topics):
             return
 
         sql, exprs, args = "SELECT * FROM messages", [], ()
         if topics:
             topics = topics if isinstance(topics, (list, tuple)) else [topics]
             topic_ids = [x["id"] for (topic, _), x in self._topics.items() if topic in topics]
             exprs += ["topic_id IN (%s)" % ", ".join(map(str, topic_ids))]
         if start_time is not None:
             exprs += ["timestamp >= ?"]
-            args  += (start_time.nanoseconds, )
+            args  += (to_nsec(to_time(start_time)), )
         if end_time is not None:
             exprs += ["timestamp <= ?"]
-            args  += (end_time.nanoseconds, )
+            args  += (to_nsec(to_time(end_time)), )
         sql += ((" WHERE " + " AND ".join(exprs)) if exprs else "")
         sql += " ORDER BY timestamp"
 
         topicmap   = {v["id"]: v for v in self._topics.values()}
-        msgtypes   = {}  # {typename: cls}
+        msgtypes   = {}  # {typename: cls or None if unavailable}
         topicset   = set(topics or [t for t, _ in self._topics])
-        errortypes = set()  # {typename failed to instantiate, }
+        typehashes = {n: h for _, n, h in self._counts} # {typename: typehash or None or ""}
         for row in self._db.execute(sql, args):
             tdata = topicmap[row["topic_id"]]
             topic, typename = tdata["name"], canonical(tdata["type"])
-            if not raw and msgtypes.get(typename, typename) is None: continue # for row
-            typehash = next(h for t, n, h in self._counts if (t, n) == (topic, typename))
+            if not raw and msgtypes.get(typename, typename) is None: continue  # for row
+            if typehashes.get(typename) is None:
+                self._ensure_types([topic])
+                selector = (h for t, n, h in self._counts if (t, n) == (topic, typename))
+                typehash = typehashes[typename] = next(selector, None)
+            else: typehash = typehashes[typename]
 
             try:
                 cls = msgtypes.get(typename) or \
                       msgtypes.setdefault(typename, get_message_class(typename))
-                if raw: msg = (typename, row["data"], typehash, cls)
+                if raw: msg = (typename, row["data"], typehash or None, cls)
                 else:   msg = rclpy.serialization.deserialize_message(row["data"], cls)
             except Exception as e:
-                errortypes.add(typename)
-                ConsolePrinter.warn("Error loading type %s in topic %s: %%s" %
-                                    (typename, topic), e, __once=True)
-                if raw: msg = (typename, row["data"], typehash, None)
+                reportfunc = ConsolePrinter.error if self._stop_on_error else ConsolePrinter.warn
+                reportfunc("Error loading type %s in topic %s: %%s" % (typename, topic),
+                           "message class not found." if cls is None else e,
+                           __once=not self._stop_on_error)
+                if self._stop_on_error: raise
+                if raw: msg = (typename, row["data"], typehash or None, msgtypes.get(typename))
                 elif set(n for n, c in msgtypes.items() if c is None) == topicset:
                     break  # for row
                 continue  # for row
-            errortypes.discard(typename)
             stamp = rclpy.time.Time(nanoseconds=row["timestamp"])
 
-            yield topic, msg, stamp
-            if not self._db:
+            api.TypeMeta.make(msg, topic, self)
+            yield self.BagMessage(topic, msg, stamp)
+            if not self._db:  # Bag has been closed in the meantime
                 break  # for row
 
 
-    def write(self, topic, msg, stamp, meta=None):
+    def write(self, topic, msg, t=None, raw=False, qoses=None, **__):
         """
         Writes a message to the bag.
 
         @param   topic  name of topic
         @param   msg    ROS2 message
-        @param   stamp  rclpy.time.Time of message publication
-        @param   meta   message metainfo dict (meta["qoses"] added to topics-table, if any)
+        @param   t      message timestamp if not using wall time, as ROS time or convertible
+                        (int/float/duration/datetime/decimal)
+        @param   qoses  topic Quality-of-Service settings, if any, as a list of dicts
         """
-        self._ensure_open(populate=True)
-        self.get_topic_info()
+        if self.closed: raise ValueError("I/O operation on closed file.")
+        if "r" == self._mode: raise io.UnsupportedOperation("write")
 
-        cursor = self._db.cursor()
-        typename = get_message_type(msg)
+        self._ensure_topics()
+        if raw:
+            typename, binary, typehash = msg[:3]
+        else:
+            typename = get_message_type(msg)
+            typehash = get_message_type_hash(msg)
+            binary   = serialize_message(msg)
         topickey = (topic, typename)
+        cursor = self._db.cursor()
         if topickey not in self._topics:
-            full_typename = make_full_typename(get_message_type(msg))
+            full_typename = make_full_typename(typename)
             sql = "INSERT INTO topics (name, type, serialization_format, offered_qos_profiles) " \
                   "VALUES (?, ?, ?, ?)"
-            qoses = yaml.safe_dump(meta["qoses"], ) if meta and meta.get("qoses") else ""
+            qoses = yaml.safe_dump(qoses) if isinstance(qoses, list) else ""
             args = (topic, full_typename, "cdr", qoses)
             cursor.execute(sql, args)
             tdata = {"id": cursor.lastrowid, "name": topic, "type": full_typename,
                      "serialization_format": "cdr", "offered_qos_profiles": qoses}
             self._topics[topickey] = tdata
 
+        timestamp = (time.time_ns() if hasattr(time, "time_ns") else int(time.time() * 10**9)) \
+                    if t is None else to_nsec(to_time(t))
         sql = "INSERT INTO messages (topic_id, timestamp, data) VALUES (?, ?, ?)"
-        args = (self._topics[topickey]["id"], stamp.nanoseconds, get_message_data(msg))
+        args = (self._topics[topickey]["id"], timestamp, binary)
         cursor.execute(sql, args)
+        countkey = (topic, typename, typehash)
+        if self._counts.get(countkey, self) is not None:
+            self._counts[countkey] = self._counts.get(countkey, 0) + 1
+        self._ttinfo = None
+
+
+    def open(self):
+        """Opens the bag file if not already open."""
+        self._ensure_open()
 
 
     def close(self):
         """Closes the bag file."""
         if self._db:
             self._db.close()
-            self._db   = None
-            self._mode = None
+            self._db     = None
+            self._mode   = None
+            self._iterer = None
+
+
+    @property
+    def closed(self):
+        """Returns whether file is closed."""
+        return not self._db
+
+
+    @property
+    def topics(self):
+        """Returns the list of topics in bag, in alphabetic order."""
+        return sorted((t for t, _, _ in self._topics), key=str.lower)
+
+
+    @property
+    def filename(self):
+        """Returns bag file path."""
+        return self._filename
 
 
     @property
     def size(self):
         """Returns current file size in bytes (including journaling files)."""
-        result = os.path.getsize(self.filename) if os.path.isfile(self.filename) else None
+        result = os.path.getsize(self._filename) if os.path.isfile(self._filename) else None
         for suffix in ("-journal", "-wal") if result else ():
-            path = self.filename + suffix
+            path = "%s%s" % (self._filename, suffix)
             result += os.path.getsize(path) if os.path.isfile(path) else 0
         return result
 
 
+    @property
+    def mode(self):
+        """Returns file open mode."""
+        return self._mode
+
+
+    def __contains__(self, key):
+        """Returns whether bag contains given topic."""
+        return any(key == t for t, _, _ in self._topics)
+
+
+    def __next__(self):
+        """Retrieves next message from bag as (topic, message, timestamp)."""
+        if self.closed: raise ValueError("I/O operation on closed file.")
+        if self._iterer is None: self._iterer = self.read_messages()
+        return next(self._iterer)
+
+
     def _ensure_open(self, populate=False):
-        """Opens bag database if not open, can populate schema if not present."""
-        if self._db:
-            return
-        if "w" == self._mode and os.path.exists(self.filename):
-            os.remove(self.filename)
-        self._db = sqlite3.connect(self.filename, detect_types=sqlite3.PARSE_DECLTYPES,
-                                   isolation_level=None, check_same_thread=False)
-        self._db.row_factory = lambda cursor, row: dict(sqlite3.Row(cursor, row))
+        """Opens bag database if not open, populates schema if specified."""
+        if not self._db:
+            if "w" == self._mode and os.path.exists(self._filename):
+                os.remove(self._filename)
+            self._db = sqlite3.connect(self._filename, detect_types=sqlite3.PARSE_DECLTYPES,
+                                       isolation_level=None, check_same_thread=False)
+            self._db.row_factory = lambda cursor, row: dict(sqlite3.Row(cursor, row))
         if populate:
             self._db.executescript(self.CREATE_SQL)
 
 
+    def _ensure_topics(self):
+        """Populates local topic struct from database, if not already available."""
+        if not self._db or self._topics or not self._has_table("topics"): return
+        for row in self._db.execute("SELECT * FROM topics ORDER BY id"):
+            topickey = (topic, typename) = row["name"], canonical(row["type"])
+            self._topics[(topic, typename)] = row
+            self._counts[(topic, typename, None)] = None
+
+
+    def _ensure_counts(self):
+        """Populates local counts values from database, if not already available."""
+        if not self._db or all(v is not None for v in self._counts.values()) \
+        or not self._has_table("messages"): return
+        self._ensure_topics()
+        topickeys = {self._topics[(t, n)]["id"]: (t, n, h) for (t, n, h) in self._counts}
+        self._counts.clear()
+        for row in self._db.execute("SELECT topic_id, COUNT(*) AS count FROM messages "
+                                    "GROUP BY topic_id").fetchall():
+            if row["topic_id"] in topickeys:
+                self._counts[topickeys[row["topic_id"]]] = row["count"]
+
+
+    def _ensure_types(self, topics=None):
+        """
+        Populates local type definitions and classes from database, if not already available.
+
+        @param   topics  selected topics to ensure types for, if not all
+        """
+        if not self._db or (not topics and topics is not None) or not self._has_table("topics") \
+        or not any(h is None for t, _, h in self._counts if topics is None or t in topics):
+            return
+        self._ensure_topics()
+        for countkey, count in list(self._counts.items()):
+            (topic, typename, typehash) = countkey
+            if typehash is None and (topics is None or topic in topics):
+                typehash = get_message_type_hash(typename)
+                self._counts.pop(countkey)
+                self._counts[(topic, typename, typehash)] = count
+
+
     def _has_table(self, name):
         """Returns whether specified table exists in database."""
         sql = "SELECT 1 FROM sqlite_master WHERE type = ? AND name = ?"
         return bool(self._db.execute(sql, ("table", name)).fetchone())
+Bag = ROS2Bag
 
 
 
 def init_node(name):
     """Initializes a ROS2 node if not already initialized."""
     global node, context, executor
     if node or not validate(live=True):
@@ -381,20 +563,24 @@
     if "2" != os.getenv("ROS_VERSION", "2"):
         ConsolePrinter.error("ROS environment not supported: need ROS_VERSION=2.")
         missing = True
     return not missing
 
 
 @memoize
-def canonical(typename):
+def canonical(typename, unbounded=False):
     """
     Returns "pkg/Type" for "pkg/msg/Type", standardizes various ROS2 formats.
 
     Converts DDS types like "octet" to "byte", and "sequence<uint8, 100>" to "uint8[100]".
+
+    @param  unbounded  drop constraints like array and string bounds,
+                       e.g. returning "uint8[]" for "uint8[10]" and "string" for "string<=8"
     """
+    if not typename: return typename
     is_array, bound, dimension = False, "", ""
 
     if "<" in typename:
         match = re.match("sequence<(.+)>", typename)
         if match:  # "sequence<uint8, 100>" or "sequence<uint8>"
             is_array = True
             typename = match.group(1)
@@ -413,15 +599,16 @@
 
     if "<=" in typename:  # "string<=5"
         typename, bound = typename.split("<=")
 
     if typename.count("/") > 1:
         typename = "%s/%s" % tuple((x[0], x[-1]) for x in [typename.split("/")])[0]
 
-    suffix = ("<=%s" % bound if bound else "") + ("[%s]" % dimension if is_array else "")
+    if unbounded: suffix = "[]" if is_array else ""
+    else: suffix = ("<=%s" % bound if bound else "") + ("[%s]" % dimension if is_array else "")
     return DDS_TYPES.get(typename, typename) + suffix
 
 
 def create_publisher(topic, cls_or_typename, queue_size):
     """Returns an rclpy.Publisher instance, with .get_num_connections() and .unregister()."""
     cls = cls_or_typename
     if isinstance(cls, str): cls = get_message_class(cls)
@@ -457,19 +644,14 @@
     sub.get_message_definition = lambda: get_message_definition(cls)
     sub.get_message_type_hash  = lambda: get_message_type_hash(cls)
     sub.get_qoses              = lambda: qosdicts
     sub.unregister             = sub.destroy
     return sub
 
 
-def deserialize_message(raw, cls):
-    """Returns ROS2 message or service request/response instantiated from serialized binary."""
-    return rclpy.serialization.deserialize_message(raw, cls)
-
-
 def format_message_value(msg, name, value):
     """
     Returns a message attribute value as string.
 
     Result is at least 10 chars wide if message is a ROS2 time/duration
     (aligning seconds and nanoseconds).
     """
@@ -480,65 +662,63 @@
 
     EXTRA = sum(v.count(x) * len(x) for x in (MatchMarkers.START, MatchMarkers.END))
     return ("%%%ds" % (LENS[name] + EXTRA)) % v  # Default %10s/%9s for secs/nanosecs
 
 
 @memoize
 def get_message_class(typename):
-    """Returns ROS2 message class."""
-    return rosidl_runtime_py.utilities.get_message(make_full_typename(typename))
-
-
-def get_message_data(msg):
-    """Returns ROS2 message as a serialized binary."""
-    with rosapi.TypeMeta.make(msg) as m:
-        if m.data is not None: return m.data
-    return rclpy.serialization.serialize_message(msg)
+    """Returns ROS2 message class, or None if unknown type."""
+    try: return rosidl_runtime_py.utilities.get_message(make_full_typename(typename))
+    except Exception: return None
 
 
 def get_message_definition(msg_or_type):
-    """Returns ROS2 message type definition full text, including subtype definitions."""
+    """
+    Returns ROS2 message type definition full text, including subtype definitions.
+
+    Returns None if unknown type.
+    """
     typename = msg_or_type if isinstance(msg_or_type, str) else get_message_type(msg_or_type)
     return _get_message_definition(canonical(typename))
 
 
 def get_message_type_hash(msg_or_type):
-    """Returns ROS2 message type MD5 hash."""
+    """Returns ROS2 message type MD5 hash, or "" if unknown type."""
     typename = msg_or_type if isinstance(msg_or_type, str) else get_message_type(msg_or_type)
     return _get_message_type_hash(canonical(typename))
 
 
 @memoize
 def _get_message_definition(typename):
-    """Returns ROS2 message type definition full text, or "" on error (internal caching method)."""
+    """Returns ROS2 message type definition full text, or None on error (internal caching method)."""
     try:
         texts, pkg = collections.OrderedDict(), typename.rsplit("/", 1)[0]
         try:
             typepath = rosidl_runtime_py.get_interface_path(make_full_typename(typename) + ".msg")
             with open(typepath) as f:
                 texts[typename] = f.read()
         except Exception:  # .msg file unavailable: parse IDL
             texts[typename] = get_message_definition_idl(typename)
         for line in texts[typename].splitlines():
             if not line or not line[0].isalpha():
                 continue  # for line
             linetype = scalar(canonical(re.sub(r"^([a-zA-Z][^\s]+)(.+)", r"\1", line)))
-            if linetype in rosapi.ROS_BUILTIN_TYPES:
+            if linetype in api.ROS_BUILTIN_TYPES:
                 continue  # for line
             linetype = linetype if "/" in linetype else "std_msgs/Header" \
                        if "Header" == linetype else "%s/%s" % (pkg, linetype)
             linedef = None if linetype in texts else get_message_definition(linetype)
             if linedef: texts[linetype] = linedef
 
         basedef = texts.pop(next(iter(texts)))
         subdefs = ["%s\nMSG: %s\n%s" % ("=" * 80, k, v) for k, v in texts.items()]
         return basedef + ("\n" if subdefs else "") + "\n".join(subdefs)
     except Exception as e:
         ConsolePrinter.warn("Error collecting type definition of %s: %s", typename, e)
-        return ""
+        return None
 
 
 @memoize
 def get_message_definition_idl(typename):
     """
     Returns ROS2 message type definition parsed from IDL file.
 
@@ -618,83 +798,90 @@
     return "\n".join(lines)
 
 
 @memoize
 def _get_message_type_hash(typename):
     """Returns ROS2 message type MD5 hash (internal caching method)."""
     msgdef = get_message_definition(typename)
-    return rosapi.calculate_definition_hash(typename, msgdef)
+    return "" if msgdef is None else api.calculate_definition_hash(typename, msgdef)
 
 
 def get_message_fields(val):
     """Returns OrderedDict({field name: field type name}) if ROS2 message, else {}."""
     if not is_ros_message(val): return {}
-    fields = {k: canonical(v) for k, v in val.get_fields_and_field_types().items()}
+    fields = ((k, canonical(v)) for k, v in val.get_fields_and_field_types().items())
     return collections.OrderedDict(fields)
 
 
 def get_message_type(msg_or_cls):
     """Returns ROS2 message type name, like "std_msgs/Header"."""
     cls = msg_or_cls if inspect.isclass(msg_or_cls) else type(msg_or_cls)
     return canonical("%s/%s" % (cls.__module__.split(".")[0], cls.__name__))
 
 
 def get_message_value(msg, name, typename):
     """Returns object attribute value, with numeric arrays converted to lists."""
     v, scalartype = getattr(msg, name), scalar(typename)
-    if isinstance(v, (bytes, array.array)) \
-    or "numpy.ndarray" == "%s.%s" % (v.__class__.__module__, v.__class__.__name__):
-        v = list(v)
+    if isinstance(v, (bytes, array.array)): v = list(v)
+    elif numpy and isinstance(v, (numpy.generic, numpy.ndarray)):
+        v = v.tolist()  # Returns value as Python type, either scalar or list
     if v and isinstance(v, (list, tuple)) and scalartype in ("byte", "uint8"):
         if isinstance(v[0], bytes):
             v = list(map(ord, v))  # In ROS2, a byte array like [0, 1] is [b"\0", b"\1"]
         elif scalartype == typename:
             v = v[0]  # In ROS2, single byte values are given as bytes()
     return v
 
 
-def get_rostime():
-    """Returns current ROS2 time, as rclpy.time.Time."""
-    return node.get_clock().now()
+def get_rostime(fallback=False):
+    """
+    Returns current ROS2 time, as rclpy.time.Time.
+
+    @param   fallback  use wall time if node not initialized
+    """
+    try: return node.get_clock().now()
+    except Exception:
+        if fallback: return make_time(time.time())
+        raise
 
 
 def get_topic_types():
     """
     Returns currently available ROS2 topics, as [(topicname, typename)].
 
     Omits topics that the current ROS2 node itself has published.
     """
     result = []
     myname, myns = node.get_name(), node.get_namespace()
     mytypes = {}  # {topic: [typename, ]}
-    for topic, typename in node.get_publisher_names_and_types_by_node(myname, myns):
-        mytypes.setdefault(topic, []).append(typename)
+    for topic, typenames in node.get_publisher_names_and_types_by_node(myname, myns):
+        mytypes.setdefault(topic, []).extend(typenames)
     for t in ("/parameter_events", "/rosout"):  # Published by all nodes
         mytypes.pop(t, None)
     for topic, typenames in node.get_topic_names_and_types():  # [(topicname, [typename, ])]
         for typename in typenames:
             if topic not in mytypes or typename not in mytypes[topic]:
                 result += [(topic, canonical(typename))]
     return result
 
 
 def is_ros_message(val, ignore_time=False):
     """
-    Returns whether value is a ROS2 message or special like ROS2 time/duration.
+    Returns whether value is a ROS2 message or special like ROS2 time/duration class or instance.
 
     @param  ignore_time  whether to ignore ROS2 time/duration types
     """
     is_message = rosidl_runtime_py.utilities.is_message(val)
-    if is_message and ignore_time:
-        is_message = not isinstance(val, tuple(ROS_TIME_CLASSES))
+    if is_message and ignore_time: is_message = not is_ros_time(val)
     return is_message
 
 
 def is_ros_time(val):
-    """Returns whether value is a ROS2 time/duration."""
+    """Returns whether value is a ROS2 time/duration class or instance."""
+    if inspect.isclass(val): return issubclass(val, tuple(ROS_TIME_CLASSES))
     return isinstance(val, tuple(ROS_TIME_CLASSES))
 
 
 def make_duration(secs=0, nsecs=0):
     """Returns an rclpy.duration.Duration."""
     return rclpy.duration.Duration(seconds=secs, nanoseconds=nsecs)
 
@@ -740,37 +927,70 @@
                 val = val.value
             elif isinstance(val, tuple(ROS_TIME_CLASSES)):
                 val = dict(zip(["sec", "nsec"], to_sec_nsec(val)))
             result[name] = val
     return [result]
 
 
+def serialize_message(msg):
+    """Returns ROS2 message as a serialized binary."""
+    with api.TypeMeta.make(msg) as m:
+        if m.data is not None: return m.data
+    return rclpy.serialization.serialize_message(msg)
+
+
+def deserialize_message(raw, cls_or_typename):
+    """Returns ROS2 message or service request/response instantiated from serialized binary."""
+    cls = cls_or_typename
+    if isinstance(cls, str): cls = get_message_class(cls)
+    return rclpy.serialization.deserialize_message(raw, cls)
+
+
 @memoize
 def scalar(typename):
     """
     Returns unbounded scalar type from ROS2 message data type
 
     Like "uint8" from "uint8[]", or "string" from "string<=10[<=5]".
     Returns type unchanged if not a collection or bounded type.
     """
-    if "["  in typename: typename = typename[:typename.index("[")]
-    if "<=" in typename: typename = typename[:typename.index("<=")]
+    if typename and "["  in typename: typename = typename[:typename.index("[")]
+    if typename and "<=" in typename: typename = typename[:typename.index("<=")]
     return typename
 
 
 def set_message_value(obj, name, value):
     """Sets message or object attribute value."""
     if is_ros_message(obj):
         # Bypass setter as it does type checking
         fieldmap = obj.get_fields_and_field_types()
         if name in fieldmap:
             name = obj.__slots__[list(fieldmap).index(name)]
     setattr(obj, name, value)
 
 
+def time_message(val, to_message=True, clock_type=None):
+    """
+    Converts ROS2 time/duration between `rclpy` and `builtin_interfaces` objects.
+
+    @param   val         ROS2 time/duration object from `rclpy` or `builtin_interfaces`
+    @param   to_message  whether to convert from `rclpy` to `builtin_interfaces` or vice versa
+    @param   clock_type  ClockType for converting to `rclpy.Time`, defaults to `ROS_TIME`
+    @return              value converted to appropriate type, or original value if not convertible
+    """
+    to_message, clock_type = bool(to_message), (clock_type or rclpy.clock.ClockType.ROS_TIME)
+    if isinstance(val, tuple(ROS_TIME_CLASSES)):
+        rcl_cls = next(k for k, v in ROS_TIME_MESSAGES.items() if isinstance(val, (k, v)))
+        is_rcl = isinstance(val, tuple(ROS_TIME_MESSAGES))
+        name = "to_msg" if to_message and is_rcl else "from_msg" if to_message == is_rcl else None
+        args = [val] + ([clock_type] if rcl_cls is rclpy.time.Time and "from_msg" == name else [])
+        return getattr(rcl_cls, name)(*args) if name else val
+    return val
+
+
 def to_nsec(val):
     """Returns value in nanoseconds if value is ROS2 time/duration, else value."""
     if not isinstance(val, tuple(ROS_TIME_CLASSES)):
         return val
     if hasattr(val, "nanoseconds"):  # rclpy.Time/Duration
         return val.nanoseconds
     return val.sec * 10**9 + val.nanosec  # builtin_interfaces.msg.Time/Duration
@@ -791,7 +1011,40 @@
     if not isinstance(val, tuple(ROS_TIME_CLASSES)):
         return val
     if hasattr(val, "seconds_nanoseconds"):  # rclpy.Time
         return val.seconds_nanoseconds()
     if hasattr(val, "nanoseconds"):  # rclpy.Duration
         return divmod(val.nanoseconds, 10**9)
     return (val.sec, val.nanosec)  # builtin_interfaces.msg.Time/Duration
+
+
+def to_time(val):
+    """
+    Returns value as ROS2 time if convertible, else value.
+
+    Convertible types: int/float/duration/datetime/decimal/builtin_interfaces.Time.
+    """
+    result = val
+    if isinstance(val, decimal.Decimal):
+        result = make_time(int(val), float(val % 1) * 10**9)
+    elif isinstance(val, datetime.datetime):
+        result = make_time(int(val.timestamp()), 1000 * val.microsecond)
+    elif isinstance(val, (float, int)):
+        result = make_time(val)
+    elif isinstance(val, rclpy.duration.Duration):
+        result = make_time(nsecs=val.nanoseconds)
+    elif isinstance(val, tuple(ROS_TIME_MESSAGES.values())):
+        result = make_time(val.sec, val.nanosec)
+    return result
+
+
+__all__ = [
+    "BAG_EXTENSIONS", "DDS_TYPES", "ROS_ALIAS_TYPES", "ROS_TIME_CLASSES", "ROS_TIME_MESSAGES",
+    "ROS_TIME_TYPES", "SKIP_EXTENSIONS", "Bag", "ROS2Bag", "context", "executor", "node",
+    "canonical", "create_publisher", "create_subscriber", "deserialize_message",
+    "format_message_value", "get_message_class", "get_message_definition",
+    "get_message_definition_idl", "get_message_fields", "get_message_type",
+    "get_message_type_hash", "get_message_value", "get_rostime", "get_topic_types", "init_node",
+    "is_ros_message", "is_ros_time", "make_duration", "make_full_typename", "make_subscriber_qos",
+    "make_time", "qos_to_dict", "scalar", "serialize_message", "set_message_value", "shutdown_node",
+    "time_message", "to_nsec", "to_sec", "to_sec_nsec", "to_time", "validate",
+]
```

## grepros/search.py

```diff
@@ -4,221 +4,354 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     28.09.2021
-@modified    18.10.2022
+@modified    02.07.2023
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.search
 import copy
 import collections
 import re
 
-from . common import MatchMarkers, filter_fields, merge_spans, wildcard_to_regex
-from . import rosapi
+from . import api
+from . import common
+from . import inputs
+
+
+class Scanner(object):
+    """
+    ROS message grepper.
+
+    In highlighted results, message field values that match search criteria are modified
+    to wrap the matching parts in {@link grepros.common.MatchMarkers MatchMarkers} tags,
+    with numeric field values converted to strings beforehand.
+    """
 
-
-class Searcher(object):
-    """ROS message grepper."""
+    ## Namedtuple of (topic name, ROS message, ROS time object, message if matched, index in topic).
+    GrepMessage = collections.namedtuple("BagMessage", "topic message timestamp match index")
 
     ## Match patterns for global any-match
     ANY_MATCHES = [((), re.compile("(.*)", re.DOTALL)), (), re.compile("(.?)", re.DOTALL)]
 
+    ## Constructor argument defaults
+    DEFAULT_ARGS = dict(PATTERN=(), CASE=False, FIXED_STRING=False, INVERT=False, HIGHLIGHT=False,
+                        NTH_MATCH=1, BEFORE=0, AFTER=0, CONTEXT=0, MAX_COUNT=0,
+                        MAX_PER_TOPIC=0, MAX_TOPICS=0, SELECT_FIELD=(), NOSELECT_FIELD=(),
+                        MATCH_WRAPPER="**")
+
 
-    def __init__(self, args):
+    def __init__(self, args=None, **kwargs):
         """
-        @param   args                     arguments object like argparse.Namespace
-        @param   args.PATTERNS            pattern(s) to find in message field values
-        @param   args.RAW                 PATTERNS are ordinary strings, not regular expressions
-        @param   args.CASE                use case-sensitive matching in PATTERNS
-        @param   args.INVERT              select non-matching messages
-        @param   args.BEFORE              number of messages of leading context to emit before match
-        @param   args.AFTER               number of messages of trailing context to emit after match
-        @param   args.MAX_MATCHES         number of matched messages to emit (per file if bag input)
-        @param   args.MAX_TOPIC_MATCHES   number of matched messages to emit from each topic
-        @param   args.MAX_TOPICS          number of topics to print matches from
-        @param   args.NTH_MATCH           emit every Nth match in topic
-        @param   args.SELECT_FIELDS       message fields to use in matching if not all
-        @param   args.NOSELECT_FIELDS     message fields to skip in matching
+        @param   args                     arguments as namespace or dictionary, case-insensitive
+        @param   args.pattern             pattern(s) to find in message field values
+        @param   args.fixed_string        pattern contains ordinary strings, not regular expressions
+        @param   args.case                use case-sensitive matching in pattern
+        @param   args.invert              select messages not matching pattern
+        @param   args.highlight           highlight matched values
+        @param   args.before              number of messages of leading context to emit before match
+        @param   args.after               number of messages of trailing context to emit after match
+        @param   args.context             number of messages of leading and trailing context to emit
+                                          around match, overrides args.before and args.after
+        @param   args.max_count           number of matched messages to emit (per file if bag input)
+        @param   args.max_per_topic       number of matched messages to emit from each topic
+        @param   args.max_topics          number of topics to emit matches from
+        @param   args.nth_match           emit every Nth match in topic
+        @param   args.select_field        message fields to use in matching if not all
+        @param   args.noselect_field      message fields to skip in matching
+        @param   args.match_wrapper       string to wrap around matched values in find() and match(),
+                                          both sides if one value, start and end if more than one,
+                                          or no wrapping if zero values (default "**")
+        @param   kwargs                   any and all arguments as keyword overrides, case-insensitive
+        <!--sep-->
+
+        Additional arguments when using match() or find(grepros.api.Bag):
+
+        @param   args.topic               ROS topics to read if not all
+        @param   args.type                ROS message types to read if not all
+        @param   args.skip_topic          ROS topics to skip
+        @param   args.skip_type           ROS message types to skip
+        @param   args.start_time          earliest timestamp of messages to read
+        @param   args.end_time            latest timestamp of messages to read
+        @param   args.start_index         message index within topic to start from
+        @param   args.end_index           message index within topic to stop at
+        @param   args.unique              emit messages that are unique in topic
+        @param   args.nth_message         read every Nth message in topic
+        @param   args.nth_interval        minimum time interval between messages in topic
+        @param   args.condition           Python expressions that must evaluate as true
+                                          for message to be processable, see ConditionMixin
+        @param   args.progress            whether to print progress bar
+        @param   args.stop_on_error       stop execution on any error like unknown message type
         """
         # {key: [(() if any field else ('nested', 'path') or re.Pattern, re.Pattern), ]}
         self._patterns = {}
         # {(topic, typename, typehash): {message ID: message}}
         self._messages = collections.defaultdict(collections.OrderedDict)
         # {(topic, typename, typehash): {message ID: ROS time}}
         self._stamps   = collections.defaultdict(collections.OrderedDict)
         # {(topic, typename, typehash): {None: processed, True: matched, False: emitted as context}}
         self._counts   = collections.defaultdict(collections.Counter)
         # {(topic, typename, typehash): {message ID: True if matched else False if emitted else None}}
         self._statuses = collections.defaultdict(collections.OrderedDict)
         # Patterns to check in message plaintext and skip full matching if not found
         self._brute_prechecks = []
-        self._passthrough     = False  # Pass all messages to sink, skip matching and highlighting
-        self._source = None  # SourceBase instance
-        self._sink   = None  # SinkBase instance
+        self._idcounter       = 0      # Counter for unique message IDs
+        self._highlight       = None   # Highlight matched values in message fields
+        self._passthrough     = False  # Emit messages without pattern-matching and highlighting
+
+        ## Source instance
+        self.source = None
+        ## Sink instance
+        self.sink   = None
 
-        self.args = copy.deepcopy(args)
+        self.args = common.ensure_namespace(args, Scanner.DEFAULT_ARGS, **kwargs)
+        if self.args.CONTEXT: self.args.BEFORE = self.args.AFTER = self.args.CONTEXT
         self._parse_patterns()
 
 
-    def search(self, source, sink):
+    def find(self, source, highlight=None):
+        """
+        Yields matched and context messages from source.
+
+        @param   source     inputs.Source or api.Bag instance
+        @param   highlight  whether to highlight matched values in message fields,
+                            defaults to flag from constructor
+        @return             GrepMessage namedtuples of
+                            (topic, message, timestamp, match, index in topic),
+                            where match is matched optionally highlighted message
+                            or `None` if yielding a context message
+        """
+        if isinstance(source, api.Bag):
+            source = inputs.BagSource(source, **vars(self.args))
+        self._prepare(source, highlight=highlight)
+        for topic, msg, stamp, matched, index in self._generate():
+            yield self.GrepMessage(topic, msg, stamp, matched, index)
+
+
+    def match(self, topic, msg, stamp, highlight=None):
+        """
+        Returns matched message if message matches search filters.
+
+        @param   topic      topic name
+        @param   msg        ROS message
+        @param   stamp      message ROS timestamp
+        @param   highlight  whether to highlight matched values in message fields,
+                            defaults to flag from constructor
+        @return             original or highlighted message on match else `None`
+        """
+        result = None
+        if not isinstance(self.source, inputs.AppSource):
+            self._prepare(inputs.AppSource(self.args), highlight=highlight)
+        if self._highlight != bool(highlight): self._configure_flags(highlight=highlight)
+
+        self.source.push(topic, msg, stamp)
+        item = self.source.read_queue()
+        if item is not None:
+            msgid = self._idcounter = self._idcounter + 1
+            topickey = api.TypeMeta.make(msg, topic).topickey
+            self._register_message(topickey, msgid, msg, stamp)
+            matched = self._is_processable(topic, msg, stamp) and self.get_match(msg)
+
+            self.source.notify(matched)
+            if matched and not self._counts[topickey][True] % (self.args.NTH_MATCH or 1):
+                self._statuses[topickey][msgid] = True
+                self._counts[topickey][True] += 1
+                result = matched
+            elif matched:  # Not NTH_MATCH, skip emitting
+                self._statuses[topickey][msgid] = True
+                self._counts[topickey][True] += 1
+            self._prune_data(topickey)
+            self.source.mark_queue(topic, msg, stamp)
+        return result
+
+
+    def work(self, source, sink):
         """
         Greps messages yielded from source and emits matched content to sink.
 
-        @param   source  inputs.SourceBase instance
-        @param   sink    outputs.SinkBase instance
+        @param   source  inputs.Source or api.Bag instance
+        @param   sink    outputs.Sink instance
         @return          count matched
         """
-        self._prepare(source, sink)
-        counter, total_matched, batch_matched, batch = 0, 0, False, None
+        if isinstance(source, api.Bag):
+            source = inputs.BagSource(source, **vars(self.args))
+        self._prepare(source, sink, highlight=self.args.HIGHLIGHT)
+        total_matched = 0
+        for topic, msg, stamp, matched, index in self._generate():
+            sink.emit_meta()
+            sink.emit(topic, msg, stamp, matched, index)
+            total_matched += bool(matched)
+        return total_matched
+
+
+    def __enter__(self):
+        """Context manager entry, does nothing, returns self."""
+        return self
+
+
+    def __exit__(self, exc_type, exc_value, traceback):
+        """Context manager exit, does nothing."""
+        return self
+
+
+    def _generate(self):
+        """
+        Yields matched and context messages from source.
 
-        for topic, msg, stamp in source.read():
-            if batch != source.get_batch():
-                total_matched += sum(x[True] for x in self._counts.values())
-                counter, batch, batch_matched = 0, source.get_batch(), False
+        @return  tuples of (topic, msg, stamp, matched optionally highlighted msg, index in topic)
+        """
+        batch_matched, batch = False, None
+        for topic, msg, stamp in self.source.read():
+            if batch != self.source.get_batch():
+                batch, batch_matched = self.source.get_batch(), False
                 if self._counts: self._clear_data()
 
-            msgid = counter = counter + 1
-            topickey = rosapi.TypeMeta.make(msg, topic).topickey
+            msgid = self._idcounter = self._idcounter + 1
+            topickey = api.TypeMeta.make(msg, topic).topickey
             self._register_message(topickey, msgid, msg, stamp)
-            matched = self._is_processable(topic, stamp, msg) and self.get_match(msg)
+            matched = self._is_processable(topic, msg, stamp) and self.get_match(msg)
 
-            source.notify(matched)
+            self.source.notify(matched)
             if matched and not self._counts[topickey][True] % (self.args.NTH_MATCH or 1):
                 self._statuses[topickey][msgid] = True
                 self._counts[topickey][True] += 1
-                sink.emit_meta()
-                self._emit_context(topickey, before=True)
-                sink.emit(topic, self._counts[topickey][None], stamp, msg, matched)
+                for x in self._generate_context(topickey, before=True): yield x
+                yield (topic, msg, stamp, matched, self._counts[topickey][None])
             elif matched:  # Not NTH_MATCH, skip emitting
                 self._statuses[topickey][msgid] = True
                 self._counts[topickey][True] += 1
             elif self.args.AFTER \
             and self._has_in_window(topickey, self.args.AFTER + 1, status=True):
-                self._emit_context(topickey, before=False)
+                for x in self._generate_context(topickey, before=False): yield x
             batch_matched = batch_matched or bool(matched)
 
             self._prune_data(topickey)
             if batch_matched and self._is_max_done():
-                sink.flush()
-                source.close_batch()
+                if self.sink: self.sink.flush()
+                self.source.close_batch()
 
-        source.close(), sink.close()
-        return total_matched + sum(x[True] for x in self._counts.values())
 
-
-    def _is_processable(self, topic, stamp, msg):
+    def _is_processable(self, topic, msg, stamp):
         """
         Returns whether processing current message in topic is acceptable:
         that topic or total maximum count has not been reached,
         and current message in topic is in configured range, if any.
         """
-        topickey = rosapi.TypeMeta.make(msg, topic).topickey
-        if self.args.MAX_MATCHES \
-        and sum(x[True] for x in self._counts.values()) >= self.args.MAX_MATCHES:
+        topickey = api.TypeMeta.make(msg, topic).topickey
+        if self.args.MAX_COUNT \
+        and sum(x[True] for x in self._counts.values()) >= self.args.MAX_COUNT:
             return False
-        if self.args.MAX_TOPIC_MATCHES \
-        and self._counts[topickey][True] >= self.args.MAX_TOPIC_MATCHES:
+        if self.args.MAX_PER_TOPIC and self._counts[topickey][True] >= self.args.MAX_PER_TOPIC:
             return False
         if self.args.MAX_TOPICS:
             topics_matched = [k for k, vv in self._counts.items() if vv[True]]
             if topickey not in topics_matched and len(topics_matched) >= self.args.MAX_TOPICS:
                 return False
-        if not self._source.is_processable(topic, self._counts[topickey][None], stamp, msg):
+        if self.source \
+        and not self.source.is_processable(topic, msg, stamp, self._counts[topickey][None]):
             return False
         return True
 
 
-    def _emit_context(self, topickey, before=False):
-        """Emits before/after context to sink for latest match."""
+    def _generate_context(self, topickey, before=False):
+        """Yields before/after context for latest match."""
         count = self.args.BEFORE + 1 if before else self.args.AFTER
         candidates = list(self._statuses[topickey])[-count:]
         current_index = self._counts[topickey][None]
         for i, msgid in enumerate(candidates) if count else ():
             if self._statuses[topickey][msgid] is None:
                 idx = current_index + i - (len(candidates) - 1 if before else 1)
-                stamp, msg = self._stamps[topickey][msgid], self._messages[topickey][msgid]
+                msg, stamp = self._messages[topickey][msgid], self._stamps[topickey][msgid]
                 self._counts[topickey][False] += 1
-                self._sink.emit(topickey[0], idx, stamp, msg, None)
+                yield topickey[0], msg, stamp, None, idx
                 self._statuses[topickey][msgid] = False
 
 
     def _clear_data(self):
         """Clears local structures."""
         for d in (self._counts, self._messages, self._stamps, self._statuses):
             d.clear()
-        rosapi.TypeMeta.clear()
+        api.TypeMeta.clear()
 
 
-    def _prepare(self, source, sink):
-        """Clears local structures, binds and registers source and sink."""
+    def _prepare(self, source, sink=None, highlight=None):
+        """Clears local structures, binds and registers source and sink, if any."""
         self._clear_data()
-        self._source, self._sink = source, sink
-        source.bind(sink), sink.bind(source)
-        self._passthrough = not sink.is_highlighting() and not self._patterns["select"] \
-                            and not self._patterns["noselect"] and not self.args.INVERT \
-                            and set(self._patterns["content"]) <= set(self.ANY_MATCHES)
+        self.source, self.sink = source, sink
+        source.bind(sink), sink and sink.bind(source)
+        source.preprocess = False
+        self._configure_flags(highlight=highlight)
 
 
     def _prune_data(self, topickey):
         """Drops history older than context window."""
         WINDOW = max(self.args.BEFORE, self.args.AFTER) + 1
         for dct in (self._messages, self._stamps, self._statuses):
             while len(dct[topickey]) > WINDOW:
                 msgid = next(iter(dct[topickey]))
                 value = dct[topickey].pop(msgid)
-                dct is self._messages and rosapi.TypeMeta.discard(value)
+                dct is self._messages and api.TypeMeta.discard(value)
 
 
     def _parse_patterns(self):
         """Parses pattern arguments into re.Patterns."""
         NOBRUTE_SIGILS = r"\A", r"\Z", "?("  # Regex specials ruling out brute precheck
         BRUTE, FLAGS = not self.args.INVERT, re.DOTALL | (0 if self.args.CASE else re.I)
         self._patterns.clear()
         del self._brute_prechecks[:]
         contents = []
-        for v in self.args.PATTERNS:
+        for v in self.args.PATTERN:
             split = v.find("=", 1, -1)
             v, path = (v[split + 1:], v[:split]) if split > 0 else (v, ())
             # Special case if '' or "": add pattern for matching empty string
-            v = (re.escape(v) if self.args.RAW else v) + ("|^$" if v in ("''", '""') else "")
+            v = "|^$" if v in ("''", '""') else (re.escape(v) if self.args.FIXED_STRING else v)
             path = re.compile(r"(^|\.)%s($|\.)" % ".*".join(map(re.escape, path.split("*")))) \
                    if path else ()
             contents.append((path, re.compile("(%s)" % v, FLAGS)))
-            if BRUTE and (self.args.RAW or not any(x in v for x in NOBRUTE_SIGILS)):
+            if BRUTE and (self.args.FIXED_STRING or not any(x in v for x in NOBRUTE_SIGILS)):
                 self._brute_prechecks.append(re.compile(v, re.I | re.M))
-        if not self.args.PATTERNS:  # Add match-all pattern
+        if not self.args.PATTERN:  # Add match-all pattern
             contents.append(self.ANY_MATCHES[0])
         self._patterns["content"] = contents
 
-        selects, noselects = self.args.SELECT_FIELDS, self.args.NOSELECT_FIELDS
+        selects, noselects = self.args.SELECT_FIELD, self.args.NOSELECT_FIELD
         for key, vals in [("select", selects), ("noselect", noselects)]:
-            self._patterns[key] = [(tuple(v.split(".")), wildcard_to_regex(v)) for v in vals]
+            self._patterns[key] = [(tuple(v.split(".")), common.wildcard_to_regex(v)) for v in vals]
 
 
     def _register_message(self, topickey, msgid, msg, stamp):
         """Registers message with local structures."""
         self._counts[topickey][None] += 1
         self._messages[topickey][msgid] = msg
         self._stamps  [topickey][msgid] = stamp
         self._statuses[topickey][msgid] = None
 
 
+    def _configure_flags(self, highlight=None):
+        """Sets highlight and passthrough flags from current settings."""
+        self._highlight = bool(highlight if highlight is not None else
+                               False if self.sink and not self.sink.is_highlighting() else
+                               self.args.HIGHLIGHT)
+        self._passthrough = not self._highlight and not self._patterns["select"] \
+                            and not self._patterns["noselect"] and not self.args.INVERT \
+                            and set(self._patterns["content"]) <= set(self.ANY_MATCHES)
+
+
     def _is_max_done(self):
         """Returns whether max match count has been reached (and message after-context emitted)."""
         result, is_maxed = False, False
-        if self.args.MAX_MATCHES:
-            is_maxed = sum(vv[True] for vv in self._counts.values()) >= self.args.MAX_MATCHES
-        if not is_maxed and self.args.MAX_TOPIC_MATCHES:
-            count_required = self.args.MAX_TOPICS or len(self._source.topics)
-            count_maxed = sum(vv[True] >= self.args.MAX_TOPIC_MATCHES
-                              or vv[None] >= (self._source.topics.get(k) or 0)
+        if self.args.MAX_COUNT:
+            is_maxed = sum(vv[True] for vv in self._counts.values()) >= self.args.MAX_COUNT
+        if not is_maxed and self.args.MAX_PER_TOPIC:
+            count_required = self.args.MAX_TOPICS or len(self.source.topics)
+            count_maxed = sum(vv[True] >= self.args.MAX_PER_TOPIC
+                              or vv[None] >= (self.source.topics.get(k) or 0)
                               for k, vv in self._counts.items())
             is_maxed = (count_maxed >= count_required)
         if is_maxed:
             result = not self.args.AFTER or \
                      not any(self._has_in_window(k, self.args.AFTER, status=True, full=True)
                              for k in self._counts)
         return result
@@ -248,51 +381,60 @@
             for i, (path, p) in enumerate(self._patterns["content"]):
                 if not path or path.search(topstr):
                     for match in (m for m in p.finditer(v1) if not v1 or m.start() != m.end()):
                         matched[i] = True
                         spans.append(match.span())
                         if self.args.INVERT:
                             break  # for match
-            spans = merge_spans(spans) if not self.args.INVERT else \
-                    [] if spans else [(0, len(v1))] if v1 or not is_collection else []
-            for a, b in reversed(spans):  # Work from last to first, indices stay the same
-                v2 = v2[:a] + MatchMarkers.START + v2[a:b] + MatchMarkers.END + v2[b:]
+            if any(WRAPS):
+                spans = common.merge_spans(spans) if not self.args.INVERT else \
+                        [] if spans else [(0, len(v1))] if v1 or not is_collection else []
+                for a, b in reversed(spans):  # Work from last to first, indices stay the same
+                    v2 = v2[:a] + WRAPS[0] + v2[a:b] + WRAPS[1] + v2[b:]
             return "[%s]" % v2 if is_collection and v != "[]" else v2
 
         def process_message(obj, top=()):
             """Recursively converts field values to pattern-matched strings; updates `matched`."""
             selects, noselects = self._patterns["select"], self._patterns["noselect"]
-            fieldmap = fieldmap0 = rosapi.get_message_fields(obj)  # Returns obj if not ROS message
+            fieldmap = fieldmap0 = api.get_message_fields(obj)  # Returns obj if not ROS message
             if fieldmap != obj:
-                fieldmap = filter_fields(fieldmap, top, include=selects, exclude=noselects)
+                fieldmap = api.filter_fields(fieldmap, top, include=selects, exclude=noselects)
             for k, t in fieldmap.items() if fieldmap != obj else ():
-                v, path = rosapi.get_message_value(obj, k, t), top + (k, )
+                v, path = api.get_message_value(obj, k, t), top + (k, )
                 is_collection = isinstance(v, (list, tuple))
-                if rosapi.is_ros_message(v):
+                if api.is_ros_message(v):
                     process_message(v, path)
-                elif v and is_collection and rosapi.scalar(t) not in rosapi.ROS_NUMERIC_TYPES:
-                    rosapi.set_message_value(obj, k, [process_message(x, path) for x in v])
+                elif v and is_collection and api.scalar(t) not in api.ROS_NUMERIC_TYPES:
+                    api.set_message_value(obj, k, [process_message(x, path) for x in v])
                 else:
                     v1 = str(list(v) if isinstance(v, (bytes, tuple)) else v)
                     v2 = wrap_matches(v1, path, is_collection)
                     if len(v1) != len(v2):
-                        rosapi.set_message_value(obj, k, v2)
-            if not rosapi.is_ros_message(obj):
+                        api.set_message_value(obj, k, v2)
+            if not api.is_ros_message(obj):
                 v1 = str(list(obj) if isinstance(obj, bytes) else obj)
                 v2 = wrap_matches(v1, top)
                 obj = v2 if len(v1) != len(v2) else obj
             if not top and not matched and not selects and not fieldmap0 and not self.args.INVERT \
             and set(self._patterns["content"]) <= set(self.ANY_MATCHES):  # Ensure Empty any-match
                 matched.update({i: True for i, _ in enumerate(self._patterns["content"])})
             return obj
 
         if self._passthrough: return msg
 
         if self._brute_prechecks:
-            text  = "\n".join("%r" % (v, ) for _, v, _ in rosapi.iter_message_fields(msg))
+            text  = "\n".join("%r" % (v, ) for _, v, _ in api.iter_message_fields(msg))
             if not all(any(p.finditer(text)) for p in self._brute_prechecks):
                 return None  # Skip detailed matching if patterns not present at all
 
+        WRAPS = [] if not self._highlight else self.args.MATCH_WRAPPER if not self.sink else \
+                (common.MatchMarkers.START, common.MatchMarkers.END)
+        WRAPS = WRAPS if isinstance(WRAPS, (list, tuple)) else [] if WRAPS is None else [WRAPS]
+        WRAPS = ((WRAPS or [""]) * 2)[:2]
+
         result, matched = copy.deepcopy(msg), {}  # {pattern index: True}
         process_message(result)
         yes = not matched if self.args.INVERT else len(matched) == len(self._patterns["content"])
-        return result if yes else None
+        return (result if self._highlight else msg) if yes else None
+
+
+__all__ = ["Scanner"]
```

## grepros/plugins/__init__.py

```diff
@@ -1,130 +1,163 @@
 # -*- coding: utf-8 -*-
 """
 Plugins interface.
 
-Allows specifying custom plugins for "source", "search" or "sink".
+Allows specifying custom plugins for "source", "scan" or "sink".
 Auto-inits any plugins in grepros.plugins.auto.
 
 Supported (but not required) plugin interface methods:
 
 - `init(args)`: invoked at startup with command-line arguments
-- `load(category, args)`: invoked with category "search" or "source" or "sink",
+- `load(category, args)`: invoked with category "scan" or "source" or "sink",
                           using returned value if not None
 
 Plugins are free to modify package internals, like adding command-line arguments
 to `main.ARGUMENTS` or sink types to `outputs.MultiSink`.
 
 Convenience methods:
 
-- `plugins.add_write_format(name, cls, label=None, options=())`:
+- `plugins.add_write_format(name, cls, label=None, options=((name, help), ))`:
    adds an output plugin to defaults
-- `plugins.add_write_options(label, [(name, help)])`: adds options for an output plugin
-- `plugins.get_argument(name)`: returns a command-line argument dictionary, or None
+- `plugins.add_output_label(label, flags)`:
+   adds plugin label to outputs enumerated in given argument help texts
+- `plugins.get_argument(name, group=None)`:
+   returns a command-line argument configuration dictionary, or None
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     18.12.2021
-@modified    04.02.2022
+@modified    26.06.2023
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.plugins
 import glob
 import os
+import re
 
-from .. common import ConsolePrinter, import_item
+import six
+
+from .. common import ConsolePrinter, ensure_namespace, get_name, import_item
 from .. outputs import MultiSink
 from . import auto
 
 
 ## {"some.module" or "some.module.Cls": <module 'some.module' from ..> or <class 'some.module.Cls'>}
 PLUGINS = {}
 
+## Added output labels to insert into argument texts, as {label: [argument flag, ]}
+OUTPUT_LABELS = {}
+
 ## Added write options, as {plugin label: [(name, help), ]}
 WRITE_OPTIONS = {}
 
+## Function argument defaults
+DEFAULT_ARGS = dict(PLUGIN=[], STOP_ON_ERROR=False)
+
 
-def init(args=None):
+def init(args=None, **kwargs):
     """
     Imports and initializes all plugins from auto and from given arguments.
 
-    @param   args           arguments object like argparse.Namespace
-    @param   args.PLUGINS   list of Python modules or classes to import,
-                            as ["my.module", "other.module.SomeClass", ]
+    @param   args                arguments as namespace or dictionary, case-insensitive
+    @param   args.plugin         list of Python modules or classes to import,
+                                 as ["my.module", "other.module.SomeClass", ],
+                                 or module or class instances
+    @param   args.stop_on_error  stop execution on any error like failing to load plugin
+    @param   kwargs              any and all arguments as keyword overrides, case-insensitive
     """
+    args = ensure_namespace(args, DEFAULT_ARGS, **kwargs)
     for f in sorted(glob.glob(os.path.join(os.path.dirname(__file__), "auto", "*"))):
         if not f.lower().endswith((".py", ".pyc")): continue  # for f
         name = os.path.splitext(os.path.split(f)[-1])[0]
         if name.startswith("__") or name in PLUGINS: continue # for f
 
         modulename = "%s.auto.%s" % (__package__, name)
         try:
             plugin = import_item(modulename)
             if callable(getattr(plugin, "init", None)): plugin.init(args)
             PLUGINS[name] = plugin
         except Exception:
             ConsolePrinter.error("Error loading plugin %s.", modulename)
+            if args.STOP_ON_ERROR: raise
     if args: configure(args)
     populate_known_plugins()
+    populate_output_arguments()
     populate_write_formats()
 
 
-def configure(args):
+def configure(args=None, **kwargs):
     """
     Imports plugin Python packages, invokes init(args) if any, raises on error.
 
-    @param   args           arguments object like argparse.Namespace
-    @param   args.PLUGINS   list of Python modules or classes to import,
-                            as ["my.module", "other.module.SomeClass", ]
+    @param   args          arguments as namespace or dictionary, case-insensitive
+    @param   args.plugin   list of Python modules or classes to import,
+                           as ["my.module", "other.module.SomeClass", ],
+                           or module or class instances
+    @param   kwargs        any and all arguments as keyword overrides, case-insensitive
     """
-    for name in (n for n in args.PLUGINS if n not in PLUGINS):
+    args = ensure_namespace(args, DEFAULT_ARGS, **kwargs)
+    for obj in args.PLUGIN:
+        name = obj if isinstance(obj, six.string_types) else get_name(obj)
+        if name in PLUGINS: continue  # for obj
         try:
-            plugin = import_item(name)
+            plugin = import_item(name) if isinstance(obj, six.string_types) else obj
             if callable(getattr(plugin, "init", None)): plugin.init(args)
             PLUGINS[name] = plugin
         except ImportWarning:
             raise
         except Exception:
             ConsolePrinter.error("Error loading plugin %s.", name)
             raise
 
 
 def load(category, args, collect=False):
     """
     Returns a plugin category instance loaded from any configured plugin, or None.
 
-    @param   category  item category like "source", "sink", or "search"
-    @param   args      arguments object like argparse.Namespace
+    @param   category  item category like "source", "sink", or "scan"
+    @param   args      arguments as namespace or dictionary, case-insensitive
     @param   collect   if true, returns a list of instances,
                        using all plugins that return something
     """
     result = []
+    args = ensure_namespace(args)
     for name, plugin in PLUGINS.items():
         if callable(getattr(plugin, "load", None)):
             try:
                 instance = plugin.load(category, args)
                 if instance is not None:
                     result.append(instance)
                     if not collect:
                         break  # for name, plugin
             except Exception:
                 ConsolePrinter.error("Error invoking %s.load(%r, args).", name, category)
                 raise
     return result if collect else result[0] if result else None
 
 
+def add_output_label(label, flags):
+    """
+    Adds plugin label to outputs enumerated in given argument help texts.
+
+    @param   label  output label to add, like "Parquet"
+    @param   flags  list of argument flags like "--emit-field" to add the output label to
+    """
+    OUTPUT_LABELS.setdefault(label, []).extend(flags)
+
+
 def add_write_format(name, cls, label=None, options=()):
     """
     Adds plugin to `--write` in main.ARGUMENTS and MultiSink formats.
 
     @param   name     format name like "csv", added to `--write .. format=FORMAT`
-    @param   cls      class providing SinkBase interface
+    @param   cls      class providing Sink interface
     @param   label    plugin label; if multiple plugins add the same option,
                       "label output" in help text is replaced with "label1/label2/.. output"
     @param   options  a sequence of (name, help) to add to --write help, like
                       [("template=/my/path.tpl", "custom template to use for HTML output")]
     """
     MultiSink.FORMAT_CLASSES[name] = cls
     if options: WRITE_OPTIONS.setdefault(label, []).extend(options)
@@ -141,14 +174,42 @@
     if group:
         return next((d for d in main.ARGUMENTS.get("groups", {}).get(group, [])
                      if name in d.get("args")), None)
     return next((d for d in main.ARGUMENTS.get("arguments", [])
                  if name in d.get("args")), None)
 
 
+def populate_output_arguments():
+    """Populates argument texts with added output labels."""
+    if not OUTPUT_LABELS: return
+    from .. import main  # Late import to avoid circular
+
+    argslist = sum(main.ARGUMENTS.get("groups", {}).values(), main.ARGUMENTS["arguments"][:])
+    args = {f: x for x in argslist for f in x["args"]}  # {flag or id(argdict): argdict}
+    args.update((id(x), x) for x in argslist)
+    arglabels = {}  # {id(argdict): [label, ]}
+
+    # First pass: collect arguments where to update output labels
+    for label, flag in ((l, f) for l, ff in OUTPUT_LABELS.items() for f in ff):
+        if flag in args: arglabels.setdefault(id(args[flag]), []).append(label)
+        else: ConsolePrinter.warn("Unknown command-line flag %r from output %r.", flag, label)
+
+    # Second pass: replace argument help with full set of output labels
+    for arg, labels in ((args[x], ll) for x, ll in arglabels.items()):
+        match = re.search(r"(\A.*?\s*in\s)(\S+)(\s+output.*\Z)", arg["help"], re.DOTALL)
+        if not match:
+            ConsolePrinter.warn("Command-line flag %s has no text on output for labels %s.",
+                                arg["args"], ", ".join(map(repr, sorted(set(labels)))))
+            continue  # for arg, labels
+        labels2 = sorted(set(labels + match[2].split("/")), key=lambda x: x.lower())
+        arg["help"] = match.expand(r"\1%s\3" % "/".join(labels2))
+
+    OUTPUT_LABELS.clear()
+
+
 def populate_known_plugins():
     """Adds known non-auto plugins to `--plugin` argument help."""
     plugins = []
     for f in sorted(glob.glob(os.path.join(os.path.dirname(__file__), "*"))):
         if not f.lower().endswith((".py", ".pyc")): continue  # for f
         name = os.path.splitext(os.path.split(f)[-1])[0]
         if not name.startswith("__"):
@@ -210,7 +271,17 @@
             texts[name] = texts[name].replace(PLACEHOLDER, labels + " output")
 
     fmt = lambda n, h: "\n".join((indent if i or "\n" == inters[n] else "") + l
                                  for i, l in enumerate(h.splitlines()))
     text = "\n".join(sorted("".join((LEADING, n, inters[n], fmt(n, h)))
                             for n, h in texts.items()))
     writearg["help"] += "\n" + text
+
+    WRITE_OPTIONS.clear()
+
+
+
+
+__all__ = [
+    "PLUGINS", "init", "configure", "load", "add_write_format", "get_argument",
+    "populate_known_plugins", "populate_write_formats",
+]
```

## grepros/plugins/embag.py

```diff
@@ -4,177 +4,280 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     19.11.2021
-@modified    20.10.2022
+@modified    26.06.2023
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.plugins.embag
 from __future__ import absolute_import
 import os
-import re
 
 try: import embag
 except ImportError: embag = None
 try: import genpy
 except ImportError: genpy = None
 
-from .. common import ConsolePrinter, Decompressor
-try: from .. import ros1
-except Exception: ros1 = None
-from .. import rosapi
+from .. import api
+from .. common import PATH_TYPES, ConsolePrinter
 
 
 
-class EmbagReader(object):
-    """embag reader interface, partially mimicking rosbag.Bag."""
+class EmbagReader(api.BaseBag):
+    """embag reader interface, providing most of rosbag.Bag interface."""
+
+    ## Supported opening modes
+    MODES = ("r", )
+
+    ## Whether bag supports reading or writing stream objects, overridden in subclasses
+    STREAMABLE = False
 
     ## ROS1 bag file header magic start bytes
     ROSBAG_MAGIC = b"#ROSBAG"
 
-    def __init__(self, filename, decompress=False, progress=False, **__):
-        """
-        @param   decompress  decompress archived bag to file directory
-        @param   progress    show progress bar during decompression
-        """
-        if Decompressor.is_compressed(filename):
-            if decompress: filename = Decompressor.decompress(filename, progress)
-            else: raise Exception("decompression not enabled")
-
-        self._topics   = {}  # {(topic, typename, typehash): message count}
-        self._types    = {}  # {(typename, typehash): message type class}
-        self._hashdefs = {}  # {(topic, typehash): typename}
-        self._typedefs = {}  # {(typename, typehash): type definition text}
-        self._view = embag.View(filename)
-
-        ## Bagfile path
-        self.filename = filename
+    def __init__(self, filename, mode="r", **__):
+        if not isinstance(filename, PATH_TYPES):
+            raise ValueError("invalid filename %r" % type(filename))
+        if mode not in self.MODES: raise ValueError("invalid mode %r" % mode)
+
+        self._topics   = {}    # {(topic, typename, typehash): message count}
+        self._types    = {}    # {(typename, typehash): message type class}
+        self._hashdefs = {}    # {(topic, typehash): typename}
+        self._typedefs = {}    # {(typename, typehash): type definition text}
+        self._iterer   = None  # Generator from read_messages() for next()
+        self._ttinfo   = None  # Cached result for get_type_and_topic_info()
+        self._view     = embag.View(filename)
+        self._filename = str(filename)
 
         self._populate_meta()
 
 
-    def get_message_count(self):
-        """Returns the number of messages in the bag."""
+    def get_message_count(self, topic_filters=None):
+        """
+        Returns the number of messages in the bag.
+
+        @param   topic_filters  list of topics or a single topic to filter by, if any
+        """
+        if topic_filters:
+            topics = topic_filters
+            topics = topics if isinstance(topics, (dict, list, set, tuple)) else [topics]
+            return sum(c for (t, _, _), c in self._topics.items() if t in topics)
         return sum(self._topics.values())
 
 
     def get_start_time(self):
-        """Returns the start time of the bag, as UNIX timestamp."""
-        if not self._topics: return None
+        """Returns the start time of the bag, as UNIX timestamp, or None if bag empty."""
+        if self.closed or not self._topics: return None
         return self._view.getStartTime().to_sec()
 
 
     def get_end_time(self):
-        """Returns the end time of the bag, as UNIX timestamp."""
-        if not self._topics: return None
+        """Returns the end time of the bag, as UNIX timestamp, or None if bag empty."""
+        if self.closed or not self._topics: return None
         return self._view.getEndTime().to_sec()
 
 
     def get_message_class(self, typename, typehash=None):
         """
-        Returns rospy message class for typename, or None if unknown type.
+        Returns rospy message class for typename, or None if unknown message type for bag.
 
         Generates class dynamically if not already generated.
 
         @param   typehash  message type definition hash, if any
         """
-        typekey = (typename, typehash or next((h for n, h in self.__types if n == typename), None))
+        typekey = (typename, typehash or next((h for n, h in self._types if n == typename), None))
         if typekey not in self._types and typekey in self._typedefs:
             for n, c in genpy.dynamic.generate_dynamic(typename, self._typedefs[typekey]).items():
                 self._types[(n, c._md5sum)] = c
-        return self._types.get(typekey) or rosapi.get_message_class(typename)
+        return self._types.get(typekey)
 
 
     def get_message_definition(self, msg_or_type):
-        """Returns ROS1 message type definition full text from bag, including subtype definitions."""
-        if rosapi.is_ros_message(msg_or_type):
+        """
+        Returns ROS1 message type definition full text from bag, including subtype definitions.
+
+        Returns None if unknown message type for bag.
+        """
+        if api.is_ros_message(msg_or_type):
             return self._typedefs.get((msg_or_type._type, msg_or_type._md5sum))
         typename = msg_or_type
         return next((d for (n, h), d in self._typedefs.items() if n == typename), None)
 
 
     def get_message_type_hash(self, msg_or_type):
-        """Returns ROS1 message type MD5 hash."""
-        if rosapi.is_ros_message(msg_or_type): return msg_or_type._md5sum
+        """Returns ROS1 message type MD5 hash, or None if unknown message type for bag."""
+        if api.is_ros_message(msg_or_type): return msg_or_type._md5sum
         typename = msg_or_type
-        return next((h for n, h in self._typedefs if n == typename), None) \
-               or rosapi.get_message_type_hash(typename)
+        return next((h for n, h in self._typedefs if n == typename), None)
 
 
-    def get_topic_info(self):
+    def get_topic_info(self, *_, **__):
         """Returns topic and message type metainfo as {(topic, typename, typehash): count}."""
         return dict(self._topics)
 
 
-    def read_messages(self, topics=None, start_time=None, end_time=None):
+    def get_type_and_topic_info(self, topic_filters=None):
+        """
+        Returns thorough metainfo on topic and message types.
+
+        @param   topic_filters  list of topics or a single topic to filter returned topics-dict by,
+                                if any
+        @return                 TypesAndTopicsTuple(msg_types, topics) namedtuple,
+                                msg_types as dict of {typename: typehash},
+                                topics as a dict of {topic: TopicTuple() namedtuple}.
+        """
+        topics = topic_filters
+        topics = topics if isinstance(topics, (list, set, tuple)) else [topics] if topics else []
+        if self._ttinfo and (not topics or set(topics) == set(t for t, _, _ in self._topics)):
+            return self._ttinfo
+        if self.closed: raise ValueError("I/O operation on closed file.")
+
+        msgtypes = {n: h for t, n, h in self._topics}
+        topicdict = {}
+
+        def median(vals):
+            """Returns median value from given sorted numbers."""
+            vlen = len(vals)
+            return None if not vlen else vals[vlen // 2] if vlen % 2 else \
+                   float(vals[vlen // 2 - 1] + vals[vlen // 2]) / 2
+
+        conns = self._view.connectionsByTopic()  # {topic: [embag.Connection, ]}
+        for (t, n, _), c in sorted(self._topics.items()):
+            if topics and t not in topics: continue  # for
+            mymedian = None
+            if c > 1:
+                stamps = sorted(m.timestamp.secs + m.timestamp.nsecs / 1E9
+                                for m in self._view.getMessages([t]))
+                mymedian = median(sorted(s1 - s0 for s1, s0 in zip(stamps[1:], stamps[:-1])))
+            freq = 1.0 / mymedian if mymedian else None
+            topicdict[t] = self.TopicTuple(n, c, len(conns.get(t, [])), freq)
+        if not topics or set(topics) == set(t for t, _, _ in self._topics):
+            self._ttinfo = self.TypesAndTopicsTuple(msgtypes, topicdict)
+        return self._ttinfo
+
+
+    def read_messages(self, topics=None, start_time=None, end_time=None, raw=False):
         """
         Yields messages from the bag, optionally filtered by topic and timestamp.
 
         @param   topics      list of topics or a single topic to filter by, if at all
-        @param   start_time  earliest timestamp of message to return, as UNIX timestamp
-        @param   end_time    latest timestamp of message to return, as UNIX timestamp
-        @return              (topic, msg, rclpy.time.Time)
+        @param   start_time  earliest timestamp of message to return, as ROS time or convertible
+                             (int/float/duration/datetime/decimal)
+        @param   end_time    latest timestamp of message to return, as ROS time or convertible
+                             (int/float/duration/datetime/decimal)
+        @param   raw         if true, then returned messages are tuples of
+                             (typename, bytes, typehash, typeclass)
+        @return              BagMessage namedtuples of (topic, message, timestamp as rospy.Time)
         """
+        if self.closed: raise ValueError("I/O operation on closed file.")
+
         topics = topics if isinstance(topics, list) else [topics] if topics else []
+        start_time, end_time = (api.to_sec(api.to_time(x)) for x in (start_time, end_time))
         for m in self._view.getMessages(topics) if topics else self._view.getMessages():
             if start_time is not None and start_time > m.timestamp.to_sec():
                 continue  # for m
             if end_time is not None and end_time < m.timestamp.to_sec():
                 continue  # for m
 
             typename = self._hashdefs[(m.topic, m.md5)]
-            msg = self._populate_message(self.get_message_class(typename, m.md5)(), m.data())
-            yield m.topic, msg, rosapi.make_time(m.timestamp.secs, m.timestamp.nsecs)
+            stamp = api.make_time(m.timestamp.secs, m.timestamp.nsecs)
+            if raw: msg = (typename, m.data(), m.md5, self.get_message_class(typename, m.md5))
+            else: msg = self._populate_message(self.get_message_class(typename, m.md5)(), m.data())
+            api.TypeMeta.make(msg, m.topic, self)
+            yield self.BagMessage(m.topic, msg, stamp)
+            if self.closed: break  # for m
+
+
+    def open(self):
+        """Opens the bag file if not already open."""
+        if not self._view: self._view = embag.View(self._filename)
 
 
     def close(self):
         """Closes the bag file."""
         if self._view:
             del self._view
-            self._view = None
+            self._view   = None
+            self._iterer = None
+
+
+    @property
+    def closed(self):
+        """Returns whether file is closed."""
+        return not self._view
+
+
+    @property
+    def topics(self):
+        """Returns the list of topics in bag, in alphabetic order."""
+        return sorted((t for t, _, _ in self._topics), key=str.lower)
+
+
+    @property
+    def filename(self):
+        """Returns bag file path."""
+        return self._filename
 
 
     @property
     def size(self):
         """Returns current file size."""
-        return os.path.getsize(self.filename) if os.path.isfile(self.filename) else None
+        return os.path.getsize(self._filename) if os.path.isfile(self._filename) else None
+
+
+    @property
+    def mode(self):
+        """Returns file open mode."""
+        return "r"
+
+
+    def __contains__(self, key):
+        """Returns whether bag contains given topic."""
+        return any(key == t for t, _, _ in self._topics)
+
+
+    def __next__(self):
+        """Retrieves next message from bag as (topic, message, timestamp)."""
+        if self.closed: raise ValueError("I/O operation on closed file.")
+        if self._iterer is None: self._iterer = self.read_messages()
+        return next(self._iterer)
 
 
     def _populate_meta(self):
         """Populates bag metainfo."""
         connections = self._view.connectionsByTopic()
         for topic in self._view.topics():
             for conn in connections.get(topic, ()):
                 topickey, typekey = (topic, conn.type, conn.md5sum), (conn.type, conn.md5sum)
                 self._topics.setdefault(topickey, 0)
                 self._topics[topickey] += conn.message_count
                 self._hashdefs[(topic, conn.md5sum)] = conn.type
                 self._typedefs[typekey] = conn.message_definition
-                subtypedefs = rosapi.parse_definition_subtypes(conn.message_definition)
+                subtypedefs = api.parse_definition_subtypes(conn.message_definition)
                 for n, d in subtypedefs.items():
-                    h = rosapi.calculate_definition_hash(n, d, tuple(subtypedefs.items()))
+                    h = api.calculate_definition_hash(n, d, tuple(subtypedefs.items()))
                     self._typedefs.setdefault((n, h), d)
 
 
     def _populate_message(self, msg, embagval):
         """Returns the ROS1 message populated from a corresponding embag.RosValue."""
-        for name, typename in rosapi.get_message_fields(msg).items():
-            v, scalarname = embagval.get(name), rosapi.scalar(typename)
-            if typename in rosapi.ROS_BUILTIN_TYPES:      # Single built-in type
+        for name, typename in api.get_message_fields(msg).items():
+            v, scalarname = embagval.get(name), api.scalar(typename)
+            if typename in api.ROS_BUILTIN_TYPES:      # Single built-in type
                 msgv = getattr(embagval, name)
-            elif scalarname in rosapi.ROS_BUILTIN_TYPES:  # List of built-in types
+            elif scalarname in api.ROS_BUILTIN_TYPES:  # List of built-in types
                 msgv = list(v)
-            elif typename in rosapi.ROS_TIME_TYPES:       # Single temporal type
-                cls = next(k for k, v in rosapi.ROS_TIME_CLASSES.items() if v == typename)
+            elif typename in api.ROS_TIME_TYPES:       # Single temporal type
+                cls = next(k for k, v in api.ROS_TIME_CLASSES.items() if v == typename)
                 msgv = cls(v.secs, v.nsecs)
-            elif scalarname in rosapi.ROS_TIME_TYPES:     # List of temporal types
-                cls = next(k for k, v in rosapi.ROS_TIME_CLASSES.items() if v == scalarname)
+            elif scalarname in api.ROS_TIME_TYPES:     # List of temporal types
+                cls = next(k for k, v in api.ROS_TIME_CLASSES.items() if v == scalarname)
                 msgv = [cls(x.secs, x.nsecs) for x in v]
             elif typename == scalarname:                  # Single subtype
                 msgv = self._populate_message(self.get_message_class(typename)(), v)
             else:                                         # List of subtypes
                 cls = self.get_message_class(scalarname)
                 msgv = [self._populate_message(cls(), x) for x in v]
             setattr(msg, name, msgv)
@@ -192,8 +295,11 @@
 
 
 def init(*_, **__):
     """Replaces ROS1 bag reader with EmbagReader. Raises ImportWarning if embag not available."""
     if not embag:
         ConsolePrinter.error("embag not available: cannot read bag files.")
         raise ImportWarning()
-    rosapi.Bag.READER_CLASSES.add(EmbagReader)
+    api.Bag.READER_CLASSES.add(EmbagReader)
+
+
+__all__ = ["EmbagReader", "init"]
```

## grepros/plugins/mcap.py

```diff
@@ -4,76 +4,117 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     14.10.2022
-@modified    20.10.2022
+@modified    29.06.2023
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.plugins.mcap
 from __future__ import absolute_import
 import atexit
 import copy
+import io
 import os
+import time
+import types
+
+from .. import api
 
 try: import mcap, mcap.reader
 except ImportError: mcap = None
 if "1" == os.getenv("ROS_VERSION"):
+    import genpy.dynamic
     try: import mcap_ros1 as mcap_ros, mcap_ros1.decoder, mcap_ros1.writer
     except ImportError: mcap_ros = None
 elif "2" == os.getenv("ROS_VERSION"):
     try: import mcap_ros2 as mcap_ros, mcap_ros2.decoder, mcap_ros2.writer
     except ImportError: mcap_ros = None
 else: mcap_ros = None
 import yaml
 
-from .. common import ConsolePrinter, format_bytes, makedirs, plural, unique_path
-from .. outputs import SinkBase
-from .. import rosapi
-ros2 = None
-if "2" == os.getenv("ROS_VERSION"):
-    from .. import ros2
+from .. import common
+from .. common import ConsolePrinter
+from .. outputs import Sink
+
+
+class McapBag(api.BaseBag):
+    """
+    MCAP bag interface, providing most of rosbag.Bag interface.
 
+    Bag cannot be appended to, and cannot be read and written at the same time
+    (MCAP API limitation).
+    """
 
-class McapReader(object):
-    """MCAP reader interface, partially mimicking rosbag.Bag."""
+    ## Supported opening modes
+    MODES = ("r", "w")
 
     ## MCAP file header magic start bytes
     MCAP_MAGIC = b"\x89MCAP\x30\r\n"
 
-    def __init__(self, filename, **__):
-        """Opens file and populates metadata."""
-        self._topics         = {}    # {(topic, typename, typehash): message count}
-        self._types          = {}    # {(typename, typehash): message type class}
-        self._typedefs       = {}    # {(typename, typehash): type definition text}
-        self._schemas        = {}    # {mcap.records.Schema.id: (typename, typehash)}
-        self._qoses          = {}    # {(topic, typename): [{qos profile dict}]}
-        self._typefields     = {}    # {(typename, typehash): {field name: type name}}
-        self._type_subtypes  = {}    # {(typename, typehash): {typename: typehash}}
-        self._field_subtypes = {}    # {(typename, typehash): {field name: (typename, typehash)}}
-        self._temporal_ctors = {}    # {typename: time/duration constructor}
-        self._start_time     = None  # Bag start time, as UNIX timestamp
-        self._end_time       = None  # Bag end time, as UNIX timestamp
-        self._file           = open(filename, "rb")
-        self._reader         = mcap.reader.make_reader(self._file)
-        self._decoder        = mcap_ros.decoder.Decoder()
+    def __init__(self, f, mode="r", **__):
+        """
+        Opens file and populates metadata.
 
-        ## Bagfile path
-        self.filename = filename
+        @param   f         bag file path, or a stream object
+        @param   mode      return reader if "r" or writer if "w"
+        """
+        if mode not in self.MODES: raise ValueError("invalid mode %r" % mode)
+        self._mode           = mode
+        self._topics         = {}     # {(topic, typename, typehash): message count}
+        self._types          = {}     # {(typename, typehash): message type class}
+        self._typedefs       = {}     # {(typename, typehash): type definition text}
+        self._schemas        = {}     # {(typename, typehash): mcap.records.Schema}
+        self._schematypes    = {}     # {mcap.records.Schema.id: (typename, typehash)}
+        self._qoses          = {}     # {(topic, typename): [{qos profile dict}]}
+        self._typefields     = {}     # {(typename, typehash): {field name: type name}}
+        self._type_subtypes  = {}     # {(typename, typehash): {typename: typehash}}
+        self._field_subtypes = {}     # {(typename, typehash): {field name: (typename, typehash)}}
+        self._temporal_ctors = {}     # {typename: time/duration constructor}
+        self._start_time     = None   # Bag start time, as UNIX timestamp
+        self._end_time       = None   # Bag end time, as UNIX timestamp
+        self._file           = None   # File handle
+        self._reader         = None   # mcap.McapReader
+        self._decoder        = None   # mcap_ros.Decoder
+        self._writer         = None   # mcap_ros.Writer
+        self._iterer         = None   # Generator from read_messages() for next()
+        self._ttinfo         = None   # Cached result for get_type_and_topic_info()
+        self._opened         = False  # Whether file has been opened at least once
+        self._filename       = None   # File path, or None if stream
+
+        if common.is_stream(f):
+            if not common.verify_io(f, mode):
+                raise io.UnsupportedOperation("read" if "r" == mode else "write")
+            self._file, self._filename = f, None
+            f.seek(0)
+        else:
+            if not isinstance(f, common.PATH_TYPES):
+                raise ValueError("invalid filename %r" % type(f))
+            if "w" == mode: common.makedirs(os.path.dirname(f))
+            self._filename = str(f)
 
-        if ros2: self._temporal_ctors.update(
-            (t, c) for c, t in rosapi.ROS_TIME_CLASSES.items() if rosapi.get_message_type(c) == t
+        if api.ROS2 and "r" == mode: self._temporal_ctors.update(
+            (t, c) for c, t in api.ROS_TIME_CLASSES.items() if api.get_message_type(c) == t
         )
-        self._populate_meta()
 
+        self.open()
+
+
+    def get_message_count(self, topic_filters=None):
+        """
+        Returns the number of messages in the bag.
 
-    def get_message_count(self):
-        """Returns the number of messages in the bag."""
+        @param   topic_filters  list of topics or a single topic to filter by, if any
+        """
+        if topic_filters:
+            topics = topic_filters
+            topics = topics if isinstance(topics, (dict, list, set, tuple)) else [topics]
+            return sum(c for (t, _, _), c in self._topics.items() if t in topics)
         return sum(self._topics.values())
 
 
     def get_start_time(self):
         """Returns the start time of the bag, as UNIX timestamp."""
         return self._start_time
 
@@ -85,99 +126,318 @@
 
     def get_message_class(self, typename, typehash=None):
         """
         Returns ROS message class for typename, or None if unknown type.
 
         @param   typehash  message type definition hash, if any
         """
-        typekey = (typename, typehash or next((t for n, t in self._types if n == typename), None))
+        typehash = typehash or next((h for t, h in self._typedefs if t == typename), None)
+        typekey = (typename, typehash)
+        if typekey not in self._types and typekey in self._typedefs:
+            if api.ROS2:
+                name = typename.split("/")[-1]
+                fields = api.parse_definition_fields(typename, self._typedefs[typekey])
+                cls = type(name, (types.SimpleNamespace, ), {
+                    "__name__": name, "__slots__": list(fields),
+                    "__repr__": message_repr, "__str__": message_repr
+                })
+                self._types[typekey] = self._patch_message_class(cls, typename, typehash)
+            else:
+                typeclses = genpy.dynamic.generate_dynamic(typename, self._typedefs[typekey])
+                self._types[typekey] = typeclses[typename]
+
         return self._types.get(typekey)
 
 
     def get_message_definition(self, msg_or_type):
         """Returns ROS message type definition full text from bag, including subtype definitions."""
-        if rosapi.is_ros_message(msg_or_type):
-            return self._typedefs.get((rosapi.get_message_type(msg_or_type),
-                                       rosapi.get_message_type_hash(msg_or_type)))
+        if api.is_ros_message(msg_or_type):
+            return self._typedefs.get((api.get_message_type(msg_or_type),
+                                       api.get_message_type_hash(msg_or_type)))
         typename = msg_or_type
         return next((d for (n, h), d in self._typedefs.items() if n == typename), None)
 
 
     def get_message_type_hash(self, msg_or_type):
         """Returns ROS message type MD5 hash."""
-        typename = rosapi.get_message_type(msg_or_type)
+        typename = msg_or_type
+        if api.is_ros_message(msg_or_type): typename = api.get_message_type(msg_or_type)
         return next((h for n, h in self._typedefs if n == typename), None)
 
 
     def get_qoses(self, topic, typename):
         """Returns topic Quality-of-Service profiles as a list of dicts, or None if not available."""
         return self._qoses.get((topic, typename))
 
 
-    def get_topic_info(self):
+    def get_topic_info(self, *_, **__):
         """Returns topic and message type metainfo as {(topic, typename, typehash): count}."""
         return dict(self._topics)
 
 
-    def read_messages(self, topics=None, start_time=None, end_time=None):
+    def get_type_and_topic_info(self, topic_filters=None):
+        """
+        Returns thorough metainfo on topic and message types.
+
+        @param   topic_filters  list of topics or a single topic to filter returned topics-dict by,
+                                if any
+        @return                 TypesAndTopicsTuple(msg_types, topics) namedtuple,
+                                msg_types as dict of {typename: typehash},
+                                topics as a dict of {topic: TopicTuple() namedtuple}.
+        """
+        topics = topic_filters
+        topics = topics if isinstance(topics, (list, set, tuple)) else [topics] if topics else []
+        if self._ttinfo and (not topics or set(topics) == set(t for t, _, _ in self._topics)):
+            return self._ttinfo
+        if self.closed: raise ValueError("I/O operation on closed file.")
+
+        topics = topic_filters
+        topics = topics if isinstance(topics, (list, set, tuple)) else [topics] if topics else []
+        msgtypes = {n: h for t, n, h in self._topics}
+        topicdict = {}
+
+        def median(vals):
+            """Returns median value from given sorted numbers."""
+            vlen = len(vals)
+            return None if not vlen else vals[vlen // 2] if vlen % 2 else \
+                   float(vals[vlen // 2 - 1] + vals[vlen // 2]) / 2
+
+        for (t, n, _), c in sorted(self._topics.items()):
+            if topics and t not in topics: continue  # for
+            mymedian = None
+            if c > 1 and self._reader:
+                stamps = sorted(m.publish_time / 1E9 for _, _, m in self._reader.iter_messages([t]))
+                mymedian = median(sorted(s1 - s0 for s1, s0 in zip(stamps[1:], stamps[:-1])))
+            freq = 1.0 / mymedian if mymedian else None
+            topicdict[t] = self.TopicTuple(n, c, len(self._qoses.get((t, n)) or []), freq)
+        result = self.TypesAndTopicsTuple(msgtypes, topicdict)
+        if not topics or set(topics) == set(t for t, _, _ in self._topics): self._ttinfo = result
+        return result
+
+
+    def read_messages(self, topics=None, start_time=None, end_time=None, raw=False):
         """
         Yields messages from the bag, optionally filtered by topic and timestamp.
 
         @param   topics      list of topics or a single topic to filter by, if at all
-        @param   start_time  earliest timestamp of message to return, as UNIX timestamp
-        @param   end_time    latest timestamp of message to return, as UNIX timestamp
-        @return              (topic, msg, rclpy.time.Time)
+        @param   start_time  earliest timestamp of message to return, as ROS time or convertible
+                             (int/float/duration/datetime/decimal)
+        @param   end_time    latest timestamp of message to return, as ROS time or convertible
+                             (int/float/duration/datetime/decimal)
+        @param   raw         if true, then returned messages are tuples of
+                             (typename, bytes, typehash, typeclass)
+        @return              BagMessage namedtuples of 
+                             (topic, message, timestamp as rospy/rclpy.Time)
         """
-        topics = topics if isinstance(topics, list) else [topics] if topics else []
-        start_ns, end_ns = (x and x * 10**9 for x in (start_time, end_time))
+        if self.closed: raise ValueError("I/O operation on closed file.")
+        if "w" == self._mode: raise io.UnsupportedOperation("read")
+
+        topics = topics if isinstance(topics, list) else [topics] if topics else None
+        start_ns, end_ns = (api.to_nsec(api.to_time(x)) for x in (start_time, end_time))
         for schema, channel, message in self._reader.iter_messages(topics, start_ns, end_ns):
-            msg = self._decode_message(message, channel, schema)
-            yield channel.topic, msg, rosapi.make_time(nsecs=message.publish_time)
+            if raw:
+                typekey = (typename, typehash) = self._schematypes[schema.id]
+                if typekey not in self._types:
+                    self._types[typekey] = self._make_message_class(schema, message)
+                msg = (typename, message.data, typehash, self._types[typekey])
+            else: msg = self._decode_message(message, channel, schema)
+            api.TypeMeta.make(msg, channel.topic, self)
+            yield self.BagMessage(channel.topic, msg, api.make_time(nsecs=message.publish_time))
+            if self.closed: break  # for schema
+
+
+    def write(self, topic, msg, t=None, raw=False, **__):
+        """
+        Writes out message to MCAP file.
+
+        @param   topic   name of topic
+        @param   msg     ROS1 message
+        @param   t       message timestamp if not using current ROS time,
+                         as ROS time type or convertible (int/float/duration/datetime/decimal)
+        @param   raw     if true, `msg` is in raw format, (typename, bytes, typehash, typeclass)
+        """
+        if self.closed: raise ValueError("I/O operation on closed file.")
+        if "r" == self._mode: raise io.UnsupportedOperation("write")
+
+        if raw:
+            typename, binary, typehash = msg[:3]
+            cls = msg[-1]
+            typedef = self._typedefs.get((typename, typehash)) or api.get_message_definition(cls)
+            msg = api.deserialize_message(binary, cls)
+        else:
+            with api.TypeMeta.make(msg, topic) as meta:
+                typename, typehash, typedef = meta.typename, meta.typehash, meta.definition
+        topickey, typekey = (topic, typename, typehash), (typename, typehash)
+
+        nanosec = (time.time_ns() if hasattr(time, "time_ns") else int(time.time() * 10**9)) \
+                  if t is None else api.to_nsec(api.to_time(t))
+        if api.ROS2:
+            if typekey not in self._schemas:
+                fullname = api.make_full_typename(typename)
+                schema = self._writer.register_msgdef(fullname, typedef)
+                self._schemas[typekey] = schema
+            schema, data = self._schemas[typekey], api.message_to_dict(msg)
+            self._writer.write_message(topic, schema, data, publish_time=nanosec)
+        else:
+            self._writer.write_message(topic, msg, publish_time=nanosec)
+
+        sec = nanosec / 1E9
+        self._start_time = sec if self._start_time is None else min(sec, self._start_time)
+        self._end_time   = sec if self._end_time   is None else min(sec, self._end_time)
+        self._topics[topickey] = self._topics.get(topickey, 0) + 1
+        self._types.setdefault(typekey, type(msg))
+        self._typedefs.setdefault(typekey, typedef)
+        self._ttinfo = None
+
+
+    def open(self):
+        """Opens the bag file if not already open."""
+        if self._reader is not None or self._writer is not None: return
+        if self._opened and "w" == self._mode:
+            raise io.UnsupportedOperation("Cannot reopen bag for writing.")
+
+        if self._file is None: self._file = open(self._filename, "%sb" % self._mode)
+        self._reader  = mcap.reader.make_reader(self._file) if "r" == self._mode else None
+        self._decoder = mcap_ros.decoder.Decoder()          if "r" == self._mode else None
+        self._writer  = mcap_ros.writer.Writer(self._file)  if "w" == self._mode else None
+        if "r" == self._mode: self._populate_meta()
+        self._opened = True
 
 
     def close(self):
         """Closes the bag file."""
-        if self._file:
+        if self._file is not None:
+            if self._writer: self._writer.finish()
             self._file.close()
-            self._file, self._reader = None, None
+            self._file, self._reader, self._writer, self._iterer = None, None, None, None
+
+
+    @property
+    def closed(self):
+        """Returns whether file is closed."""
+        return self._file is None
+
+
+    @property
+    def topics(self):
+        """Returns the list of topics in bag, in alphabetic order."""
+        return sorted((t for t, _, _ in self._topics), key=str.lower)
+
+
+    @property
+    def filename(self):
+        """Returns bag file path."""
+        return self._filename
 
 
     @property
     def size(self):
         """Returns current file size."""
-        return os.path.getsize(self.filename) if os.path.isfile(self.filename) else None
+        if self._filename is None and self._file is not None:
+            pos, _  = self._file.tell(), self._file.seek(0, os.SEEK_END)
+            size, _ = self._file.tell(), self._file.seek(pos)
+            return size
+        return os.path.getsize(self._filename) if os.path.isfile(self._filename) else None
+
+
+    @property
+    def mode(self):
+        """Returns file open mode."""
+        return self._mode
+
+
+    def __contains__(self, key):
+        """Returns whether bag contains given topic."""
+        return any(key == t for t, _, _ in self._topics)
+
+
+    def __next__(self):
+        """Retrieves next message from bag as (topic, message, timestamp)."""
+        if self.closed: raise ValueError("I/O operation on closed file.")
+        if self._iterer is None: self._iterer = self.read_messages()
+        return next(self._iterer)
 
 
     def _decode_message(self, message, channel, schema):
         """
         Returns ROS message deserialized from binary data.
 
         @param   message  mcap.records.Message instance
         @param   channel  mcap.records.Channel instance for message
-        @param   shcema   mcap.records.Schema instance for message type
+        @param   schema   mcap.records.Schema instance for message type
         """
-        typekey = (typename, typehash) = self._schemas[schema.id]
-        if ros2 and typekey not in self._types:
-            try:  # Try loading class from disk for full compatibility
-                cls = rosapi.get_message_class(typename)
-                clshash = rosapi.get_message_type_hash(cls)
-                if typehash == clshash: self._types[typekey] = cls
-            except Exception: pass  # ModuleNotFoundError, AttributeError etc
-        if ros2 and typekey in self._types:
-            msg = ros2.deserialize_message(message.data, self._types[typekey])
+        cls = self._make_message_class(schema, message, generate=False)
+        if api.ROS2 and not issubclass(cls, types.SimpleNamespace):
+            msg = api.deserialize_message(message.data, cls)
         else:
             msg = self._decoder.decode(schema=schema, message=message)
-            if ros2:  # MCAP ROS2 message classes need monkey-patching with expected API
-                msg = self._patch_message(msg, *self._schemas[schema.id])
+            if api.ROS2:  # MCAP ROS2 message classes need monkey-patching with expected API
+                msg = self._patch_message(msg, *self._schematypes[schema.id])
                 # Register serialized binary, as MCAP does not support serializing its own creations
-                rosapi.TypeMeta.make(msg, channel.topic, data=message.data)
+                api.TypeMeta.make(msg, channel.topic, self, data=message.data)
+        typekey = self._schematypes[schema.id]
         if typekey not in self._types: self._types[typekey] = type(msg)
         return msg
 
 
+    def _make_message_class(self, schema, message, generate=True):
+        """
+        Returns message type class, generating if not already available.
+
+        @param   schema    mcap.records.Schema instance for message type
+        @param   message   mcap.records.Message instance
+        @param   generate  generate message class dynamically if not available
+        """
+        typekey = (typename, typehash) = self._schematypes[schema.id]
+        if api.ROS2 and typekey not in self._types:
+            try:  # Try loading class from disk for full compatibility
+                cls = api.get_message_class(typename)
+                clshash = api.get_message_type_hash(cls)
+                if typehash == clshash: self._types[typekey] = cls
+            except Exception: pass  # ModuleNotFoundError, AttributeError etc
+        if typekey not in self._types and generate:
+            if api.ROS2:  # MCAP ROS2 message classes need monkey-patching with expected API
+                msg = self._decoder.decode(schema=schema, message=message)
+                self._types[typekey] = self._patch_message_class(type(msg), typename, typehash)
+            else:
+                typeclses = genpy.dynamic.generate_dynamic(typename, schema.data.decode())
+                self._types[typekey] = typeclses[typename]
+        return self._types.get(typekey)
+
+
+    def _patch_message_class(self, cls, typename, typehash):
+        """
+        Patches MCAP ROS2 message class with expected attributes and methods, recursively.
+
+        @param   cls       ROS message class as returned from mcap_ros2.decoder
+        @param   typename  ROS message type name, like "std_msgs/Bool"
+        @param   typehash  ROS message type hash
+        @return            patched class
+        """
+        typekey = (typename, typehash)
+        if typekey not in self._typefields:
+            fields = api.parse_definition_fields(typename, self._typedefs[typekey])
+            self._typefields[typekey] = fields
+            self._field_subtypes[typekey] = {n: (s, h) for n, t in fields.items()
+                                             for s in [api.scalar(t)]
+                                             if s not in api.ROS_BUILTIN_TYPES
+                                             for h in [self._type_subtypes[typekey][s]]}
+
+        # mcap_ros2 creates a dynamic class for each message, having __slots__
+        # but no other ROS2 API hooks; even the class module is "mcap_ros2.dynamic".
+        cls.__module__ = typename.split("/", maxsplit=1)[0]
+        cls.SLOT_TYPES = ()  # rosidl_runtime_py.utilities.is_message() checks for presence
+        cls._fields_and_field_types = dict(self._typefields[typekey])
+        cls.get_fields_and_field_types = message_get_fields_and_field_types
+        cls.__copy__     = copy_with_slots  # MCAP message classes lack support for copy
+        cls.__deepcopy__ = deepcopy_with_slots
+
+        return cls
+
+
     def _patch_message(self, message, typename, typehash):
         """
         Patches MCAP ROS2 message with expected attributes and methods, recursively.
 
         @param   message   ROS message instance as returned from mcap_ros2.decoder
         @param   typename  ROS message type name, like "std_msgs/Bool"
         @param   typehash  ROS message type hash
@@ -194,30 +454,15 @@
                 # Convert temporal types to ROS2 temporal types for grepros to recognize
                 msg2 = self._temporal_ctors[typename](sec=msg.sec, nanosec=msg.nanosec)
                 if msg is message: result = msg2                     # Replace input message
                 elif len(path) == 1: setattr(parent, path[0], msg2)  # Set scalar field
                 else: getattr(parent, path[0])[path[1]] = msg2       # Set array field element
                 continue  # while stack
 
-            if typekey not in self._typefields:
-                fields = rosapi.parse_definition_fields(typename, self._typedefs[typekey])
-                self._typefields[typekey] = fields
-                self._field_subtypes[typekey] = {n: (s, h) for n, t in fields.items()
-                                                 for s in [rosapi.scalar(t)]
-                                                 if s not in rosapi.ROS_BUILTIN_TYPES
-                                                 for h in [self._type_subtypes[typekey][s]]}
-
-            # mcap_ros2 creates a dynamic class for each message, having __slots__
-            # but no other ROS2 API hooks; even the class module is "mcap_ros2.dynamic".
-            mycls.__module__ = typename.split("/", maxsplit=1)[0]
-            mycls.SLOT_TYPES = ()  # rosidl_runtime_py.utilities.is_message() checks for presence
-            mycls._fields_and_field_types = dict(self._typefields[typekey])
-            mycls.get_fields_and_field_types = message_get_fields_and_field_types
-            mycls.__copy__     = copy_with_slots  # MCAP message classes lack support for copy
-            mycls.__deepcopy__ = deepcopy_with_slots
+            self._patch_message_class(mycls, *typekey)
 
             for name, subtypekey in self._field_subtypes[typekey].items():
                 v = getattr(msg, name)
                 if isinstance(v, list):  # Queue each array element for patching
                     stack.extend((x, subtypekey, msg, (name, i)) for i, x in enumerate(v))
                 else:  # Queue scalar field for patching
                     stack.append((v, subtypekey, msg, (name, )))
@@ -230,21 +475,20 @@
         summary = self._reader.get_summary()
         self._start_time = summary.statistics.message_start_time / 1E9
         self._end_time   = summary.statistics.message_end_time   / 1E9
 
         defhashes = {}  # Cached {type definition full text: type hash}
         for cid, channel in summary.channels.items():
             schema = summary.schemas[channel.schema_id]
-            topic, typename = channel.topic, schema.name
-            if ros2: typename = ros2.canonical(typename)
+            topic, typename = channel.topic, api.canonical(schema.name)
 
             typedef = schema.data.decode("utf-8")  # Full definition including subtype definitions
-            subtypedefs, nesting = rosapi.parse_definition_subtypes(typedef, nesting=True)
+            subtypedefs, nesting = api.parse_definition_subtypes(typedef, nesting=True)
             typehash = channel.metadata.get("md5sum") or \
-                       rosapi.calculate_definition_hash(typename, typedef,
+                       api.calculate_definition_hash(typename, typedef,
                                                         tuple(subtypedefs.items()))
             topickey, typekey = (topic, typename, typehash), (typename, typehash)
 
             qoses = None
             if channel.metadata.get("offered_qos_profiles"):
                 try: qoses = yaml.safe_load(channel.metadata["offered_qos_profiles"])
                 except Exception as e:
@@ -253,35 +497,41 @@
 
             self._topics.setdefault(topickey, 0)
             self._topics[topickey] += summary.statistics.channel_message_counts[cid]
             self._typedefs[typekey] = typedef
             defhashes[typedef] = typehash
             for t, d in subtypedefs.items():  # Populate subtype definitions and hashes
                 if d in defhashes: h = defhashes[d]
-                else: h = rosapi.calculate_definition_hash(t, d, tuple(subtypedefs.items()))
+                else: h = api.calculate_definition_hash(t, d, tuple(subtypedefs.items()))
                 self._typedefs.setdefault((t, h), d)
                 self._type_subtypes.setdefault(typekey, {})[t] = h
                 defhashes[d] = h
             for t, subtypes in nesting.items():  # Populate all nested type references
                 h = self._type_subtypes[typekey][t]
                 for t2 in subtypes:
                     h2 = self._type_subtypes[typekey][t2]
                     self._type_subtypes.setdefault((t, h), {})[t2] = h2
 
             if qoses: self._qoses[topickey] = qoses
-            self._schemas[schema.id] = typekey
+            self._schemas[typekey] = schema
+            self._schematypes[schema.id] = typekey
 
 
     @classmethod
-    def autodetect(cls, filename):
+    def autodetect(cls, f):
         """Returns whether file is readable as MCAP format."""
-        result = os.path.isfile(filename)
-        if result:
-            with open(filename, "rb") as f:
+        if common.is_stream(f):
+            pos, _ = f.tell(), f.seek(0)
+            result, _ = (f.read(len(cls.MCAP_MAGIC)) == cls.MCAP_MAGIC), f.seek(pos)
+        elif os.path.isfile(f) and os.path.getsize(f):
+            with open(f, "rb") as f:
                 result = (f.read(len(cls.MCAP_MAGIC)) == cls.MCAP_MAGIC)
+        else:
+            ext = os.path.splitext(f or "")[-1].lower()
+            result = ext in McapSink.FILE_EXTENSIONS
         return result
 
 
 
 def copy_with_slots(self):
     """Returns a shallow copy, with slots populated manually."""
     result = self.__class__.__new__(self.__class__)
@@ -299,100 +549,130 @@
 
 
 def message_get_fields_and_field_types(self):
     """Returns a map of message field names and types (patch method for MCAP message classes)."""
     return self._fields_and_field_types.copy()
 
 
+def message_repr(self):
+    """Returns a string representation of ROS message."""
+    fields = ", ".join("%s=%r" % (n, getattr(self, n)) for n in self.__slots__)
+    return "%s(%s)" % (self.__name__, fields)
 
-class McapSink(SinkBase):
-    """Writes messages to MCAP files."""
+
+
+class McapSink(Sink):
+    """Writes messages to MCAP file."""
 
     ## Auto-detection file extensions
     FILE_EXTENSIONS = (".mcap", )
 
+    ## Constructor argument defaults
+    DEFAULT_ARGS = dict(META=False, WRITE_OPTIONS={}, VERBOSE=False)
 
-    def __init__(self, args):
+
+    def __init__(self, args=None, **kwargs):
         """
-        @param   args                 arguments object like argparse.Namespace
-        @param   args.META            whether to print metainfo
-        @param   args.WRITE           base name of MCAP files to write
-        @param   args.WRITE_OPTIONS   {"overwrite": whether to overwrite existing file
+        @param   args                 arguments as namespace or dictionary, case-insensitive;
+                                      or a single path as the file to write
+        @param   args.write           base name of MCAP files to write
+        @param   args.write_options   {"overwrite": whether to overwrite existing file
                                                     (default false)}
-        @param   args.VERBOSE         whether to print debug information
+        @param   args.meta            whether to print metainfo
+        @param   args.verbose         whether to print debug information
+        @param   kwargs               any and all arguments as keyword overrides, case-insensitive
         """
+        args = {"WRITE": str(args)} if isinstance(args, common.PATH_TYPES) else args
+        args = common.ensure_namespace(args, McapSink.DEFAULT_ARGS, **kwargs)
         super(McapSink, self).__init__(args)
 
         self._filename      = None  # Output filename
         self._file          = None  # Open file() object
         self._writer        = None  # mcap_ros.writer.Writer object
         self._schemas       = {}    # {(typename, typehash): mcap.records.Schema}
-        self._overwrite     = (args.WRITE_OPTIONS.get("overwrite") == "true")
+        self._overwrite     = (args.WRITE_OPTIONS.get("overwrite") in (True, "true"))
         self._close_printed = False
 
         atexit.register(self.close)
 
 
     def validate(self):
         """
         Returns whether required libraries are available (mcap, mcap_ros1/mcap_ros2)
-        and overwrite is valid.
+        and overwrite is valid and file is writable.
         """
+        if self.valid is not None: return self.valid
         ok, mcap_ok, mcap_ros_ok = True, bool(mcap), bool(mcap_ros)
-        if self.args.WRITE_OPTIONS.get("overwrite") not in (None, "true", "false"):
+        if self.args.WRITE_OPTIONS.get("overwrite") not in (None, True, False, "true", "false"):
             ConsolePrinter.error("Invalid overwrite option for MCAP: %r. "
                                  "Choose one of {true, false}.",
                                  self.args.WRITE_OPTIONS["overwrite"])
             ok = False
         if not mcap_ok:
             ConsolePrinter.error("mcap not available: cannot work with MCAP files.")
         if not mcap_ros_ok:
             ConsolePrinter.error("mcap_ros%s not available: cannot work with MCAP files.",
-                                 os.getenv("ROS_VERSION", ""))
-        return ok and mcap_ok and mcap_ros_ok
+                                 api.ROS_VERSION or "")
+        if not common.verify_io(self.args.WRITE, "w"):
+            ok = False
+        self.valid = ok and mcap_ok and mcap_ros_ok
+        return self.valid
 
 
-    def emit(self, topic, index, stamp, msg, match):
+    def emit(self, topic, msg, stamp=None, match=None, index=None):
         """Writes out message to MCAP file."""
+        if not self.validate(): raise Exception("invalid")
         self._ensure_open()
-        kwargs = dict(publish_time=rosapi.to_nsec(stamp), sequence=index)
-        if ros2:
-            with rosapi.TypeMeta.make(msg, topic) as m:
+        stamp, index = self._ensure_stamp_index(topic, msg, stamp, index)
+        kwargs = dict(publish_time=api.to_nsec(stamp), sequence=index)
+        if api.ROS2:
+            with api.TypeMeta.make(msg, topic) as m:
                 typekey = m.typekey
                 if typekey not in self._schemas:
-                    fullname = ros2.make_full_typename(m.typename)
+                    fullname = api.make_full_typename(m.typename)
                     self._schemas[typekey] = self._writer.register_msgdef(fullname, m.definition)
-            schema, data = self._schemas[typekey], rosapi.message_to_dict(msg)
+            schema, data = self._schemas[typekey], api.message_to_dict(msg)
             self._writer.write_message(topic, schema, data, **kwargs)
         else:
             self._writer.write_message(topic, msg, **kwargs)
-        super(McapSink, self).emit(topic, index, stamp, msg, match)
+        super(McapSink, self).emit(topic, msg, stamp, match, index)
 
 
     def close(self):
         """Closes output file if open."""
-        if self._writer:
-            self._writer.finish()
-            self._file.close()
-            self._file, self._writer = None, None
-        if not self._close_printed and self._counts:
-            self._close_printed = True
-            ConsolePrinter.debug("Wrote %s in %s to %s (%s).",
-                                 plural("message", sum(self._counts.values())),
-                                 plural("topic", self._counts), self._filename,
-                                 format_bytes(os.path.getsize(self._filename)))
-        super(McapSink, self).close()
+        try:
+            if self._writer:
+                self._writer.finish()
+                self._file.close()
+                self._file, self._writer = None, None
+        finally:
+            if not self._close_printed and self._counts:
+                self._close_printed = True
+                try: sz = common.format_bytes(os.path.getsize(self._filename))
+                except Exception as e:
+                    ConsolePrinter.warn("Error getting size of %s: %s", self._filename, e)
+                    sz = "error getting size"
+                ConsolePrinter.debug("Wrote %s in %s to %s (%s).",
+                                     common.plural("message", sum(self._counts.values())),
+                                     common.plural("topic", self._counts), self._filename, sz)
+            super(McapSink, self).close()
 
 
     def _ensure_open(self):
         """Opens output file if not already open."""
         if self._file: return
 
-        self._filename = self.args.WRITE if self._overwrite else unique_path(self.args.WRITE)
-        makedirs(os.path.dirname(self._filename))
+        filename = self.args.WRITE
+        if not self._overwrite and os.path.isfile(filename) and os.path.getsize(filename):
+            filename = common.unique_path(filename)
+            if self.args.VERBOSE:
+                ConsolePrinter.debug("Making unique filename %r, as %s does not support "
+                                     "appending.", filename, type(self).__name___)
+        self._filename = filename
+        common.makedirs(os.path.dirname(self._filename))
         if self.args.VERBOSE:
             sz = os.path.exists(self._filename) and os.path.getsize(self._filename)
             action = "Overwriting" if sz and self._overwrite else "Creating"
             ConsolePrinter.debug("%s %s.", action, self._filename)
         self._file = open(self._filename, "wb")
         self._writer = mcap_ros.writer.Writer(self._file)
 
@@ -403,9 +683,13 @@
         ConsolePrinter.error("mcap libraries not available: cannot work with MCAP files.")
         raise ImportWarning()
     from .. import plugins  # Late import to avoid circular
     plugins.add_write_format("mcap", McapSink, "MCAP", [
         ("overwrite=true|false",  "overwrite existing file in MCAP output\n"
                                   "instead of appending unique counter (default false)"),
     ])
-    rosapi.BAG_EXTENSIONS += McapSink.FILE_EXTENSIONS
-    rosapi.Bag.READER_CLASSES.add(McapReader)
+    api.BAG_EXTENSIONS += McapSink.FILE_EXTENSIONS
+    api.Bag.READER_CLASSES.add(McapBag)
+    api.Bag.WRITER_CLASSES.add(McapBag)
+
+
+__all__ = ["McapBag", "McapSink", "init"]
```

## grepros/plugins/parquet.py

```diff
@@ -1,38 +1,41 @@
 # -*- coding: utf-8 -*-
 """
-Parquet output for search results.
+Parquet output plugin.
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     14.12.2021
-@modified    06.02.2022
+@modified    29.06.2023
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.plugins.parquet
+import itertools
 import json
 import os
 import re
+import uuid
 
 try: import pandas
 except ImportError: pandas = None
 try: import pyarrow
 except ImportError: pyarrow = None
 try: import pyarrow.parquet
 except ImportError: pass
+import six
 
-from .. common import ConsolePrinter, format_bytes, makedirs, plural, unique_path
-from .. outputs import SinkBase
-from .. import rosapi
+from .. import api, common
+from .. common import ConsolePrinter
+from .. outputs import Sink
 
 
-class ParquetSink(SinkBase):
+class ParquetSink(Sink):
     """Writes messages to Apache Parquet files."""
 
     ## Auto-detection file extensions
     FILE_EXTENSIONS = (".parquet", )
 
     ## Number of dataframes to cache before writing, per type
     CHUNK_SIZE = 100
@@ -69,155 +72,244 @@
     ## Fallback pyarrow type if mapped type not found
     DEFAULT_TYPE = pyarrow.string() if pyarrow else None
 
     ## Default columns for message type tables
     MESSAGE_TYPE_BASECOLS  = [("_topic",      "string"),
                               ("_timestamp",  "time"), ]
 
+    ## Additional default columns for messaga type tables with nesting output
+    MESSAGE_TYPE_NESTCOLS  = [("_id",          "string"),
+                              ("_parent_type", "string"),
+                              ("_parent_id",   "string"), ]
+
     ## Custom arguments for pyarrow.parquet.ParquetWriter
     WRITER_ARGS = {"version": "2.6"}
 
+    ## Constructor argument defaults
+    DEFAULT_ARGS = dict(EMIT_FIELD=(), META=False, NOEMIT_FIELD=(), WRITE_OPTIONS={},
+                        VERBOSE=False)
+
 
-    def __init__(self, args):
+    def __init__(self, args=None, **kwargs):
         """
-        @param   args                 arguments object like argparse.Namespace
-        @param   args.META            whether to print metainfo
-        @param   args.WRITE           base name of Parquet files to write
-        @param   args.WRITE_OPTIONS   {"writer-*": arguments passed to ParquetWriter,
+        @param   args                 arguments as namespace or dictionary, case-insensitive;
+                                      or a single path as the base name of Parquet files to write
+        @param   args.emit_field      message fields to emit in output if not all
+        @param   args.noemit_field    message fields to skip in output
+        @param   args.write           base name of Parquet files to write
+        @param   args.write_options   ```
+                                      {"column": additional columns as {name: (rostype, value)},
+                                       "type": {rostype: PyArrow type or typename like "uint8"},
+                                       "writer": dictionary of arguments passed to ParquetWriter,
+                                       "idgenerator": callable or iterable for producing message IDs
+                                                      like uuid.uuid4 or itertools.count();
+                                                      nesting uses UUID values by default,
+                                       "column-k=rostype:v": one "column"-argument
+                                                             in flat string form,
+                                       "type-k=v: one "type"-argument in flat string form,
+                                       "writer-k=v": one "writer"-argument in flat string form,
+                                       "nesting": "array" to recursively insert arrays
+                                                  of nested types, or "all" for any nesting,
                                        "overwrite": whether to overwrite existing file
                                                     (default false)}
-        @param   args.VERBOSE         whether to print debug information
+                                      ```
+        @param   args.meta            whether to print metainfo
+        @param   args.verbose         whether to print debug information
+        @param   kwargs               any and all arguments as keyword overrides, case-insensitive
         """
+        args = {"WRITE": str(args)} if isinstance(args, common.PATH_TYPES) else args
+        args = common.ensure_namespace(args, ParquetSink.DEFAULT_ARGS, **kwargs)
         super(ParquetSink, self).__init__(args)
 
         self._filebase       = args.WRITE
-        self._overwrite      = (args.WRITE_OPTIONS.get("overwrite") == "true")
+        self._overwrite      = (args.WRITE_OPTIONS.get("overwrite") in (True, "true"))
         self._filenames      = {}  # {(typename, typehash): Parquet file path}
         self._caches         = {}  # {(typename, typehash): [{data}, ]}
         self._schemas        = {}  # {(typename, typehash): pyarrow.Schema}
         self._writers        = {}  # {(typename, typehash): pyarrow.parquet.ParquetWriter}
-        self._extra_basecols = []  # [(name, value)]
+        self._extra_basecols = []  # [(name, rostype)]
+        self._extra_basevals = []  # [(name, value)]
+        self._patterns       = {}  # {key: [(() if any field else ('path', ), re.Pattern), ]}
+        self._nesting        = args.WRITE_OPTIONS.get("nesting")
+        self._idgenerator    = iter(lambda: str(uuid.uuid4()), self) if self._nesting else None
 
-        self._configure_ok  = True
         self._close_printed = False
 
-        self._configure()
-
 
     def validate(self):
         """
-        Returns whether required libraries are available (pandas and pyarrow) and overwrite is valid.
+        Returns whether required libraries are available (pandas and pyarrow) and overwrite is valid
+        and file base is writable.
         """
-        ok, pandas_ok, pyarrow_ok = self._configure_ok, bool(pandas), bool(pyarrow)
-        if self.args.WRITE_OPTIONS.get("overwrite") not in (None, "true", "false"):
+        if self.valid is not None: return self.valid
+        ok, pandas_ok, pyarrow_ok = self._configure(), bool(pandas), bool(pyarrow)
+        if self.args.WRITE_OPTIONS.get("overwrite") not in (None, True, False, "true", "false"):
             ConsolePrinter.error("Invalid overwrite option for Parquet: %r. "
                                  "Choose one of {true, false}.",
                                  self.args.WRITE_OPTIONS["overwrite"])
             ok = False
+        if self.args.WRITE_OPTIONS.get("nesting") not in (None, "", "array", "all"):
+            ConsolePrinter.error("Invalid nesting option for Parquet: %r. "
+                                 "Choose one of {array,all}.",
+                                 self.args.WRITE_OPTIONS["nesting"])
+            ok = False
         if not pandas_ok:
             ConsolePrinter.error("pandas not available: cannot write Parquet files.")
         if not pyarrow_ok:
             ConsolePrinter.error("PyArrow not available: cannot write Parquet files.")
-        return ok and pandas_ok and pyarrow_ok
+        if not common.verify_io(self.args.WRITE, "w"):
+            ok = False
+        self.valid = ok and pandas_ok and pyarrow_ok
+        return self.valid
 
 
-    def emit(self, topic, index, stamp, msg, match):
+    def emit(self, topic, msg, stamp=None, match=None, index=None):
         """Writes message to a Parquet file."""
+        if not self.validate(): raise Exception("invalid")
+        stamp, index = self._ensure_stamp_index(topic, msg, stamp, index)
         self._process_type(topic, msg)
-        self._process_message(topic, stamp, msg)
-        super(ParquetSink, self).emit(topic, index, stamp, msg, match)
+        self._process_message(topic, index, stamp, msg, match)
 
 
     def close(self):
         """Writes out any remaining messages, closes writers, clears structures."""
-        for k, vv in list(self._caches.items()):
-            vv and self._write_table(k)
-        for k in list(self._writers):
-            self._writers.pop(k).close()
-        if not self._close_printed and self._counts:
-            self._close_printed = True
-            sizes = {n: os.path.getsize(n) for n in self._filenames.values()}
-            ConsolePrinter.debug("Wrote %s in %s to %s (%s):",
-                                 plural("message", sum(self._counts.values())),
-                                 plural("topic", self._counts), plural("Parquet file", sizes),
-                                 format_bytes(sum(sizes.values())))
-            for (t, h), name in self._filenames.items():
-                count = sum(c for (_, t_, h_), c in self._counts.items() if (t, h) == (t_, h_))
-                ConsolePrinter.debug("- %s (%s, %s)", name,
-                                    format_bytes(sizes[name]), plural("message", count))
-        self._caches.clear()
-        self._schemas.clear()
-        self._filenames.clear()
+        try:
+            for k, vv in list(self._caches.items()):
+                vv and self._write_table(k)
+            for k in list(self._writers):
+                self._writers.pop(k).close()
+        finally:
+            if not self._close_printed and self._counts:
+                self._close_printed = True
+                sizes = {n: None for n in self._filenames.values()}
+                for n in self._filenames.values():
+                    try: sizes[n] = os.path.getsize(n)
+                    except Exception as e: ConsolePrinter.warn("Error getting size of %s: %s", n, e)
+                ConsolePrinter.debug("Wrote %s in %s to %s (%s):",
+                                     common.plural("message", sum(self._counts.values())),
+                                     common.plural("topic", self._counts),
+                                     common.plural("Parquet file", sizes),
+                                     common.format_bytes(sum(filter(bool, sizes.values()))))
+                for (t, h), name in self._filenames.items():
+                    count = sum(c for (_, t_, h_), c in self._counts.items() if (t, h) == (t_, h_))
+                    ConsolePrinter.debug("- %s (%s, %s)", name,
+                                         "error getting size" if sizes[name] is None else
+                                         common.format_bytes(sizes[name]),
+                                         common.plural("message", count))
+            self._caches.clear()
+            self._schemas.clear()
+            self._filenames.clear()
 
 
-    def _process_type(self, topic, msg):
+    def _process_type(self, topic, msg, rootmsg=None):
         """Prepares Parquet schema and writer if not existing."""
-        with rosapi.TypeMeta.make(msg, topic) as m:
+        rootmsg = rootmsg or msg
+        with api.TypeMeta.make(msg, root=rootmsg) as m:
             typename, typehash, typekey = (m.typename, m.typehash, m.typekey)
-        if (topic, typename, typehash) not in self._counts and self.args.VERBOSE:
+        if topic and (topic, typename, typehash) not in self._counts and self.args.VERBOSE:
             ConsolePrinter.debug("Adding topic %s in Parquet output.", topic)
         if typekey in self._writers: return
 
         basedir, basename = os.path.split(self._filebase)
         pathname = os.path.join(basedir, re.sub(r"\W", "__", "%s__%s" % (typename, typehash)))
         filename = os.path.join(pathname, basename)
         if not self._overwrite:
-            filename = unique_path(filename)
+            filename = common.unique_path(filename)
 
         cols = []
         scalars = set(x for x in self.COMMON_TYPES if "[" not in x)
-        for path, value, subtype in rosapi.iter_message_fields(msg, scalars=scalars):
+        fltrs = dict(include=self._patterns["print"], exclude=self._patterns["noprint"])
+        for path, value, subtype in api.iter_message_fields(msg, scalars=scalars, **fltrs):
             coltype = self._make_column_type(subtype)
             cols += [(".".join(path), coltype)]
+        MSGCOLS = self.MESSAGE_TYPE_BASECOLS + (self.MESSAGE_TYPE_NESTCOLS if self._nesting else [])
         cols += [(c, self._make_column_type(t, fallback="int64" if "time" == t else None))
-                 for c, t in self.MESSAGE_TYPE_BASECOLS]
+                 for c, t in MSGCOLS + self._extra_basecols]
 
         if self.args.VERBOSE:
             sz = os.path.isfile(filename) and os.path.getsize(filename)
             action = "Overwriting" if sz and self._overwrite else "Adding"
             ConsolePrinter.debug("%s type %s in Parquet output.", action, typename)
-        makedirs(pathname)
+        common.makedirs(pathname)
 
         schema = pyarrow.schema(cols)
         writer = pyarrow.parquet.ParquetWriter(filename, schema, **self.WRITER_ARGS)
         self._caches[typekey]    = []
         self._filenames[typekey] = filename
         self._schemas[typekey]   = schema
         self._writers[typekey]   = writer
 
+        nesteds = api.iter_message_fields(msg, messages_only=True, **fltrs) if self._nesting else ()
+        for path, submsgs, subtype in nesteds:
+            scalartype = api.scalar(subtype)
+            if subtype == scalartype and "all" != self._nesting:
+                continue  # for path
+            subtypehash = not submsgs and self.source.get_message_type_hash(scalartype)
+            if not isinstance(submsgs, (list, tuple)): submsgs = [submsgs]
+            [submsg] = submsgs[:1] or [self.source.get_message_class(scalartype, subtypehash)()]
+            self._process_type(None, submsg, rootmsg)
+
 
-    def _process_message(self, topic, stamp, msg):
+    def _process_message(self, topic, index, stamp, msg, match=None,
+                         rootmsg=None, parent_type=None, parent_id=None):
         """
         Converts message to pandas dataframe, adds to cache.
 
         Writes cache to disk if length reached chunk size.
+
+        If nesting is enabled, processes nested messages for subtypes in message.
+        If IDs are used, returns generated ID.
         """
-        data = {}
-        typekey = rosapi.TypeMeta.make(msg, topic).typekey
-        for p, v, t in rosapi.iter_message_fields(msg, scalars=set(self.COMMON_TYPES)):
+        data, myid, rootmsg = {}, None, (rootmsg or None)
+        if self._idgenerator: myid = next(self._idgenerator)
+        with api.TypeMeta.make(msg, topic, root=rootmsg) as m:
+            typename, typekey = m.typename, m.typekey
+        fltrs = dict(include=self._patterns["print"], exclude=self._patterns["noprint"])
+        for p, v, t in api.iter_message_fields(msg, scalars=set(self.COMMON_TYPES), **fltrs):
             data[".".join(p)] = self._make_column_value(v, t)
         data.update(_topic=topic, _timestamp=self._make_column_value(stamp, "time"))
-        data.update(self._extra_basecols)
+        if self._idgenerator: data.update(_id=myid)
+        if self._nesting:
+            COLS = [k for k, _ in self.MESSAGE_TYPE_NESTCOLS if "parent" in k]
+            data.update(zip(COLS, [parent_type, parent_id]))
+        data.update(self._extra_basevals)
         self._caches[typekey].append(data)
+        super(ParquetSink, self).emit(topic, msg, stamp, match, index)
+
+        subids = {}  # {message field path: [ids]}
+        nesteds = api.iter_message_fields(msg, messages_only=True, **fltrs) if self._nesting else ()
+        for path, submsgs, subtype in nesteds:
+            scalartype = api.scalar(subtype)
+            if subtype == scalartype and "all" != self._nesting:
+                continue  # for path
+            if isinstance(submsgs, (list, tuple)):
+                subids[path] = []
+            for submsg in submsgs if isinstance(submsgs, (list, tuple)) else [submsgs]:
+                subid = self._process_message(topic, index, stamp, submsg,
+                                              rootmsg=rootmsg, parent_type=typename, parent_id=myid)
+                if isinstance(submsgs, (list, tuple)):
+                    subids[path].append(subid)
+        data.update(subids)
+
         if len(self._caches[typekey]) >= self.CHUNK_SIZE:
             self._write_table(typekey)
+        return myid
 
 
     def _make_column_type(self, typename, fallback=None):
         """
         Returns pyarrow type for ROS type.
 
         @param  fallback  fallback typename to use for lookup if typename not found
         """
-        scalartype = rosapi.scalar(typename)
-        timetype   = rosapi.get_ros_time_category(scalartype)
-        coltype    = self.COMMON_TYPES.get(typename)
+        noboundtype = api.canonical(typename, unbounded=True)
+        scalartype  = api.scalar(typename)
+        timetype    = api.get_ros_time_category(scalartype)
+        coltype     = self.COMMON_TYPES.get(typename) or self.COMMON_TYPES.get(noboundtype)
 
-        if not coltype and "[" not in typename and scalartype in self.COMMON_TYPES:
-            coltype = self.COMMON_TYPES[scalartype]  # Bounded type like "string<=10"
         if not coltype and scalartype in self.COMMON_TYPES:
             coltype = pyarrow.list_(self.COMMON_TYPES[scalartype])
         if not coltype and timetype in self.COMMON_TYPES:
             if typename != scalartype:
                 coltype = pyarrow.list_(self.COMMON_TYPES[timetype])
             else:
                 coltype = self.COMMON_TYPES[timetype]
@@ -228,99 +320,172 @@
         return coltype
 
 
     def _make_column_value(self, value, typename=None):
         """Returns column value suitable for adding to Parquet file."""
         v = value
         if isinstance(v, (list, tuple)):
-            if v and rosapi.is_ros_time(v[0]):
-                v = [rosapi.to_nsec(x) for x in v]
-            elif rosapi.scalar(typename) not in rosapi.ROS_BUILTIN_TYPES:
-                v = str([rosapi.message_to_dict(m) for m in v])
-            elif pyarrow.binary() == self.COMMON_TYPES.get(typename):
+            noboundtype = api.canonical(typename, unbounded=True)
+            if v and api.is_ros_time(v[0]):
+                v = [api.to_nsec(x) for x in v]
+            elif api.scalar(typename) not in api.ROS_BUILTIN_TYPES:
+                v = str([api.message_to_dict(m) for m in v])
+            elif pyarrow.binary() in (self.COMMON_TYPES.get(typename),
+                                      self.COMMON_TYPES.get(noboundtype)):
                 v = bytes(bytearray(v))  # Py2/Py3 compatible
             else:
                 v = list(v)  # Ensure lists not tuples
-        elif rosapi.is_ros_time(v):
-            v = rosapi.to_nsec(v)
-        elif typename and typename not in rosapi.ROS_BUILTIN_TYPES:
-            v = str(rosapi.message_to_dict(v))
+        elif api.is_ros_time(v):
+            v = api.to_nsec(v)
+        elif typename and typename not in api.ROS_BUILTIN_TYPES:
+            v = str(api.message_to_dict(v))
         return v
 
 
     def _write_table(self, typekey):
         """Writes out cached messages for type."""
         dicts = self._caches[typekey][:]
         del self._caches[typekey][:]
         mapping = {k: [d[k] for d in dicts] for k in dicts[0]}
         table = pyarrow.Table.from_pydict(mapping, self._schemas[typekey])
         self._writers[typekey].write_table(table)
 
 
     def _configure(self):
-        """Parses args.WRITE_OPTIONS."""
-        ok = True
+        """Parses args.WRITE_OPTIONS, returns success."""
+        ok = self._configure_ids()
+
+        def process_column(name, rostype, value):  # Parse "column-name=rostype:value"
+            v, myok = value, True
+            if "string" not in rostype:
+                v = json.loads(v)
+            if not name:
+                myok = False
+                ConsolePrinter.error("Invalid name option in %s=%s:%s", name, rostype, v)
+            if rostype not in api.ROS_BUILTIN_TYPES:
+                myok = False
+                ConsolePrinter.error("Invalid type option in %s=%s:%s", name, rostype, v)
+            if myok:
+                self._extra_basecols.append((name, rostype))
+                self._extra_basevals.append((name, value))
+
+        def process_type(rostype, arrowtype):  # Eval pyarrow datatype from value like "float64()"
+            if arrowtype not in self.ARROW_TYPES.values():
+                arrowtype = eval(compile(arrowtype, "", "eval"), {"__builtins__": self.ARROW_TYPES})
+            self.COMMON_TYPES[rostype] = arrowtype
+
+        for key, vals in [("print", self.args.EMIT_FIELD), ("noprint", self.args.NOEMIT_FIELD)]:
+            self._patterns[key] = [(tuple(v.split(".")), common.wildcard_to_regex(v)) for v in vals]
 
         # Populate ROS type aliases like "byte" and "char"
         for rostype in list(self.COMMON_TYPES):
-            alias = rosapi.get_type_alias(rostype)
+            alias = api.get_type_alias(rostype)
             if alias:
                 self.COMMON_TYPES[alias] = self.COMMON_TYPES[rostype]
             if alias and rostype + "[]" in self.COMMON_TYPES:
                 self.COMMON_TYPES[alias + "[]"] = self.COMMON_TYPES[rostype + "[]"]
 
         for k, v in self.args.WRITE_OPTIONS.items():
-            if k.startswith("column-"):
-                # Parse "column-name=rostype:value"
-                try:
-                    name, (rostype, value), myok = k[len("column-"):], v.split(":", 1), True
-                    if "string" not in rostype:
-                        value = json.loads(value)
-                    if not name:
-                        ok = myok = False
-                        ConsolePrinter.error("Invalid name option in %s=%s", k, v)
-                    if rostype not in rosapi.ROS_BUILTIN_TYPES:
-                        ok = myok = False
-                        ConsolePrinter.error("Invalid type option in %s=%s", k, v)
-                    if myok:
-                        self.MESSAGE_TYPE_BASECOLS.append((name, rostype))
-                        self._extra_basecols.append((name, value))
-                except Exception as e:
-                    ok = False
-                    ConsolePrinter.error("Invalid column option in %s=%s: %s", k, v, e)
-            elif k.startswith("type-"):
-                # Eval pyarrow datatype from value like "float64()" or "list(uint8())"
-                if not k[len("type-"):]:
-                    ok = False
-                    ConsolePrinter.error("Invalid type option in %s=%s: %s", k, v)
+            if "column" == k and v and isinstance(v, dict):
+                for name, (rostype, value) in v.items():
+                    if not process_column(name, rostype, value): ok = False
+            elif "type" == k and v and isinstance(v, dict):
+                for name, value in v.items():
+                    if not process_type(name, value): ok = False
+            elif "writer" == k and v and isinstance(v, dict):
+                self.WRITER_ARGS.update(v)
+            elif isinstance(k, str) and "-" in k:
+                category, name = k.split("-", 1)
+                if category not in ("column", "type", "writer"):
+                    ConsolePrinter.warn("Unknown %r option in %s=%s", category, k, v)
                     continue  # for k, v
                 try:
-                    arrowtype = eval(compile(v, "", "eval"), {"__builtins__": self.ARROW_TYPES})
-                    self.COMMON_TYPES[k[len("type-"):]] = arrowtype
+                    if not name: raise Exception("empty name")
+                    if "column" == category:    # column-name=rostype:value
+                        if not process_column(name, *v.split(":", 1)): ok = False
+                    elif "type" == category:    # type-rostype=arrowtype
+                        process_type(name, v)
+                    elif "writer" == category:  # writer-argname=argvalue
+                        try: v = json.loads(v)
+                        except Exception: pass
+                        self.WRITER_ARGS[name] = v
                 except Exception as e:
                     ok = False
-                    ConsolePrinter.error("Invalid type option in %s=%s: %s", k, v, e)
-            elif k.startswith("writer-"):
-                if not k[len("writer-"):]:
-                    ok = False
-                    ConsolePrinter.error("Invalid name in %s=%s: %s", k, v)
-                    continue  # for k, v
-                try: v = json.loads(v)
+                    ConsolePrinter.error("Invalid %s option in %s=%s: %s", category, k, v, e)
+        return ok
+
+
+    def _configure_ids(self):
+        """Configures ID generator from args.WRITE_OPTIONS, returns success."""
+        ok = True
+
+        k, v = "idgenerator", self.args.WRITE_OPTIONS.get("idgenerator")
+        if k in self.args.WRITE_OPTIONS:
+            val, ex, ns = v, None, dict(self.ARROW_TYPES, itertools=itertools, uuid=uuid)
+            for root in v.split(".", 1)[:1]:
+                try: ns[root] = common.import_item(root)  # Provide root module
                 except Exception: pass
-                self.WRITER_ARGS[k[len("writer-"):]] = v
-        self._configure_ok = ok
+            try: common.import_item(re.sub(r"\(.+", "", v))  # Ensure nested imports
+            except Exception: pass
+            try: val = eval(compile(v, "", "eval"), ns)
+            except Exception as e: ok, ex = False, e
+            if isinstance(val, (six.binary_type, six.text_type)): ok = False
+
+            if ok:
+                try: self._idgenerator = iter(val)
+                except Exception as e:
+                    try: self._idgenerator = iter(val, self)  # (callable=val, sentinel=self)
+                    except Exception as e: ok, ex = False, e
+            if not ok:
+                ConsolePrinter.error("Invalid value in %s=%s%s", k, v, (": %s" % ex if ex else ""))
+            elif not self._nesting:
+                self.MESSAGE_TYPE_BASECOLS.append(("_id", "string"))
+
+            if ok and self._idgenerator:  # Detect given ID column type
+                fval, typename, generator = next(self._idgenerator), None, self._idgenerator
+                if api.is_ros_time(fval):
+                    typename = "time" if "time" in str(type(fval)).lower() else "duration"
+                elif isinstance(fval, six.integer_types):
+                    typename = "int64"
+                elif isinstance(fval, float):
+                    typename = "float64"
+                elif not isinstance(fval, str):  # Cast whatever it is to string
+                    fval, self._idgenerator = str(fval), (str(x) for x in generator)
+                if typename:
+                    repl = lambda n, t: (n, typename) if "_id" in n else (n, t)
+                    self.MESSAGE_TYPE_BASECOLS = [repl(*x) for x in self.MESSAGE_TYPE_BASECOLS]
+                    self.MESSAGE_TYPE_NESTCOLS = [repl(*x) for x in self.MESSAGE_TYPE_NESTCOLS]
+                self._idgenerator = itertools.chain([fval], self._idgenerator)
+        return ok
+
 
 
 def init(*_, **__):
-    """Adds Parquet output format support."""
+    """Adds Parquet output format support. Raises ImportWarning if libraries not available."""
+    if not pandas or not pyarrow:
+        ConsolePrinter.error("pandas or PyArrow not available: cannot write Parquet files.")
+        raise ImportWarning()
     from .. import plugins  # Late import to avoid circular
     plugins.add_write_format("parquet", ParquetSink, "Parquet", [
-        ("column-name=rostype:value",  "additional column to add in Parquet output,\n"
+        ("column-NAME=ROSTYPE:VALUE",  "additional column to add in Parquet output,\n"
                                        "like column-bag_hash=string:26dfba2c"),
+        ("idgenerator=CALLABLE",       "callable or iterable for producing message IDs \n"
+                                       "in Parquet output, like 'uuid.uuid4' or 'itertools.count()';\n"
+                                       "nesting uses UUID values by default"),
+        ("nesting=array|all",          "create tables for nested message types\n"
+                                       "in Parquet output,\n"
+                                       'only for arrays if "array" \n'
+                                       "else for any nested types\n"
+                                       "(array fields in parent will be populated \n"
+                                       " with foreign keys instead of messages as JSON)"),
         ("overwrite=true|false",       "overwrite existing file in Parquet output\n"
                                        "instead of appending unique counter (default false)"),
-        ("type-rostype=arrowtype",     "custom mapping between ROS and pyarrow type\n"
+        ("type-ROSTYPE=ARROWTYPE",     "custom mapping between ROS and pyarrow type\n"
                                        "for Parquet output, like type-time=\"timestamp('ns')\"\n"
                                        "or type-uint8[]=\"list(uint8())\""),
-        ("writer-argname=argvalue",    "additional arguments for Parquet output\n"
+        ("writer-ARGNAME=ARGVALUE",    "additional arguments for Parquet output\n"
                                        "given to pyarrow.parquet.ParquetWriter"),
     ])
+    plugins.add_output_label("Parquet", ["--emit-field", "--no-emit-field"])
+
+
+__all__ = ["ParquetSink", "init"]
```

## grepros/plugins/sql.py

```diff
@@ -1,35 +1,37 @@
 # -*- coding: utf-8 -*-
 """
-SQL schema output for search results.
+SQL schema output plugin.
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     20.12.2021
-@modified    04.02.2022
+@modified    04.07.2023
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.plugins.sql
 import atexit
 import collections
 import datetime
 import os
-import sys
 
-from .. import rosapi
-from .. common import ConsolePrinter, format_bytes, makedirs, plural, unique_path
-from .. outputs import SinkBase
+from .. import __title__
+from .. import api
+from .. import common
+from .. import main
+from .. common import ConsolePrinter, plural
+from .. outputs import Sink
 from . auto.sqlbase import SqlMixin
 
 
 
-class SqlSink(SinkBase, SqlMixin):
+class SqlSink(Sink, SqlMixin):
     """
     Writes SQL schema file for message type tables and topic views.
 
     Output will have:
     - table "pkg/MsgType" for each topic message type, with ordinary columns for
       scalar fields, and structured columns for list fields;
       plus underscore-prefixed fields for metadata, like `_topic` as the topic name.
@@ -43,116 +45,139 @@
     ## Auto-detection file extensions
     FILE_EXTENSIONS = (".sql", )
 
     ## Default columns for message type tables, as [(column name, ROS type)]
     MESSAGE_TYPE_BASECOLS  = [("_topic",      "string"),
                               ("_timestamp",  "time"), ]
 
+    ## Constructor argument defaults
+    DEFAULT_ARGS = dict(WRITE_OPTIONS={}, VERBOSE=False)
 
-    def __init__(self, args):
+
+    def __init__(self, args=None, **kwargs):
         """
-        @param   args                 arguments object like argparse.Namespace
-        @param   args.WRITE_OPTIONS   {"dialect": SQL dialect if not default,
+        @param   args                 arguments as namespace or dictionary, case-insensitive;
+                                      or a single path as the file to write
+        @param   args.write           output file path
+        @param   args.write_options   ```
+                                      {"dialect": SQL dialect if not default,
                                        "nesting": true|false to created nested type tables,
                                        "overwrite": whether to overwrite existing file
                                                     (default false)}
+                                      ```
+        @param   args.meta            whether to emit metainfo
+        @param   args.verbose         whether to emit debug information
+        @param   kwargs               any and all arguments as keyword overrides, case-insensitive
         """
+        args = {"WRITE": str(args)} if isinstance(args, common.PATH_TYPES) else args
+        args = common.ensure_namespace(args, SqlSink.DEFAULT_ARGS, **kwargs)
         super(SqlSink, self).__init__(args)
         SqlMixin.__init__(self, args)
 
         self._filename      = None   # Unique output filename
         self._file          = None   # Open file() object
         self._batch         = None   # Current source batch
         self._nested_types  = {}     # {(typename, typehash): "CREATE TABLE .."}
         self._batch_metas   = []     # [source batch metainfo string, ]
-        self._overwrite     = (args.WRITE_OPTIONS.get("overwrite") == "true")
+        self._overwrite     = (args.WRITE_OPTIONS.get("overwrite") in (True, "true"))
         self._close_printed = False
 
         # Whether to create tables for nested message types,
         # "array" if to do this only for arrays of nested types, or
         # "all" for any nested type, including those fully flattened into parent fields.
         self._nesting = args.WRITE_OPTIONS.get("nesting")
 
         atexit.register(self.close)
 
 
     def validate(self):
         """
-        Returns whether "dialect" and "nesting" and "overwrite" parameters contain supported values.
+        Returns whether "dialect" and "nesting" and "overwrite" parameters contain supported values
+        and file is writable.
         """
+        if self.valid is not None: return self.valid
         ok, sqlconfig_ok = True, SqlMixin.validate(self)
         if self.args.WRITE_OPTIONS.get("nesting") not in (None, "array", "all"):
             ConsolePrinter.error("Invalid nesting option for SQL: %r. "
                                  "Choose one of {array,all}.",
                                  self.args.WRITE_OPTIONS["nesting"])
             ok = False
-        if self.args.WRITE_OPTIONS.get("overwrite") not in (None, "true", "false"):
+        if self.args.WRITE_OPTIONS.get("overwrite") not in (None, True, False, "true", "false"):
             ConsolePrinter.error("Invalid overwrite option for SQL: %r. "
                                  "Choose one of {true, false}.",
                                  self.args.WRITE_OPTIONS["overwrite"])
             ok = False
-        return sqlconfig_ok and ok
+        if not common.verify_io(self.args.WRITE, "w"):
+            ok = False
+        self.valid = sqlconfig_ok and ok
+        return self.valid
 
 
-    def emit(self, topic, index, stamp, msg, match):
+    def emit(self, topic, msg, stamp=None, match=None, index=None):
         """Writes out message type CREATE TABLE statements to SQL schema file."""
+        if not self.validate(): raise Exception("invalid")
         batch = self.source.get_batch()
         if not self._batch_metas or batch != self._batch:
             self._batch = batch
             self._batch_metas.append(self.source.format_meta())
         self._ensure_open()
         self._process_type(msg)
         self._process_topic(topic, msg)
 
 
     def close(self):
         """Rewrites out everything to SQL schema file, ensuring all source metas."""
-        if self._file:
-            self._file.seek(0)
-            self._write_header()
-            for key in sorted(self._types):
-                self._write_entity("table", self._types[key])
-            for key in sorted(self._topics):
-                self._write_entity("view", self._topics[key])
-            self._file.close()
-            self._file = None
-        if not self._close_printed and self._types:
-            self._close_printed = True
-            ConsolePrinter.debug("Wrote %s and %s to SQL %s (%s).",
-                                 plural("message type table",
-                                        len(self._types) - len(self._nested_types)),
-                                 plural("topic view", self._topics), self._filename,
-                                 format_bytes(os.path.getsize(self._filename)))
-            if self._nested_types:
-                ConsolePrinter.debug("Wrote %s to SQL %s.",
-                                     plural("nested message type table", self._nested_types),
-                                     self._filename)
-        self._nested_types.clear()
-        del self._batch_metas[:]
-        SqlMixin.close(self)
-        super(SqlSink, self).close()
+        try:
+            if self._file:
+                self._file.seek(0)
+                self._write_header()
+                for key in sorted(self._types):
+                    self._write_entity("table", self._types[key])
+                for key in sorted(self._topics):
+                    self._write_entity("view", self._topics[key])
+                self._file.close()
+                self._file = None
+        finally:
+            if not self._close_printed and self._types:
+                self._close_printed = True
+                try: sz = common.format_bytes(os.path.getsize(self._filename))
+                except Exception as e:
+                    ConsolePrinter.warn("Error getting size of %s: %s", self._filename, e)
+                    sz = "error getting size"
+                ConsolePrinter.debug("Wrote %s and %s to SQL %s (%s).",
+                                     plural("message type table",
+                                            len(self._types) - len(self._nested_types)),
+                                     plural("topic view", self._topics), self._filename, sz)
+                if self._nested_types:
+                    ConsolePrinter.debug("Wrote %s to SQL %s.",
+                                         plural("nested message type table", self._nested_types),
+                                         self._filename)
+            self._nested_types.clear()
+            del self._batch_metas[:]
+            SqlMixin.close(self)
+            super(SqlSink, self).close()
 
 
     def _ensure_open(self):
         """Opens output file if not already open, writes header."""
         if self._file: return
 
-        self._filename = self.args.WRITE if self._overwrite else unique_path(self.args.WRITE)
-        makedirs(os.path.dirname(self._filename))
+        self._filename = self.args.WRITE if self._overwrite else common.unique_path(self.args.WRITE)
+        common.makedirs(os.path.dirname(self._filename))
         if self.args.VERBOSE:
             sz = os.path.exists(self._filename) and os.path.getsize(self._filename)
             action = "Overwriting" if sz and self._overwrite else "Creating"
             ConsolePrinter.debug("%s %s.", action, self._filename)
         self._file = open(self._filename, "wb")
         self._write_header()
 
 
     def _process_topic(self, topic, msg):
         """Builds and writes CREATE VIEW statement for topic if not already built."""
-        topickey = rosapi.TypeMeta.make(msg, topic).topickey
+        topickey = api.TypeMeta.make(msg, topic).topickey
         if topickey in self._topics:
             return
 
         self._topics[topickey] = self._make_topic_data(topic, msg)
         self._write_entity("view", self._topics[topickey])
 
 
@@ -161,15 +186,15 @@
         Builds and writes CREATE TABLE statement for message type if not already built.
 
         Builds statements recursively for nested types if configured.
 
         @return   built SQL, or None if already built
         """
         rootmsg = rootmsg or msg
-        typekey = rosapi.TypeMeta.make(msg, root=rootmsg).typekey
+        typekey = api.TypeMeta.make(msg, root=rootmsg).typekey
         if typekey in self._types:
             return None
 
         extra_cols = [(c, self._make_column_type(t, fallback="int64" if "time" == t else None))
                       for c, t in self.MESSAGE_TYPE_BASECOLS]
         self._types[typekey] = self._make_type_data(msg, extra_cols, rootmsg)
         self._schema[typekey] = collections.OrderedDict(self._types[typekey].pop("cols"))
@@ -177,45 +202,45 @@
         self._write_entity("table", self._types[typekey])
         if self._nesting: self._process_nested(msg, rootmsg)
         return self._types[typekey]["sql"]
 
 
     def _process_nested(self, msg, rootmsg):
         """Builds anr writes CREATE TABLE statements for nested types."""
-        nesteds = rosapi.iter_message_fields(msg, messages_only=True) if self._nesting else ()
+        nesteds = api.iter_message_fields(msg, messages_only=True) if self._nesting else ()
         for path, submsgs, subtype in nesteds:
-            scalartype = rosapi.scalar(subtype)
+            scalartype = api.scalar(subtype)
             if subtype == scalartype and "all" != self._nesting: continue  # for path
 
             subtypehash = self.source.get_message_type_hash(scalartype)
             subtypekey = (scalartype, subtypehash)
             if subtypekey in self._types: continue  # for path
 
             if not isinstance(submsgs, (list, tuple)): submsgs = [submsgs]
             [submsg] = submsgs[:1] or [self.source.get_message_class(scalartype, subtypehash)()]
             subsql = self._process_type(submsg, rootmsg)
             if subsql: self._nested_types[subtypekey] = subsql
 
 
     def _write_header(self):
         """Writes header to current file."""
-        args = {
-            "dialect":  self._dialect,
-            "args":      " ".join(sys.argv[1:]),
-            "source":   "\n\n".join("-- Source:\n" +
-                                    "\n".join("-- " + x for x in s.strip().splitlines())
-                                    for s in self._batch_metas),
-            "dt":       datetime.datetime.now().strftime("%Y-%m-%d %H:%M"),
-        }
+        values = {"title":    __title__,
+                  "dialect":  self._dialect,
+                  "args":     " ".join(main.CLI_ARGS or []),
+                  "source":   "\n\n".join("-- Source:\n" +
+                                          "\n".join("-- " + x for x in s.strip().splitlines())
+                                          for s in self._batch_metas),
+                  "dt":       datetime.datetime.now().strftime("%Y-%m-%d %H:%M"), }
         self._file.write((
             "-- SQL dialect: {dialect}.\n"
-            "-- Written by grepros on {dt}.\n"
-            "-- Command: grepros {args}.\n"
-            "\n{source}\n\n"
-        ).format(**args).encode("utf-8"))
+            "-- Written by {title} on {dt}.\n"
+        ).format(**values).encode("utf-8"))
+        if values["args"]:
+            self._file.write("-- Command: grepros {args}.\n".format(**values).encode("utf-8"))
+        self._file.write("\n{source}\n\n".format(**values).encode("utf-8"))
 
 
     def _write_entity(self, category, item):
         """Writes table or view SQL statement to file."""
         self._file.write(b"\n")
         if "table" == category:
             self._file.write(("-- Message type %(type)s (%(md5)s)\n--\n" % item).encode("utf-8"))
@@ -239,7 +264,10 @@
         ("nesting=array|all",     "create tables for nested message types\n"
                                   "in SQL output,\n"
                                   'only for arrays if "array" \n'
                                   "else for any nested types"),
         ("overwrite=true|false",  "overwrite existing file in SQL output\n"
                                   "instead of appending unique counter (default false)")
     ])
+
+
+__all__ = ["SqlSink", "init"]
```

## grepros/plugins/auto/csv.py

```diff
@@ -1,118 +1,145 @@
 # -*- coding: utf-8 -*-
 """
-CSV output for search results.
+CSV output plugin.
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     03.12.2021
-@modified    16.03.2022
+@modified    02.07.2023
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.plugins.auto.csv
 from __future__ import absolute_import
 import atexit
 import csv
-import io
 import itertools
 import os
-import sys
 
-from ... common import ConsolePrinter, format_bytes, makedirs, plural, unique_path
-from ... import rosapi
-from ... outputs import SinkBase
+import six
 
+from ... import api
+from ... import common
+from ... common import ConsolePrinter, plural
+from ... outputs import Sink
 
-class CsvSink(SinkBase):
+
+class CsvSink(Sink):
     """Writes messages to CSV files, each topic to a separate file."""
 
     ## Auto-detection file extensions
     FILE_EXTENSIONS = (".csv", )
 
-    def __init__(self, args):
+    ## Constructor argument defaults
+    DEFAULT_ARGS = dict(EMIT_FIELD=(), META=False, NOEMIT_FIELD=(), WRITE_OPTIONS={}, VERBOSE=False)
+
+    def __init__(self, args=None, **kwargs):
         """
-        @param   args                 arguments object like argparse.Namespace
-        @param   args.WRITE           base name of CSV file to write,
+        @param   args                 arguments as namespace or dictionary, case-insensitive;
+                                      or a single path as the base name of CSV files to write
+        @param   args.emit_field      message fields to emit in output if not all
+        @param   args.noemit_field    message fields to skip in output
+        @param   args.write           base name of CSV files to write,
                                       will add topic name like "name.__my__topic.csv" for "/my/topic",
                                       will add counter like "name.__my__topic.2.csv" if exists
-        @param   args.WRITE_OPTIONS   {"overwrite": whether to overwrite existing files
+        @param   args.write_options   {"overwrite": whether to overwrite existing files
                                                     (default false)}
-        @param   args.VERBOSE         whether to print debug information
+        @param   args.meta            whether to emit metainfo
+        @param   args.verbose         whether to emit debug information
+        @param   kwargs               any and all arguments as keyword overrides, case-insensitive
         """
+        args = {"WRITE": str(args)} if isinstance(args, common.PATH_TYPES) else args
+        args = common.ensure_namespace(args, CsvSink.DEFAULT_ARGS, **kwargs)
         super(CsvSink, self).__init__(args)
         self._filebase      = args.WRITE  # Filename base, will be made unique
         self._files         = {}          # {(topic, typename, typehash): file()}
         self._writers       = {}          # {(topic, typename, typehash): CsvWriter}
+        self._patterns      = {}          # {key: [(() if any field else ('path', ), re.Pattern), ]}
         self._lasttopickey  = None        # Last (topic, typename, typehash) emitted
-        self._overwrite     = (args.WRITE_OPTIONS.get("overwrite") == "true")
+        self._overwrite     = (args.WRITE_OPTIONS.get("overwrite") in (True, "true"))
         self._close_printed = False
 
+        for key, vals in [("print", args.EMIT_FIELD), ("noprint", args.NOEMIT_FIELD)]:
+            self._patterns[key] = [(tuple(v.split(".")), common.wildcard_to_regex(v)) for v in vals]
         atexit.register(self.close)
 
-    def emit(self, topic, index, stamp, msg, match):
+    def emit(self, topic, msg, stamp=None, match=None, index=None):
         """Writes message to output file."""
+        if not self.validate(): raise Exception("invalid")
+        stamp, index = self._ensure_stamp_index(topic, msg, stamp, index)
         data = (v for _, v in self._iter_fields(msg))
-        metadata = [rosapi.to_sec(stamp), rosapi.to_datetime(stamp), rosapi.get_message_type(msg)]
+        metadata = [api.to_sec(stamp), api.to_datetime(stamp), api.get_message_type(msg)]
         self._make_writer(topic, msg).writerow(itertools.chain(metadata, data))
-        super(CsvSink, self).emit(topic, index, stamp, msg, match)
+        super(CsvSink, self).emit(topic, msg, stamp, match, index)
 
     def validate(self):
-        """Returns whether overwrite option is valid."""
+        """Returns whether overwrite option is valid and file base is writable."""
+        if self.valid is not None: return self.valid
         result = True
-        if self.args.WRITE_OPTIONS.get("overwrite") not in (None, "true", "false"):
+        if self.args.WRITE_OPTIONS.get("overwrite") not in (None, True, False, "true", "false"):
             ConsolePrinter.error("Invalid overwrite option for CSV: %r. "
                                  "Choose one of {true, false}.",
                                  self.args.WRITE_OPTIONS["overwrite"])
             result = False
-        return result
+        if not common.verify_io(self.args.WRITE, "w"):
+            result = False
+        self.valid = result
+        return self.valid
 
     def close(self):
         """Closes output file(s), if any."""
-        names = {k: f.name for k, f in self._files.items()}
-        for k in names:
-            self._files.pop(k).close()
-        self._writers.clear()
-        self._lasttopickey = None
-        if not self._close_printed and self._counts:
-            self._close_printed = True
-            sizes = {k: os.path.getsize(n) for k, n in names.items()}
-            ConsolePrinter.debug("Wrote %s in %s to CSV (%s):",
-                                 plural("message", sum(self._counts.values())),
-                                 plural("topic", self._counts), format_bytes(sum(sizes.values())))
-            for topickey, name in names.items():
-                ConsolePrinter.debug("- %s (%s, %s)", name,
-                                    format_bytes(sizes[topickey]),
-                                    plural("message", self._counts[topickey]))
-        super(CsvSink, self).close()
+        try:
+            names = {k: f.name for k, f in self._files.items()}
+            for k in names:
+                self._files.pop(k).close()
+            self._writers.clear()
+            self._lasttopickey = None
+        finally:
+            if not self._close_printed and self._counts:
+                self._close_printed = True
+                sizes = {k: None for k in names.values()}
+                for k, n in names.items():
+                    try: sizes[k] = os.path.getsize(n)
+                    except Exception as e: ConsolePrinter.warn("Error getting size of %s: %s", n, e)
+                ConsolePrinter.debug("Wrote %s in %s to CSV (%s):",
+                                     plural("message", sum(self._counts.values())),
+                                     plural("topic", self._counts),
+                                     common.format_bytes(sum(filter(bool, sizes.values()))))
+                for topickey, name in names.items():
+                    ConsolePrinter.debug("- %s (%s, %s)", name,
+                                         "error getting size" if sizes[topickey] is None else
+                                         common.format_bytes(sizes[topickey]),
+                                         plural("message", self._counts[topickey]))
+            super(CsvSink, self).close()
 
     def _make_writer(self, topic, msg):
         """
         Returns a csv.writer for writing topic data.
 
         File is populated with header if not created during this session.
         """
-        topickey = rosapi.TypeMeta.make(msg, topic).topickey
+        topickey = api.TypeMeta.make(msg, topic).topickey
         if not self._lasttopickey:
-            makedirs(os.path.dirname(self._filebase))
+            common.makedirs(os.path.dirname(self._filebase))
         if self._lasttopickey and topickey != self._lasttopickey:
             self._files[self._lasttopickey].close()  # Avoid hitting ulimit
         if topickey not in self._files or self._files[topickey].closed:
             name = self._files[topickey].name if topickey in self._files else None
             action = "Creating"  # Or "Overwriting"
             if not name:
                 base, ext = os.path.splitext(self._filebase)
                 name = "%s.%s%s" % (base, topic.lstrip("/").replace("/", "__"), ext)
                 if self._overwrite:
                     if os.path.isfile(name) and os.path.getsize(name): action = "Overwriting"
                     open(name, "w").close()
-                else: name = unique_path(name)
-            flags = {"mode": "ab"} if sys.version_info < (3, 0) else {"mode": "a", "newline": ""}
+                else: name = common.unique_path(name)
+            flags = {"mode": "ab"} if six.PY2 else {"mode": "a", "newline": ""}
             f = open(name, **flags)
             w = CsvWriter(f)
             if topickey not in self._files:
                 if self.args.VERBOSE:
                     ConsolePrinter.debug("%s %s.", action, name)
                 header = (topic + "/" + ".".join(map(str, p)) for p, _ in self._iter_fields(msg))
                 metaheader = ["__time", "__datetime", "__type"]
@@ -123,27 +150,29 @@
 
     def _iter_fields(self, msg, top=()):
         """
         Yields ((nested, path), scalar value) from ROS message.
 
         Lists are returned as ((nested, path, index), value), e.g. (("data", 0), 666).
         """
-        fieldmap, identity = rosapi.get_message_fields(msg), lambda x: x
+        prints, noprints = self._patterns["print"], self._patterns["noprint"]
+        fieldmap, identity = api.get_message_fields(msg), lambda x: x
+        fieldmap = api.filter_fields(fieldmap, top, include=prints, exclude=noprints)
         for k, t in fieldmap.items() if fieldmap != msg else ():
-            v, path, baset = rosapi.get_message_value(msg, k, t), top + (k, ), rosapi.scalar(t)
-            is_sublist = isinstance(v, (list, tuple)) and baset not in rosapi.ROS_BUILTIN_TYPES
-            cast = rosapi.to_sec if baset in rosapi.ROS_TIME_TYPES else identity
+            v, path, baset = api.get_message_value(msg, k, t), top + (k, ), api.scalar(t)
+            is_sublist = isinstance(v, (list, tuple)) and baset not in api.ROS_BUILTIN_TYPES
+            cast = api.to_sec if baset in api.ROS_TIME_TYPES else identity
             if isinstance(v, (list, tuple)) and not is_sublist:
                 for i, lv in enumerate(v):
                     yield path + (i, ), cast(lv)
             elif is_sublist:
                 for i, lmsg in enumerate(v):
                     for lp, lv in self._iter_fields(lmsg, path + (i, )):
                         yield lp, lv
-            elif rosapi.is_ros_message(v, ignore_time=True):
+            elif api.is_ros_message(v, ignore_time=True):
                 for mp, mv in self._iter_fields(v, path):
                     yield mp, mv
             else:
                 yield path, cast(v)
 
 
 class CsvWriter(object):
@@ -152,21 +181,21 @@
     def __init__(self, csvfile, dialect="excel", **fmtparams):
         """
         @param   csvfile    file-like object with `write()` method
         @param   dialect    CSV dialect to use, one from `csv.list_dialects()`
         @param   fmtparams  override individual format parameters in dialect
         """
         self._file    = csvfile
-        self._buffer  = io.BytesIO() if sys.version_info < (3, 0) else io.StringIO()
+        self._buffer  = six.BytesIO() if "b" in csvfile.mode else six.StringIO()
         self._writer  = csv.writer(self._buffer, dialect, **dict(fmtparams, lineterminator=""))
         self._dialect = csv.writer(self._buffer, dialect, **fmtparams).dialect
         self._format  = lambda v: int(v) if isinstance(v, bool) else v
-        if sys.version_info < (3, 0):  # Py2, CSV is written in binary mode
+        if six.PY2:  # In Py2, CSV is written in binary mode
             self._format = lambda v: int(v) if isinstance(v, bool) else \
-                                     v.encode("utf-8") if isinstance(v, unicode) else v
+                                     v.encode("utf-8") if isinstance(v, six.text_type) else v
 
     @property
     def dialect(self):
         """A read-only description of the dialect in use by the writer."""
         return self._dialect
 
     def writerow(self, row):
@@ -182,20 +211,21 @@
             """Writes columns to file, returns number of bytes written."""
             count = self._file.write(inter) if inter else 0
             self._writer.writerow(cols)                         # Hack: use csv.writer to format
             count += self._file.write(self._buffer.getvalue())  # a slice at a time, as it can get
             self._buffer.seek(0); self._buffer.truncate()       # very memory-hungry for huge rows
             return count
 
-        result, chunk, inter, STEP = 0, [], "", 10000
+        result, chunk, inter, DELIM, STEP = 0, [], "", self.dialect.delimiter, 10000
+        if "b" in self._file.mode: DELIM = six.binary_type(self.dialect.delimiter)
         for v in row:
             chunk.append(self._format(v))
             if len(chunk) >= STEP:
                 result += write_columns(chunk, inter)
-                chunk, inter = [], self.dialect.delimiter
+                chunk, inter = [], DELIM
         if chunk: result += write_columns(chunk, inter)
         result += self._file.write(self.dialect.lineterminator)
         return result
 
     def writerows(self, rows):
         """
         Writes the rows to the writers file object.
@@ -211,7 +241,11 @@
 def init(*_, **__):
     """Adds CSV output format support."""
     from ... import plugins  # Late import to avoid circular
     plugins.add_write_format("csv", CsvSink, "CSV", [
         ("overwrite=true|false",  "overwrite existing files in CSV output\n"
                                   "instead of appending unique counter (default false)")
     ])
+    plugins.add_output_label("CSV", ["--emit-field", "--no-emit-field"])
+
+
+__all__ = ["CsvSink", "CsvWriter", "init"]
```

## grepros/plugins/auto/dbbase.py

```diff
@@ -4,28 +4,28 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     11.12.2021
-@modified    07.01.2022
+@modified    28.06.2023
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.plugins.auto.dbbase
 import atexit
 import collections
 
-from ... import rosapi
-from ... common import ConsolePrinter, plural
-from ... outputs import SinkBase
+from ... import api
+from ... common import PATH_TYPES, ConsolePrinter, ensure_namespace, plural
+from ... outputs import Sink
 from . sqlbase import SqlMixin, quote
 
 
-class DataSinkBase(SinkBase, SqlMixin):
+class BaseDataSink(Sink, SqlMixin):
     """
     Base class for writing messages to a database.
 
     Output will have:
     - table "topics", with topic and message type names
     - table "types", with message type definitions
 
@@ -56,29 +56,40 @@
                               ("_timestamp",   "INTEGER"),
                               ("_id",          "INTEGER NOT NULL "
                                                "PRIMARY KEY AUTOINCREMENT"), ]
     ## Additional default columns for pkg/MsgType tables with nesting output
     MESSAGE_TYPE_NESTCOLS  = [("_parent_type", "TEXT"),
                               ("_parent_id",   "INTEGER"), ]
 
+    ## Constructor argument defaults
+    DEFAULT_ARGS = dict(META=False, WRITE_OPTIONS={}, VERBOSE=False)
 
-    def __init__(self, args):
+
+    def __init__(self, args=None, **kwargs):
         """
-        @param   args                 arguments object like argparse.Namespace
-        @param   args.WRITE           database connection string
-        @param   args.WRITE_OPTIONS   {"commit-interval": transaction size (0 is autocommit),
+        @param   args                 arguments as namespace or dictionary, case-insensitive;
+                                      or a single item as the database connection string
+        @param   args.write           database connection string
+        @param   args.write_options   ```
+                                      {"commit-interval": transaction size (0 is autocommit),
                                       "nesting": "array" to recursively insert arrays
                                                   of nested types, or "all" for any nesting)}
-        @param   args.META            whether to print metainfo
-        @param   args.VERBOSE         whether to print debug information
-        """
-        super(DataSinkBase, self).__init__(args)
+                                      ```
+        @param   args.meta            whether to emit metainfo
+        @param   args.verbose         whether to emit debug information
+        @param   kwargs               any and all arguments as keyword overrides, case-insensitive
+        """
+        args = {"WRITE": str(args)} if isinstance(args, PATH_TYPES) else args
+        args = ensure_namespace(args, BaseDataSink.DEFAULT_ARGS, **kwargs)
+        super(BaseDataSink, self).__init__(args)
         SqlMixin.__init__(self, args)
 
-        self._db            = None   # Database connection
+        ## Database connection
+        self.db = None
+
         self._cursor        = None   # Database cursor or connection
         self._dialect       = self.ENGINE.lower()  # Override SqlMixin._dialect
         self._close_printed = False
 
         # Whether to create tables and rows for nested message types,
         # "array" if to do this only for arrays of nested types, or
         # "all" for any nested type, including those fully flattened into parent fields.
@@ -90,84 +101,89 @@
         self._nested_counts = {}  # {(typename, typehash): count}
 
         atexit.register(self.close)
 
 
     def validate(self):
         """
-        Returns whether args.WRITE_OPTIONS has valid values, if any.
+        Returns whether args.write_options has valid values, if any.
 
         Checks parameters "commit-interval" and "nesting".
         """
         ok, sqlconfig_ok = True, SqlMixin._validate_dialect_file(self)
         if "commit-interval" in self.args.WRITE_OPTIONS:
             try: ok = int(self.args.WRITE_OPTIONS["commit-interval"]) >= 0
             except Exception: ok = False
             if not ok:
                 ConsolePrinter.error("Invalid commit-interval option for %s: %r.",
                                      self.ENGINE, self.args.WRITE_OPTIONS["commit-interval"])
-        if self.args.WRITE_OPTIONS.get("nesting") not in (None, "", "array", "all"):
+        if self.args.WRITE_OPTIONS.get("nesting") not in (None, False, "", "array", "all"):
             ConsolePrinter.error("Invalid nesting option for %s: %r. "
                                  "Choose one of {array,all}.",
                                  self.ENGINE, self.args.WRITE_OPTIONS["nesting"])
             ok = False
         return ok and sqlconfig_ok
 
 
-    def emit(self, topic, index, stamp, msg, match):
+    def emit(self, topic, msg, stamp=None, match=None, index=None):
         """Writes message to database."""
-        if not self._db:
+        if not self.validate(): raise Exception("invalid")
+        if not self.db:
             self._init_db()
+        stamp, index = self._ensure_stamp_index(topic, msg, stamp, index)
         self._process_type(msg)
         self._process_topic(topic, msg)
         self._process_message(topic, msg, stamp)
-        super(DataSinkBase, self).emit(topic, index, stamp, msg, match)
+        super(BaseDataSink, self).emit(topic, msg, stamp, match, index)
 
 
     def close(self):
         """Closes database connection, if any."""
-        if self._db:
-            for sql in list(self._sql_queue):
-                self._executemany(sql, self._sql_queue.pop(sql))
-            self._db.commit()
-            self._cursor.close()
-            self._cursor = None
-            self._db.close()
-            self._db = None
-        if not self._close_printed and self._counts:
-            self._close_printed = True
-            target = self._make_db_label()
-            ConsolePrinter.debug("Wrote %s in %s to %s database %s.",
-                                 plural("message", sum(self._counts.values())),
-                                 plural("topic", self._counts), self.ENGINE, target)
-            if self._nested_counts:
+        try:
+            if self.db:
+                for sql in list(self._sql_queue):
+                    self._executemany(sql, self._sql_queue.pop(sql))
+                self.db.commit()
+                self._cursor.close()
+                self._cursor = None
+                self.db.close()
+                self.db = None
+        finally:
+            if not self._close_printed and self._counts:
+                self._close_printed = True
+                target = self._make_db_label()
                 ConsolePrinter.debug("Wrote %s in %s to %s database %s.",
-                                     plural("nested message", sum(self._nested_counts.values())),
-                                     plural("nested message type", self._nested_counts),
-                                     self.ENGINE, target)
-        self._checkeds.clear()
-        self._nested_counts.clear()
-        SqlMixin.close(self)
-        super(DataSinkBase, self).close()
+                                     plural("message", sum(self._counts.values())),
+                                     plural("topic", self._counts), self.ENGINE, target)
+                if self._nested_counts:
+                    ConsolePrinter.debug("Wrote %s in %s to %s database %s.",
+                        plural("nested message", sum(self._nested_counts.values())),
+                        plural("nested message type", self._nested_counts),
+                        self.ENGINE, target
+                    )
+            self._checkeds.clear()
+            self._nested_counts.clear()
+            SqlMixin.close(self)
+            super(BaseDataSink, self).close()
 
 
     def _init_db(self):
         """Opens database connection, and populates schema if not already existing."""
-        baseattrs = dir(SinkBase(None))
+        baseattrs = dir(Sink())
         for attr in (getattr(self, k, None) for k in dir(self)
                      if not k.isupper() and k not in baseattrs):
             isinstance(attr, dict) and attr.clear()
         self._close_printed = False
 
         if "commit-interval" in self.args.WRITE_OPTIONS:
             self.COMMIT_INTERVAL = int(self.args.WRITE_OPTIONS["commit-interval"])
-        self._db = self._connect()
+        self.db = self._connect()
         self._cursor = self._make_cursor()
         self._executescript(self._get_dialect_option("base_schema"))
-        self._db.commit()
+        self.db.commit()
         self._load_schema()
         TYPECOLS = self.MESSAGE_TYPE_TOPICCOLS + self.MESSAGE_TYPE_BASECOLS
         if self._nesting: TYPECOLS += self.MESSAGE_TYPE_NESTCOLS
         self._ensure_columns(TYPECOLS)
 
 
     def _load_schema(self):
@@ -186,15 +202,15 @@
     def _process_topic(self, topic, msg):
         """
         Inserts topics-row and creates view `/topic/name` if not already existing.
 
         Also creates types-row and pkg/MsgType table for this message if not existing.
         If nesting enabled, creates types recursively.
         """
-        topickey = rosapi.TypeMeta.make(msg, topic).topickey
+        topickey = api.TypeMeta.make(msg, topic).topickey
         if topickey in self._checkeds:
             return
 
         if topickey not in self._topics:
             exclude_cols = list(self.MESSAGE_TYPE_TOPICCOLS)
             if self._nesting: exclude_cols += self.MESSAGE_TYPE_NESTCOLS
             tdata = self._make_topic_data(topic, msg, exclude_cols)
@@ -202,29 +218,29 @@
             self._executescript(tdata["sql"])
 
             sql, args = self._make_topic_insert_sql(topic, msg)
             if self.args.VERBOSE:
                 ConsolePrinter.debug("Adding topic %s in %s output.", topic, self.ENGINE)
             self._topics[topickey]["id"] = self._execute_insert(sql, args)
 
-            if self.COMMIT_INTERVAL: self._db.commit()
+            if self.COMMIT_INTERVAL: self.db.commit()
         self._checkeds[topickey] = True
 
 
     def _process_type(self, msg, rootmsg=None):
         """
         Creates types-row and pkg/MsgType table if not already existing.
 
         @return   created types-row, or None if already existed
         """
         rootmsg = rootmsg or msg
-        with rosapi.TypeMeta.make(msg, root=rootmsg) as m:
+        with api.TypeMeta.make(msg, root=rootmsg) as m:
             typename, typekey = (m.typename, m.typekey)
         if typekey in self._checkeds:
-            return
+            return None
 
         if typekey not in self._types:
             if self.args.VERBOSE:
                 ConsolePrinter.debug("Adding type %s in %s output.", typename, self.ENGINE)
             extra_cols = self.MESSAGE_TYPE_TOPICCOLS + self.MESSAGE_TYPE_BASECOLS
             if self._nesting: extra_cols += self.MESSAGE_TYPE_NESTCOLS
             tdata = self._make_type_data(msg, extra_cols)
@@ -233,17 +249,17 @@
 
             self._executescript(tdata["sql"])
             sql, args = self._make_type_insert_sql(msg)
             tdata["id"] = self._execute_insert(sql, args)
 
 
         nested_tables = self._types[typekey].get("nested_tables") or {}
-        nesteds = rosapi.iter_message_fields(msg, messages_only=True) if self._nesting else ()
+        nesteds = api.iter_message_fields(msg, messages_only=True) if self._nesting else ()
         for path, submsgs, subtype in nesteds:
-            scalartype = rosapi.scalar(subtype)
+            scalartype = api.scalar(subtype)
             if subtype == scalartype and "all" != self._nesting:
                 continue  # for path
 
             subtypehash = not submsgs and self.source.get_message_type_hash(scalartype)
             if not isinstance(submsgs, (list, tuple)): submsgs = [submsgs]
             [submsg] = submsgs[:1] or [self.source.get_message_class(scalartype, subtypehash)()]
             subdata = self._process_type(submsg, rootmsg)
@@ -265,47 +281,47 @@
         Commits transaction if interval due.
         """
         self._populate_type(topic, msg, stamp)
         if self.COMMIT_INTERVAL:
             do_commit = sum(len(v) for v in self._sql_queue.values()) >= self.COMMIT_INTERVAL
             for sql in list(self._sql_queue) if do_commit else ():
                 self._executemany(sql, self._sql_queue.pop(sql))
-            do_commit and self._db.commit()
+            do_commit and self.db.commit()
 
 
     def _populate_type(self, topic, msg, stamp,
                        rootmsg=None, parent_type=None, parent_id=None):
         """
         Inserts pkg/MsgType row for message.
 
         If nesting is enabled, inserts sub-rows for subtypes in message,
         and returns inserted ID.
         """
         rootmsg = rootmsg or msg
-        with rosapi.TypeMeta.make(msg, root=rootmsg) as m:
+        with api.TypeMeta.make(msg, root=rootmsg) as m:
             typename, typekey = m.typename, m.typekey
-        with rosapi.TypeMeta.make(rootmsg) as m:
+        with api.TypeMeta.make(rootmsg) as m:
             topic_id = self._topics[m.topickey]["id"]
         table_name = self._types[typekey]["table_name"]
 
         myid = self._get_next_id(table_name) if self._nesting else None
         coldefs = self.MESSAGE_TYPE_TOPICCOLS + self.MESSAGE_TYPE_BASECOLS[:-1]
-        colvals = [topic, topic_id, rosapi.to_datetime(stamp), rosapi.to_nsec(stamp)]
+        colvals = [topic, topic_id, api.to_datetime(stamp), api.to_nsec(stamp)]
         if self._nesting:
             coldefs += self.MESSAGE_TYPE_BASECOLS[-1:] + self.MESSAGE_TYPE_NESTCOLS
             colvals += [myid, parent_type, parent_id]
         extra_cols = list(zip([c for c, _ in coldefs], colvals))
         sql, args = self._make_message_insert_sql(topic, msg, extra_cols)
         self._ensure_execute(sql, args)
         if parent_type: self._nested_counts[typekey] = self._nested_counts.get(typekey, 0) + 1
 
         subids = {}  # {message field path: [ids]}
-        nesteds = rosapi.iter_message_fields(msg, messages_only=True) if self._nesting else ()
+        nesteds = api.iter_message_fields(msg, messages_only=True) if self._nesting else ()
         for subpath, submsgs, subtype in nesteds:
-            scalartype = rosapi.scalar(subtype)
+            scalartype = api.scalar(subtype)
             if subtype == scalartype and "all" != self._nesting:
                 continue  # for subpath
             if isinstance(submsgs, (list, tuple)):
                 subids[subpath] = []
             for submsg in submsgs if isinstance(submsgs, (list, tuple)) else [submsgs]:
                 subid = self._populate_type(topic, submsg, stamp, rootmsg, typename, myid)
                 if isinstance(submsgs, (list, tuple)):
@@ -324,15 +340,15 @@
             table_name = self._types[typekey]["table_name"]
             for c, t in ((c, t) for c, t in cols if c not in typecols):
                 sql = "ALTER TABLE %s ADD COLUMN %s %s;" % (quote(table_name), c, t)
                 sqls.append(sql)
                 typecols[c] = t
         if sqls:
             self._executescript("\n".join(sqls))
-            self._db.commit()
+            self.db.commit()
 
 
     def _ensure_execute(self, sql, args):
         """Executes SQL if in autocommit mode, else caches arguments for batch execution."""
         args = tuple(args) if isinstance(args, list) else args
         if self.COMMIT_INTERVAL:
             self._sql_queue.setdefault(sql, []).append(args)
@@ -363,13 +379,16 @@
     def _get_next_id(self, table):
         """Returns next ID value for table."""
         raise NotImplementedError()
 
 
     def _make_cursor(self):
         """Returns new database cursor."""
-        return self._db.cursor()
+        return self.db.cursor()
 
 
     def _make_db_label(self):
         """Returns formatted label for database."""
         return self.args.WRITE
+
+
+__all__ = ["BaseDataSink"]
```

## grepros/plugins/auto/html.py

```diff
@@ -1,176 +1,214 @@
 # -*- coding: utf-8 -*-
 """
-HTML output for search results.
+HTML output plugin.
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     03.12.2021
-@modified    15.03.2022
+@modified    04.07.2023
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.plugins.auto.html
 import atexit
-import copy
 import os
 try: import queue  # Py3
 except ImportError: import Queue as queue  # Py2
 import re
-import sys
 import threading
 
-from ... common import ConsolePrinter, MatchMarkers, format_bytes, makedirs, plural, unique_path
-from ... import rosapi
-from ... outputs import SinkBase, TextSinkMixin
+from ... import api
+from ... import common
+from ... import main
+from ... common import ConsolePrinter, MatchMarkers, plural
+from ... outputs import Sink, TextSinkMixin
 from ... vendor import step
 
 
-class HtmlSink(SinkBase, TextSinkMixin):
+class HtmlSink(Sink, TextSinkMixin):
     """Writes messages to an HTML file."""
 
     ## Auto-detection file extensions
     FILE_EXTENSIONS = (".htm", ".html")
 
     ## HTML template path
     TEMPLATE_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), "html.tpl")
 
     ## Character wrap width for message YAML
     WRAP_WIDTH = 120
 
-    def __init__(self, args):
+    ## Constructor argument defaults
+    DEFAULT_ARGS = dict(META=False, WRITE_OPTIONS={}, HIGHLIGHT=True, MATCH_WRAPPER=None,
+                        ORDERBY=None, VERBOSE=False, COLOR=True, EMIT_FIELD=(), NOEMIT_FIELD=(), 
+                        MAX_FIELD_LINES=None, START_LINE=None, END_LINE=None,
+                        MAX_MESSAGE_LINES=None, LINES_AROUND_MATCH=None, MATCHED_FIELDS_ONLY=False,
+                        WRAP_WIDTH=None)
+
+    def __init__(self, args=None, **kwargs):
         """
-        @param   args                  arguments object like argparse.Namespace
-        @param   args.META             whether to print metainfo
-        @param   args.WRITE            name of HTML file to write,
-                                       will add counter like .2 to filename if exists
-        @param   args.WRITE_OPTIONS    {"template": path to custom HTML template, if any,
-                                        "overwrite": whether to overwrite existing file
-                                                     (default false)}
-        @param   args.VERBOSE          whether to print debug information
-        @param   args.MATCH_WRAPPER    string to wrap around matched values,
-                                       both sides if one value, start and end if more than one,
-                                       or no wrapping if zero values
-        @param   args.ORDERBY          "topic" or "type" if any to group results by
+        @param   args                       arguments as namespace or dictionary, case-insensitive;
+                                            or a single path as the name of HTML file to write
+        @param   args.write                 name of HTML file to write,
+                                            will add counter like .2 to filename if exists
+        @param   args.write_options         ```
+                                            {"template": path to custom HTML template, if any,
+                                             "overwrite": whether to overwrite existing file
+                                                          (default false)}
+                                            ```
+        @param   args.highlight             highlight matched values (default true)
+        @param   args.orderby               "topic" or "type" if any to group results by
+        @param   args.color                 False or "never" for not using colors in replacements
+        @param   args.emit_field            message fields to emit if not all
+        @param   args.noemit_field          message fields to skip in output
+        @param   args.max_field_lines       maximum number of lines to output per field
+        @param   args.start_line            message line number to start output from
+        @param   args.end_line              message line number to stop output at
+        @param   args.max_message_lines     maximum number of lines to output per message
+        @param   args.lines_around_match    number of message lines around matched fields to output
+        @param   args.matched_fields_only   output only the fields where match was found
+        @param   args.wrap_width            character width to wrap message YAML output at
+        @param   args.match_wrapper         string to wrap around matched values,
+                                            both sides if one value, start and end if more than one,
+                                            or no wrapping if zero values
+        @param   args.meta                  whether to emit metainfo
+        @param   args.verbose               whether to emit debug information
+        @param   kwargs                     any and all arguments as keyword overrides,
+                                            case-insensitive
         """
-        args = copy.deepcopy(args)
+        args = {"WRITE": str(args)} if isinstance(args, common.PATH_TYPES) else args
+        args = common.ensure_namespace(args, HtmlSink.DEFAULT_ARGS, **kwargs)
         args.WRAP_WIDTH = self.WRAP_WIDTH
-        args.COLOR = "always"
+        args.COLOR = bool(args.HIGHLIGHT)
 
         super(HtmlSink, self).__init__(args)
         TextSinkMixin.__init__(self, args)
         self._queue     = queue.Queue()
         self._writer    = None        # threading.Thread running _stream()
         self._filename  = args.WRITE  # Filename base, will be made unique
-        self._overwrite = (args.WRITE_OPTIONS.get("overwrite") == "true")
+        self._overwrite = (args.WRITE_OPTIONS.get("overwrite") in (True, "true"))
         self._template_path = args.WRITE_OPTIONS.get("template") or self.TEMPLATE_PATH
         self._close_printed = False
 
         WRAPS = ((args.MATCH_WRAPPER or [""]) * 2)[:2]
-        self._tag_repls = {MatchMarkers.START:            '<span class="match">' +
-                                                          step.escape_html(WRAPS[0]),
-                           MatchMarkers.END:              step.escape_html(WRAPS[1]) + '</span>',
+        START = ('<span class="match">' + step.escape_html(WRAPS[0])) if args.HIGHLIGHT else ""
+        END = (step.escape_html(WRAPS[1]) + '</span>') if args.HIGHLIGHT else ""
+        self._tag_repls = {MatchMarkers.START: START,
+                           MatchMarkers.END:   END,
                            ConsolePrinter.STYLE_LOWLIGHT: '<span class="lowlight">',
                            ConsolePrinter.STYLE_RESET:    '</span>'}
         self._tag_rgx = re.compile("(%s)" % "|".join(map(re.escape, self._tag_repls)))
 
         self._format_repls.clear()
         atexit.register(self.close)
 
-    def emit(self, topic, index, stamp, msg, match):
+    def emit(self, topic, msg, stamp=None, match=None, index=None):
         """Writes message to output file."""
-        self._queue.put((topic, index, stamp, msg, match))
+        if not self.validate(): raise Exception("invalid")
+        stamp, index = self._ensure_stamp_index(topic, msg, stamp, index)
+        self._queue.put((topic, msg, stamp, match, index))
         if not self._writer:
             self._writer = threading.Thread(target=self._stream)
             self._writer.start()
         if self._queue.qsize() > 100: self._queue.join()
 
     def validate(self):
         """
-        Returns whether write options are valid and ROS environment is set, prints error if not.
+        Returns whether write options are valid and ROS environment is set and file is writable,
+        emits error if not.
         """
+        if self.valid is not None: return self.valid
         result = True
         if self.args.WRITE_OPTIONS.get("template") and not os.path.isfile(self._template_path):
             result = False
             ConsolePrinter.error("Template does not exist: %s.", self._template_path)
-        if self.args.WRITE_OPTIONS.get("overwrite") not in (None, "true", "false"):
+        if self.args.WRITE_OPTIONS.get("overwrite") not in (None, True, False, "true", "false"):
             ConsolePrinter.error("Invalid overwrite option for HTML: %r. "
                                  "Choose one of {true, false}.",
                                  self.args.WRITE_OPTIONS["overwrite"])
             result = False
-        return rosapi.validate() and result
+        if not common.verify_io(self.args.WRITE, "w"):
+            result = False
+        self.valid = api.validate() and result
+        return self.valid
 
     def close(self):
         """Closes output file, if any."""
-        if self._writer:
-            writer, self._writer = self._writer, None
-            self._queue.put(None)
-            writer.is_alive() and writer.join()
-        if not self._close_printed and self._counts:
-            self._close_printed = True
-            ConsolePrinter.debug("Wrote %s in %s to %s (%s).",
-                                 plural("message", sum(self._counts.values())),
-                                 plural("topic", self._counts), self._filename,
-                                 format_bytes(os.path.getsize(self._filename)))
-        super(HtmlSink, self).close()
+        try:
+            if self._writer:
+                writer, self._writer = self._writer, None
+                self._queue.put(None)
+                writer.is_alive() and writer.join()
+        finally:
+            if not self._close_printed and self._counts:
+                self._close_printed = True
+                try: sz = common.format_bytes(os.path.getsize(self._filename))
+                except Exception as e:
+                    ConsolePrinter.warn("Error getting size of %s: %s", self._filename, e)
+                    sz = "error getting size"
+                ConsolePrinter.debug("Wrote %s in %s to %s (%s).",
+                                     plural("message", sum(self._counts.values())),
+                                     plural("topic", self._counts), self._filename, sz)
+            super(HtmlSink, self).close()
 
     def flush(self):
         """Writes out any pending data to disk."""
         self._queue.join()
 
     def format_message(self, msg, highlight=False):
-        """Returns message as formatted string, optionally highlighted for matches."""
-        text = TextSinkMixin.format_message(self, msg, highlight)
+        """Returns message as formatted string, optionally highlighted for matches if configured."""
+        text = TextSinkMixin.format_message(self, msg, self.args.HIGHLIGHT and highlight)
         text = "".join(self._tag_repls.get(x) or step.escape_html(x)
                        for x in self._tag_rgx.split(text))
         return text
 
     def is_highlighting(self):
-        """Returns True (requires highlighted matches)."""
-        return True
+        """Returns True if sink is configured to highlight matched values."""
+        return bool(self.args.HIGHLIGHT)
 
     def _stream(self):
         """Writer-loop, streams HTML template to file."""
         if not self._writer:
             return
 
         try:
             with open(self._template_path, "r") as f: tpl = f.read()
             template = step.Template(tpl, escape=True, strip=False)
-            ns = dict(source=self.source, sink=self, args=["grepros"] + sys.argv[1:],
-                      timeline=not self.args.ORDERBY, messages=self._produce())
-            makedirs(os.path.dirname(self.args.WRITE))
+            ns = dict(source=self.source, sink=self, messages=self._produce(),
+                      args=None, timeline=not self.args.ORDERBY)
+            if main.CLI_ARGS: ns.update(args=main.CLI_ARGS)
+            common.makedirs(os.path.dirname(self.args.WRITE))
             if not self._overwrite:
-                self._filename = unique_path(self.args.WRITE, empty_ok=True)
+                self._filename = common.unique_path(self.args.WRITE, empty_ok=True)
             if self.args.VERBOSE:
                 sz = os.path.isfile(self._filename) and os.path.getsize(self._filename)
                 action = "Overwriting" if sz and self._overwrite else "Creating"
                 ConsolePrinter.debug("%s %s.", action, self._filename)
             with open(self._filename, "wb") as f:
                 template.stream(f, ns, unbuffered=True)
         except Exception as e:
             self.thread_excepthook("Error writing HTML output %r: %r" % (self._filename, e), e)
         finally:
             self._writer = None
 
     def _produce(self):
-        """Yields messages from emit queue, as (topic, index, stamp, msg, match)."""
+        """Yields messages from emit queue, as (topic, msg, stamp, match, index)."""
         while True:
             entry = self._queue.get()
             if entry is None:
                 self._queue.task_done()
                 break  # while
-            (topic, index, stamp, msg, match) = entry
-            topickey = rosapi.TypeMeta.make(msg, topic).topickey
+            (topic, msg, stamp, match, index) = entry
+            topickey = api.TypeMeta.make(msg, topic).topickey
             if self.args.VERBOSE and topickey not in self._counts:
                 ConsolePrinter.debug("Adding topic %s in HTML output.", topic)
             yield entry
-            super(HtmlSink, self).emit(topic, index, stamp, msg, match)
+            super(HtmlSink, self).emit(topic, msg, stamp, match, index)
             self._queue.task_done()
         try:
             while self._queue.get_nowait() or True: self._queue.task_done()
         except queue.Empty: pass
 
 
 
@@ -178,7 +216,13 @@
     """Adds HTML output format support."""
     from ... import plugins  # Late import to avoid circular
     plugins.add_write_format("html", HtmlSink, "HTML", [
         ("template=/my/path.tpl",  "custom template to use for HTML output"),
         ("overwrite=true|false",   "overwrite existing file in HTML output\n"
                                    "instead of appending unique counter (default false)")
     ])
+    plugins.add_output_label("HTML", ["--emit-field", "--no-emit-field", "--matched-fields-only",
+                                      "--lines-around-match", "--lines-per-field", "--start-line",
+                                      "--end-line", "--lines-per-message", "--match-wrapper"])
+
+
+__all__ = ["HtmlSink", "init"]
```

## grepros/plugins/auto/html.tpl

```diff
@@ -1,38 +1,38 @@
 <%
 """
 HTML export template.
 
-@param   source     inputs.SourceBase instance
+@param   source     inputs.Source instance
 @param   sink       inputs.HtmlSink instance
-@param   args       list of command-line arguments
+@param   args       list of command-line arguments, if any
 @param   timeline   whether to create timeline
-@param   messages   iterable yielding (topic, index, stamp, msg, match)
+@param   messages   iterable yielding (topic, msg, stamp, match, index)
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     06.11.2021
-@modified    18.12.2021
+@modified    03.07.2023
 ------------------------------------------------------------------------------
 """
 import datetime, os, re
-from grepros import __version__, rosapi
+from grepros import __title__, __version__, api
 
 dt =  datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
 sourcemeta = source.get_meta()
 subtitle = os.path.basename(sourcemeta["file"]) if "file" in sourcemeta else "live"
 %>
 <!DOCTYPE HTML><html lang="">
 <head>
   <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
-  <meta name="generator" content="grepros {{ __version__ }}" />
-  <title>grepros {{ subtitle }} {{ dt }}</title>
+  <meta name="generator" content="{{ __title__ }} {{ __version__ }}" />
+  <title>{{ __title__ }} {{ subtitle }} {{ dt }}</title>
   <style>
     body {
       background:             #300A24;
       color:                  white;
       font-family:            monospace;
       position:               relative;
     }
@@ -388,19 +388,19 @@
       Object.keys(opts || {}).forEach(function(x) { result.setAttribute(x, opts[x]); });
       return result;
     };
 
 
     /**
      * Returns formatted timestamp for [seconds, nanoseconds],
-     * format defaulting to "%Y-%m-%d %H:%M:%s.%f",
+     * format defaulting to "%Y-%m-%d %H:%M:%S.%f",
      * with trailing zeros stripped from ending fractional seconds.
      */
     var formatStamp = function(secs_nsecs, format) {
-      format = format || "%Y-%m-%d %H:%M:%s.%f";
+      format = format || "%Y-%m-%d %H:%M:%S.%f";
       var result = makeDate(secs_nsecs).strftime(format);
       return RegExp("((%f)|(%n))$", "i").test(format) ? result.replace(/\\.?0+$/, "") : result;
     };
 
 
     /** Returns full height of an element, including paddings, margins, and border. */
     var getOuterHeight = function(elem) {
@@ -971,16 +971,20 @@
   </script>
 </head>
 
 
 <body>
 
 <div id="header">
+%if source.format_meta().strip() or isdef("args") and args:
   <div id="meta">{{ source.format_meta().strip() }}
-Command: {{ " ".join(args) }}
+    %if isdef("args") and args:
+Command: {{ __title__ }} {{ " ".join(args) }}
+    %endif
+%endif
   </div>
   <div id="topics">
     <span title="Toggle contents" onclick="return toggleClass('toc', 'collapsed', document.getElementById('toggle_topics'))">
       Contents: <span id="toggle_topics" class="toggle collapsed"></span>
     </span>
     <table id="toc" class="collapsed">
       <thead>
@@ -1018,15 +1022,15 @@
   <span title="Toggle timeline" onclick="return toggleClass('timeline', 'collapsed', document.getElementById('toggle_timeline'))">
     Timeline: <span id="toggle_timeline" class="toggle"></span>
   </span>
   <ul></ul>
 </div>
 
 
-<div id="footer">Written by grepros on {{ dt }}.</div>
+<div id="footer">Written by {{ __title__ }} on {{ dt }}.</div>
 
 
 <div id="content">
   <table id="messages">
     <thead><tr>
       <th>#</th>
       <th>Topic</th>
@@ -1036,18 +1040,18 @@
       <th><span class="toggle all" title="Toggle all messages" onclick="return Messages.toggleAllMessages()"></span></th>
     </tr></thead>
     <tbody>
 <%
 topic_idx = {}  # {(topic, typename, typehash), ]
 selector = lambda v: re.sub(r"([^\w\-])", r"\\\1", v)
 %>
-%for i, (topic, index, stamp, msg, match) in enumerate(messages, 1):
+%for i, (topic, msg, stamp, match, index) in enumerate(messages, 1):
     <%
-secs, nsecs = divmod(rosapi.to_nsec(stamp), 10**9)
-meta = source.get_message_meta(topic, index, stamp, msg)
+secs, nsecs = divmod(api.to_nsec(stamp), 10**9)
+meta = source.get_message_meta(topic, msg, stamp, index)
 topickey = (topic, meta["type"], meta["hash"])
     %>
     <tr class="meta {{ selector(topic) }} {{ selector(meta["type"]) }} {{ selector(meta["hash"]) }}" id="{{ i }}" onclick="return Messages.onClickHeader({{ i }}, event)">
       <td>
         {{ "{0:,d}".format(index) }}{{ ("/{0:,d}".format(meta["total"])) if "total" in meta else "" }}
         <span class="index">{{ i }}</span>
       </td>
```

## grepros/plugins/auto/postgres.py

```diff
@@ -1,38 +1,37 @@
 # -*- coding: utf-8 -*-
 """
-Sink plugin for dumping messages to a Postgres database.
+Postgres output plugin.
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     02.12.2021
-@modified    04.02.2022
+@modified    28.06.2023
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.plugins.auto.postgres
 import collections
 import json
 
 try:
     import psycopg2
     import psycopg2.extensions
     import psycopg2.extras
-    import psycopg2.pool
 except ImportError:
     psycopg2 = None
 
-from ... import rosapi
+from ... import api
 from ... common import ConsolePrinter
-from . dbbase import DataSinkBase, quote
+from . dbbase import BaseDataSink, quote
 
 
-class PostgresSink(DataSinkBase):
+class PostgresSink(BaseDataSink):
     """
     Writes messages to a Postgres database.
 
     Output will have:
     - table "topics", with topic and message type names
     - table "types", with message type definitions
 
@@ -73,37 +72,49 @@
                               ("_timestamp",   "NUMERIC"),
                               ("_id",          "BIGSERIAL PRIMARY KEY"), ]
     ## Additional default columns for pkg/MsgType tables with nesting output
     MESSAGE_TYPE_NESTCOLS  = [("_parent_type", "TEXT"),
                               ("_parent_id",   "BIGINT"), ]
 
 
-    def __init__(self, args):
+    def __init__(self, args=None, **kwargs):
         """
-        @param   args                 arguments object like argparse.Namespace
-        @param   args.WRITE           Postgres connection string postgresql://user@host/db
-        @param   args.WRITE_OPTIONS   {"commit-interval": transaction size (0 is autocommit),
+        @param   args                 arguments as namespace or dictionary, case-insensitive;
+                                      or a single item as the database connection string
+        @param   args.write           Postgres connection string like "postgresql://user@host/db"
+        @param   args.write_options   ```
+                                      {"commit-interval": transaction size (0 is autocommit),
                                        "nesting": "array" to recursively insert arrays
                                                   of nested types, or "all" for any nesting)}
-        @param   args.META            whether to print metainfo
-        @param   args.VERBOSE         whether to print debug information
+                                      ```
+        @param   args.meta            whether to emit metainfo
+        @param   args.verbose         whether to emit debug information
+        @param   kwargs               any and all arguments as keyword overrides, case-insensitive
         """
-        super(PostgresSink, self).__init__(args)
+        super(PostgresSink, self).__init__(args, **kwargs)
         self._id_queue = collections.defaultdict(collections.deque)  # {table name: [next ID, ]}
 
 
     def validate(self):
         """
         Returns whether Postgres driver is available,
-        and "commit-interval" and "nesting" in args.WRITE_OPTIONS have valid value, if any.
+        and "commit-interval" and "nesting" in args.write_options have valid value, if any,
+        and database is connectable.
         """
-        driver_ok, config_ok = bool(psycopg2), super(PostgresSink, self).validate()
+        if self.valid is not None: return self.valid
+        db_ok, driver_ok, config_ok = False, bool(psycopg2), super(PostgresSink, self).validate()
         if not driver_ok:
             ConsolePrinter.error("psycopg2 not available: cannot write to Postgres.")
-        return driver_ok and config_ok
+        else:
+            try:
+                with self._connect(): db_ok = True
+            except Exception as e:
+                ConsolePrinter.error("Error connecting Postgres: %s", e)
+        self.valid = db_ok and driver_ok and config_ok
+        return self.valid
 
 
     def _init_db(self):
         """Opens the database file, and populates schema if not already existing."""
         psycopg2.extensions.register_type(psycopg2.extensions.UNICODE)
         psycopg2.extensions.register_type(psycopg2.extensions.UNICODEARRAY)
         psycopg2.extras.register_default_jsonb(globally=True, loads=json.loads)
@@ -157,52 +168,53 @@
                 self._id_queue[table].append(self._cursor.fetchone()["id"])
         return self._id_queue[table].popleft()
 
 
     def _make_column_value(self, value, typename=None):
         """Returns column value suitable for inserting to database."""
         TYPES = self._get_dialect_option("types")
-        plaintype = typename and rosapi.scalar(typename)  # "string<=10" -> "string"
+        plaintype = api.scalar(typename)  # "string<=10" -> "string"
         v = value
         # Common in JSON but disallowed in Postgres
         replace = {float("inf"): None, float("-inf"): None, float("nan"): None}
         if not typename:
             v = psycopg2.extras.Json(v, json.dumps)
         elif isinstance(v, (list, tuple)):
-            scalartype = rosapi.scalar(typename)
-            if scalartype in rosapi.ROS_TIME_TYPES:
+            scalartype = api.scalar(typename)
+            if scalartype in api.ROS_TIME_TYPES:
                 v = [self._convert_time_value(x, scalartype) for x in v]
-            elif scalartype not in rosapi.ROS_BUILTIN_TYPES:
+            elif scalartype not in api.ROS_BUILTIN_TYPES:
                 if self._nesting: v = None
-                else: v = psycopg2.extras.Json([rosapi.message_to_dict(m, replace)
+                else: v = psycopg2.extras.Json([api.message_to_dict(m, replace)
                                                 for m in v], json.dumps)
-            elif "BYTEA" == TYPES.get(typename):
+            elif "BYTEA" in (TYPES.get(typename),
+                             TYPES.get(api.canonical(typename, unbounded=True))):
                 v = psycopg2.Binary(bytes(bytearray(v)))  # Py2/Py3 compatible
             else:
                 v = list(self._convert_column_value(v, typename))  # Ensure not-tuple for psycopg2
-        elif rosapi.is_ros_time(v):
+        elif api.is_ros_time(v):
             v = self._convert_time_value(v, typename)
-        elif plaintype not in rosapi.ROS_BUILTIN_TYPES:
-            v = psycopg2.extras.Json(rosapi.message_to_dict(v, replace), json.dumps)
+        elif plaintype not in api.ROS_BUILTIN_TYPES:
+            v = psycopg2.extras.Json(api.message_to_dict(v, replace), json.dumps)
         else:
             v = self._convert_column_value(v, plaintype)
         return v
 
 
     def _make_db_label(self):
         """Returns formatted label for database."""
         target = self.args.WRITE
-        if not target.startswith("postgresql://"): target = repr(target)
+        if not target.startswith(("postgres://", "postgresql://")): target = repr(target)
         return target
 
 
     @classmethod
     def autodetect(cls, target):
         """Returns true if target is recognizable as a Postgres connection string."""
-        return (target or "").startswith("postgresql://")
+        return (target or "").startswith(("postgres://", "postgresql://"))
 
 
 
 def init(*_, **__):
     """Adds Postgres output format support."""
     from ... import plugins  # Late import to avoid circular
     plugins.add_write_format("postgres", PostgresSink, "Postgres", [
@@ -215,7 +227,10 @@
         ("nesting=array|all",    "create tables for nested message types\n"
                                  "in Postgres output,\n"
                                  'only for arrays if "array" \n'
                                  "else for any nested types\n"
                                  "(array fields in parent will be populated \n"
                                  " with foreign keys instead of messages as JSON)"),
     ])
+
+
+__all__ = ["PostgresSink", "init"]
```

## grepros/plugins/auto/sqlbase.py

```diff
@@ -4,48 +4,53 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     03.01.2022
-@modified    06.02.2022
+@modified    28.06.2023
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.plugins.auto.sqlbase
-import copy
 import json
 import re
 
 import yaml
 
-from ... import rosapi
-from ... common import ConsolePrinter, ellipsize, import_item, merge_dicts
+from ... import api
+from ... common import ConsolePrinter, ellipsize, ensure_namespace, import_item, merge_dicts
 
 
 
 class SqlMixin(object):
     """
     Base class for producing SQL for topics and messages.
 
     Can load additional SQL dialects or additional options for existing dialects
     from a YAML/JSON file.
     """
 
     ## Default SQL dialect used if dialect not specified
     DEFAULT_DIALECT = "sqlite"
 
+    ## Constructor argument defaults
+    DEFAULT_ARGS = dict(META=False, WRITE_OPTIONS={}, MATCH_WRAPPER=None, VERBOSE=False)
 
-    def __init__(self, args):
+
+    def __init__(self, args=None, **kwargs):
         """
-        @param   args                 arguments object like argparse.Namespace
-        @param   args.WRITE_OPTIONS   {"dialect": SQL dialect if not default,
+        @param   args                 arguments as namespace or dictionary, case-insensitive
+        @param   args.write_options   ```
+                                      {"dialect": SQL dialect if not default,
                                        "nesting": true|false to created nested type tables}
+                                      ```
+        @param   kwargs               any and all arguments as keyword overrides, case-insensitive
         """
-        self._args      = copy.deepcopy(args)
+        self._args      = ensure_namespace(args, SqlMixin.DEFAULT_ARGS, **kwargs)
         self._topics    = {}  # {(topic, typename, typehash): {name, table_name, view_name, sql, ..}}
         self._types     = {}  # {(typename, typehash): {type, table_name, sql, ..}}
         self._schema    = {}  # {(typename, typehash): {cols}}
         self._sql_cache = {}  # {table: "INSERT INTO table VALUES (%s, ..)"}
         self._dialect   = args.WRITE_OPTIONS.get("dialect", self.DEFAULT_DIALECT)
         self._nesting   = args.WRITE_OPTIONS.get("nesting")
 
@@ -80,15 +85,15 @@
             except Exception as e:
                 ok = False
                 ConsolePrinter.error("Error reading SQL dialect file %r: %s", filename, e)
 
         # Populate ROS type aliases like "byte" and "char"
         for opts in self.DIALECTS.values() if ok else ():
             for rostype in list(opts.get("types", {})):
-                alias = rosapi.get_type_alias(rostype)
+                alias = api.get_type_alias(rostype)
                 if alias:
                     opts["types"][alias] = opts["types"][rostype]
                 if alias and rostype + "[]" in opts["types"]:
                     opts["types"][alias + "[]"] = opts["types"][rostype + "[]"]
 
         return ok
 
@@ -119,15 +124,15 @@
         Returns full data dictionary for topic, including view name and SQL.
 
         @param   exclude_cols  list of column names to exclude from view SELECT, if any
         @return                {"name": topic name, "type": message type name as "pkg/Cls",
                                 "table_name": message type table name, "view_name": topic view name,
                                 "md5": message type definition MD5 hash, "sql": "CREATE VIEW .."}
         """
-        with rosapi.TypeMeta.make(msg, topic) as m:
+        with api.TypeMeta.make(msg, topic) as m:
             typename, typehash, typekey = (m.typename, m.typehash, m.typekey)
 
         table_name = self._types[typekey]["table_name"]
         pkgname, clsname = typename.split("/", 1)
         nameargs = {"topic": topic, "type": typename, "hash": typehash,
                     "package": pkgname, "class": clsname}
         view_name = self._make_entity_name("view", nameargs)
@@ -153,72 +158,72 @@
         @return              {"type": message type name as "pkg/Cls",
                               "table_name": message type table name,
                               "definition": message type definition,
                               "cols": [(column name, column type)],
                               "md5": message type definition MD5 hash, "sql": "CREATE TABLE .."}
         """
         rootmsg = rootmsg or msg
-        with rosapi.TypeMeta.make(msg, root=rootmsg) as m:
+        with api.TypeMeta.make(msg, root=rootmsg) as m:
             typename, typehash = (m.typename, m.typehash)
 
         cols = []
-        scalars = set(x for x in self._get_dialect_option("types") if x == rosapi.scalar(x))
-        for path, value, subtype in rosapi.iter_message_fields(msg, scalars=scalars):
+        scalars = set(x for x in self._get_dialect_option("types") if x == api.scalar(x))
+        for path, value, subtype in api.iter_message_fields(msg, scalars=scalars):
             coltype = self._make_column_type(subtype)
             cols += [(".".join(path), coltype)]
         cols.extend(extra_cols or [])
         cols = list(zip(self._make_column_names([c for c, _ in cols]), [t for _, t in cols]))
         namewidth = 2 + max(len(n) for n, _ in cols)
         coldefs = ["%s  %s" % (quote(n).ljust(namewidth), t) for n, t in cols]
 
         pkgname, clsname = typename.split("/", 1)
         nameargs = {"type": typename, "hash": typehash, "package": pkgname, "class": clsname}
         table_name = self._make_entity_name("table", nameargs)
 
         sqlargs = dict(nameargs, table=quote(table_name), cols="\n  %s\n" % ",\n  ".join(coldefs))
         sql = self._get_dialect_option("table_template").strip().format(**sqlargs)
         return {"type": typename, "md5": typehash,
-                "definition": rosapi.TypeMeta.make(msg).definition,
+                "definition": api.TypeMeta.make(msg).definition,
                 "table_name": table_name, "cols": cols, "sql": sql}
 
 
     def _make_topic_insert_sql(self, topic, msg):
         """Returns ("INSERT ..", [args]) for inserting into topics-table."""
         POSARG = self._get_dialect_option("posarg")
-        topickey = rosapi.TypeMeta.make(msg, topic).topickey
+        topickey = api.TypeMeta.make(msg, topic).topickey
         tdata = self._topics[topickey]
 
         sql  = self._get_dialect_option("insert_topic").strip().replace("%s", POSARG)
         args = [tdata[k] for k in ("name", "type", "md5", "table_name", "view_name")]
         return sql, args
 
 
     def _make_type_insert_sql(self, msg):
         """Returns ("INSERT ..", [args]) for inserting into types-table."""
         POSARG = self._get_dialect_option("posarg")
-        typekey = rosapi.TypeMeta.make(msg).typekey
+        typekey = api.TypeMeta.make(msg).typekey
         tdata = self._types[typekey]
 
         sql  = self._get_dialect_option("insert_type").strip().replace("%s", POSARG)
         args = [tdata[k] for k in ("type", "definition", "md5", "table_name")]
         return sql, args
 
 
     def _make_message_insert_sql(self, topic, msg, extra_cols=()):
         """
         Returns ("INSERT ..", [args]) for inserting into message type table.
 
         @param   extra_cols  list of additional table columns, as [(name, value)]
         """
-        typekey = rosapi.TypeMeta.make(msg, topic).typekey
+        typekey = api.TypeMeta.make(msg, topic).typekey
         table_name = self._types[typekey]["table_name"]
         sql, cols, args = self._sql_cache.get(table_name), [], []
 
-        scalars = set(x for x in self._get_dialect_option("types") if x == rosapi.scalar(x))
-        for p, v, t in rosapi.iter_message_fields(msg, scalars=scalars):
+        scalars = set(x for x in self._get_dialect_option("types") if x == api.scalar(x))
+        for p, v, t in api.iter_message_fields(msg, scalars=scalars):
             if not sql: cols.append(".".join(p))
             args.append(self._make_column_value(v, t))
         args = tuple(args) + tuple(v for _, v in extra_cols)
 
         if not sql:
             POSARG = self._get_dialect_option("posarg")
             if extra_cols: cols.extend(c for c, _ in extra_cols)
@@ -288,26 +293,26 @@
 
     def _make_column_value(self, value, typename=None):
         """Returns column value suitable for inserting to database."""
         if not typename: return value
 
         v = value
         if isinstance(v, (list, tuple)):
-            scalartype = rosapi.scalar(typename)
-            if scalartype in rosapi.ROS_TIME_TYPES:
+            scalartype = api.scalar(typename)
+            if scalartype in api.ROS_TIME_TYPES:
                 v = [self._convert_time(x) for x in v]
-            elif scalartype not in rosapi.ROS_BUILTIN_TYPES:
+            elif scalartype not in api.ROS_BUILTIN_TYPES:
                 if self._nesting: v = []
-                else: v = [rosapi.message_to_dict(x) for x in v]
+                else: v = [api.message_to_dict(x) for x in v]
             else:
                 v = self._convert_column_value(v, typename)
-        elif rosapi.is_ros_time(v):
+        elif api.is_ros_time(v):
             v = self._convert_time_value(v, typename)
-        elif typename not in rosapi.ROS_BUILTIN_TYPES:
-            v = json.dumps(rosapi.message_to_dict(v))
+        elif typename not in api.ROS_BUILTIN_TYPES:
+            v = json.dumps(api.message_to_dict(v))
         else:
             v = self._convert_column_value(v, typename)
         return v
 
 
     def _make_column_type(self, typename, fallback=None):
         """
@@ -315,20 +320,18 @@
 
         @param  fallback  fallback typename to use for lookup if no mapping for typename
         """
         TYPES         = self._get_dialect_option("types")
         ARRAYTEMPLATE = self._get_dialect_option("arraytype_template")
         DEFAULTTYPE   = self._get_dialect_option("defaulttype")
 
-        scalartype = rosapi.scalar(typename)
-        timetype   = rosapi.get_ros_time_category(scalartype)
-        coltype    = TYPES.get(typename)
+        scalartype = api.scalar(typename)
+        timetype   = api.get_ros_time_category(scalartype)
+        coltype    = TYPES.get(typename) or TYPES.get(api.canonical(typename, unbounded=True))
 
-        if not coltype and "[" not in typename and scalartype in TYPES:
-            coltype = TYPES[scalartype]  # Bounded type like "string<=10"
         if not coltype and scalartype in TYPES:
             coltype = ARRAYTEMPLATE.format(type=TYPES[scalartype])
         if not coltype and timetype in TYPES:
             if typename != scalartype:
                 coltype = ARRAYTEMPLATE.format(type=TYPES[timetype])
             else:
                 coltype = TYPES[timetype]
@@ -342,30 +345,30 @@
     def _convert_column_value(self, value, typename):
         """Returns ROS value converted to dialect value."""
         ADAPTERS = self._get_dialect_option("adapters")
         if not ADAPTERS: return value
 
         adapter, iterate = ADAPTERS.get(typename), False
         if not adapter and isinstance(value, (list, tuple)):
-            adapter, iterate = ADAPTERS.get(rosapi.scalar(typename)), True
+            adapter, iterate = ADAPTERS.get(api.scalar(typename)), True
         if adapter:
             value = [adapter(x) for x in value] if iterate else adapter(value)
         return value
 
 
     def _convert_time_value(self, value, typename):
         """Returns ROS time/duration value converted to dialect value."""
         adapter = self._get_dialect_option("adapters").get(typename)
         if adapter:
             try: is_int = issubclass(adapter, int)
             except Exception: is_int = False
-            v = rosapi.to_sec(value) if is_int else "%d.%09d" % rosapi.to_sec_nsec(value)
+            v = api.to_sec(value) if is_int else "%d.%09d" % api.to_sec_nsec(value)
             result = adapter(v)
         else:
-            result = rosapi.to_decimal(value)
+            result = api.to_decimal(value)
         return result
 
 
     def _get_dialect_option(self, option):
         """Returns option for current SQL dialect, falling back to default dialect."""
         return self.DIALECTS[self._dialect].get(option, self.DIALECTS[None].get(option))
 
@@ -501,15 +504,15 @@
             },
             "defaulttype":         "String",
             "arraytype_template":  "Array({type})",
         },
     }
 
 
-    ## Words that need quoting if in name context, e.g. table name.
+    ## Words that need quoting if in name context, like table name.
     ## Combined from reserved words for Postgres, SQLite, MSSQL, Oracle et al.
     KEYWORDS = [
         "A", "ABORT", "ABS", "ABSOLUTE", "ACCESS", "ACTION", "ADA", "ADD", "ADMIN", "AFTER",
         "AGGREGATE", "ALIAS", "ALL", "ALLOCATE", "ALSO", "ALTER", "ALWAYS", "ANALYSE", "ANALYZE",
         "AND", "ANY", "ARE", "ARRAY", "AS", "ASC", "ASENSITIVE", "ASSERTION", "ASSIGNMENT",
         "ASYMMETRIC", "AT", "ATOMIC", "ATTACH", "ATTRIBUTE", "ATTRIBUTES", "AUDIT",
         "AUTHORIZATION", "AUTOINCREMENT", "AUTO_INCREMENT", "AVG", "AVG_ROW_LENGTH", "BACKUP",
@@ -633,7 +636,10 @@
                     contains only alphanumerics, and is not a reserved keyword)
     """
     result = name
     if force or result.upper() in SqlMixin.KEYWORDS \
     or re.search(r"(^[\W\d])|(?=\W)", result, re.U):
         result = '"%s"' % result.replace('"', '""')
     return result
+
+
+__all__ = ["SqlMixin", "quote"]
```

## grepros/plugins/auto/sqlite.py

```diff
@@ -1,33 +1,34 @@
 # -*- coding: utf-8 -*-
 """
-SQLite output for search results.
+SQLite output plugin.
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     03.12.2021
-@modified    06.02.2022
+@modified    28.06.2023
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.plugins.auto.sqlite
 import collections
 import json
 import os
 import sqlite3
-import sys
 
-from ... common import ConsolePrinter, format_bytes, makedirs
-from ... import rosapi
-from . dbbase import DataSinkBase, quote
+import six
 
+from ... import api
+from ... common import ConsolePrinter, format_bytes, makedirs, verify_io
+from . dbbase import BaseDataSink, quote
 
-class SqliteSink(DataSinkBase):
+
+class SqliteSink(BaseDataSink):
     """
     Writes messages to an SQLite database.
 
     Output will have:
     - table "messages", with all messages as serialized binary data
     - table "types", with message definitions
     - table "topics", with topic information
@@ -51,91 +52,101 @@
 
     ## Auto-detection file extensions
     FILE_EXTENSIONS = (".sqlite", ".sqlite3")
 
     ## Maximum integer size supported in SQLite, higher values inserted as string
     MAX_INT = 2**63 - 1
 
+    ## Constructor argument defaults
+    DEFAULT_ARGS = dict(META=False, WRITE_OPTIONS={}, VERBOSE=False)
+
 
-    def __init__(self, args):
+    def __init__(self, args=None, **kwargs):
         """
-        @param   args                 arguments object like argparse.Namespace
-        @param   args.META            whether to print metainfo
-        @param   args.WRITE           name of SQLite file to write, will be appended to if exists
-        @param   args.WRITE_OPTIONS   {"commit-interval": transaction size (0 is autocommit),
+        @param   args                 arguments as namespace or dictionary, case-insensitive;
+                                      or a single path as the name of SQLitefile to write
+        @param   args.write           name of SQLite file to write, will be appended to if exists
+        @param   args.write_options   ```
+                                      {"commit-interval": transaction size (0 is autocommit),
                                        "message-yaml": populate messages.yaml (default true),
                                        "nesting": "array" to recursively insert arrays
                                                   of nested types, or "all" for any nesting),
                                        "overwrite": whether to overwrite existing file
                                                     (default false)}
-        @param   args.VERBOSE         whether to print debug information
+                                      ```
+        @param   args.meta            whether to emit metainfo
+        @param   args.verbose         whether to emit debug information
+        @param   kwargs               any and all arguments as keyword overrides, case-insensitive
         """
-        super(SqliteSink, self).__init__(args)
+        super(SqliteSink, self).__init__(args, **kwargs)
 
-        self._filename    = args.WRITE
-        self._do_yaml     = (args.WRITE_OPTIONS.get("message-yaml") != "false")
-        self._overwrite   = (args.WRITE_OPTIONS.get("overwrite") == "true")
+        self._filename    = self.args.WRITE
+        self._do_yaml     = (self.args.WRITE_OPTIONS.get("message-yaml") != "false")
+        self._overwrite   = (self.args.WRITE_OPTIONS.get("overwrite") in (True, "true"))
         self._id_counters = {}  # {table next: max ID}
 
 
     def validate(self):
         """
-        Returns "commit-interval" and "nesting" in args.WRITE_OPTIONS have valid value, if any;
-        parses "message-yaml" from args.WRITE_OPTIONS.
+        Returns whether "commit-interval" and "nesting" in args.write_options have valid value, if any,
+        and file is writable; parses "message-yaml" and "overwrite" from args.write_options.
         """
-        config_ok = super(SqliteSink, self).validate()
-        if self.args.WRITE_OPTIONS.get("message-yaml") not in (None, "true", "false"):
+        if self.valid is not None: return self.valid
+        ok = super(SqliteSink, self).validate()
+        if self.args.WRITE_OPTIONS.get("message-yaml") not in (None, True, False, "true", "false"):
             ConsolePrinter.error("Invalid message-yaml option for %s: %r. "
                                  "Choose one of {true, false}.",
                                  self.ENGINE, self.args.WRITE_OPTIONS["message-yaml"])
-            config_ok = False
-        if self.args.WRITE_OPTIONS.get("overwrite") not in (None, "true", "false"):
+            ok = False
+        if self.args.WRITE_OPTIONS.get("overwrite") not in (None, True, False, "true", "false"):
             ConsolePrinter.error("Invalid overwrite option for %s: %r. "
                                  "Choose one of {true, false}.",
                                  self.ENGINE, self.args.WRITE_OPTIONS["overwrite"])
-            config_ok = False
-        return config_ok
+            ok = False
+        if not verify_io(self.args.WRITE, "w"):
+            ok = False
+        self.valid = ok
+        return self.valid
 
 
     def _init_db(self):
         """Opens the database file and populates schema if not already existing."""
         for t in (dict, list, tuple): sqlite3.register_adapter(t, json.dumps)
-        sqlite3.register_adapter(int, lambda x: str(x) if abs(x) > self.MAX_INT else x)
-        if sys.version_info < (3, ):
-            sqlite3.register_adapter(long, lambda x: str(x) if abs(x) > self.MAX_INT else x)
+        for t in six.integer_types:
+            sqlite3.register_adapter(t, lambda x: str(x) if abs(x) > self.MAX_INT else x)
         sqlite3.register_converter("JSON", json.loads)
         if self.args.VERBOSE:
             sz = os.path.exists(self._filename) and os.path.getsize(self._filename)
             action = "Overwriting" if sz and self._overwrite else \
                      "Appending to" if sz else "Creating"
             ConsolePrinter.debug("%s %s%s.", action, self._filename,
                                  (" (%s)" % format_bytes(sz)) if sz else "")
         super(SqliteSink, self)._init_db()
 
 
     def _load_schema(self):
         """Populates instance attributes with schema metainfo."""
         super(SqliteSink, self)._load_schema()
-        for row in self._db.execute("SELECT name FROM sqlite_master "
+        for row in self.db.execute("SELECT name FROM sqlite_master "
                                     "WHERE type = 'table' AND name LIKE '%/%'"):
-            cols = self._db.execute("PRAGMA table_info(%s)" % quote(row["name"])).fetchall()
+            cols = self.db.execute("PRAGMA table_info(%s)" % quote(row["name"])).fetchall()
             typerow = next((x for x in self._types.values()
                             if x["table_name"] == row["name"]), None)
             if not typerow: continue  # for row
             typekey = (typerow["type"], typerow["md5"])
             self._schema[typekey] = collections.OrderedDict([(c["name"], c) for c in cols])
 
 
     def _process_message(self, topic, msg, stamp):
         """Inserts message to messages-table, and to pkg/MsgType tables."""
-        with rosapi.TypeMeta.make(msg, topic) as m:
+        with api.TypeMeta.make(msg, topic) as m:
             topic_id, typename = self._topics[m.topickey]["id"], m.typename
-        margs = dict(dt=rosapi.to_datetime(stamp), timestamp=rosapi.to_nsec(stamp),
+        margs = dict(dt=api.to_datetime(stamp), timestamp=api.to_nsec(stamp),
                      topic=topic, name=topic, topic_id=topic_id, type=typename,
-                     yaml=str(msg) if self._do_yaml else "", data=rosapi.get_message_data(msg))
+                     yaml=str(msg) if self._do_yaml else "", data=api.serialize_message(msg))
         self._ensure_execute(self._get_dialect_option("insert_message"), margs)
         super(SqliteSink, self)._process_message(topic, msg, stamp)
 
 
     def _connect(self):
         """Returns new database connection."""
         makedirs(os.path.dirname(self._filename))
@@ -162,19 +173,28 @@
         self._cursor.executescript(sql)
 
 
     def _get_next_id(self, table):
         """Returns next ID value for table, using simple auto-increment."""
         if not self._id_counters.get(table):
             sql = "SELECT COALESCE(MAX(_id), 0) AS id FROM %s" % quote(table)
-            self._id_counters[table] = self._db.execute(sql).fetchone()["id"]
+            self._id_counters[table] = self.db.execute(sql).fetchone()["id"]
         self._id_counters[table] += 1
         return self._id_counters[table]
 
 
+    def _make_db_label(self):
+        """Returns formatted label for database, with file path and size."""
+        try: sz = format_bytes(os.path.getsize(self._filename))
+        except Exception as e:
+            ConsolePrinter.warn("Error getting size of %s: %s", self._filename, e)
+            sz = "error getting size"
+        return "%s (%s)" % (self._filename, sz)
+
+
 
 def init(*_, **__):
     """Adds SQLite output format support."""
     from ... import plugins  # Late import to avoid circular
     plugins.add_write_format("sqlite", SqliteSink, "SQLite", [
         ("commit-interval=NUM",      "transaction size for SQLite output\n"
                                      "(default 1000, 0 is autocommit)"),
@@ -189,7 +209,10 @@
                                      'only for arrays if "array" \n'
                                      "else for any nested types\n"
                                      "(array fields in parent will be populated \n"
                                      " with foreign keys instead of messages as JSON)"),
         ("overwrite=true|false",     "overwrite existing file in SQLite output\n"
                                      "instead of appending to file (default false)")
     ])
+
+
+__all__ = ["SqliteSink", "init"]
```

## Comparing `grepros/rosapi.py` & `grepros/api.py`

 * *Files 27% similar despite different names*

```diff
@@ -4,49 +4,72 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS1 bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     01.11.2021
-@modified    17.10.2022
+@modified    02.07.2023
 ------------------------------------------------------------------------------
 """
-## @namespace grepros.rosapi
+## @namespace grepros.api
+import abc
 import collections
 import datetime
 import decimal
 import hashlib
+import inspect
 import os
 import re
+import sys
 import time
 
-from . common import ConsolePrinter, Decompressor, filter_fields, memoize
+import six
+
+from . common import ConsolePrinter, LenIterable, format_bytes, memoize
 #from . import ros1, ros2  # Imported conditionally
 
 
 ## Node base name for connecting to ROS (will be anonymized).
 NODE_NAME = "grepros"
 
 ## Bagfile extensions to seek, including leading dot, populated after init
 BAG_EXTENSIONS  = ()
 
 ## Bagfile extensions to skip, including leading dot, populated after init
 SKIP_EXTENSIONS = ()
 
+## Flag denoting ROS1 environment, populated on validate()
+ROS1 = None
+
+## Flag denoting ROS2 environment, populated on validate()
+ROS2 = None
+
+## ROS version from environment, populated on validate() as integer
+ROS_VERSION = None
+
+## ROS Python module family, "rospy" or "rclpy", populated on validate()
+ROS_FAMILY = None
+
 ## All built-in numeric types in ROS
 ROS_NUMERIC_TYPES = ["byte", "char", "int8", "int16", "int32", "int64", "uint8",
                      "uint16", "uint32", "uint64", "float32", "float64", "bool"]
 
 ## All built-in string types in ROS
 ROS_STRING_TYPES = ["string", "wstring"]
 
 ## All built-in basic types in ROS
 ROS_BUILTIN_TYPES = ROS_NUMERIC_TYPES + ROS_STRING_TYPES
 
+## Python constructors for ROS built-in types, as {ROS name: type class}
+ROS_BUILTIN_CTORS = {"byte":   int,  "char":   int, "int8":    int,   "int16":   int,
+                     "int32":  int,  "int64":  int, "uint8":   int,   "uint16":  int,
+                     "uint32": int,  "uint64": int, "float32": float, "float64": float,
+                     "bool":   bool, "string": str, "wstring": str}
+
 ## ROS time/duration types, populated after init
 ROS_TIME_TYPES = []
 
 ## ROS1 time/duration types mapped to type names, populated after init
 ROS_TIME_CLASSES = {}
 
 ## All built-in basic types plus time types in ROS, populated after init
@@ -55,42 +78,374 @@
 ## Mapping between type aliases and real types, like {"byte": "int8"} in ROS1
 ROS_ALIAS_TYPES = {}
 
 ## Module grepros.ros1 or grepros.ros2
 realapi = None
 
 
+class BaseBag(object):
+    """
+    ROS bag interface.
+
+    %Bag can be used a context manager, is an iterable providing (topic, message, timestamp) tuples
+    and supporting `len(bag)`; and supports topic-based membership
+    (`if mytopic in bag`, `for t, m, s in bag[mytopic]`, `len(bag[mytopic])`).
+
+    Extra methods and properties compared with rosbag.Bag: Bag.get_message_class(),
+    Bag.get_message_definition(), Bag.get_message_type_hash(), Bag.get_topic_info();
+    Bag.closed and Bag.topics.
+    """
+
+    ## Returned from read_messages() as (topic name, ROS message, ROS timestamp object).
+    BagMessage = collections.namedtuple("BagMessage", "topic message timestamp")
+
+    ## Returned from get_type_and_topic_info() as
+    ## (typename, message count, connection count, median frequency).
+    TopicTuple = collections.namedtuple("TopicTuple", ["msg_type", "message_count",
+                                                       "connections", "frequency"])
+
+    ## Returned from get_type_and_topic_info() as ({typename: typehash}, {topic name: TopicTuple}).
+    TypesAndTopicsTuple = collections.namedtuple("TypesAndTopicsTuple", ["msg_types", "topics"])
+
+    ## Supported opening modes, overridden in subclasses
+    MODES = ("r", "w", "a")
+
+    ## Whether bag supports reading or writing stream objects, overridden in subclasses
+    STREAMABLE = True
+
+    def __iter__(self):
+        """Iterates over all messages in the bag."""
+        return self.read_messages()
+
+    def __enter__(self):
+        """Context manager entry, opens bag if not open."""
+        self.open()
+        return self
+
+    def __exit__(self, exc_type, exc_value, traceback):
+        """Context manager exit, closes bag."""
+        self.close()
+
+    def __len__(self):
+        """Returns the number of messages in the bag."""
+        return self.get_message_count()
+
+    def __next__(self):
+        """Retrieves next message from bag as (topic, message, timestamp)."""
+        raise NotImplementedError
+    if sys.version_info < (3, ): next = __next__
+
+    def __nonzero__(self): return True  # Iterables by default use len() for bool() [Py2]
+
+    def __bool__   (self): return True  # Iterables by default use len() for bool() [Py3]
+
+    def __contains__(self, key):
+        """Returns whether bag contains given topic."""
+        raise NotImplementedError
+
+    def __copy__(self): return self
+
+    def __deepcopy__(self, memo=None): return self
+
+    def __getitem__(self, key):
+        """Returns an iterator yielding messages from the bag in given topic, supporting len()."""
+        if key not in self: return LenIterable([], 0)
+        get_count = lambda: sum(c for (t, _, _), c in self.get_topic_info().items() if t == key)
+        return LenIterable(self.read_messages(key), get_count)
+
+    def __str__(self):
+        """Returns informative text for bag, with a full overview of topics and types."""
+
+        indent  = lambda s, n: ("\n%s" % (" " * n)).join(s.splitlines())
+        # Returns UNIX timestamp as "Dec 22 2021 23:13:44.44"
+        fmttime = lambda x: datetime.datetime.fromtimestamp(x).strftime("%b %d %Y %H:%M:%S.%f")[:-4]
+        def fmtdur(secs):
+            """Returns duration seconds as text like "1hr 1:12s (3672s)" or "51.8s"."""
+            result = ""
+            hh, rem = divmod(secs, 3600)
+            mm, ss  = divmod(rem, 60)
+            if hh: result += "%dhr " % hh
+            if mm: result += "%d:"   % mm
+            result += "%ds" % ss
+            if hh or mm: result += " (%ds)" % secs
+            return result
+
+        entries = {}
+        counts = self.get_topic_info()
+        start, end = self.get_start_time(), self.get_end_time()
+
+        entries["path"] = self.filename or "<stream>"
+        if None not in (start, end):
+            entries["duration"] = fmtdur(end - start)
+            entries["start"] = "%s (%.2f)" % (fmttime(start), start)
+            entries["end"]   = "%s (%.2f)" % (fmttime(end),   end)
+        entries["size"] = format_bytes(self.size)
+        if any(counts.values()):
+            entries["messages"] = str(sum(counts.values()))
+        if counts:
+            nhs = sorted(set((n, h) for _, n, h in counts))
+            namew = max(len(n) for n, _ in nhs)
+            # "pkg/Msg <PAD>[typehash]"
+            entries["types"] = "\n".join("%s%s [%s]" % (n, " " * (namew - len(n)), h) for n, h in nhs)
+        if counts:
+            topicw = max(len(t) for t, _, _ in counts)
+            typew  = max(len(n) for _, n, _ in counts)
+            countw = max(len(str(c)) for c in counts.values())
+            lines = []
+            for (t, n, _), c in sorted(counts.items()):
+                qq = self.get_qoses(t, n) or []
+                # "/my/topic<PAD>"
+                line  = "%s%s" % (t, " " * (topicw - len(t)))
+                # "   <PAD>13 msgs" or "   <PAD>1 msg "
+                line += "   %s%s msg%s" % (" " * (countw - len(str(c))), c, " " if 1 == c else "s")
+                # "    : pkg/Msg"
+                line += "    : %s" % n
+                # "<PAD> (2 connections)" if >1 connections
+                line += "%s (%s connections)" % (" " * (typew - len(n)), len(qq)) if len(qq) > 1 else ""
+                lines.append(line)
+            entries["topics"] = "\n".join(lines)
+
+        labelw = max(map(len, entries))
+        return "\n".join("%s:%s %s" % (k, " " * (labelw - len(k)), indent(v, labelw + 2))
+                         for k, v in entries.items())
+
+    def get_message_count(self, topic_filters=None):
+        """
+        Returns the number of messages in the bag.
+
+        @param   topic_filters  list of topics or a single topic to filter by, if any
+        """
+        raise NotImplementedError
+
+    def get_start_time(self):
+        """Returns the start time of the bag, as UNIX timestamp, or None if bag empty."""
+        raise NotImplementedError
+
+    def get_end_time(self):
+        """Returns the end time of the bag, as UNIX timestamp, or None if bag empty."""
+        raise NotImplementedError
+
+    def get_topic_info(self, counts=True):
+        """
+        Returns topic and message type metainfo as {(topic, typename, typehash): count}.
+
+        @param   counts  if false, counts may be returned as None if lookup is costly
+        """
+        raise NotImplementedError
+
+    def get_type_and_topic_info(self, topic_filters=None):
+        """
+        Returns thorough metainfo on topic and message types.
+
+        @param   topic_filters  list of topics or a single topic to filter returned topics-dict by,
+                                if any
+        @return                 TypesAndTopicsTuple(msg_types, topics) namedtuple,
+                                msg_types as dict of {typename: typehash},
+                                topics as a dict of {topic: TopicTuple() namedtuple}.
+        """
+        raise NotImplementedError
+
+    def get_qoses(self, topic, typename):
+        """
+        Returns topic Quality-of-Service profiles as a list of dicts, or None if not available.
+
+        Functional only in ROS2.
+        """
+        return None
+
+    def get_message_class(self, typename, typehash=None):
+        """Returns ROS message type class, or None if unknown message type for bag."""
+        return None
+
+    def get_message_definition(self, msg_or_type):
+        """
+        Returns ROS message type definition full text, including subtype definitions.
+
+        Returns None if unknown message type for bag.
+        """
+        return None
+
+    def get_message_type_hash(self, msg_or_type):
+        """Returns ROS message type MD5 hash, or None if unknown message type for bag."""
+        return None
+
+    def read_messages(self, topics=None, start_time=None, end_time=None, raw=False, **__):
+        """
+        Yields messages from the bag, optionally filtered by topic and timestamp.
+
+        @param   topics      list of topics or a single topic to filter by, if any
+        @param   start_time  earliest timestamp of message to return, as ROS time or convertible
+                             (int/float/duration/datetime/decimal)
+        @param   end_time    latest timestamp of message to return, as ROS time
+                             (int/float/duration/datetime/decimal)
+        @param   raw         if true, then returned messages are tuples of
+                             (typename, bytes, typehash, typeclass)
+                             or (typename, bytes, typehash, position, typeclass),
+                             depending on file format
+        @return              BagMessage namedtuples of (topic, message, timestamp as ROS time)
+        """
+        raise NotImplementedError
+
+    def write(self, topic, msg, t=None, raw=False, **kwargs):
+        """
+        Writes a message to the bag.
+
+        @param   topic   name of topic
+        @param   msg     ROS message
+        @param   t       message timestamp if not using current wall time, as ROS time
+                         or convertible (int/float/duration/datetime/decimal)
+        @param   raw     if true, `msg` is in raw format, (typename, bytes, typehash, typeclass)
+        @param   kwargs  ROS version-specific arguments,
+                         like `connection_header` for ROS1 or `qoses` for ROS2
+        """
+        raise NotImplementedError
+
+    def open(self):
+        """Opens the bag file if not already open."""
+        raise NotImplementedError
+
+    def close(self):
+        """Closes the bag file."""
+        raise NotImplementedError
+
+    def flush(self):
+        """Ensures all changes are written to bag file."""
+
+    @property
+    def topics(self):
+        """Returns the list of topics in bag, in alphabetic order."""
+        raise NotImplementedError
+
+    @property
+    def filename(self):
+        """Returns bag file path."""
+        raise NotImplementedError
+
+    @property
+    def size(self):
+        """Returns current file size in bytes."""
+        raise NotImplementedError
+
+    @property
+    def mode(self):
+        """Returns file open mode."""
+        raise NotImplementedError
+
+    @property
+    def closed(self):
+        """Returns whether file is closed."""
+        raise NotImplementedError
+
+    @property
+    def stop_on_error(self):
+        """Whether raising read error on unknown message type (ROS2 SQLite .db3 specific)."""
+        return getattr(self, "_stop_on_error", None)
+
+    @stop_on_error.setter
+    def stop_on_error(self, flag):
+        """Sets whether to raise read error on unknown message type (ROS2 SQLite .db3 specific)."""
+        setattr(self, "_stop_on_error", flag)
+
+
+@six.add_metaclass(abc.ABCMeta)
+class Bag(BaseBag):
+    """
+    %Bag factory metaclass.
+
+    Result is a format-specific class instance, auto-detected from file extension or content:
+    an extended rosbag.Bag for ROS1 bags, otherwise an object with a conforming interface.
+
+    E.g. {@link grepros.plugins.mcap.McapBag McapBag} if {@link grepros.plugins.mcap mcap}
+    plugin loaded and file recognized as MCAP format.
+
+    User plugins can add their own format support to READER_CLASSES and WRITER_CLASSES.
+    Classes can have a static/class method `autodetect(filename)`
+    returning whether given file is in recognizable format for the plugin class.
+    """
+
+    ## Bag reader classes, as {Cls, }
+    READER_CLASSES = set()
+
+    ## Bag writer classes, as {Cls, }
+    WRITER_CLASSES = set()
+
+    def __new__(cls, f, mode="r", reindex=False, progress=False, **kwargs):
+        """
+        Returns an object for reading or writing ROS bags.
+
+        Suitable Bag class is auto-detected by file extension or content.
+
+        @param   f         bag file path, or a stream object
+                           (streams not supported for ROS2 .db3 SQLite bags)
+        @param   mode      return reader if "r" else writer
+        @param   reindex   reindex unindexed bag (ROS1 only), making a backup if indexed format
+        @param   progress  show progress bar with reindexing status
+        @param   kwargs    additional keyword arguments for format-specific Bag constructor,
+                           like `compression` for ROS1 bag
+        """
+        classes, errors = set(cls.READER_CLASSES if "r" == mode else cls.WRITER_CLASSES), []
+        for detect, bagcls in ((d, c) for d in (True, False) for c in list(classes)):
+            use, discard = not detect, False  # Try auto-detecting suitable class first
+            try:
+                if detect and callable(getattr(bagcls, "autodetect", None)):
+                    use, discard = bagcls.autodetect(f), True
+                if use:
+                    return bagcls(f, mode, reindex=reindex, progress=progress, **kwargs)
+            except Exception as e:
+                discard = True
+                errors.append("Failed to open %r for %s with %s: %s." %
+                              (f, "reading" if "r" == mode else "writing", bagcls, e))
+            discard and classes.discard(bagcls)
+        for err in errors: ConsolePrinter.warn(err)
+        raise Exception("No suitable %s class available" % ("reader" if "r" == mode else "writer"))
+
+
+    @classmethod
+    def autodetect(cls, f):
+        """Returns registered bag class auto-detected from file, or None."""
+        for bagcls in cls.READER_CLASSES | cls.WRITER_CLASSES:
+            if callable(vars(bagcls).get("autodetect")) and bagcls.autodetect(f):
+                return bagcls
+        return None
+
+
+    @classmethod
+    def __subclasshook__(cls, C):
+        return True if issubclass(C, BaseBag) else NotImplemented
+
+
 class TypeMeta(object):
     """
     Container for caching and retrieving message type metadata.
 
     All property values are lazy-loaded upon request.
     """
 
-    ## SourceBase instance
-    SOURCE = None
-
-    ## Seconds before auto-clearing message from cache
+    ## Seconds before auto-clearing message from cache, <=0 disables
     LIFETIME = 2
 
-    ## {id(msg): MessageMeta()}
+    ## Max size to constrain cache to, <=0 disables
+    POPULATION = 0
+
+    ## {id(msg): TypeMeta()}
     _CACHE = {}
 
     ## {id(msg): [id(nested msg), ]}
     _CHILDREN = {}
 
     ## {id(msg): time.time() of registering}
     _TIMINGS = {}
 
     ## time.time() of last cleaning of stale messages
     _LASTSWEEP = time.time()
 
-    def __init__(self, msg, topic=None, data=None):
+    def __init__(self, msg, topic=None, source=None, data=None):
         self._msg      = msg
         self._topic    = topic
+        self._source   = source
         self._data     = data
         self._type     = None  # Message typename as "pkg/MsgType"
         self._def      = None  # Message type definition with full subtype definitions
         self._hash     = None  # Message type definition MD5 hash
         self._cls      = None  # Message class object
         self._topickey = None  # (topic, typename, typehash)
         self._typekey  = None  # (typename, typehash)
@@ -108,36 +463,37 @@
             self._type = realapi.get_message_type(self._msg)
         return self._type
 
     @property
     def typehash(self):
         """Returns message type definition MD5 hash."""
         if not self._hash:
-            hash = self.SOURCE and self.SOURCE.get_message_type_hash(self._msg)
+            hash = self._source and self._source.get_message_type_hash(self._msg)
             self._hash = hash or realapi.get_message_type_hash(self._msg)
         return self._hash
 
     @property
     def definition(self):
         """Returns message type definition text with full subtype definitions."""
         if not self._def:
-            typedef = self.SOURCE and self.SOURCE.get_message_definition(self._msg)
+            typedef = self._source and self._source.get_message_definition(self._msg)
             self._def = typedef or realapi.get_message_definition(self._msg)
         return self._def
 
     @property
     def data(self):
         """Returns message serialized binary, as bytes(), or None if not cached."""
         return self._data
 
     @property
     def typeclass(self):
         """Returns message class object."""
         if not self._cls:
-            cls = self.SOURCE and self.SOURCE.get_message_class(self.typename, self.typehash)
+            cls = type(self._msg) if realapi.is_ros_message(self._msg) else \
+                  self._source and self._source.get_message_class(self.typename, self.typehash)
             self._cls = cls or realapi.get_message_class(self.typename)
         return self._cls
 
     @property
     def topickey(self):
         """Returns (topic, typename, typehash) for message."""
         if not self._topickey:
@@ -148,23 +504,25 @@
     def typekey(self):
         """Returns (typename, typehash) for message."""
         if not self._typekey:
             self._typekey = (self.typename, self.typehash)
         return self._typekey
 
     @classmethod
-    def make(cls, msg, topic=None, root=None, data=None):
+    def make(cls, msg, topic=None, source=None, root=None, data=None):
         """
         Returns TypeMeta instance, registering message in cache if not present.
 
         Other parameters are only required for first registration.
 
-        @param   topic  topic the message is in if root message
-        @param   root   root message that msg is a nested value of, if any
-        @param   data   message serialized binary, if any
+        @param   topic   topic the message is in if root message
+        @param   source  message source like TopicSource or Bag,
+                         for looking up message type metadata
+        @param   root    root message that msg is a nested value of, if any
+        @param   data    message serialized binary, if any
         """
         msgid = id(msg)
         if msgid not in cls._CACHE:
             cls._CACHE[msgid] = TypeMeta(msg, topic, data)
             if root and root is not msg:
                 cls._CHILDREN.setdefault(id(root), set()).add(msgid)
             cls._TIMINGS[msgid] = time.time()
@@ -179,17 +537,24 @@
         cls._CACHE.pop(msgid, None), cls._TIMINGS.pop(msgid, None)
         for childid in cls._CHILDREN.pop(msgid, []):
             cls._CACHE.pop(childid, None), cls._TIMINGS.pop(childid, None)
         cls.sweep()
 
     @classmethod
     def sweep(cls):
-        """Discards stale messages from cache."""
+        """Discards stale or surplus messages from cache."""
+        if cls.POPULATION > 0 and len(cls._CACHE) > cls.POPULATION:
+            count = len(cls._CACHE) - cls.POPULATION
+            for msgid, tm in sorted(x[::-1] for x in cls._TIMINGS.items())[:count]:
+                cls._CACHE.pop(msgid, None), cls._TIMINGS.pop(msgid, None)
+                for childid in cls._CHILDREN.pop(msgid, []):
+                    cls._CACHE.pop(childid, None), cls._TIMINGS.pop(childid, None)
+
         now = time.time()
-        if not cls.LIFETIME or cls._LASTSWEEP < now - cls.LIFETIME: return
+        if cls.LIFETIME <= 0 or cls._LASTSWEEP < now - cls.LIFETIME: return
 
         for msgid, tm in list(cls._TIMINGS.items()):
             drop = (tm > now) or (tm < now - cls.LIFETIME)
             drop and (cls._CACHE.pop(msgid, None), cls._TIMINGS.pop(msgid, None))
             for childid in cls._CHILDREN.pop(msgid, []) if drop else ():
                 cls._CACHE.pop(childid, None), cls._TIMINGS.pop(childid, None)
         cls._LASTSWEEP = now
@@ -198,65 +563,14 @@
     def clear(cls):
         """Clears entire cache."""
         cls._CACHE.clear()
         cls._CHILDREN.clear()
         cls._TIMINGS.clear()
 
 
-class Bag(object):
-    """ROS bag creation wrapper."""
-
-    ## Bag reader classes, as {Cls, }
-    READER_CLASSES = set()
-
-    ## Bag writer classes, as {Cls, }
-    WRITER_CLASSES = set()
-
-    def __new__(cls, filename, mode="r", decompress=False, reindex=False, progress=False):
-        """
-        Returns an object for reading or writing ROS bags.
-
-        Result is rosbag.Bag in ROS1, or an object with a partially conforming API
-        if using embag in ROS1, or if using ROS2.
-
-        Plugins can add their own format support to READER_CLASSES and WRITER_CLASSES.
-        Classes can have a static/class method `autodetect(filename)`
-        returning whether given file is readable for the plugin class.
-
-        Extra methods compared with rosbag.Bag: get_message_class(),
-        get_message_definition(), get_message_type_hash(), and get_topic_info().
-
-        @param   mode         return reader if "r" else writer
-        @param   decompress   decompress archived bag to file directory
-        @param   reindex      reindex unindexed bag (ROS1 only), making a backup if indexed format
-        @param   progress     show progress bar with decompression or reindexing status
-        """
-        if Decompressor.is_compressed(filename):
-            if decompress: filename = Decompressor.decompress(filename, progress)
-            else: raise Exception("decompression not enabled")
-
-        if "a" == mode and (not os.path.exists(filename) or not os.path.getsize(filename)):
-            mode = "w"  # rosbag raises error on append if no file or empty file
-            os.path.exists(filename) and os.remove(filename)
-        classes = set(cls.READER_CLASSES if "r" == mode else cls.WRITER_CLASSES)
-        for detect, mycls in ((d, c) for d in (True, False) for c in list(classes)):
-            use, discard = not detect, False
-            try:  # Try auto-detecting suitable class first
-                if detect and callable(getattr(mycls, "autodetect", None)):
-                    use, discard = mycls.autodetect(filename), True
-                if use:
-                    result = mycls(filename, mode=mode, reindex=reindex, progress=progress)
-                    if result: return result
-            except Exception as e:
-                discard = True
-                ConsolePrinter.warn("Failed to open %r for %s with %s: %s.",
-                                    filename, "reading" if "r" == mode else "writing", mycls, e)
-            discard and classes.discard(mycls)
-        raise Exception("No suitable %s class available" % ("reader" if "r" == mode else "writer"))
-
 
 def init_node(name=None):
     """
     Initializes a ROS1 or ROS2 node if not already initialized.
 
     In ROS1, blocks until ROS master available.
     """
@@ -266,42 +580,44 @@
 def shutdown_node():
     """Shuts down live ROS node."""
     realapi and realapi.shutdown_node()
 
 
 def validate(live=False):
     """
-    Returns whether ROS environment is set, prints error if not.
+    Initializes ROS bindings, returns whether ROS environment set, prints or raises error if not.
 
     @param   live  whether environment must support launching a ROS node
     """
-    global realapi, BAG_EXTENSIONS, SKIP_EXTENSIONS, \
+    global realapi, BAG_EXTENSIONS, SKIP_EXTENSIONS, ROS1, ROS2, ROS_VERSION, ROS_FAMILY, \
            ROS_COMMON_TYPES, ROS_TIME_TYPES, ROS_TIME_CLASSES, ROS_ALIAS_TYPES
     if realapi:
         return True
 
     success, version = False, os.getenv("ROS_VERSION")
     if "1" == version:
         from . import ros1
         realapi = ros1
         success = realapi.validate()
+        ROS1, ROS2, ROS_VERSION, ROS_FAMILY = True, False, 1, "rospy"
     elif "2" == version:
         from . import ros2
         realapi = ros2
         success = realapi.validate(live)
+        ROS1, ROS2, ROS_VERSION, ROS_FAMILY = False, True, 2, "rclpy"
     elif not version:
         ConsolePrinter.error("ROS environment not set: missing ROS_VERSION.")
     else:
         ConsolePrinter.error("ROS environment not supported: unknown ROS_VERSION %r.", version)
     if success:
         BAG_EXTENSIONS, SKIP_EXTENSIONS = realapi.BAG_EXTENSIONS, realapi.SKIP_EXTENSIONS
         ROS_COMMON_TYPES = ROS_BUILTIN_TYPES + realapi.ROS_TIME_TYPES
         ROS_TIME_TYPES   = realapi.ROS_TIME_TYPES
         ROS_TIME_CLASSES = realapi.ROS_TIME_CLASSES
-        ROS_ALIAS_TYPES = realapi.ROS_ALIAS_TYPES
+        ROS_ALIAS_TYPES  = realapi.ROS_ALIAS_TYPES
         Bag.READER_CLASSES.add(realapi.Bag)
         Bag.WRITER_CLASSES.add(realapi.Bag)
     return success
 
 
 @memoize
 def calculate_definition_hash(typename, msgdef, extradefs=()):
@@ -341,14 +657,26 @@
                 subtype = scalartype if "/" in scalartype else "std_msgs/Header" \
                           if "Header" == scalartype else "%s/%s" % (pkg, scalartype)
                 typestr = calculate_definition_hash(subtype, subtypedefs[subtype], extradefs)
             lines.append("%s %s" % (typestr, namestr))
     return hashlib.md5("\n".join(lines).encode()).hexdigest()
 
 
+def canonical(typename, unbounded=False):
+    """
+    Returns "pkg/Type" for "pkg/subdir/Type", standardizes various ROS2 formats.
+
+    Converts ROS2 DDS types like "octet" to "byte", and "sequence<uint8, 100>" to "uint8[100]".
+
+    @param   unbounded  drop constraints like array bounds, and string bounds in ROS2,
+                        e.g. returning "uint8[]" for "uint8[10]"
+    """
+    return realapi.canonical(typename, unbounded)
+
+
 def create_publisher(topic, cls_or_typename, queue_size):
     """Returns a ROS publisher instance, with .get_num_connections() and .unregister()."""
     return realapi.create_publisher(topic, cls_or_typename, queue_size)
 
 
 def create_subscriber(topic, cls_or_typename, handler, queue_size):
     """
@@ -356,69 +684,106 @@
 
     Supplemented with .unregister(), .get_message_class(), .get_message_definition(),
     .get_message_type_hash(), and .get_qoses().
     """
     return realapi.create_subscriber(topic, cls_or_typename, handler, queue_size)
 
 
+def filter_fields(fieldmap, top=(), include=(), exclude=()):
+    """
+    Returns fieldmap filtered by include and exclude patterns.
+
+    @param   fieldmap   {field name: field type name}
+    @param   top        parent path as (rootattr, ..)
+    @param   include    [((nested, path), re.Pattern())] to require in parent path
+    @param   exclude    [((nested, path), re.Pattern())] to reject in parent path
+    """
+    NESTED_RGX = re.compile(".+/.+|" + "|".join("^%s$" % re.escape(x) for x in ROS_TIME_TYPES))
+    result = type(fieldmap)() if include or exclude else fieldmap
+    for k, v in fieldmap.items() if not result else ():
+        trailstr = ".".join(map(str, top + (k, )))
+        for is_exclude, patterns in enumerate((include, exclude)):
+            # Nested fields need filtering on deeper level
+            matches = not is_exclude and NESTED_RGX.match(v) \
+                      or any(r.match(trailstr) for _, r in patterns)
+            if patterns and (not matches if is_exclude else matches):
+                result[k] = v
+            elif patterns and is_exclude and matches:
+                result.pop(k, None)
+            if include and exclude and k not in result:  # Failing to include takes precedence
+                break  # for is_exclude
+    return result
+
+
 def format_message_value(msg, name, value):
     """
     Returns a message attribute value as string.
 
     Result is at least 10 chars wide if message is a ROS time/duration
     (aligning seconds and nanoseconds).
     """
     return realapi.format_message_value(msg, name, value)
 
 
 def get_message_class(typename):
-    """Returns ROS message class."""
+    """Returns ROS message class, or None if unavailable."""
     return realapi.get_message_class(typename)
 
 
-def get_message_data(msg):
-    """Returns ROS message as a serialized binary."""
-    return realapi.get_message_data(msg)
-
-
 def get_message_definition(msg_or_type):
-    """Returns ROS message type definition full text, including subtype definitions."""
+    """
+    Returns ROS message type definition full text, including subtype definitions.
+
+    Returns None if unknown type.
+    """
     return realapi.get_message_definition(msg_or_type)
 
 
 def get_message_type_hash(msg_or_type):
-    """Returns ROS message type MD5 hash."""
+    """Returns ROS message type MD5 hash, or "" if unknown type."""
     return realapi.get_message_type_hash(msg_or_type)
 
 
 def get_message_fields(val):
-    """Returns OrderedDict({field name: field type name}) if ROS message, else {}."""
+    """
+    Returns OrderedDict({field name: field type name}) if ROS message, else {}.
+
+    @param   val  ROS message class or instance
+    """
     return realapi.get_message_fields(val)
 
 
 def get_message_type(msg_or_cls):
     """Returns ROS message type name, like "std_msgs/Header"."""
     return realapi.get_message_type(msg_or_cls)
 
 
 def get_message_value(msg, name, typename):
     """Returns object attribute value, with numeric arrays converted to lists."""
     return realapi.get_message_value(msg, name, typename)
 
 
-def get_rostime():
-    """Returns current ROS time."""
-    return realapi.get_rostime()
+def get_rostime(fallback=False):
+    """
+    Returns current ROS time, as rospy.Time or rclpy.time.Time.
 
+    @param   fallback  use wall time if node not initialized
+    """
+    return realapi.get_rostime(fallback=fallback)
 
-def get_ros_time_category(typename):
-    """Returns "time" or "duration" for time/duration type, else typename."""
-    if typename in ROS_TIME_TYPES:
-        return "duration" if "duration" in typename.lower() else "time"
-    return typename
+
+def get_ros_time_category(msg_or_type):
+    """Returns "time" or "duration" for time/duration type or instance, else original argument."""
+    cls = msg_or_type if inspect.isclass(msg_or_type) else \
+          type(msg_or_type) if is_ros_message(msg_or_type) else None
+    if cls is None:
+        cls = next((x for x in ROS_TIME_CLASSES if get_message_type(x) == msg_or_type), None)
+    if cls in ROS_TIME_CLASSES:
+        return "duration" if "duration" in ROS_TIME_CLASSES[cls].lower() else "time"
+    return msg_or_type
 
 
 def get_topic_types():
     """
     Returns currently available ROS topics, as [(topicname, typename)].
 
     Omits topics that the current ROS node itself has published.
@@ -442,36 +807,40 @@
     In ROS1, byte and char are aliases for int8 and uint8; in ROS2 the reverse.
     """
     return ROS_ALIAS_TYPES.get(typename)
 
 
 def is_ros_message(val, ignore_time=False):
     """
-    Returns whether value is a ROS message or special like ROS time/duration.
+    Returns whether value is a ROS message or special like ROS time/duration class or instance.
 
-    @param  ignore_time  whether to ignore ROS time/duration types
+    @param   ignore_time  whether to ignore ROS time/duration types
     """
     return realapi.is_ros_message(val, ignore_time)
 
 
 def is_ros_time(val):
-    """Returns whether value is a ROS2 time/duration."""
+    """Returns whether value is a ROS time/duration class or instance."""
     return realapi.is_ros_time(val)
 
 
-def iter_message_fields(msg, messages_only=False, scalars=(), top=()):
+def iter_message_fields(msg, messages_only=False, scalars=(), include=(), exclude=(), top=()):
     """
     Yields ((nested, path), value, typename) from ROS message.
 
-    @param  messages_only  whether to yield only values that are ROS messages themselves
-                           or lists of ROS messages, else will yield scalar and list values
-    @param  scalars        sequence of ROS types to consider as scalars, like ("time", duration")
+    @param   messages_only  whether to yield only values that are ROS messages themselves
+                            or lists of ROS messages, else will yield scalar and list values
+    @param   scalars        sequence of ROS types to consider as scalars, like ("time", duration")
+    @param   include        [((nested, path), re.Pattern())] to require in field path, if any
+    @param   exclude        [((nested, path), re.Pattern())] to reject in field path, if any
+    @param   top            internal recursion helper
     """
     fieldmap = realapi.get_message_fields(msg)
-    if fieldmap is msg: return
+    if include or exclude: fieldmap = filter_fields(fieldmap, (), include, exclude)
+    if not fieldmap: return
     if messages_only:
         for k, t in fieldmap.items():
             v, scalart = realapi.get_message_value(msg, k, t), realapi.scalar(t)
             is_sublist = isinstance(v, (list, tuple)) and scalart not in ROS_COMMON_TYPES
             is_forced_scalar = get_ros_time_category(scalart) in scalars
             if not is_forced_scalar and realapi.is_ros_message(v):
                 for p2, v2, t2 in iter_message_fields(v, True, scalars, top=top + (k, )):
@@ -485,39 +854,72 @@
             if not is_forced_scalar and realapi.is_ros_message(v):
                 for p2, v2, t2 in iter_message_fields(v, False, scalars, top=top + (k, )):
                     yield p2, v2, t2
             else:
                 yield top + (k, ), v, t
 
 
+def make_full_typename(typename, category="msg"):
+    """
+    Returns "pkg/msg/Type" for "pkg/Type".
+
+    @param   category  type category like "msg" or "srv"
+    """
+    INTER, FAMILY = "/%s/" % category, "rospy" if ROS1 else "rclpy"
+    if INTER in typename or "/" not in typename or typename.startswith("%s/" % FAMILY):
+        return typename
+    return INTER.join(next((x[0], x[-1]) for x in [typename.split("/")]))
+
+
 def make_bag_time(stamp, bag):
     """
-    Returns timestamp string or datetime instance as ROS time.
+    Returns value as ROS timestamp, conditionally as relative to bag start/end time.
+
+    Stamp is interpreted as relative offset from bag start/end time
+    if numeric string with sign prefix, or timedelta, or ROS duration.
 
-    Stamp interpreted as delta from bag start/end time if numeric string with sign prefix.
+    @param   stamp   converted to ROS timestamp if int/float/str/duration/datetime/timedelta/decimal
+    @param   bag     an open bag to use for relative start/end time
     """
-    if isinstance(stamp, datetime.datetime):
-        stamp, shift = time.mktime(stamp.timetuple()) + stamp.microsecond / 1E6, 0
-    else:
-        stamp, sign = float(stamp), ("+" == stamp[0] if stamp[0] in "+-" else None)
+    shift = 0
+    if is_ros_time(stamp):
+        if "duration" != get_ros_time_category(stamp): return stamp
+        shift = bag.get_start_time() if stamp >= 0 else bag.get_end_time()
+    elif isinstance(stamp, datetime.datetime):
+        stamp = time.mktime(stamp.timetuple()) + stamp.microsecond / 1E6
+    elif isinstance(stamp, datetime.timedelta):
+        stamp = stamp.total_seconds()
+        shift = bag.get_start_time() if stamp >= 0 else bag.get_end_time()
+    elif isinstance(stamp, (six.binary_type, six.text_type)):
+        sign = stamp[0] in ("+", b"+") if six.text_type(stamp[0]) in "+-" else None
         shift = 0 if sign is None else bag.get_start_time() if sign else bag.get_end_time()
+        stamp = float(stamp)
     return make_time(stamp + shift)
 
 
 def make_live_time(stamp):
     """
-    Returns timestamp string or datetime instance as ROS time.
+    Returns value as ROS timestamp, conditionally as relative to system time.
 
-    Stamp interpreted as delta from system time if numeric string with sign prefix.
+    Stamp is interpreted as relative offset from system time
+    if numeric string with sign prefix, or timedelta, or ROS duration.
+
+    @param   stamp   converted to ROS timestamp if int/float/str/duration/datetime/timedelta/decimal
     """
-    if isinstance(stamp, datetime.datetime):
-        stamp, shift = time.mktime(stamp.timetuple()) + stamp.microsecond / 1E6, 0
-    else:
-        stamp, sign = float(stamp), ("+" == stamp[0] if stamp[0] in "+-" else None)
-        shift = 0 if sign is None else time.time()
+    shift = 0
+    if is_ros_time(stamp):
+        if "duration" != get_ros_time_category(stamp): return stamp
+        shift = time.time()
+    elif isinstance(stamp, datetime.datetime):
+        stamp = time.mktime(stamp.timetuple()) + stamp.microsecond / 1E6
+    elif isinstance(stamp, datetime.timedelta):
+        stamp, shift = stamp.total_seconds(), time.time()
+    elif isinstance(stamp, (six.binary_type, six.text_type)):
+        sign = stamp[0] in ("+", b"+") if six.text_type(stamp[0]) in "+-" else None
+        stamp, shift = float(stamp), (0 if sign is None else time.time())
     return make_time(stamp + shift)
 
 
 def make_duration(secs=0, nsecs=0):
     """Returns a ROS duration."""
     return realapi.make_duration(secs=secs, nsecs=nsecs)
 
@@ -563,34 +965,62 @@
     @param   replace  mapping of {value: replaced value},
                       e.g. {math.nan: None, math.inf: None}
     """
     result = {} if realapi.is_ros_message(msg) else msg
     for name, typename in realapi.get_message_fields(msg).items():
         v = realapi.get_message_value(msg, name, typename)
         if realapi.is_ros_time(v):
-            v = dict(zip(["secs", "nsecs"], realapi.to_sec_nsec(v)))
+            v = dict(zip(realapi.get_message_fields(v), realapi.to_sec_nsec(v)))
         elif realapi.is_ros_message(v):
             v = message_to_dict(v)
         elif isinstance(v, (list, tuple)):
             if realapi.scalar(typename) not in ROS_BUILTIN_TYPES:
                 v = [message_to_dict(x) for x in v]
             elif replace:
                 v = [replace.get(x, x) for x in v]
         elif replace:
             v = replace.get(v, v)
         result[name] = v
     return result
 
 
+def dict_to_message(dct, msg):
+    """
+    Returns given ROS message populated from Python dictionary.
+
+    Raises TypeError on attribute value type mismatch.
+    """
+    for name, typename in realapi.get_message_fields(msg).items():
+        if name not in dct:
+            continue  # for
+        v, msgv = dct[name], realapi.get_message_value(msg, name, typename)
+
+        if realapi.is_ros_message(msgv):
+            v = dict_to_message(v, msgv)
+        elif isinstance(msgv, (list, tuple)):
+            scalarname = realapi.scalar(typename)
+            if scalarname in ROS_BUILTIN_TYPES:
+                cls = ROS_BUILTIN_CTORS[scalarname]
+                v = [x if isinstance(x, cls) else cls(x) for x in v]
+            else:
+                cls = realapi.get_message_class(scalarname)
+                v = [dict_to_message(x, cls()) for x in v]
+        else:
+            v = type(msgv)(v)
+
+        setattr(msg, name, v)
+    return msg
+
+
 @memoize
 def parse_definition_fields(typename, typedef):
     """
     Returns field names and type names from a message definition text.
 
-    Dpes not recurse into subtypes.
+    Does not recurse into subtypes.
 
     @param   typename  ROS message type name, like "my_pkg/MyCls"
     @param   typedef   ROS message definition, like "Header header\nbool a\nMyCls2 b"
     @return            ordered {field name: type name},
                        like {"header": "std_msgs/Header", "a": "bool", "b": "my_pkg/MyCls2"}
     """
     result = collections.OrderedDict()  # {subtypename: subtypedef}
@@ -615,15 +1045,14 @@
 
 
 @memoize
 def parse_definition_subtypes(typedef, nesting=False):
     """
     Returns subtype names and type definitions from a full message definition.
 
-    @param   typename   message type name
     @param   typedef    message type definition including all subtype definitions
     @param   nesting    whether to additionally return type nesting information as
                         {typename: [typename contained in parent]}
     @return             {"pkg/MsgType": "full definition for MsgType including subtypes"}
                         or ({typedefs}, {nesting}) if nesting
     """
     result  = collections.OrderedDict()      # {subtypename: subtypedef}
@@ -661,14 +1090,24 @@
                     addendum = "%s\nMSG: %s\n%s" % ("=" * 80, fulltype, result[fulltype])
                     result[subtype] = result[subtype].rstrip() + ("\n\n%s\n" % addendum)
                     nesteds[subtype].append(fulltype)
                     seen.add(fulltype)
     return (result, nesteds) if nesting else result
 
 
+def serialize_message(msg):
+    """Returns ROS message as a serialized binary."""
+    return realapi.serialize_message(msg)
+
+
+def deserialize_message(msg, cls_or_typename):
+    """Returns ROS message or service request/response instantiated from serialized binary."""
+    return realapi.deserialize_message(msg, cls_or_typename)
+
+
 def scalar(typename):
     """
     Returns scalar type from ROS message data type, like "uint8" from uint8-array.
 
     Returns type unchanged if an ordinary type. In ROS2, returns unbounded type,
     e.g. "string" from "string<=10[<=5]".
     """
@@ -676,14 +1115,29 @@
 
 
 def set_message_value(obj, name, value):
     """Sets message or object attribute value."""
     realapi.set_message_value(obj, name, value)
 
 
+def time_message(val, to_message=True, clock_type=None):
+    """
+    Converts ROS2 time/duration between `rclpy` and `builtin_interfaces` objects.
+
+    Returns input value as-is in ROS1.
+
+    @param   val         ROS2 time/duration object from `rclpy` or `builtin_interfaces`
+    @param   to_message  whether to convert from `rclpy` to `builtin_interfaces` or vice versa
+    @param   clock_type  ClockType for converting to `rclpy.Time`, defaults to `ROS_TIME`
+    @return              value converted to appropriate type, or original value if not convertible
+    """
+    if ROS1: return val
+    return realapi.time_message(val, to_message, clock_type=clock_type)
+
+
 def to_datetime(val):
     """Returns value as datetime.datetime if value is ROS time/duration, else value."""
     sec = realapi.to_sec(val)
     return datetime.datetime.fromtimestamp(sec) if sec is not val else val
 
 
 def to_decimal(val):
@@ -702,7 +1156,29 @@
     """Returns value in seconds if value is ROS time/duration, else value."""
     return realapi.to_sec(val)
 
 
 def to_sec_nsec(val):
     """Returns value as (seconds, nanoseconds) if value is ROS time/duration, else value."""
     return realapi.to_sec_nsec(val)
+
+
+def to_time(val):
+    """Returns value as ROS time if convertible (int/float/duration/datetime/decimal), else value."""
+    return realapi.to_time(val)
+
+
+__all___ = [
+    "BAG_EXTENSIONS", "NODE_NAME", "ROS_ALIAS_TYPES", "ROS_BUILTIN_CTORS", "ROS_BUILTIN_TYPES",
+    "ROS_COMMON_TYPES", "ROS_FAMILY", "ROS_NUMERIC_TYPES", "ROS_STRING_TYPES", "ROS_TIME_CLASSES",
+    "ROS_TIME_TYPES", "SKIP_EXTENSIONS", "Bag", "BaseBag", "TypeMeta",
+    "calculate_definition_hash", "canonical", "create_publisher", "create_subscriber",
+    "deserialize_message", "dict_to_message", "filter_fields", "format_message_value",
+    "get_alias_type", "get_message_class", "get_message_definition", "get_message_fields",
+    "get_message_type", "get_message_type_hash", "get_message_value", "get_ros_time_category",
+    "get_rostime", "get_topic_types", "get_type_alias", "init_node", "is_ros_message",
+    "is_ros_time", "iter_message_fields", "make_bag_time", "make_duration", "make_live_time",
+    "make_message_hash", "make_time", "message_to_dict", "parse_definition_fields",
+    "parse_definition_subtypes", "scalar", "deserialize_message", "set_message_value",
+    "shutdown_node", "time_message", "to_datetime", "to_decimal", "to_nsec", "to_sec",
+    "to_sec_nsec", "to_time", "validate",
+]
```

## Comparing `grepros-0.5.0.dist-info/LICENSE.md` & `grepros-1.0.0.dist-info/LICENSE.md`

 * *Files identical despite different names*

## Comparing `grepros-0.5.0.dist-info/METADATA` & `grepros-1.0.0.dist-info/METADATA`

 * *Files 16% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 Metadata-Version: 2.1
 Name: grepros
-Version: 0.5.0
-Summary: grep for ROS bag files and live topics
+Version: 1.0.0
+Summary: grep for ROS bag files and live topics: read, filter, export
 Home-page: https://github.com/suurjaak/grepros
 Author: Erki Suurjaak
 Author-email: erki@lap.ee
 License: BSD
 Keywords: ROS ROS1 ROS2 rosbag grep
 Platform: any
 Classifier: Development Status :: 4 - Beta
@@ -17,70 +17,53 @@
 Classifier: Topic :: Scientific/Engineering
 Classifier: Topic :: Utilities
 Classifier: Programming Language :: Python :: 2
 Classifier: Programming Language :: Python :: 2.7
 Classifier: Programming Language :: Python :: 3
 Requires-Python: >=2.7
 Description-Content-Type: text/markdown
+License-File: LICENSE.md
 Requires-Dist: pyyaml
+Requires-Dist: six
 
 grepros
 =======
 
-grep for ROS bag files and live topics.
+grep for ROS bag files and live topics: read, filter, export.
 
 Searches through ROS messages and matches any message field value by regular
 expression patterns or plain text, regardless of field type.
 Can also look for specific values in specific message fields only.
 
 By default, matches are printed to console. Additionally, matches can be written
 to a bagfile or HTML/CSV/MCAP/Parquet/Postgres/SQL/SQLite, or published to live topics.
 
 Supports both ROS1 and ROS2. ROS environment variables need to be set, at least `ROS_VERSION`.
 
+Supported bag formats: `.bag` (ROS1), `.db3` (ROS2), `.mcap` (ROS1, ROS2).
+
 In ROS1, messages can be grepped even if Python packages for message types are not installed.
 Using ROS1 live topics requires ROS master to be running.
 
 Using ROS2 requires Python packages for message types to be available in path.
 
 Supports loading custom plugins, mainly for additional output formats.
 
-[![Screenshot](https://raw.githubusercontent.com/suurjaak/grepros/gh-pages/img/th_screen.png)](https://raw.githubusercontent.com/suurjaak/grepros/gh-pages/img/screen.png)
+Usable as a Python library, see doc/LIBRARY.md.
+Full API documentation available at https://suurjaak.github.io/grepros.
 
-API documentation available at https://suurjaak.github.io/grepros.
+[![Screenshot](https://raw.githubusercontent.com/suurjaak/grepros/gh-pages/img/th_screen.png)](https://raw.githubusercontent.com/suurjaak/grepros/gh-pages/img/screen.png)
 
 
 - Example usage
 - Installation
-  - Using pip
-  - Using apt
-  - Using catkin
-  - Using colcon
 - Inputs
-  - bag
-  - live
 - Outputs
-  - console
-  - bag
-  - live
-  - csv
-  - html
-  - postgres
-  - sqlite
-  - console / html message formatting
 - Matching and filtering
-  - Limits
-  - Filtering
-  - Conditions
 - Plugins
-  - embag
-  - mcap
-  - parquet
-  - sql
-- SQL dialects
 - Notes on ROS1 vs ROS2
 - All command-line arguments
 - Dependencies
 - Attribution
 - License
 
 
@@ -114,20 +97,20 @@
     grepros key=0xA002 --topic *diagnostics --path /tmp
 
 Find diagnostics_msgs messages in bags in current directory,
 containing "navigation" in fields "name" or "message",
 print only header stamp and values:
 
     grepros --type diagnostic_msgs/* --select-field name message \
-            --print-field header.stamp status.values -- navigation
+            --emit-field header.stamp status.values -- navigation
 
-Print first message from each lidar topic on host 1.2.3.4:
+Print first message from each lidar topic on host 1.2.3.4, without highlight:
 
     ROS_MASTER_URI=http://1.2.3.4::11311 \
-    grepros --live --topic *lidar* --max-per-topic 1
+    grepros --live --topic *lidar* --max-per-topic 1 --no-highlight
 
 Export all bag messages to SQLite and Postgres, print only export progress:
 
     grepros -n my.bag --write my.bag.sqlite --no-console-output --no-verbose --progress
 
     grepros -n my.bag --write postgresql://user@host/dbname \
             --no-console-output --no-verbose --progress
@@ -145,17 +128,16 @@
 
 
 Installation
 ------------
 
 grepros is written in Python, supporting both Python 2 and Python 3.
 
-Developed and tested under ROS1 noetic and ROS2 foxy,
-it should also work in later ROS2 versions;
-it _may_ work in earlier ROS1 versions. 
+Developed and tested under ROS1 Noetic and ROS2 Foxy,
+should also work in later ROS2 versions; _may_ work in earlier ROS1 versions.
 
 
 ### Using pip
 
     pip install grepros
 
 This will add the `grepros` command to path.
@@ -208,14 +190,16 @@
 
 Input is either from one or more ROS bag files (default), or from live ROS topics.
 
 ### bag
 
 Read messages from ROS bag files, by default all in current directory.
 
+For reading bags in MCAP format, see the doc/DETAIL.md#mcap.
+
 Recurse into subdirectories when looking for bagfiles:
 
     -r
     --recursive
 
 Read specific filenames (supports * wildcards):
 
@@ -242,14 +226,31 @@
 
 Order bag messages first by topic or type, and only then by time:
 
     --order-bag-by topic
     --order-bag-by type
 
 
+### live
+
+    --live
+
+Read messages from live ROS topics instead of bagfiles.
+
+Requires `ROS_MASTER_URI` and `ROS_ROOT` to be set in environment if ROS1.
+
+Set custom queue size for subscribing (default 10):
+
+    --queue-size-in 100
+
+Use ROS time instead of system time for incoming message timestamps:
+
+    --ros-time-in
+
+
 Outputs
 -------
 
 There can be any number of outputs: printing to console (default),
 publishing to live ROS topics, or writing to file or database.
 
 ### console
@@ -270,316 +271,86 @@
 accept raw control characters (`more -f` or `less -R`).
 
 
 ### bag
 
     --write path/to/my.bag [format=bag] [overwrite=true|false]
 
-Write messages to a ROS bag file, the custom `.bag` format in ROS1
+Write messages to a ROS bag file, the custom `.bag` format in ROS1,
 or the `.db3` SQLite database format in ROS2. If the bagfile already exists,
 it is appended to, unless specified to overwrite.
 
 Specifying `format=bag` is not required
 if the filename ends with `.bag` in ROS1 or `.db3` in ROS2.
 
+For writing bags in MCAP format, see the doc/DETAIL.md#mcap.
 
-### live
 
-    --live
+### live
 
-Read messages from live ROS topics instead of bagfiles.
+    --publish
 
-Requires `ROS_MASTER_URI` and `ROS_ROOT` to be set in environment if ROS1.
+Publish messages to live ROS topics. Topic prefix and suffix can be changed,
+or topic name set to one specific name:
 
-Set custom queue size for subscribing (default 10):
+    --publish-prefix  /myroot
+    --publish-suffix  /myend
+    --publish-fixname /my/singular/name
 
-    --queue-size-in 100
+One of the above arguments needs to be specified if publishing to live ROS topics
+while grepping from live ROS topics, to avoid endless loops.
 
-Use ROS time instead of system time for incoming message timestamps:
+Set custom queue size for publishers (default 10):
 
-    --ros-time-in
+    --queue-size-out 100
 
 
 ### csv
 
     --write path/to/my.csv [format=csv] [overwrite=true|false]
 
 Write messages to CSV files, each topic to a separate file, named
 `path/to/my.full__topic__name.csv` for `/full/topic/name`.
 
-Output mimicks CSVs compatible with PlotJuggler, all messages values flattened
-to a single list, with header fields like `/topic/field.subfield.listsubfield.0.data.1`.
-
-If a file already exists, a unique counter is appended to the name of the new file,
-e.g. `my.full__topic__name.2.csv`, unless specified to overwrite.
-
-Specifying `format=csv` is not required if the filename ends with `.csv`.
-
 
 ### html
 
     --write path/to/my.html [format=html] [overwrite=true|false]
             [template=/path/to/html.template]
 
 Write messages to an HTML file, with a linked table of contents,
 message timeline, message type definitions, and a topically traversable message list.
 
 [![Screenshot](https://raw.githubusercontent.com/suurjaak/grepros/gh-pages/img/th_screen_html.png)](https://raw.githubusercontent.com/suurjaak/grepros/gh-pages/img/screen_html.png)
 
-Note: resulting file may be large, and take a long time to open in browser.
-
-If the file already exists, a unique counter is appended to the name of the new file,
-e.g. `my.2.html`, unless specified to overwrite.
-
-Specifying `format=html` is not required if the filename ends with `.htm` or `.html`.
-
-A custom template file can be specified, in [step](https://github.com/dotpy/step) syntax:
-
-    --write path/to/my.html template=/my/html.template
-
 
 ### postgres
 
     --write postgresql://username@host/dbname [format=postgres]
             [commit-interval=NUM] [nesting=array|all]
             [dialect-file=path/to/dialects.yaml]
 
 Write messages to a Postgres database, with tables `pkg/MsgType` for each ROS message type,
 and views `/full/topic/name` for each topic.
-Plus table `topics` with a list of topics, `types` with message types and definitions,
-and `meta` with table/view/column name changes from shortenings and conflicts, if any
-(Postgres name length is limited to 63 characters).
-
-ROS primitive types are inserted as Postgres data types (time/duration types as NUMERIC),
-uint8[] arrays as BYTEA, other primitive arrays as ARRAY, and arrays of subtypes as JSONB.
-
-If the database already exists, it is appended to. If there are conflicting names
-(same package and name but different message type definition),
-table/view name becomes "name (MD5 hash of type definition)".
-
-Specifying `format=postgres` is not required if the parameter uses the
-Postgres URI scheme `postgresql://`.
-
-Requires [psycopg2](https://pypi.org/project/psycopg2).
-
-Parameter `--write` can also use the Postgres keyword=value format,
-e.g. `"host=localhost port=5432 dbname=mydb username=postgres connect_timeout=10"`.
-
-Standard Postgres environment variables are also supported (PGPASSWORD et al).
 
 [![Screenshot](https://raw.githubusercontent.com/suurjaak/grepros/gh-pages/img/th_screen_postgres.png)](https://raw.githubusercontent.com/suurjaak/grepros/gh-pages/img/screen_postgres.png)
 
-A custom transaction size can be specified (default is 1000; 0 is autocommit):
-
-    --write postgresql://username@host/dbname commit-interval=NUM
-
-Updates to Postgres SQL dialect can be loaded from a YAML or JSON file:
-
-    --write postgresql://username@host/dbname dialect-file=path/to/dialects.yaml
-
-More on SQL dialects.
-
-
-#### Nested messages
-
-Nested message types can be recursively populated to separate tables, linked
-to parent messages via foreign keys.
-
-To recursively populate nested array fields:
-
-    --write postgresql://username@host/dbname nesting=array
-
-E.g. for `diagnostic_msgs/DiagnosticArray`, this would populate the following tables:
-
-```sql
-CREATE TABLE "diagnostic_msgs/DiagnosticArray" (
-  "header.seq"          BIGINT,
-  "header.stamp.secs"   INTEGER,
-  "header.stamp.nsecs"  INTEGER,
-  "header.frame_id"     TEXT,
-  status                JSONB,       -- [_id from "diagnostic_msgs/DiagnosticStatus", ]
-  _topic                TEXT,
-  _timestamp            NUMERIC,
-  _id                   BIGSERIAL,
-  _parent_type          TEXT,
-  _parent_id            BIGINT
-);
-
-CREATE TABLE "diagnostic_msgs/DiagnosticStatus" (
-  level                 SMALLINT,
-  name                  TEXT,
-  message               TEXT,
-  hardware_id           TEXT,
-  "values"              JSONB,       -- [_id from "diagnostic_msgs/KeyValue", ]
-  _topic                TEXT,        -- _topic from "diagnostic_msgs/DiagnosticArray"
-  _timestamp            NUMERIC,     -- _timestamp from "diagnostic_msgs/DiagnosticArray"
-  _id                   BIGSERIAL,
-  _parent_type          TEXT,        -- "diagnostic_msgs/DiagnosticArray"
-  _parent_id            BIGINT       -- _id from "diagnostic_msgs/DiagnosticArray"
-);
-
-CREATE TABLE "diagnostic_msgs/KeyValue" (
-  "key"                 TEXT,
-  value                 TEXT,
-  _topic                TEXT,        -- _topic from "diagnostic_msgs/DiagnosticStatus"
-  _timestamp            NUMERIC,     -- _timestamp from "diagnostic_msgs/DiagnosticStatus"
-  _id                   BIGSERIAL,
-  _parent_type          TEXT,        -- "diagnostic_msgs/DiagnosticStatus"
-  _parent_id            BIGINT       -- _id from "diagnostic_msgs/DiagnosticStatus"
-);
-```
-
-Without nesting, array field values are inserted as JSON with full subtype content.
-
-To recursively populate all nested message types:
-
-    --write postgresql://username@host/dbname nesting=all
-
-E.g. for `diagnostic_msgs/DiagnosticArray`, this would, in addition to the above, populate:
-
-```sql
-CREATE TABLE "std_msgs/Header" (
-  seq                   BIGINT,
-  "stamp.secs"          INTEGER,
-  "stamp.nsecs"         INTEGER,
-  frame_id              TEXT,
-  _topic                TEXT,       -- _topic from "diagnostic_msgs/DiagnosticArray"
-  _timestamp            NUMERIC,    -- _timestamp from "diagnostic_msgs/DiagnosticArray"
-  _id                   BIGSERIAL,
-  _parent_type          TEXT,       -- "diagnostic_msgs/DiagnosticArray"
-  _parent_id            BIGINT      -- _id from "diagnostic_msgs/DiagnosticArray"
-);
-```
 
 ### sqlite
 
     --write path/to/my.sqlite [format=sqlite] [overwrite=true|false]
             [commit-interval=NUM] [message-yaml=true|false] [nesting=array|all]
             [dialect-file=path/to/dialects.yaml]
 
 Write an SQLite database with tables `pkg/MsgType` for each ROS message type
 and nested type, and views `/full/topic/name` for each topic.
-If the database already exists, it is appended to, unless specified to overwrite.
-
-Output is compatible with ROS2 `.db3` bagfiles, supplemented with
-full message YAMLs, and message type definition texts. Note that a database
-dumped from a ROS1 source will most probably not be usable as a ROS2 bag,
-due to breaking changes in ROS2 standard built-in types and message types.
-
-Specifying `format=sqlite` is not required
-if the filename ends with `.sqlite` or `.sqlite3`.
 
 [![Screenshot](https://raw.githubusercontent.com/suurjaak/grepros/gh-pages/img/th_screen_sqlite.png)](https://raw.githubusercontent.com/suurjaak/grepros/gh-pages/img/screen_sqlite.png)
 
-A custom transaction size can be specified (default is 1000; 0 is autocommit):
-
-    --write path/to/my.sqlite commit-interval=NUM
-
-By default, table `messages` is populated with full message YAMLs, unless:
-
-    --write path/to/my.sqlite message-yaml=false
-
-Updates to SQLite SQL dialect can be loaded from a YAML or JSON file:
-
-    --write path/to/my.sqlite dialect-file=path/to/dialects.yaml
-
-More on SQL dialects.
-
-
-#### Nested messages
-
-Nested message types can be recursively populated to separate tables, linked
-to parent messages via foreign keys.
-
-To recursively populate nested array fields:
-
-    --write path/to/my.sqlite nesting=array
-
-E.g. for `diagnostic_msgs/DiagnosticArray`, this would populate the following tables:
-
-```sql
-CREATE TABLE "diagnostic_msgs/DiagnosticArray" (
-  "header.seq"          INTEGER,
-  "header.stamp.secs"   INTEGER,
-  "header.stamp.nsecs"  INTEGER,
-  "header.frame_id"     TEXT,
-  -- [_id from "diagnostic_msgs/DiagnosticStatus", ]
-  status                "DIAGNOSTIC_MSGS/DIAGNOSTICSTATUS[]",
-  _topic                TEXT,
-  _timestamp            INTEGER,
-  _id                   INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,
-  _parent_type          TEXT,
-  _parent_id            INTEGER
-);
-
-CREATE TABLE "diagnostic_msgs/DiagnosticStatus" (
-  level                 SMALLINT,
-  name                  TEXT,
-  message               TEXT,
-  hardware_id           TEXT,
-  -- [_id from "diagnostic_msgs/KeyValue", ]
-  "values"              "DIAGNOSTIC_MSGS/KEYVALUE[]",
-  _topic                TEXT,        -- _topic from "diagnostic_msgs/DiagnosticArray"
-  _timestamp            INTEGER,     -- _timestamp from "diagnostic_msgs/DiagnosticArray"
-  _id                   INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,
-  _parent_type          TEXT,        -- "diagnostic_msgs/DiagnosticArray"
-  _parent_id            INTEGER      -- _id from "diagnostic_msgs/DiagnosticArray"
-);
-
-CREATE TABLE "diagnostic_msgs/KeyValue" (
-  "key"                 TEXT,
-  value                 TEXT,
-  _topic                TEXT,        -- _topic from "diagnostic_msgs/DiagnosticStatus"
-  _timestamp            INTEGER,     -- _timestamp from "diagnostic_msgs/DiagnosticStatus"
-  _id                   INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,
-  _parent_type          TEXT,        -- "diagnostic_msgs/DiagnosticStatus"
-  _parent_id            INTEGER      -- _id from "diagnostic_msgs/DiagnosticStatus"
-);
-```
-
-Without nesting, array field values are inserted as JSON with full subtype content.
-
-To recursively populate all nested message types:
-
-    --write path/to/my.sqlite nesting=all
-
-E.g. for `diagnostic_msgs/DiagnosticArray`, this would, in addition to the above, populate:
-
-```sql
-CREATE TABLE "std_msgs/Header" (
-  seq                   UINT32,
-  "stamp.secs"          INT32,
-  "stamp.nsecs"         INT32,
-  frame_id              TEXT,
-  _topic                STRING,     -- _topic from "diagnostic_msgs/DiagnosticArray"
-  _timestamp            INTEGER,    -- _timestamp from "diagnostic_msgs/DiagnosticArray"
-  _id                   INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,
-  _parent_type          TEXT,       -- "diagnostic_msgs/DiagnosticArray"
-  _parent_id            INTEGER     -- _id from "diagnostic_msgs/DiagnosticArray"
-);
-```
-
-
-### live
-
-    --publish
-
-Publish messages to live ROS topics. Topic prefix and suffix can be changed,
-or topic name set to one specific name:
-
-    --publish-prefix  /myroot
-    --publish-suffix  /myend
-    --publish-fixname /my/singular/name
-
-One of the above arguments needs to be specified if publishing to live ROS topics
-while grepping from live ROS topics, to avoid endless loops.
-
-Set custom queue size for publishers (default 10):
-
-    --queue-size-out 100
+More on outputs in doc/DETAIL.md#outputs.
 
 
 ### console / html message formatting
 
 Set maximum number of lines to output per message:
 
     --lines-per-message 5
@@ -599,19 +370,19 @@
 
 Output only matched fields and specified number of lines around match:
 
     --lines-around-match 5
 
 Output only specific message fields (supports nested.paths and * wildcards):
 
-    --print-field *data
+    --emit-field *data
 
 Skip outputting specific message fields (supports nested.paths and * wildcards):
 
-    --no-print-field header.stamp
+    --no-emit-field header.stamp
 
 Wrap matches in custom texts:
 
     --match-wrapper @@@
     --match-wrapper "<<<<" ">>>>"
 
 Set custom width for wrapping message YAML printed to console (auto-detected from terminal by default):
@@ -656,45 +427,45 @@
 ### Limits
 
 Stop after matching a specified number of messages (per each file if bag input):
 
     -m          100
     --max-count 100
 
-Scan only a specified number of topics (per each file if bag input):
+Read only a specified number of topics (per each file if bag input):
 
     --max-topics 10
 
 Emit a specified number of matches per topic (per each file if bag input):
 
     --max-per-topic 20
 
 Emit every Nth match in topic:
 
     --every-nth-match 10  # (skips 9 matches in topic after each match emitted)
 
 
 ### Filtering
 
-Scan specific topics only (supports * wildcards):
+Read specific topics only (supports * wildcards):
 
     -t      *lidar* *ins*
     --topic /robot/sensors/*
 
 Skip specific topics (supports * wildcards):
 
     -nt        *lidar* *ins*
     --no-topic /robot/sensors/*
 
-Scan specific message types only (supports * wildcards):
+Read specific message types only (supports * wildcards):
 
     -d     *Twist*
     --type sensor_msgs/*
 
-Skip specific message types from scanning (supports * wildcards):
+Skip specific message types from reading (supports * wildcards):
 
     -nd       *Twist*
     --no-type sensor_msgs/*
 
 Set specific message fields to scan (supports nested.paths and * wildcards):
 
     -sf            twist.linear
@@ -706,43 +477,43 @@
     --no-select-field *data
 
 Only emit matches that are unique in topic,
 taking `--select-field` and `--no-select-field` into account (per each file if bag input):
 
     --unique-only
 
-Start scanning from a specific timestamp:
+Start reading from a specific timestamp:
 
     -t0          2021-11     # (using partial ISO datetime)
     --start-time 1636900000  # (using UNIX timestamp)
     --start-time +100        # (seconds from bag start time, or from script startup time if live input)
     --start-time -100        # (seconds from bag end time, or script startup time if live input)
 
-Stop scanning at a specific timestamp:
+Stop reading at a specific timestamp:
 
     -t1        2021-11     # (using partial ISO datetime)
     --end-time 1636900000  # (using UNIX timestamp)
     --end-time +100        # (seconds from bag start time, or from script startup time if live input)
     --end-time -100        # (seconds from bag end time, or from script startup time if live input)
 
-Start scanning from a specific message index in topic:
+Start reading from a specific message index in topic:
 
     -n0           -100  # (counts back from topic total message count in bag)
     --start-index   10  # (1-based index)
 
-Stop scanning at a specific message index in topic:
+Stop reading at a specific message index in topic:
 
     -n1         -100  # (counts back from topic total message count in bag)
     --end-index   10  # (1-based index)
 
-Scan every Nth message in topic:
+Read every Nth message in topic:
 
     --every-nth-message 10  # (skips 9 messages in topic with each step)
 
-Scan messages in topic with timestamps at least N seconds apart:
+Read messages in topic with timestamps at least N seconds apart:
 
     --every-nth-interval 5  # (samples topic messages no more often than every 5 seconds)
 
 
 ## Conditions
 
     --condition "PYTHON EXPRESSION"
@@ -785,28 +556,14 @@
 
 Plugins
 -------
 
     --plugin some.python.module some.other.module.Class
 
 Load one or more Python modules or classes as plugins.
-Supported (but not required) plugin interface methods:
-
-- `init(args)`: invoked at startup with command-line arguments
-- `load(category, args)`: invoked with category "search" or "source" or "sink",
-                          using returned value for specified component if not None
-
-Plugins are free to modify `grepros` internals, like adding command-line arguments
-to `grepros.main.ARGUMENTS` or adding sink types to `grepros.outputs.MultiSink`.
-
-Convenience methods:
-
-- `plugins.add_write_format(name, cls, label=None, options=())`:
-   adds an output plugin to defaults
-- `plugins.get_argument(name)`: returns a command-line argument dictionary, or None
 
 Specifying `--plugin someplugin` and `--help` will include plugin options in printed help.
 
 Built-in plugins:
 
 ### embag
 
@@ -819,161 +576,103 @@
 
 ### mcap
 
     --plugin grepros.plugins.mcap
 
 Read or write messages in [MCAP](https://mcap.dev) format.
 
-Requires [mcap](https://pypi.org/project/mcap), and
-[mcap_ros1_support](https://pypi.org/project/mcap-ros1-support) for ROS1
-or [mcap_ros2_support](https://pypi.org/project/mcap-ros2-support) for ROS2.
-
-In ROS2, messages grepped from MCAP files can only be published to live topics
-if the same message type packages are locally installed.
-
-Write bags in MCAP format:
-
-    --plugin grepros.plugins.mcap \
-    --write path/to/my.mcap [format=mcap] [overwrite=true|false]
-
-If the file already exists, a unique counter is appended to the name of the new file,
-e.g. `my.2.mcap`, unless specified to overwrite.
-
-Specifying write `format=mcap` is not required if the filename ends with `.mcap`.
-
 
 ### parquet
 
     --plugin grepros.plugins.parquet \
     --write path/to/my.parquet [format=parquet] [overwrite=true|false] \
             [column-name=rostype:value] [type-rostype=arrowtype] \
-            [writer-argname=argvalue]
+            [idgenerator=callable] [nesting=array|all] [writer-argname=argvalue]
 
 Write messages to Apache Parquet files (columnar storage format, version 2.6),
-each message type to a separate file, named `path/to/package__MessageType__typehash/my.parquet`
-for `package/MessageType` (typehash is message type definition MD5 hashsum).
-Adds fields `_topic string()` and `_timestamp timestamp("ns")` to each type.
-
-If a file already exists, a unique counter is appended to the name of the new file,
-e.g. `package__MessageType__typehash/my.2.parquet`, unless specified to overwrite.
+each message type to a separate file.
 
-Specifying `format=parquet` is not required if the filename ends with `.parquet`.
-
-Requires [pandas](https://pypi.org/project/pandas) and [pyarrow](https://pypi.org/project/pyarrow).
-
-Supports adding supplementary columns with fixed values to Parquet files:
-
-    --write path/to/my.parquet column-bag_hash=string:26dfba2c
-
-Supports custom mapping between ROS and pyarrow types with `type-rostype=arrowtype`:
+To recursively populate nested array fields:
 
-    --write path/to/my.parquet type-time="timestamp('ns')"
-    --write path/to/my.parquet type-uint8[]="list(uint8())"
+    --write path/to/my.parquet nesting=array
 
-Time/duration types are flattened into separate integer columns `secs` and `nsecs`,
-unless they are mapped to pyarrow types explicitly, like:
+E.g. for `diagnostic_msgs/DiagnosticArray`, this would populate files with following schemas:
 
-    --write path/to/my.parquet type-time="timestamp('ns')" type-duration="duration('ns')"
+```python
+diagnostic_msgs__DiagnosticArray = pyarrow.schema([
+  ("header.seq",          pyarrow.int64()),
+  ("header.stamp.secs",   pyarrow.int32()),
+  ("header.stamp.nsecs",  pyarrow.int32()),
+  ("header.frame_id",     pyarrow.string()),
+  ("status",              pyarrow.string()),   # [_id from "diagnostic_msgs/DiagnosticStatus", ]
+  ("_topic",              pyarrow.string()),
+  ("_timestamp",          pyarrow.int64()),
+  ("_id",                 pyarrow.string()),
+  ("_parent_type",        pyarrow.string()),
+  ("_parent_id",          pyarrow.string()),
+])
+
+diagnostic_msgs__DiagnosticStatus = pyarrow.schema([
+  ("level",               pyarrow.int16()),
+  ("name",                pyarrow.string()),
+  ("message",             pyarrow.string()),
+  ("hardware_id",         pyarrow.string()),
+  ("values"",             pyarrow.string()),   # [_id from "diagnostic_msgs/KeyValue", ]
+  ("_topic",              pyarrow.string()),   # _topic from "diagnostic_msgs/DiagnosticArray"
+  ("_timestamp",          pyarrow.int64()),    # _timestamp from "diagnostic_msgs/DiagnosticArray"
+  ("_id",                 pyarrow.string()),
+  ("_parent_type",        pyarrow.string()),   # "diagnostic_msgs/DiagnosticArray"
+  ("_parent_id",          pyarrow.string()),   # _id from "diagnostic_msgs/DiagnosticArray"
+])
+
+diagnostic_msgs__KeyValue = pyarrow.schema([
+  ("key"                  pyarrow.string()),
+  ("value",               pyarrow.string()),
+  ("_topic",              pyarrow.string()),   # _topic from "diagnostic_msgs/DiagnosticStatus"
+  ("_timestamp",          pyarrow.int64()),    # _timestamp from "diagnostic_msgs/DiagnosticStatus"
+  ("_id",                 pyarrow.string()),
+  ("_parent_type",        pyarrow.string()),   # "diagnostic_msgs/DiagnosticStatus"
+  ("_parent_id",          pyarrow.string()),   # _id from "diagnostic_msgs/DiagnosticStatus"
+])
+```
 
-Supports additional arguments given to [pyarrow.parquet.ParquetWriter](
-https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetWriter.html), as:
+Without nesting, array field values are inserted as JSON with full subtype content.
 
-    --write path/to/my.parquet writer-argname=argvalue
+To recursively populate all nested message types:
 
-For example, specifying no compression:
+    --write path/to/my.parquet nesting=all
 
-    --write path/to/my.parquet writer-compression=null
+E.g. for `diagnostic_msgs/DiagnosticArray`, this would, in addition to the above, populate:
 
-The value is interpreted as JSON if possible, e.g. `writer-use_dictionary=false`.
+```python
+std_msgs__Header = pyarrow.schema([
+  "seq",                  pyarrow.int64()),
+  "stamp.secs",           pyarrow.int32()),
+  "stamp.nsecs",          pyarrow.int32()),
+  "frame_id",             pyarrow.string()),
+  "_topic",               pyarrow.string()),   # _topic from "diagnostic_msgs/DiagnosticArray"
+  "_timestamp",           pyarrow.int64()),    # _timestamp from "diagnostic_msgs/DiagnosticArray"
+  "_id",                  pyarrow.string()),
+  "_parent_type",         pyarrow.string()),   # "diagnostic_msgs/DiagnosticArray"
+  "_parent_id",           pyarrow.string()),   # _id from "diagnostic_msgs/DiagnosticArray"
+])
+```
 
 
 ### sql
 
     --plugin grepros.plugins.sql \
     --write path/to/my.sql [format=sql] [overwrite=true|false] \
             [nesting=array|all] [dialect=clickhouse|postgres|sqlite] \
             [dialect-file=path/to/dialects.yaml]
 
 Write SQL schema to output file, CREATE TABLE for each message type
 and CREATE VIEW for each topic.
 
-If the file already exists, a unique counter is appended to the name of the new file,
-e.g. `my.2.sql`, unless specified to overwrite.
-
-Specifying `format=sql` is not required if the filename ends with `.sql`.
-
-To create tables for nested array message type fields:
-
-    --write path/to/my.sql nesting=array
-
-To create tables for all nested message types:
-
-    --write path/to/my.sql nesting=all
-
-A specific SQL dialect can be specified:
-
-    --write path/to/my.sql dialect=clickhouse|postgres|sqlite
-
-Additional dialects, or updates for existing dialects, can be loaded from a YAML or JSON file:
-
-    --write path/to/my.sql dialect=mydialect dialect-file=path/to/dialects.yaml
-
-
-SQL dialects
-------------
-
-Postgres, SQLite and SQL outputs support loading additional options for SQL dialect.
-
-Dialect file format:
-
-```yaml
-dialectname:
-  table_template:       CREATE TABLE template; args: table, cols, type, hash, package, class
-  view_template:        CREATE VIEW template; args: view, cols, table, topic, type, hash, package, class
-  table_name_template:  message type table name template; args: type, hash, package, class
-  view_name_template:   topic view name template; args: topic, type, hash, package, class
-  types:                Mapping between ROS and SQL common types for table columns,
-                        e.g. {"uint8": "SMALLINT", "uint8[]": "BYTEA", ..}
-  adapters:             Mapping between ROS types and callable converters for table columns,
-                        e.g. {"time": "decimal.Decimal"}
-  defaulttype:          Fallback SQL type if no mapped type for ROS type;
-                        if no mapped and no default type, column type will be ROS type as-is
-  arraytype_template:   Array type template; args: type
-  maxlen_entity:        Maximum table/view name length, 0 disables
-  maxlen_column:        Maximum column name length, 0 disables
-  invalid_char_regex:   Regex for matching invalid characters in name, if any
-  invalid_char_repl:    Replacement for invalid characters in name
-```
-
-Template parameters like `table_name_template` use Python `str.format()` keyword syntax,
-e.g. `{"table_name_template": "{type}", "view_name_template": "{topic}"}`.
-
-Time/duration types are flattened into separate integer columns `secs` and `nsecs`,
-unless the dialect maps them to SQL types explicitly, e.g. `{"time": "BIGINT"}`.
-
-Any dialect options not specified in the given dialect or built-in dialects,
-will be taken from the default dialect configuration:
-
-```yaml
-  table_template:       'CREATE TABLE IF NOT EXISTS {table} ({cols});'
-  view_template:        'DROP VIEW IF EXISTS {view};
-                         CREATE VIEW {view} AS
-                         SELECT {cols}
-                         FROM {table}
-                         WHERE _topic = {topic};'
-  table_name_template:  '{type}',
-  view_name_template:   '{topic}',
-  types:                {}
-  defaulttype:          null
-  arraytype_template:   '{type}[]'
-  maxlen_entity:        0
-  maxlen_column:        0
-  invalid_char_regex:   null
-  invalid_char_repl:    '__'
-```
+More on plugins in doc/DETAIL.md#plugins.
 
 
 Notes on ROS1 vs ROS2
 ---------------------
 
 In ROS1, message type packages do not need to be installed locally to be able to
 read messages from bags or live topics, as bags and topic publishers provide
@@ -982,18 +681,18 @@
 and so does grepros.
 
 Additionally, each ROS1 message type has a hash code computed from its type
 definition text, available both in live topic metadata, and bag metadata.
 The message type definition hash code allows to recognize changes
 in message type packages and use the correct version of the message type.
 
-ROS2 does not provide message type definitions, neither in bagfiles nor in live
-topics. Due to this, the message type packages always need to be installed.
-Also, ROS2 does not provide options for generating type classes at run-time,
-and it does not have the concept of a message type hash.
+ROS2 does not provide message type definitions, neither in the .db3 bagfiles
+nor in live topics. Due to this, the message type packages always need to be
+installed. Also, ROS2 does not provide options for generating type classes
+at run-time, and it does not have the concept of a message type hash.
 
 These are serious limitations in ROS2 compared to ROS1, at least with versions
 up to ROS2 Humble and counting, and require extra work to smooth over.
 Without knowing which version of a message type package a bag was recorded with,
 reading bag messages with changed definitions can result in undefined behaviour.
 
 If the serialized message structure happens to match (e.g. a change swapped
@@ -1003,14 +702,20 @@
 
 Because of this, it is prudent to always include a snapshot archive of used
 message type packages, when recording ROS2 bags.
 
 grepros does provide the message type hash itself in ROS2 exports, by calculating
 the ROS2 message type hash on its own from the locally installed type definition.
 
+The situation in ROS2 with the newer MCAP format is a bit better: at least
+parsed message data can be read from MCAP bags without needing the specific message
+packages installed. However, reading from MCAP bags yields only data structs,
+not usable as ROS messages e.g. for publishing to live topics.
+grepros tries to smooth over this difference by defaulting to locally installed
+message classes if available, with definitions matching message types in bag.
 
 
 All command-line arguments
 --------------------------
 
 ```
 positional arguments:
@@ -1019,89 +724,105 @@
                         can specify message field as NAME=PATTERN
                         (supports nested.paths and * wildcards)
 
 optional arguments:
   -h, --help            show this help message and exit
   -F, --fixed-strings   PATTERNs are ordinary strings, not regular expressions
   -I, --no-ignore-case  use case-sensitive matching in PATTERNs
-  -v, --invert-match    select non-matching messages
+  -v, --invert-match    select messages not matching PATTERN
   --version             display version information and exit
   --live                read messages from live ROS topics instead of bagfiles
   --publish             publish matched messages to live ROS topics
-  --write TARGET [format=bag|csv|html|postgres|sqlite] [KEY=VALUE ...]
+  --write TARGET [format=bag|csv|html|mcap|parquet|postgres|sql|sqlite] [KEY=VALUE ...]
                         write matched messages to specified output,
                         format is autodetected from TARGET if not specified.
                         Bag or database will be appended to if it already exists.
                         Keyword arguments are given to output writer.
+                          column-NAME=ROSTYPE:VALUE
+                                                   additional column to add in Parquet output,
+                                                   like column-bag_hash=string:26dfba2c
                           commit-interval=NUM      transaction size for Postgres/SQLite output
                                                    (default 1000, 0 is autocommit)
                           dialect-file=path/to/dialects.yaml
                                                    load additional SQL dialect options
-                                                   for Postgres/SQLite output
+                                                   for Postgres/SQL/SQLite output
                                                    from a YAML or JSON file
+                          dialect=clickhouse|postgres|sqlite
+                                                   use specified SQL dialect in SQL output
+                                                   (default "sqlite")
+                          idgenerator=CALLABLE     callable or iterable for producing message IDs 
+                                                   in Parquet output, like 'uuid.uuid4' or 'itertools.count()';
+                                                   nesting uses UUID values by default
                           message-yaml=true|false  whether to populate table field messages.yaml
                                                    in SQLite output (default true)
                           nesting=array|all        create tables for nested message types
-                                                   in Postgres/SQLite output,
+                                                   in Parquet/Postgres/SQL/SQLite output,
                                                    only for arrays if "array" 
                                                    else for any nested types
                                                    (array fields in parent will be populated 
                                                     with foreign keys instead of messages as JSON)
-                          overwrite=true|false     overwrite existing file in bag/CSV/HTML/SQLite output
+                          overwrite=true|false     overwrite existing file 
+                                                   in bag/CSV/HTML/MCAP/Parquet/SQL/SQLite output
                                                    instead of appending to if bag or database
                                                    or appending unique counter to file name
                                                    (default false)
                           template=/my/path.tpl    custom template to use for HTML output
+                          type-ROSTYPE=ARROWTYPE   custom mapping between ROS and pyarrow type
+                                                   for Parquet output, like type-time="timestamp('ns')"
+                                                   or type-uint8[]="list(uint8())"
+                          writer-ARGNAME=ARGVALUE  additional arguments for Parquet output
+                                                   given to pyarrow.parquet.ParquetWriter
   --plugin PLUGIN [PLUGIN ...]
                         load a Python module or class as plugin
                         (built-in plugins: grepros.plugins.embag, 
                          grepros.plugins.mcap, grepros.plugins.parquet, 
                          grepros.plugins.sql)
+  --stop-on-error       stop further execution on any error like unknown message type
 
 Filtering:
   -t TOPIC [TOPIC ...], --topic TOPIC [TOPIC ...]
-                        ROS topics to scan if not all (supports * wildcards)
+                        ROS topics to read if not all (supports * wildcards)
   -nt TOPIC [TOPIC ...], --no-topic TOPIC [TOPIC ...]
                         ROS topics to skip (supports * wildcards)
   -d TYPE [TYPE ...], --type TYPE [TYPE ...]
-                        ROS message types to scan if not all (supports * wildcards)
+                        ROS message types to read if not all (supports * wildcards)
   -nd TYPE [TYPE ...], --no-type TYPE [TYPE ...]
                         ROS message types to skip (supports * wildcards)
   --condition CONDITION [CONDITION ...]
                         extra conditions to require for matching messages,
                         as ordinary Python expressions, can refer to last messages
-                        in topics as {topic /my/topic}; topic name can contain wildcards.
-                        E.g. --condition "{topic /robot/enabled}.data" matches
+                        in topics as <topic /my/topic>; topic name can contain wildcards.
+                        E.g. --condition "<topic /robot/enabled>.data" matches
                         messages only while last message in '/robot/enabled' has data=true.
   -t0 TIME, --start-time TIME
-                        earliest timestamp of messages to scan
+                        earliest timestamp of messages to read
                         as relative seconds if signed,
                         or epoch timestamp or ISO datetime
                         (for bag input, relative to bag start time
                         if positive or end time if negative,
                         for live input relative to system time,
                         datetime may be partial like 2021-10-14T12)
   -t1 TIME, --end-time TIME
-                        latest timestamp of messages to scan
+                        latest timestamp of messages to read
                         as relative seconds if signed,
                         or epoch timestamp or ISO datetime
                         (for bag input, relative to bag start time
                         if positive or end time if negative,
                         for live input relative to system time,
                         datetime may be partial like 2021-10-14T12)
   -n0 INDEX, --start-index INDEX
                         message index within topic to start from
                         (1-based if positive, counts back from bag total if negative)
   -n1 INDEX, --end-index INDEX
                         message index within topic to stop at
                         (1-based if positive, counts back from bag total if negative)
   --every-nth-message NUM
-                        scan every Nth message within topic
+                        read every Nth message within topic
   --every-nth-interval SECONDS
-                        scan messages at least N seconds apart within topic
+                        read messages at least N seconds apart within topic
   --every-nth-match NUM
                         emit every Nth match in topic
   -sf FIELD [FIELD ...], --select-field FIELD [FIELD ...]
                         message fields to use in matching if not all
                         (supports nested.paths and * wildcards)
   -ns FIELD [FIELD ...], --no-select-field FIELD [FIELD ...]
                         message fields to skip in matching
@@ -1119,56 +840,57 @@
   -B NUM, --before-context NUM
                         emit NUM messages of leading context before match
   -A NUM, --after-context NUM
                         emit NUM messages of trailing context after match
   -C NUM, --context NUM
                         emit NUM messages of leading and trailing context
                         around match
-  -pf FIELD [FIELD ...], --print-field FIELD [FIELD ...]
-                        message fields to print in console output if not all
+  -ef FIELD [FIELD ...], --emit-field FIELD [FIELD ...]
+                        message fields to emit in console/CSV/HTML/Parquet output if not all
                         (supports nested.paths and * wildcards)
-  -np FIELD [FIELD ...], --no-print-field FIELD [FIELD ...]
-                        message fields to skip in console output
+  -nf FIELD [FIELD ...], --no-emit-field FIELD [FIELD ...]
+                        message fields to skip in console/CSV/HTML/Parquet output
                         (supports nested.paths and * wildcards)
   -mo, --matched-fields-only
-                        print only the fields where PATTERNs find a match
+                        emit only the fields where PATTERNs find a match in console/HTML output
   -la NUM, --lines-around-match NUM
-                        print only matched fields and NUM message lines
-                        around match
+                        emit only matched fields and NUM message lines
+                        around match in console/HTML output
   -lf NUM, --lines-per-field NUM
-                        maximum number of lines to print per field
+                        maximum number of lines to emit per field in console/HTML output
   -l0 NUM, --start-line NUM
-                        message line number to start printing from
+                        message line number to start emitting from in console/HTML output
                         (1-based if positive, counts back from total if negative)
   -l1 NUM, --end-line NUM
-                        message line number to stop printing at
+                        message line number to stop emitting at in console/HTML output
                         (1-based if positive, counts back from total if negative)
   -lm NUM, --lines-per-message NUM
-                        maximum number of lines to print per message
+                        maximum number of lines to emit per message in console/HTML output
   --match-wrapper [STR [STR ...]]
-                        string to wrap around matched values,
+                        string to wrap around matched values in console/HTML output,
                         both sides if one value, start and end if more than one,
                         or no wrapping if zero values
                         (default "**" in colorless output)
-  --wrap-width NUM      character width to wrap message YAML output at,
+  --wrap-width NUM      character width to wrap message YAML console output at,
                         0 disables (defaults to detected terminal width)
   --color {auto,always,never}
                         use color output in console (default "always")
   --no-meta             do not print source and message metainfo to console
   --no-filename         do not print bag filename prefix on each console message line
+  --no-highlight        do not highlight matched values
   --no-console-output   do not print matches to console
   --progress            show progress bar when not printing matches to console
   --verbose             print status messages during console output
                         for publishing and writing
   --no-verbose          do not print status messages during console output
                         for publishing and writing
 
 Bag input control:
   -n FILE [FILE ...], --filename FILE [FILE ...]
-                        names of ROS bagfiles to scan if not all in directory
+                        names of ROS bagfiles to read if not all in directory
                         (supports * wildcards)
   -p PATH [PATH ...], --path PATH [PATH ...]
                         paths to scan if not current directory
                         (supports * wildcards)
   -r, --recursive       recurse into subdirectories when looking for bagfiles
   --order-bag-by {topic,type}
                         order bag messages by topic or type first and then by time
@@ -1195,14 +917,15 @@
 Dependencies
 ------------
 
 grepros requires Python 3.8+ or Python 2.7,
 and the following 3rd-party Python packages:
 
 - pyyaml (https://pypi.org/project/PyYAML)
+- six (https://pypi.org/project/six)
 - ROS1: rospy, roslib, rosbag, genpy
 - ROS2: rclpy, rosidl_parser, rosidl_runtime_py, builtin_interfaces
 
 Optional, for decompressing archived bags:
 
 - zstandard (https://pypi.org/project/zstandard)
 
@@ -1226,14 +949,20 @@
 - mcap_ros2_support (https://pypi.org/project/mcap_ros2_support)
 
 Optional, for generating API documentation:
 
 - doxypypy (https://pypi.org/project/doxypypy;
             needs latest master: `pip install git+https://github.com/Feneric/doxypypy`)
 
+All dependencies other than rospy/rclpy can be installed with:
+
+    pip install pyyaml six zstandard embag psycopg2 pandas pyarrow \
+                mcap mcap_ros1_support mcap_ros2_support \
+                git+https://github.com/Feneric/doxypypy
+
 
 Attribution
 -----------
 
 Includes a modified version of step, Simple Template Engine for Python,
 (c) 2012, Daniele Mazzocchio, https://github.com/dotpy/step,
 released under the MIT license.
@@ -1241,9 +970,7 @@
 
 License
 -------
 
 Copyright (c) 2021 by Erki Suurjaak.
 Released as free open source software under the BSD License,
 see LICENSE.md for full details.
-
-
```

## Comparing `grepros-0.5.0.dist-info/RECORD` & `grepros-1.0.0.dist-info/RECORD`

 * *Files 26% similar despite different names*

```diff
@@ -1,31 +1,32 @@
-grepros/__init__.py,sha256=-j-8hoCzmx6mr9dg5mue_zVNlvBKNryb2CumWfEJROs,476
+grepros/__init__.py,sha256=dgstg6TV_BDROHndsCbzlNzvbOsTmaK6NwjAKeWFEeg,533
 grepros/__main__.py,sha256=mr_Xu92us0Ya3d5IedjZPN6hkOd8dedbOB_sJOicwnI,452
-grepros/common.py,sha256=yRWyVqjVc6Ss6kUNVojmrB9DR77pPn3J2ycA-h3aRNk,33237
-grepros/inputs.py,sha256=IebAsYzNQHmXQmhH2P8_l-Paiz_wRgd7ZP-r9jT_xEo,38736
-grepros/main.py,sha256=CPVnm2WNKtXhuHmZywWvDSL90T5jtC9jFeqrA8Gbq6o,23668
-grepros/outputs.py,sha256=h7g3IvkdpM7-1zQfOABug_QiQx87gM2O79yAEbznIuw,26754
-grepros/ros1.py,sha256=DkXJU6Q_c22kcwkdDwDyze3KWNOwPW0xUwVccvmXOgE,20128
-grepros/ros2.py,sha256=wPuWrKhyadDPu5NOUalTu8bqh14NNQHFJCZVQzS7wh0,31524
-grepros/rosapi.py,sha256=U2shjHfYUsnJHKdRdfSpBe-pjo7_Q4DrYmzURxOKKEM,26654
-grepros/search.py,sha256=TLlJFtH_psMWehOdKJDzzeJ6ownydegr1f87fNFaeMY,14793
-grepros/plugins/__init__.py,sha256=_AxCeNGbQ4aA8w7xMY1USEd58hi2xqmcFyPppSPlrbI,8546
-grepros/plugins/embag.py,sha256=-sOdlKTud1RMcxw5lBmkh3P_gC1t0yefq0_fD8tJTdE,8138
-grepros/plugins/mcap.py,sha256=KkbdeQXhYFAzOiykD6jFsypZiO9f0csN-I1drr6Tobo,18317
-grepros/plugins/parquet.py,sha256=GpFvwlF0ATk95QJw1O70DbEvVFQncS_gGJ4bl23LsVA,14962
-grepros/plugins/sql.py,sha256=1xUkIUWvPlqfe39fal-0J-QRcda9OCkqax003hMtxoU,10524
+grepros/api.py,sha256=yKGDTqp_9F1kQsTeIix29X6z6BRRcYPKITbqyrc9JEg,46566
+grepros/common.py,sha256=_qWdoSPmv6HgmXZEFM8ZRwYHMQp39Bpq8XgRAKngFn8,42514
+grepros/inputs.py,sha256=G4DPJ-lWsOhsesdlKhAokiU0CjQLiczZlEewuWYvtm4,53136
+grepros/library.py,sha256=k0nJ69y-rqfNFpyXKn-QdmqzdXR3dBtv9vPjozhjYek,18891
+grepros/main.py,sha256=I5ukoqRabnkXVnphkf6WIycvM-HZak_A68GNHzSmFa4,25233
+grepros/outputs.py,sha256=8wAt9CPHKjQK2WlWjH5x9xgu9C9dQBMv6jNxXaazTfI,36173
+grepros/ros1.py,sha256=FhMn1nXhpxUU-xmhIx2R1Szir8MPknKytgaL2UEV_qo,30697
+grepros/ros2.py,sha256=X6KstfHAaXK2BN6uVnrTtIGJGl4N0MXfG0UaGCyINVs,43615
+grepros/search.py,sha256=szv5_n1BdbJ_ZdzRfg_7kAZRSNonwqEY08YfNtgZfpc,21881
+grepros/plugins/__init__.py,sha256=lX5qzipdVu2l5GRyBcYU2Kq7Nx5wzHLD6DBwO7R3xGM,11620
+grepros/plugins/embag.py,sha256=V0Z0TpRZdzbNgDTClFm87p-3Tivz80NcGexjqXcrTJU,12525
+grepros/plugins/mcap.py,sha256=NXMAJj9MdO2yeoLTedbK52GnywaYiUN8quloIE1oOHA,31324
+grepros/plugins/parquet.py,sha256=8FP_8r9atRjQ4nXYfuN5c70R16EZIGhE07JtCzWhQys,25014
+grepros/plugins/sql.py,sha256=S_wTUDJaubFWUcWYz1dJVs4GYR9doOtx-YNFfexpuc4,12063
 grepros/plugins/auto/__init__.py,sha256=LQiEql_xKJooVHXQnGL9MgFaEyw_tWCttLC9d3-SxbA,426
-grepros/plugins/auto/csv.py,sha256=Pg90CLHNfZMdVIwjX_1CftIBL106ByxNUZ3zIyO7NT8,9788
-grepros/plugins/auto/dbbase.py,sha256=o1co-iR_ZAiVnZEqrITM0Kl2wivEUH43TKAEL-kzIzI,15784
-grepros/plugins/auto/html.py,sha256=imDWTIgQSXfS939xEvXO_sffMR9k6ERVk9msw12KOXE,8059
-grepros/plugins/auto/html.tpl,sha256=H-INhr56GiWhi6d1kwACX_zNDOQFUX2LXVGsy6mxLTA,42323
-grepros/plugins/auto/postgres.py,sha256=6U2kYEVChY4v5s-zLphkoaqW8UVdw40881Jxc0MvWHk,9442
-grepros/plugins/auto/sqlbase.py,sha256=DAp_kK73sTWSTlwia1oPj-j6f6AuJraN7noFtlTRu4k,32302
-grepros/plugins/auto/sqlite.py,sha256=uUGKl2i752CWrVBJD6jNDk3gOj9MDdjvHjmg_L4jsxM,8754
+grepros/plugins/auto/csv.py,sha256=nTNqyQJZIzsbZoBMXx674IbMay3PmmBqct_pbipEWM8,11900
+grepros/plugins/auto/dbbase.py,sha256=SCEMvRBWryqYP1xtx7b6Sv9O9OwpI2lX_E2736ax1RI,16591
+grepros/plugins/auto/html.py,sha256=vmQpZ_ud3FAFNsd160JPywUMHoQq91-O-l258Q-EPoM,11073
+grepros/plugins/auto/html.tpl,sha256=FEQlm9gCURQw0QjqKcmZ83OkNOl_NM5cAREVqtZ8ARM,42482
+grepros/plugins/auto/postgres.py,sha256=ZDSHrGVc5cjQuGBXSZKqJpAXH8DMexZo5pStOQndrvs,10151
+grepros/plugins/auto/sqlbase.py,sha256=pQR137yjL8eRlZ8BzFi2S-r6QDL2D82gbzUdeix5lI4,32550
+grepros/plugins/auto/sqlite.py,sha256=WrJUsbm3eoz0yetDPNzHwl3jXB4JNnGVnPPzTgI5j2o,9694
 grepros/vendor/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 grepros/vendor/step.py,sha256=zgGgMdxFujPUeT6uFB4WMrxmgFooVSYmlEIjsm4bgrY,7903
-grepros-0.5.0.dist-info/LICENSE.md,sha256=qlDZaCV5FQE44nTvlsLWm9av1VhgIlKbu3h4TK6wknI,1523
-grepros-0.5.0.dist-info/METADATA,sha256=udJsHN2g4_vcSTmjyzvaty7I3BQhizaq-vniz52FY0w,47262
-grepros-0.5.0.dist-info/WHEEL,sha256=6T3TYZE4YFi2HTS1BeZHNXAi8N52OZT4O-dJ6-ome_4,116
-grepros-0.5.0.dist-info/entry_points.txt,sha256=5emWEbHZTH6c0sAAHZtwCLs_lut1qsd0VNcG7-9GNEE,46
-grepros-0.5.0.dist-info/top_level.txt,sha256=DfFuozU8--EUKxcjRvbaWrUwm9uWPK6vsRPJUyRQ8yU,8
-grepros-0.5.0.dist-info/RECORD,,
+grepros-1.0.0.dist-info/LICENSE.md,sha256=qlDZaCV5FQE44nTvlsLWm9av1VhgIlKbu3h4TK6wknI,1523
+grepros-1.0.0.dist-info/METADATA,sha256=uKmllSXrWwkeyj155g-hdKGIbZKWADAa1jwMQ3VK2OI,38013
+grepros-1.0.0.dist-info/WHEEL,sha256=a-zpFRIJzOq5QfuhBzbhiA1eHTzNCJn8OdRvhdNX0Rk,110
+grepros-1.0.0.dist-info/entry_points.txt,sha256=mUxuxh9wJ4i0dHqhB8DVJ9FpngADYtrDBceLAtRc1vE,45
+grepros-1.0.0.dist-info/top_level.txt,sha256=DfFuozU8--EUKxcjRvbaWrUwm9uWPK6vsRPJUyRQ8yU,8
+grepros-1.0.0.dist-info/RECORD,,
```

