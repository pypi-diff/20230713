# Comparing `tmp/gs_coordinator-0.23.0-py2.py3-none-macosx_11_0_x86_64.whl.zip` & `tmp/gs_coordinator-0.24.0a20230712-py2.py3-none-macosx_11_0_x86_64.whl.zip`

## zipinfo {}

```diff
@@ -1,34 +1,34 @@
-Zip file size: 94733 bytes, number of entries: 32
--rw-r--r--  2.0 unx      694 b- defN 23-Jul-04 08:42 graphscope_runtime/__init__.py
--rw-rw-r--  2.0 unx     2829 b- defN 23-Jul-04 11:01 gs_coordinator-0.23.0.dist-info/RECORD
--rw-r--r--  2.0 unx      140 b- defN 23-Jul-04 10:44 gs_coordinator-0.23.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       33 b- defN 23-Jul-04 10:44 gs_coordinator-0.23.0.dist-info/top_level.txt
--rw-r--r--  2.0 unx    22349 b- defN 23-Jul-04 10:44 gs_coordinator-0.23.0.dist-info/METADATA
--rw-r--r--  2.0 unx    58500 b- defN 23-Jul-04 08:42 gscoordinator/kubernetes_launcher.py
--rw-r--r--  2.0 unx    37646 b- defN 23-Jul-04 08:42 gscoordinator/coordinator.py
--rw-r--r--  2.0 unx     5620 b- defN 23-Jul-04 08:42 gscoordinator/dag_manager.py
--rw-r--r--  2.0 unx     1045 b- defN 23-Jul-04 08:42 gscoordinator/version.py
--rw-r--r--  2.0 unx     6949 b- defN 23-Jul-04 08:42 gscoordinator/monitor.py
--rw-r--r--  2.0 unx    40836 b- defN 23-Jul-04 08:42 gscoordinator/op_executor.py
--rw-r--r--  2.0 unx     2653 b- defN 23-Jul-04 08:42 gscoordinator/object_manager.py
--rw-r--r--  2.0 unx      990 b- defN 23-Jul-04 08:42 gscoordinator/__init__.py
--rw-r--r--  2.0 unx    23213 b- defN 23-Jul-04 08:42 gscoordinator/cluster_builder.py
--rw-r--r--  2.0 unx    20588 b- defN 23-Jul-04 08:42 gscoordinator/local_launcher.py
--rw-r--r--  2.0 unx        7 b- defN 23-Jul-04 08:42 gscoordinator/VERSION
--rw-r--r--  2.0 unx    77420 b- defN 23-Jul-04 08:42 gscoordinator/utils.py
--rw-r--r--  2.0 unx     4091 b- defN 23-Jul-04 08:42 gscoordinator/io_utils.py
--rw-r--r--  2.0 unx     5090 b- defN 23-Jul-04 08:42 gscoordinator/launcher.py
--rw-r--r--  2.0 unx     2450 b- defN 23-Jul-04 08:42 gscoordinator/learning.py
--rw-r--r--  2.0 unx       77 b- defN 23-Jul-04 08:42 gscoordinator/__main__.py
--rw-r--r--  2.0 unx      646 b- defN 23-Jul-04 08:42 gscoordinator/template/__init__.py
--rw-r--r--  2.0 unx     4340 b- defN 23-Jul-04 08:42 gscoordinator/template/pregel.pxd.template
--rw-r--r--  2.0 unx     4957 b- defN 23-Jul-04 08:42 gscoordinator/template/pie.pxd.template
--rw-r--r--  2.0 unx    21172 b- defN 23-Jul-04 08:42 gscoordinator/template/CMakeLists.template
--rw-r--r--  2.0 unx      646 b- defN 23-Jul-04 08:42 gscoordinator/builtin/__init__.py
--rw-r--r--  2.0 unx    24514 b- defN 23-Jul-04 08:42 gscoordinator/builtin/app/.gs_conf.yaml
--rw-r--r--  2.0 unx     5673 b- defN 23-Jul-04 10:44 gscoordinator/builtin/app/builtin_app.gar
--rw-r--r--  2.0 unx      646 b- defN 23-Jul-04 08:42 gscoordinator/builtin/app/__init__.py
--rw-r--r--  2.0 unx      646 b- defN 23-Jul-04 08:42 gscoordinator/hook/__init__.py
--rw-r--r--  2.0 unx      646 b- defN 23-Jul-04 08:42 gscoordinator/hook/prestop/__init__.py
--rw-r--r--  2.0 unx     1560 b- defN 23-Jul-04 08:42 gscoordinator/hook/prestop/__main__.py
-32 files, 378666 bytes uncompressed, 90213 bytes compressed:  76.2%
+Zip file size: 95526 bytes, number of entries: 32
+-rw-r--r--  2.0 unx      694 b- defN 23-Jul-12 19:05 graphscope_runtime/__init__.py
+-rw-rw-r--  2.0 unx     2866 b- defN 23-Jul-12 21:17 gs_coordinator-0.24.0a20230712.dist-info/RECORD
+-rw-r--r--  2.0 unx      140 b- defN 23-Jul-12 21:00 gs_coordinator-0.24.0a20230712.dist-info/WHEEL
+-rw-r--r--  2.0 unx       33 b- defN 23-Jul-12 21:00 gs_coordinator-0.24.0a20230712.dist-info/top_level.txt
+-rw-r--r--  2.0 unx    22367 b- defN 23-Jul-12 21:00 gs_coordinator-0.24.0a20230712.dist-info/METADATA
+-rw-r--r--  2.0 unx    59125 b- defN 23-Jul-12 19:05 gscoordinator/kubernetes_launcher.py
+-rw-r--r--  2.0 unx    39201 b- defN 23-Jul-12 19:05 gscoordinator/coordinator.py
+-rw-r--r--  2.0 unx     5620 b- defN 23-Jul-12 19:05 gscoordinator/dag_manager.py
+-rw-r--r--  2.0 unx     1045 b- defN 23-Jul-12 19:05 gscoordinator/version.py
+-rw-r--r--  2.0 unx     6949 b- defN 23-Jul-12 19:05 gscoordinator/monitor.py
+-rw-r--r--  2.0 unx    40836 b- defN 23-Jul-12 19:05 gscoordinator/op_executor.py
+-rw-r--r--  2.0 unx     2653 b- defN 23-Jul-12 19:05 gscoordinator/object_manager.py
+-rw-r--r--  2.0 unx      990 b- defN 23-Jul-12 19:05 gscoordinator/__init__.py
+-rw-r--r--  2.0 unx    23337 b- defN 23-Jul-12 19:05 gscoordinator/cluster_builder.py
+-rw-r--r--  2.0 unx    21016 b- defN 23-Jul-12 19:05 gscoordinator/local_launcher.py
+-rw-r--r--  2.0 unx       16 b- defN 23-Jul-12 19:13 gscoordinator/VERSION
+-rw-r--r--  2.0 unx    78642 b- defN 23-Jul-12 19:05 gscoordinator/utils.py
+-rw-r--r--  2.0 unx     4091 b- defN 23-Jul-12 19:05 gscoordinator/io_utils.py
+-rw-r--r--  2.0 unx     5090 b- defN 23-Jul-12 19:05 gscoordinator/launcher.py
+-rw-r--r--  2.0 unx     2450 b- defN 23-Jul-12 19:05 gscoordinator/learning.py
+-rw-r--r--  2.0 unx       77 b- defN 23-Jul-12 19:05 gscoordinator/__main__.py
+-rw-r--r--  2.0 unx      646 b- defN 23-Jul-12 19:05 gscoordinator/template/__init__.py
+-rw-r--r--  2.0 unx     4340 b- defN 23-Jul-12 19:05 gscoordinator/template/pregel.pxd.template
+-rw-r--r--  2.0 unx     4957 b- defN 23-Jul-12 19:05 gscoordinator/template/pie.pxd.template
+-rw-r--r--  2.0 unx    21530 b- defN 23-Jul-12 19:05 gscoordinator/template/CMakeLists.template
+-rw-r--r--  2.0 unx      646 b- defN 23-Jul-12 19:05 gscoordinator/builtin/__init__.py
+-rw-r--r--  2.0 unx    24514 b- defN 23-Jul-12 19:05 gscoordinator/builtin/app/.gs_conf.yaml
+-rw-r--r--  2.0 unx     5673 b- defN 23-Jul-12 21:00 gscoordinator/builtin/app/builtin_app.gar
+-rw-r--r--  2.0 unx      646 b- defN 23-Jul-12 19:05 gscoordinator/builtin/app/__init__.py
+-rw-r--r--  2.0 unx      646 b- defN 23-Jul-12 19:05 gscoordinator/hook/__init__.py
+-rw-r--r--  2.0 unx      646 b- defN 23-Jul-12 19:05 gscoordinator/hook/prestop/__init__.py
+-rw-r--r--  2.0 unx     1560 b- defN 23-Jul-12 19:05 gscoordinator/hook/prestop/__main__.py
+32 files, 383042 bytes uncompressed, 90934 bytes compressed:  76.3%
```

## zipnote {}

```diff
@@ -1,20 +1,20 @@
 Filename: graphscope_runtime/__init__.py
 Comment: 
 
-Filename: gs_coordinator-0.23.0.dist-info/RECORD
+Filename: gs_coordinator-0.24.0a20230712.dist-info/RECORD
 Comment: 
 
-Filename: gs_coordinator-0.23.0.dist-info/WHEEL
+Filename: gs_coordinator-0.24.0a20230712.dist-info/WHEEL
 Comment: 
 
-Filename: gs_coordinator-0.23.0.dist-info/top_level.txt
+Filename: gs_coordinator-0.24.0a20230712.dist-info/top_level.txt
 Comment: 
 
-Filename: gs_coordinator-0.23.0.dist-info/METADATA
+Filename: gs_coordinator-0.24.0a20230712.dist-info/METADATA
 Comment: 
 
 Filename: gscoordinator/kubernetes_launcher.py
 Comment: 
 
 Filename: gscoordinator/coordinator.py
 Comment:
```

## gscoordinator/kubernetes_launcher.py

```diff
@@ -95,14 +95,17 @@
         mars_worker_mem=None,
         with_dataset=False,
         namespace=None,
         num_workers=None,
         preemptive=None,
         service_type=None,
         timeout_seconds=None,
+        kube_timeout_seconds=1,
+        retry_time_seconds=2,
+        del_retry_time_seconds=1,
         vineyard_cpu=None,
         vineyard_deployment=None,
         vineyard_image=None,
         vineyard_mem=None,
         vineyard_shared_mem=None,
         volumes=None,
         waiting_for_delete=None,
@@ -158,14 +161,20 @@
 
         self._with_dataset = with_dataset
         self._preemptive = preemptive
         self._service_type = service_type
 
         assert timeout_seconds is not None
         self._timeout_seconds = timeout_seconds
+        # timeout seconds waiting for kube service ready
+        self._kube_timeout_seconds = kube_timeout_seconds
+        # retry time when waiting for kube service ready
+        self._retry_time_seconds = retry_time_seconds
+        # retry time when deleting dangling coordinators
+        self._del_retry_time_seconds = del_retry_time_seconds
 
         self._waiting_for_delete = waiting_for_delete
 
         self._with_analytical = False
         self._with_analytical_java = False
         self._with_interactive = False
         self._with_learning = False
@@ -632,20 +641,21 @@
             self._session_workspace,
             str(object_id),
             schema_path,
             hosts,
             container,
             str(self._interactive_port),  # executor port
             str(self._interactive_port + 1),  # executor rpc port
-            str(self._interactive_port + 2),  # frontend port
+            str(self._interactive_port + 2),  # frontend gremlin port
+            str(self._interactive_port + 3),  # frontend cypher port
             self._coordinator_name,
             engine_selector,
             params,
         ]
-        self._interactive_port += 3
+        self._interactive_port += 4
         logger.info("Create GIE instance with command: %s", " ".join(cmd))
         process = subprocess.Popen(
             cmd,
             start_new_session=True,
             cwd=os.getcwd(),
             env=env,
             encoding="utf-8",
@@ -943,15 +953,15 @@
         response = self._apps_api.create_namespaced_deployment(
             self._namespace, deployment
         )
         self._resource_object.append(response)
 
     def _create_frontend_service(self):
         logger.info("Creating frontend service...")
-        service = self._engine_cluster.get_interactive_frontend_service(8233)
+        service = self._engine_cluster.get_interactive_frontend_service(8233, 7687)
         service.metadata.owner_references = self._owner_references
         response = self._core_api.create_namespaced_service(self._namespace, service)
         self._resource_object.append(response)
 
     def _create_learning_service(self, object_id):
         logger.info("Creating learning service...")
         service = self._engine_cluster.get_learning_service(
@@ -1013,29 +1023,29 @@
                     for pod in pods.items:
                         pod_name = pod.metadata.name
                         field_selector = "involvedObject.name=" + pod_name
                         stream = kube_watch.Watch().stream(
                             self._core_api.list_namespaced_event,
                             namespace,
                             field_selector=field_selector,
-                            timeout_seconds=1,
+                            timeout_seconds=self._kube_timeout_seconds,
                         )
                         for event in stream:
                             msg = f"[{pod_name}]: {event['object'].message}"
                             if msg not in event_messages:
                                 event_messages.append(msg)
                                 logger.info(msg)
                                 if event["object"].reason == "Failed":
                                     raise RuntimeError("Kubernetes event error: " + msg)
 
             if service_available:
                 break
             if self._timeout_seconds + start_time < time.time():
                 raise TimeoutError("GraphScope Engines launching timeout.")
-            time.sleep(2)
+            time.sleep(self._retry_time_seconds)
 
         self._pod_name_list = []
         self._pod_ip_list = []
         self._pod_host_ip_list = []
         pods = self._core_api.list_namespaced_pod(
             namespace=namespace, label_selector=selector
         )
@@ -1239,20 +1249,20 @@
                     if ex.status != 404:
                         logger.exception(
                             "Deleting dangling coordinator %s failed",
                             self._coordinator_name,
                         )
                     break
                 else:
-                    time.sleep(1)
                     if time.time() - start_time > self._timeout_seconds:
                         logger.error(
                             "Deleting dangling coordinator %s timeout",
                             self._coordinator_name,
                         )
+                    time.sleep(self._del_retry_time_seconds)
 
     def _get_owner_reference_as_json(self):
         owner_reference = [
             {
                 "apiVersion": self._owner_references[0].api_version,
                 "kind": self._owner_references[0].kind,
                 "name": self._owner_references[0].name,
@@ -1357,26 +1367,27 @@
                                 if ex.status != 404:
                                     logger.exception(
                                         "Deleting dangling namespace %s failed",
                                         self._namespace,
                                     )
                                 break
                             else:
-                                time.sleep(1)
                                 if time.time() - start_time > self._timeout_seconds:
                                     logger.error(
                                         "Deleting namespace %s timeout", self._namespace
                                     )
+                                time.sleep(self._del_retry_time_seconds)
+
                 else:
                     # delete coordinator deployment and service
                     self._delete_dangling_coordinator()
             self._serving = False
             logger.info("Kubernetes launcher stopped")
 
-    def _allocate_learining_engine(self, object_id):
+    def _allocate_learning_engine(self, object_id):
         # check the learning engine flag
         if not self._with_learning:
             raise NotImplementedError("Learning engine not enabled")
 
         # allocate learning engine based on the mode
         if self._deploy_mode == "eager":
             return self._pod_name_list, self._pod_ip_list, self._pod_host_ip_list
@@ -1437,15 +1448,15 @@
         self._learning_start_port += len(pod_name_list)
         # parse the service hosts and ports
         return self._engine_cluster.get_graphlearn_service_endpoint(
             self._api_client, object_id, pod_host_ip_list
         )
 
     def create_learning_instance(self, object_id, handle, config):
-        pod_name_list, _, pod_host_ip_list = self._allocate_learining_engine(object_id)
+        pod_name_list, _, pod_host_ip_list = self._allocate_learning_engine(object_id)
         if not pod_name_list or not pod_host_ip_list:
             raise RuntimeError("Failed to allocate learning engine")
         return self._distribute_learning_process(
             pod_name_list, pod_host_ip_list, object_id, handle, config
         )
 
     def close_learning_instance(self, object_id):
```

## gscoordinator/coordinator.py

```diff
@@ -60,15 +60,15 @@
 from gscoordinator.kubernetes_launcher import KubernetesClusterLauncher
 from gscoordinator.monitor import Monitor
 from gscoordinator.object_manager import InteractiveInstanceManager
 from gscoordinator.object_manager import LearningInstanceManager
 from gscoordinator.object_manager import ObjectManager
 from gscoordinator.op_executor import OperationExecutor
 from gscoordinator.utils import GS_GRPC_MAX_MESSAGE_LENGTH
-from gscoordinator.utils import check_gremlin_server_ready
+from gscoordinator.utils import check_server_ready
 from gscoordinator.utils import create_single_op_dag
 from gscoordinator.utils import str2bool
 from gscoordinator.version import __version__
 
 
 def catch_unknown_errors(response_on_error=None, using_yield=False):
     """A catcher that catches all (unknown) exceptions in gRPC handlers to ensure
@@ -168,14 +168,16 @@
 
         # control log fetching
         self._streaming_logs = False
         self._pipe_merged = PipeMerger(sys.stdout, sys.stderr)
 
         # dangling check
         self._dangling_timeout_seconds = dangling_timeout_seconds
+        self._comm_timeout_seconds = 120
+        self._poll_timeout_seconds = 2
         self._dangling_detecting_timer = None
         self._cleanup_instance = False
 
         self._session_id = self._generate_session_id()
         self._launcher.set_session_workspace(self._session_id)
         if not self._launcher.start():
             raise RuntimeError("Coordinator launching instance failed.")
@@ -233,14 +235,17 @@
             context.set_details(error_msg)
             return message_pb2.ConnectSessionResponse()
 
         # Connect to serving coordinator.
         self._connected = True
         # Cleanup after timeout seconds
         self._dangling_timeout_seconds = request.dangling_timeout_seconds
+        # other timeout seconds
+        self._comm_timeout_seconds = getattr(request, "comm_timeout_seconds", 120)
+        self._poll_timeout_seconds = getattr(request, "poll_timeout_seconds", 2)
         # If true, also delete graphscope instance (such as pods) in closing process
         self._cleanup_instance = request.cleanup_instance
 
         # Session connected, fetch logs via gRPC.
         self._streaming_logs = True
         sys.stdout.drop(False)
         sys.stderr.drop(False)
@@ -378,15 +383,17 @@
     RunStepWrapped = catch_unknown_errors(
         message_pb2.RunStepResponse(head=message_pb2.RunStepResponseHead()), True
     )(_RunStep)
 
     def FetchLogs(self, request, context):
         while self._streaming_logs:
             try:
-                info_message, error_message = self._pipe_merged.poll(timeout=2)
+                info_message, error_message = self._pipe_merged.poll(
+                    timeout=self._poll_timeout_seconds
+                )
             except queue.Empty:
                 info_message, error_message = "", ""
             except Exception as e:
                 info_message, error_message = (
                     f"WARNING: failed to read log: {e}. The traceback is: {traceback.format_exc()}",
                     "",
                 )
@@ -437,18 +444,24 @@
             for line in lines.split("\n"):
                 rlt = re.findall(pattern, line)
                 if rlt:
                     return rlt[0].strip()
             return ""
 
         # frontend endpoint pattern
-        FRONTEND_PATTERN = re.compile("(?<=FRONTEND_ENDPOINT:).*$")
+        FRONTEND_GREMLIN_PATTERN = re.compile("(?<=FRONTEND_GREMLIN_ENDPOINT:).*$")
+        FRONTEND_CYPHER_PATTERN = re.compile("(?<=FRONTEND_CYPHER_ENDPOINT:).*$")
         # frontend external endpoint, for clients that are outside of cluster to connect
         # only available in kubernetes mode, exposed by NodePort or LoadBalancer
-        FRONTEND_EXTERNAL_PATTERN = re.compile("(?<=FRONTEND_EXTERNAL_ENDPOINT:).*$")
+        FRONTEND_EXTERNAL_GREMLIN_PATTERN = re.compile(
+            "(?<=FRONTEND_EXTERNAL_GREMLIN_ENDPOINT:).*$"
+        )
+        FRONTEND_EXTERNAL_CYPHER_PATTERN = re.compile(
+            "(?<=FRONTEND_EXTERNAL_CYPHER_ENDPOINT:).*$"
+        )
 
         # create instance
         object_id = request.object_id
         schema_path = request.schema_path
         params = request.params
         try:
             proc = self._launcher.create_interactive_instance(
@@ -456,39 +469,64 @@
             )
             gie_manager = InteractiveInstanceManager(object_id)
             # Put it to object_manager to ensure it could be killed during coordinator cleanup
             # If coordinator is shutdown by force when creating interactive instance
             self._object_manager.put(object_id, gie_manager)
             # 60 seconds is enough, see also GH#1024; try 120
             # already add errs to outs
-            outs, _ = proc.communicate(timeout=120)  # throws TimeoutError
+            outs, _ = proc.communicate(
+                timeout=self._comm_timeout_seconds
+            )  # throws TimeoutError
             return_code = proc.poll()
             if return_code != 0:
                 raise RuntimeError(f"Error code: {return_code}, message {outs}")
-            # match frontend endpoint and check for ready
-            endpoint = _match_frontend_endpoint(FRONTEND_PATTERN, outs)
+            # match frontend endpoints and check for ready
+            gremlin_endpoint = _match_frontend_endpoint(FRONTEND_GREMLIN_PATTERN, outs)
+            cypher_endpoint = _match_frontend_endpoint(FRONTEND_CYPHER_PATTERN, outs)
+            logger.debug("Got endpoints: %s %s", gremlin_endpoint, cypher_endpoint)
             # coordinator use internal endpoint
-            gie_manager.set_endpoint(endpoint)
-            if check_gremlin_server_ready(endpoint):  # throws TimeoutError
+            gie_manager.set_endpoint(gremlin_endpoint)
+            if check_server_ready(
+                gremlin_endpoint, server="gremlin"
+            ) and check_server_ready(
+                cypher_endpoint, server="cypher"
+            ):  # throws TimeoutError
                 logger.info(
-                    "Built interactive frontend %s for graph %ld", endpoint, object_id
+                    "Built interactive frontend gremlin: %s & cypher: %s for graph %ld",
+                    gremlin_endpoint,
+                    cypher_endpoint,
+                    object_id,
                 )
         except Exception as e:
             context.set_code(grpc.StatusCode.ABORTED)
             context.set_details(
                 f"Create interactive instance failed: ${e}. The traceback is: {traceback.format_exc()}"
             )
             self._launcher.close_interactive_instance(object_id)
             self._object_manager.pop(object_id)
             return message_pb2.CreateInteractiveInstanceResponse()
-        external_endpoint = _match_frontend_endpoint(FRONTEND_EXTERNAL_PATTERN, outs)
+        external_gremlin_endpoint = _match_frontend_endpoint(
+            FRONTEND_EXTERNAL_GREMLIN_PATTERN, outs
+        )
+        external_cypher_endpoint = _match_frontend_endpoint(
+            FRONTEND_EXTERNAL_CYPHER_PATTERN, outs
+        )
+        logger.debug(
+            "Got external endpoints: %s %s",
+            external_gremlin_endpoint,
+            external_cypher_endpoint,
+        )
+
         # client use external endpoint (k8s mode), or internal endpoint (standalone mode)
-        endpoint = external_endpoint or endpoint
+        gremlin_endpoint = external_gremlin_endpoint or gremlin_endpoint
+        cypher_endpoint = external_cypher_endpoint or cypher_endpoint
         return message_pb2.CreateInteractiveInstanceResponse(
-            gremlin_endpoint=endpoint, object_id=object_id
+            gremlin_endpoint=gremlin_endpoint,
+            cypher_endpoint=cypher_endpoint,
+            object_id=object_id,
         )
 
     def CreateLearningInstance(self, request, context):
         object_id = request.object_id
         logger.info("Create learning instance with object id %ld", object_id)
         handle, config = request.handle, request.config
         try:
```

## gscoordinator/cluster_builder.py

```diff
@@ -573,18 +573,21 @@
         deployment_spec = ResourceBuilder.get_deployment_spec(
             template_spec, replicas, self._frontend_labels
         )
         return ResourceBuilder.get_deployment(
             self._namespace, name, deployment_spec, self._frontend_labels
         )
 
-    def get_interactive_frontend_service(self, port):
+    def get_interactive_frontend_service(self, gremlin_port, cypher_port):
         name = self.frontend_deployment_name
         service_type = self._service_type
-        ports = [kube_client.V1ServicePort(name="gremlin", port=port)]
+        ports = [
+            kube_client.V1ServicePort(name="gremlin", port=gremlin_port),
+            kube_client.V1ServicePort(name="cypher", port=cypher_port),
+        ]
         service_spec = ResourceBuilder.get_service_spec(
             service_type, ports, self._frontend_labels, None
         )
         service = ResourceBuilder.get_service(
             self._namespace, name, service_spec, self._frontend_labels, _annotations
         )
         return service
```

## gscoordinator/local_launcher.py

```diff
@@ -57,29 +57,32 @@
         etcd_listening_client_port: int,
         etcd_listening_peer_port: int,
         vineyard_socket: str,
         shared_mem: str,
         log_level: str,
         instance_id: str,
         timeout_seconds: int,
+        close_timeout_seconds: int = 60,
+        retry_time_seconds: int = 1,
     ):
         super().__init__()
         self._num_workers = num_workers
         self._hosts = hosts
 
         self._external_etcd_addr = etcd_addrs
         self._etcd_listening_client_port = etcd_listening_client_port
         self._etcd_listening_peer_port = etcd_listening_peer_port
         self._vineyard_socket = vineyard_socket
         self._shared_mem = shared_mem
 
         self._glog_level = parse_as_glog_level(log_level)
         self._instance_id = instance_id
         self._timeout_seconds = timeout_seconds
-
+        self._close_timeout_seconds = close_timeout_seconds
+        self._retry_time_seconds = retry_time_seconds
         self._vineyard_socket_prefix = os.path.join(get_tempdir(), "vineyard.sock.")
 
         # A graphscope instance may have multiple session by reconnecting to coordinator
         self._instance_workspace = os.path.join(WORKSPACE, self._instance_id)
         os.makedirs(self._instance_workspace, exist_ok=True)
         # setting during client connect to coordinator
         self._session_workspace = None
@@ -180,18 +183,19 @@
         setattr(process, "stdout_watcher", stdout_watcher)
         setattr(process, "stderr_watcher", stderr_watcher)
 
         self._analytical_engine_process = process
 
         start_time = time.time()
         while is_free_port(rpc_port):
-            time.sleep(1)
             if self._timeout_seconds + start_time < time.time():
                 self._analytical_engine_process.kill()
                 raise RuntimeError("Launch analytical engine failed due to timeout.")
+            time.sleep(self._retry_time_seconds)
+
         logger.info(
             "Analytical engine is listening on %s", self._analytical_engine_endpoint
         )
 
     def create_interactive_instance(
         self, object_id: int, schema_path: str, params: dict
     ):
@@ -228,20 +232,21 @@
             "create_gremlin_instance_on_local",
             self._session_workspace,
             str(object_id),
             schema_path,
             str(num_workers),  # server size
             str(self._interactive_port),  # executor port
             str(self._interactive_port + 1),  # executor rpc port
-            str(self._interactive_port + 2 * num_workers),  # frontend port
+            str(self._interactive_port + 2 * num_workers),  # frontend gremlin port
+            str(self._interactive_port + 2 * num_workers + 1),  # frontend cypher port
             self.vineyard_socket,
             params,
         ]
         logger.info("Create GIE instance with command: %s", " ".join(cmd))
-        self._interactive_port += 3
+        self._interactive_port += 2 * num_workers + 2
         process = subprocess.Popen(
             cmd,
             start_new_session=True,
             cwd=os.getcwd(),
             env=env,
             encoding="utf-8",
             errors="replace",
@@ -336,15 +341,15 @@
             env=env,
             encoding="utf-8",
             errors="replace",
             universal_newlines=True,
             bufsize=1,
         )
         # 60 seconds is enough
-        process.wait(timeout=60)
+        process.wait(timeout=self._close_timeout_seconds)
         return process
 
     def close_learning_instance(self, object_id):
         if object_id not in self._learning_instance_processes:
             return
 
         # terminate the process
@@ -419,22 +424,23 @@
             suppressed=False,
         )
         setattr(process, "stdout_watcher", stdout_watcher)
         self._etcd_process = process
 
         start_time = time.time()
         while is_free_port(self._etcd_client_port):
-            time.sleep(1)
             if self._timeout_seconds + start_time < time.time():
                 self._etcd_process.kill()
                 _, errs = self._etcd_process.communicate()
                 logger.error("Start etcd timeout, %s", errs)
                 msg = "Launch etcd service failed due to timeout: "
                 msg += "\n".join([line for line in stdout_watcher.poll_all()])
                 raise RuntimeError(msg)
+            time.sleep(self._retry_time_seconds)
+
         stdout_watcher.drop(True)
         stdout_watcher.suppress(not logger.isEnabledFor(logging.DEBUG))
         logger.info("Etcd is ready, endpoint is %s", self._etcd_endpoint)
 
     def launch_vineyard(self):
         if self.vineyard_socket is not None:
             logger.info("Found existing vineyard socket: %s", self.vineyard_socket)
@@ -489,29 +495,30 @@
             suppressed=False,
         )
         setattr(process, "stdout_watcher", stdout_watcher)
         self._vineyardd_process = process
 
         start_time = time.time()
         if len(hosts) > 1:
-            time.sleep(5)  # should be OK
+            time.sleep(5 * self._retry_time_seconds)  # should be OK
         else:
             while not os.path.exists(self._vineyard_socket):
-                time.sleep(1)
                 if self._vineyardd_process.poll() is not None:
                     msg = "Launch vineyardd failed: "
                     msg += "\n".join([line for line in stdout_watcher.poll_all()])
                     msg += "\nRerun with `graphscope.set_option(log_level='debug')`,"
                     msg += " to get verbosed vineyardd logs."
                     raise RuntimeError(msg)
                 if self._timeout_seconds + start_time < time.time():
                     self._vineyardd_process.kill()
                     # outs, _ = self._vineyardd_process.communicate()
                     # logger.error("Start vineyardd timeout, %s", outs)
                     raise RuntimeError("Launch vineyardd failed due to timeout.")
+                time.sleep(self._retry_time_seconds)
+
         stdout_watcher.drop(True)
         stdout_watcher.suppress(not logger.isEnabledFor(logging.DEBUG))
         logger.info(
             "Vineyardd is ready, ipc socket is %s",
             self._vineyard_socket,
         )
```

## gscoordinator/VERSION

```diff
@@ -1 +1 @@
-0.23.0
+0.24.0a20230712
```

## gscoordinator/utils.py

```diff
@@ -1992,46 +1992,79 @@
 def check_argument(condition, message=None):
     if not condition:
         if message is None:
             message = "in '%s'" % inspect.stack()[1].code_context[0]
         raise ValueError(f"Check failed: {message}")
 
 
-def check_gremlin_server_ready(endpoint):
-    def _check_task(endpoint):
+def check_server_ready(endpoint, server="gremlin"):
+    def _check_gremlin_task(endpoint):
         from gremlin_python.driver.client import Client
 
         if "MY_POD_NAME" in os.environ:
             # inner kubernetes env
             if endpoint == "localhost" or endpoint == "127.0.0.1":
                 # now, used in macOS with docker-desktop kubernetes cluster,
                 # which external ip is 'localhost' when service type is 'LoadBalancer'
+                logger.info("In kubernetes env, gremlin server is ready.")
                 return True
 
         try:
             client = Client(f"ws://{endpoint}/gremlin", "g")
             # May throw
             client.submit("g.V().limit(1)").all().result()
+            logger.info("Gremlin server is ready.")
         finally:
             try:
                 client.close()
             except:  # noqa: E722
                 pass
         return True
 
+    def _check_cypher_task(endpoint):
+        from neo4j import GraphDatabase
+
+        if "MY_POD_NAME" in os.environ:
+            # inner kubernetes env
+            if endpoint == "localhost" or endpoint == "127.0.0.1":
+                logger.info("In kubernetes env, cypher server is ready.")
+                return True
+
+        try:
+            logger.debug("Try to connect to cypher server.")
+            driver = GraphDatabase.driver(f"neo4j://{endpoint}", auth=("", ""))
+            # May throw
+            driver.verify_connectivity()
+            logger.info("Checked connectivity to cypher server.")
+        finally:
+            try:
+                driver.close()
+            except:  # noqa: E722
+                pass
+        return True
+
     executor = ThreadPoolExecutor(max_workers=20)
 
     begin_time = time.time()
     while True:
-        t = executor.submit(_check_task, endpoint)
+        if server == "gremlin":
+            t = executor.submit(_check_gremlin_task, endpoint)
+        elif server == "cypher":
+            t = executor.submit(_check_cypher_task, endpoint)
+        else:
+            raise ValueError(
+                f"Unsupported server type: {server} other than 'gremlin' or 'cypher'"
+            )
         try:
             _ = t.result(timeout=30)
         except Exception as e:
             t.cancel()
             error_message = str(e)
         else:
             executor.shutdown(wait=False)
             return True
         time.sleep(3)
         if time.time() - begin_time > INTERACTIVE_INSTANCE_TIMEOUT_SECONDS:
             executor.shutdown(wait=False)
-            raise TimeoutError(f"Gremlin check query failed: {error_message}")
+            raise TimeoutError(
+                f"{server.capitalize()} check query failed: {error_message}"
+            )
```

## gscoordinator/template/CMakeLists.template

```diff
@@ -292,17 +292,25 @@
 # in recent version of brew installed protobuf on MacOS
 if (APPLE)
     # find bundled protobuf library
     if (EXISTS "${ANALYTICAL_ENGINE_HOME}/lib/libprotobuf.dylib")
         set(Protobuf_LIBRARIES "${ANALYTICAL_ENGINE_HOME}/lib/libprotobuf.dylib")
     elseif(EXISTS "${GRAPHSCOPE_ANALYTICAL_HOME}/lib/libprotobuf.dylib")
         set(Protobuf_LIBRARIES "${GRAPHSCOPE_ANALYTICAL_HOME}/lib/libprotobuf.dylib")
-    else()
-        # search libprotobuf.xx.xxx.dylib in graphscope.libs
-        file(GLOB PROTOBUF_LIBS "${ANALYTICAL_ENGINE_HOME}/../graphscope.dylibs/lib/libprotobuf.*.dylib")
+    endif()
+    if (NOT Protobuf_LIBRARIES)
+        # search libprotobuf.xx.xxx.dylib in graphscope.dylibs
+        file(GLOB PROTOBUF_LIBS "${ANALYTICAL_ENGINE_HOME}/../graphscope.dylibs/libprotobuf.*.dylib")
+        if (PROTOBUF_LIBS)
+            list(GET PROTOBUF_LIBS 0 Protobuf_LIBRARIES)
+        endif()
+    endif()
+    if (NOT Protobuf_LIBRARIES)
+        # search libprotobuf.xx.xxx.dylib in graphscope_runtime/.dylibs
+        file(GLOB PROTOBUF_LIBS "${ANALYTICAL_ENGINE_HOME}/../graphscope_runtime/.dylibs/libprotobuf.*.dylib")
         if (PROTOBUF_LIBS)
             list(GET PROTOBUF_LIBS 0 Protobuf_LIBRARIES)
         endif()
     endif()
     if (NOT Protobuf_LIBRARIES)
         # search system-wide protobuf
         find_package (Protobuf QUIET)
```

## gscoordinator/builtin/app/builtin_app.gar

### zipinfo {}

```diff
@@ -1,5 +1,5 @@
 Zip file size: 5673 bytes, number of entries: 3
--rw-r--r--  2.0 unx    24514 b- defN 23-Jul-04 08:42 .gs_conf.yaml
--rw-r--r--  2.0 unx      646 b- defN 23-Jul-04 08:42 __init__.py
--rw-r--r--  2.0 unx     2712 b- defN 23-Jul-04 10:44 builtin_app.zip
+-rw-r--r--  2.0 unx    24514 b- defN 23-Jul-12 19:05 .gs_conf.yaml
+-rw-r--r--  2.0 unx      646 b- defN 23-Jul-12 19:05 __init__.py
+-rw-r--r--  2.0 unx     2712 b- defN 23-Jul-12 21:00 builtin_app.zip
 3 files, 27872 bytes uncompressed, 5345 bytes compressed:  80.8%
```

### builtin_app.zip

 * *Command `'zipinfo {}'` failed with exit code 9. Standard output:*

 * *    Archive:  /tmp/diffoscope_opxetgf2_/tmpkxevz3as_ZipContainer/builtin_app.zip*

 * *    [â€¦]*

 * *Archive contents identical but files differ, possibly due to different compression levels. Falling back to binary comparison.*

```diff
@@ -1,8 +1,8 @@
-00000000: 504b 0304 1400 0000 0800 4e45 e456 535f  PK........NE.VS_
+00000000: 504b 0304 1400 0000 0800 a898 ec56 535f  PK...........VS_
 00000010: 95b7 aa08 0000 c25f 0000 0d00 0000 2e67  ......._.......g
 00000020: 735f 636f 6e66 2e79 616d 6ccd 9cdd 729b  s_conf.yaml...r.
 00000030: 3814 80ef f729 f202 2c33 e95d ee1c d26c  8....)..,3.]...l
 00000040: 334d 5a4f 9cee eeec 0d23 8362 6b23 038b  3MZO.....#.bk#..
 00000050: 845b f7e9 f748 20c0 18f0 919d 5a5c c4f1  .[...H .....Z\..
 00000060: 189d 9f4f 473f 4742 40b2 ece6 b7ab 2bef  ...OG?GB@.....+.
 00000070: 8af0 557a 7395 9115 cd49 f206 3f5d 5dc9  ..Uzs....I..?]].
@@ -135,16 +135,16 @@
 00000860: 7a7c 2108 be1d 7b80 611f 4dab 77b3 856e  z|!...{.a.M.w..n
 00000870: b387 8e1d db97 6e06 7730 6b71 5707 7046  ......n.w0kqW.pF
 00000880: 9f78 dae7 7173 4347 5884 6781 0e8f 7013  .x..qsCGX.g...p.
 00000890: 1e61 159e 053e 3cc2 5578 9639 0325 f816  .a...><.Ux.9.%..
 000008a0: a78b 63db 9c2e ec8e caa6 2769 0174 67d2  ..c.......'i.tg.
 000008b0: a5dd 040c c4c3 2c65 8945 4651 c8b9 12c0  ......,e.EFQ....
 000008c0: 8ee5 85f4 b401 a774 360f 7e56 7cd8 e8d5  .......t6.~V|...
-000008d0: 8017 09e0 ff50 4b03 0414 0000 0008 004e  .....PK........N
-000008e0: 45e4 567c bd2d f09a 0100 0086 0200 000b  E.V|.-..........
+000008d0: 8017 09e0 ff50 4b03 0414 0000 0008 00a8  .....PK.........
+000008e0: 98ec 567c bd2d f09a 0100 0086 0200 000b  ..V|.-..........
 000008f0: 0000 005f 5f69 6e69 745f 5f2e 7079 6592  ...__init__.pye.
 00000900: 416f db30 0c85 effd 156f c965 1b92 38c8  Ao.0.....o.e..8.
 00000910: 2e43 77f2 d26c 3516 3840 9cae e851 b669  .Cw..l5.8@...Q.i
 00000920: 9b80 2369 925c 37ff 7e94 9b01 2da6 8b20  ..#i.\7.~...-.. 
 00000930: f1f1 e923 a9f9 0724 8377 49c9 3a21 fd0c  ...#...$.wI.:!..
 00000940: 7b09 9dd1 5f6e e658 7e5e a232 35eb f616  {..._n.X~^.25...
 00000950: 4368 965f e3cd cd5c 225b 632f 8edb 2e60  Ch._...\"[c/...`
```

#### builtin_app.zip

```diff
@@ -1,8 +1,8 @@
-00000000: 504b 0304 1400 0000 0800 4e45 e456 535f  PK........NE.VS_
+00000000: 504b 0304 1400 0000 0800 a898 ec56 535f  PK...........VS_
 00000010: 95b7 aa08 0000 c25f 0000 0d00 0000 2e67  ......._.......g
 00000020: 735f 636f 6e66 2e79 616d 6ccd 9cdd 729b  s_conf.yaml...r.
 00000030: 3814 80ef f729 f202 2c33 e95d ee1c d26c  8....)..,3.]...l
 00000040: 334d 5a4f 9cee eeec 0d23 8362 6b23 038b  3MZO.....#.bk#..
 00000050: 845b f7e9 f748 20c0 18f0 919d 5a5c c4f1  .[...H .....Z\..
 00000060: 189d 9f4f 473f 4742 40b2 ece6 b7ab 2bef  ...OG?GB@.....+.
 00000070: 8af0 557a 7395 9115 cd49 f206 3f5d 5dc9  ..Uzs....I..?]].
@@ -135,16 +135,16 @@
 00000860: 7a7c 2108 be1d 7b80 611f 4dab 77b3 856e  z|!...{.a.M.w..n
 00000870: b387 8e1d db97 6e06 7730 6b71 5707 7046  ......n.w0kqW.pF
 00000880: 9f78 dae7 7173 4347 5884 6781 0e8f 7013  .x..qsCGX.g...p.
 00000890: 1e61 159e 053e 3cc2 5578 9639 0325 f816  .a...><.Ux.9.%..
 000008a0: a78b 63db 9c2e ec8e caa6 2769 0174 67d2  ..c.......'i.tg.
 000008b0: a5dd 040c c4c3 2c65 8945 4651 c8b9 12c0  ......,e.EFQ....
 000008c0: 8ee5 85f4 b401 a774 360f 7e56 7cd8 e8d5  .......t6.~V|...
-000008d0: 8017 09e0 ff50 4b03 0414 0000 0008 004e  .....PK........N
-000008e0: 45e4 567c bd2d f09a 0100 0086 0200 000b  E.V|.-..........
+000008d0: 8017 09e0 ff50 4b03 0414 0000 0008 00a8  .....PK.........
+000008e0: 98ec 567c bd2d f09a 0100 0086 0200 000b  ..V|.-..........
 000008f0: 0000 005f 5f69 6e69 745f 5f2e 7079 6592  ...__init__.pye.
 00000900: 416f db30 0c85 effd 156f c965 1b92 38c8  Ao.0.....o.e..8.
 00000910: 2e43 77f2 d26c 3516 3840 9cae e851 b669  .Cw..l5.8@...Q.i
 00000920: 9b80 2369 925c 37ff 7e94 9b01 2da6 8b20  ..#i.\7.~...-.. 
 00000930: f1f1 e923 a9f9 0724 8377 49c9 3a21 fd0c  ...#...$.wI.:!..
 00000940: 7b09 9dd1 5f6e e658 7e5e a232 35eb f616  {..._n.X~^.25...
 00000950: 4368 965f e3cd cd5c 225b 632f 8edb 2e60  Ch._...\"[c/...`
```

## Comparing `gs_coordinator-0.23.0.dist-info/RECORD` & `gs_coordinator-0.24.0a20230712.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,32 +1,32 @@
 graphscope_runtime/__init__.py,sha256=w_Me0NHI5zy8GitMCD7mzOi0Qz7ZfeE2zUI5wGfZIyM,694
-gs_coordinator-0.23.0.dist-info/RECORD,,
-gs_coordinator-0.23.0.dist-info/WHEEL,sha256=fw_hGi-Yx84a3ggAIzpSq5odRE77o5K1cXAmZvJuyLk,140
-gs_coordinator-0.23.0.dist-info/top_level.txt,sha256=GiJfpWt7WpC-H1BxUy5V39G7WAtPut6eqH3yAciJhqQ,33
-gs_coordinator-0.23.0.dist-info/METADATA,sha256=murl1bBk89MjVIzYKUGPTCOyPOFQtdpAtbqyN5mCGuI,22349
-gscoordinator/kubernetes_launcher.py,sha256=InmnHLCry7tOln00G4rzICpDbWHGCmuQ-09fJvYianY,58500
-gscoordinator/coordinator.py,sha256=t5qeE39lbTbT9mLqdh1icghBM0ooQdYSu71OWtQYVRA,37646
+gs_coordinator-0.24.0a20230712.dist-info/RECORD,,
+gs_coordinator-0.24.0a20230712.dist-info/WHEEL,sha256=fw_hGi-Yx84a3ggAIzpSq5odRE77o5K1cXAmZvJuyLk,140
+gs_coordinator-0.24.0a20230712.dist-info/top_level.txt,sha256=GiJfpWt7WpC-H1BxUy5V39G7WAtPut6eqH3yAciJhqQ,33
+gs_coordinator-0.24.0a20230712.dist-info/METADATA,sha256=W4Nn09VfONMka22OVXYFdeXWQZaIb2W3UOun7ur4bWg,22367
+gscoordinator/kubernetes_launcher.py,sha256=tPMX-N9w1bMpY2me9-3H5tAC2VnyefEwv4ODsai4AGY,59125
+gscoordinator/coordinator.py,sha256=jVoGtEYxKE3LkoL8GU_ATvZ7qT7k8LRNCbj_4OyBxIw,39201
 gscoordinator/dag_manager.py,sha256=NQCH_Ntf9ttj7mlIiDs9doJ5guTR2v-fK_WfW6fur-Q,5620
 gscoordinator/version.py,sha256=bS71qRjwM-xk4LAFTExdiWJOaJEXaeNdlJYCAqtD98U,1045
 gscoordinator/monitor.py,sha256=2ZRcZI3_i5UILjMPEO5mn7Ha3Cfw7gXbiF56AXPFdos,6949
 gscoordinator/op_executor.py,sha256=XFGHsDeZi-NalfI4QXeKfowWA_2M-ay1g0jbMcn_grk,40836
 gscoordinator/object_manager.py,sha256=e-5KcWigw8VM9o1WSMZ9BIQHIATvWD4gCXfiZVjPR6U,2653
 gscoordinator/__init__.py,sha256=nUqENyB6dtqMgYfbJjblAdV2nemmlsaTMp_yTteBnOw,990
-gscoordinator/cluster_builder.py,sha256=fs81DMs6EZHQPBGU_Vm4DeGzUeZCzlxzfKr8R48ApdM,23213
-gscoordinator/local_launcher.py,sha256=khGAv5X1Ui3kpPcZZdeAcKuapmdAjTLSxA6TYaTnzos,20588
-gscoordinator/VERSION,sha256=0ZlOSUKwbsOY7X2fqR7M5Aec0_hf_p4EWIhmKRJjFJw,7
-gscoordinator/utils.py,sha256=outYdoGzVv1oOE8S-LJ3pkAHpYIf8GQGOJ9BS2sc9TM,77420
+gscoordinator/cluster_builder.py,sha256=4GTCQpSV0PO-m99WnSd2tEmZebTpSgx5W9zVoA-t88I,23337
+gscoordinator/local_launcher.py,sha256=WZE4cWKOUyRGJaYJGP4-_7ul3D2OnZKVc7anLAKr-x0,21016
+gscoordinator/VERSION,sha256=IXZtEedpa3S_uhdrxJJFsf-chaTy45fgewVT6TgRKFw,16
+gscoordinator/utils.py,sha256=fW64gAfX5UiflaVFkZdWy6n9oK16u11abI4hEYhuvbs,78642
 gscoordinator/io_utils.py,sha256=8lxUPh-DfBj4yPIAPY9ovv2Iweg7MG8V77LscRlPM8o,4091
 gscoordinator/launcher.py,sha256=bgH4X47BZIr0E6mdw7VftUeZMYr2oaSilIMLZ82H-lo,5090
 gscoordinator/learning.py,sha256=M9MOv4ywKjZVYv1o1SB0fXN0ToTLD1EhfPiiBU1ic94,2450
 gscoordinator/__main__.py,sha256=pfC-UJ0gz8i-p1lGAs2jBB8zALAz6PaBokLa9v6AYQ4,77
 gscoordinator/template/__init__.py,sha256=TsAsMQRDcEJ9nRXDAeF-pJPapJrZPwVeb-TxxVp42aY,646
 gscoordinator/template/pregel.pxd.template,sha256=Eh4tESR8KlmM30Mcw-eIUfiQNeadDj3WWGCUzkIsT2M,4340
 gscoordinator/template/pie.pxd.template,sha256=-hYphyhUzTnyQELecGEsCK_S-fEXLWGiTV91-gDqAU8,4957
-gscoordinator/template/CMakeLists.template,sha256=Ijgd8rCc7Avr2qUpiLntRpbzbB9rGawL3Dikpn-u1yQ,21172
+gscoordinator/template/CMakeLists.template,sha256=hb52newLXIsq0vMBAlzaIIWrD6Q5d5vhO1vdZQ4DKOw,21530
 gscoordinator/builtin/__init__.py,sha256=TsAsMQRDcEJ9nRXDAeF-pJPapJrZPwVeb-TxxVp42aY,646
 gscoordinator/builtin/app/.gs_conf.yaml,sha256=Yy_-JLIOdZ4FoTPNlUWN9VN2fUTU9vdYzVICOIPX5fY,24514
-gscoordinator/builtin/app/builtin_app.gar,sha256=ssyOSq3ys2AoxBJv7yR_IN_hJ1AhwGhbI5pvzPsgzWw,5673
+gscoordinator/builtin/app/builtin_app.gar,sha256=HCALzivqBQ1a9OhrK81MrBqrX_xxpi0CTjsnsBHJ9oI,5673
 gscoordinator/builtin/app/__init__.py,sha256=TsAsMQRDcEJ9nRXDAeF-pJPapJrZPwVeb-TxxVp42aY,646
 gscoordinator/hook/__init__.py,sha256=jzzLJxC1gBgRLMb4xzX6vucYuNdosuWOtZKRj7Cal_o,646
 gscoordinator/hook/prestop/__init__.py,sha256=jzzLJxC1gBgRLMb4xzX6vucYuNdosuWOtZKRj7Cal_o,646
 gscoordinator/hook/prestop/__main__.py,sha256=1FmS0g2hwl85NPMOtbAvW_IGn4GFnldmjbSSwaj9GrE,1560
```

## Comparing `gs_coordinator-0.23.0.dist-info/METADATA` & `gs_coordinator-0.24.0a20230712.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: gs-coordinator
-Version: 0.23.0
+Version: 0.24.0a20230712
 Home-page: https://github.com/alibaba/GraphScope
 Author: GraphScope Team, Damo Academy
 Author-email: graphscope@alibaba-inc.com
 License: Apache License 2.0
 Keywords: GraphScope,Graph Computations
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Intended Audience :: Developers
@@ -26,15 +26,15 @@
 Requires-Dist: grpcio-tools (>=1.49)
 Requires-Dist: kubernetes (>=24.2.0)
 Requires-Dist: protobuf (>=4)
 Requires-Dist: PyYAML
 Requires-Dist: prometheus-client (>=0.14.1)
 Requires-Dist: packaging
 Requires-Dist: tqdm
-Requires-Dist: graphscope-client (==0.23.0)
+Requires-Dist: graphscope-client (==0.24.0a20230712)
 Requires-Dist: vineyard (==0.15.0) ; sys_platform != "win32"
 Requires-Dist: vineyard-io (==0.15.0) ; sys_platform != "win32"
 Provides-Extra: dev
 Requires-Dist: black (>=23.3.0) ; extra == 'dev'
 Requires-Dist: flake8 (==4.0.1) ; extra == 'dev'
 Requires-Dist: isort (==5.10.1) ; extra == 'dev'
 Requires-Dist: setuptools (==65.7.0) ; extra == 'dev'
```

